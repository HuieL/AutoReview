{"name": "ICLR_2017_10.pdf", "metadata": {"source": "CRF", "title": "Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC", "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Model-free reinforcement learning is a promising approach for solving arbitrary goal-directed sequential decision-making problems with only high-level reward signals and no supervision. It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016). Deep neural network parametrization minimizes the need for manual feature and policy engineering, and allows learning end-to-end policies mapping from high-dimensional inputs, such as images, directly to actions. However, such expressive parametrization also introduces a number of practical problems. Deep reinforcement learning algorithms tend to be sensitive to hyperparameter settings, often requiring extensive hyperparameter sweeps to find good values. Poor hyperparameter settings tend to produce unstable or non-convergent learning. Deep RL algorithms also tend to exhibit high sample complexity, often to the point of being impractical to run on real physical systems. Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.\nModel-free reinforcement learning consists of on- and off-policy methods. Monte Carlo policy gradient methods (Peters & Schaal, 2006; Schulman et al., 2015) are popular on-policy methods that\ndirectly maximize the cumulative future returns with respect to the policy. While these algorithms can offer unbiased (or nearly unbiased, as discussed in Section 2.1) estimates of the gradient, they rely on Monte Carlo estimation and often suffer from high variance. To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing value-based back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al., 2015). However, these methods all tend to require very large numbers of samples to deal with the high variance when estimating gradients of high-dimensional neural network policies. The crux of the problem with policy gradient methods is that they can only effectively use on-policy samples, which means that they require collecting large amounts of on-policy experiences after each parameter update to the policy. This makes them very sample intensive. Offpolicy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay. Such methods are much more sample-efficient. However, convergence of these algorithms is in general not guaranteed with non-linear function approximators, and practical convergence and instability issues typically mean that extensive hyperparameter tuning is required to attain good results.\nIn order to make deep reinforcement learning practical as a tool for tackling real-world tasks, we must develop methods that are both data efficient and stable. In this paper, we propose Q-Prop, a step in this direction that combines the advantages of on-policy policy gradient methods with the efficiency of off-policy learning. Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al., 2016) which fit the value function on-policy, Q-Prop learns the action-value function off-policy. The core idea is to use the first-order Taylor expansion of the critic as a control variate, resulting in an analytical gradient term through the critic and a Monte Carlo policy gradient term consisting of the residuals in advantage approximations. The method helps unify policy gradient and actor-critic methods: it can be seen as using the off-policy critic to reduce variance in policy gradient or using on-policy Monte Carlo returns to correct for bias in the critic gradient. We further provide theoretical analysis of the control variate, and derive two additional variants of Q-Prop. The method can be easily incorporated into any policy gradient algorithm. We show that Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) across a repertoire of continuous control tasks."}, {"heading": "2 BACKGROUND", "text": "Reinforcement learning (RL) aims to learn a policy for an agent such that it behaves optimally according to a reward function. At a time step t and state st , the agent chooses an action at according to its policy \u03c0(at |st), the state of the agent and the environment changes to new state st+1 according to dynamics p(st+1|st ,at), the agent receives a reward r(st ,at), and the process continues. Let Rt denote a \u03b3-discounted cumulative return from t for an infinite horizon problem, i.e Rt = \u2211\u221et \u2032=t \u03b3\nt \u2032\u2212tr(st \u2032 ,at \u2032). The goal of reinforcement learning is to maximize the expected return J(\u03b8) = E\u03c0\u03b8 [R0] with respect to the policy parameters \u03b8 . In this section, we review several standard techniques for performing this optimization, and in the next section, we will discuss our proposed Q-Prop algorithm that combines the strengths of these approaches to achieve efficient, stable RL. Monte Carlo policy gradient refers to policy gradient methods that use full Monte Carlo returns, e.g. REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), and policy gradient with function approximation refers to actor-critic methods (Sutton et al., 1999) which optimize the policy against a critic, e.g. deterministic policy gradient (Silver et al., 2014; Lillicrap et al., 2016)."}, {"heading": "2.1 MONTE CARLO POLICY GRADIENT METHODS", "text": "Monte Carlo policy gradient methods apply direct gradient-based optimization to the reinforcement learning objective. This involves directly differentiating the J(\u03b8) objective with respect to the policy\nparameters \u03b8 . The standard form, known as the REINFORCE algorithm (Williams, 1992), is shown below:\n\u2207\u03b8 J(\u03b8) = E\u03c0 [ \u221e\n\u2211 t=0\n\u2207\u03b8 log\u03c0\u03b8 (at |st)\u03b3 tRt ] = E\u03c0 [ \u221e\n\u2211 t=0 \u03b3 t\u2207\u03b8 log\u03c0\u03b8 (at |st)(Rt \u2212b(st))], (1)\nwhere b(st) is known as the baseline. For convenience of later derivations, Eq. 1 can also be written as below, where \u03c1\u03c0(s) = \u2211\u221et=0 \u03b3 t p(st = s) is the unnormalized discounted state visitation frequency,\n\u2207\u03b8 J(\u03b8) = Est\u223c\u03c1\u03c0 (\u00b7),at\u223c\u03c0(\u00b7|st )[\u2207\u03b8 log\u03c0\u03b8 (at |st)(Rt \u2212b(st))]. (2) Eq. 2 is an unbiased gradient of the RL objective. However, in practice, most policy gradient methods effectively use undiscounted state visitation frequencies, i.e. \u03b3 = 1 in the equal for \u03c1\u03c0 , and are therefore biased; in fact, making them unbiased often hurts performance (Thomas, 2014). In this paper, we mainly discuss bias due to function approximation, off-policy learning, and value back-ups.\nThe gradient is estimated using Monte Carlo samples in practice and has very high variance. A proper choice of baseline is necessary to reduce the variance sufficiently such that learning becomes feasible. A common choice is to estimate the value function of the state V\u03c0(st) to use as the baseline, which provides an estimate of advantage function A\u03c0(st ,at), which is a centered action-value function Q\u03c0(st ,at), as defined below:\nV\u03c0(st) = E\u03c0 [Rt ] = E\u03c0\u03b8 (at |st )[Q\u03c0(st ,at)] Q\u03c0(st ,at) = r(st ,at)+ \u03b3E\u03c0 [Rt+1] = r(st ,at)+ \u03b3Ep(st+1|st ,at )[V\u03c0(st+1)] A\u03c0(st ,at) = Q\u03c0(st ,at)\u2212V\u03c0(st).\n(3)\nQ\u03c0(st ,at) summarizes the performance of each action from a given state, assuming it follows \u03c0 thereafter, and A\u03c0(st ,at) provides a measure of how each action compares to the average performance at the state st , which is given by V\u03c0(st). Using A\u03c0(st ,at) centers the learning signal and reduces variance significantly.\nBesides high variance, another problem with the policy gradient is that it requires on-policy samples. This makes policy gradient optimization very sample intensive. To achieve similar sample efficiency as off-policy methods, we can attempt to include off-policy data. Prior attempts use importance sampling to include off-policy trajectories; however, these are known to be difficult scale to highdimensional action spaces because of rapidly degenerating importance weights (Precup, 2000)."}, {"heading": "2.2 POLICY GRADIENT WITH FUNCTION APPROXIMATION", "text": "Policy gradient methods with function approximation (Sutton et al., 1999), or actor-critic methods, include a policy evaluation step, which often uses temporal difference (TD) learning to fit a critic Qw for the current policy \u03c0(\u03b8), and a policy improvement step which greedily optimizes the policy \u03c0 against the critic estimate Qw. Significant gains in sample efficiency may be achievable using offpolicy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), typically by means of experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).\nOne particularly relevant example of such a method is the deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016). The updates for this method are given below, where \u03c0\u03b8 (at |st) = \u03b4 (at = \u00b5\u03b8 (st)) is a deterministic policy, \u03b2 is arbitrary exploration distribution, and \u03c1\u03b2 corresponds to sampling from a replay buffer. Q(\u00b7, \u00b7) is the target network that slowly tracks Qw (Lillicrap et al., 2016).\nw = argmin w Est\u223c\u03c1\u03b2 (\u00b7),at\u223c\u03b2 (\u00b7|st )[(r(st ,at)+ \u03b3Q(st+1,\u00b5\u03b8 (st+1))\u2212Qw(st ,at)) 2]\n\u03b8 = argmax \u03b8\nEst\u223c\u03c1\u03b2 (\u00b7)[Qw(st ,\u00b5\u03b8 (st))] (4)\nWhen the critic and policy are parametrized with neural networks, full optimization is expensive, and instead stochastic gradient optimization is used. The gradient in the policy improvement phase is given below, which is generally a biased gradient of J(\u03b8).\n\u2207\u03b8 J(\u03b8)\u2248 Est\u223c\u03c1\u03b2 (\u00b7)[\u2207aQw(st ,a)|a=\u00b5\u03b8 (st )\u2207\u03b8\u00b5\u03b8 (st)] (5)\nThe crucial benefits of DDPG are that it does not rely on high variance REINFORCE gradients and is trainable on off-policy data. These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016). However, the use of a biased policy gradient estimator makes analyzing its convergence and stability properties difficult."}, {"heading": "3 Q-PROP", "text": "In this section, we derive the Q-Prop estimator for policy gradient. The key idea from this estimator comes from observing Equations 2 and 5 and noting that the former provides an almost unbiased (see Section 2.1), but high variance gradient, while the latter provides a deterministic, but biased gradient. By using the deterministic biased estimator as a particular form of control variate (Ross, 2006; Paisley et al., 2012) for the Monte Carlo policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that in practice exhibits improved sample efficiency through the inclusion of off-policy samples while preserving the stability of on-policy Monte Carlo policy gradient."}, {"heading": "3.1 Q-PROP ESTIMATOR", "text": "To derive the Q-Prop gradient estimator, we start by using the first-order Taylor expansion of an arbitrary function f (st ,at), f\u0304 (st ,at) = f (st , a\u0304t)+\u2207a f (st ,a)|a=a\u0304t (at \u2212 a\u0304t) as the control variate for the policy gradient estimator. We use Q\u0302(st ,at) = \u2211\u221et \u2032=t \u03b3\nt \u2032\u2212tr(st \u2032 ,at \u2032) to denote Monte Carlo return from state st and action at , i.e. E\u03c0 [Q\u0302(st ,at)] = r(st ,at) + \u03b3Ep[V\u03c0(st+1)], and \u00b5\u03b8 (st) = E\u03c0\u03b8 (at |st )[at ] to denote the expected action of a stochastic policy \u03c0\u03b8 . Full derivation is in Appendix A.\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 f\u0304 (st ,at)]+E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st) f\u0304 (st ,at)] = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 f\u0304 (st ,at)]+E\u03c1\u03c0 [\u2207a f (st ,a)|a=a\u0304t \u2207\u03b8\u00b5\u03b8 (st)] (6)\nEq. 6 is general for arbitrary function f (st ,at) that is differentiable with respect to at at an arbitrary value of a\u0304t ; however, a sensible choice is to use the critic Qw for f and \u00b5\u03b8 (st) for a\u0304t to get,\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 Q\u0304w(st ,at)]+E\u03c1\u03c0 [\u2207aQw(st ,a)|a=\u00b5\u03b8 (st )\u2207\u03b8\u00b5\u03b8 (st)]. (7)\nFinally, since in practice we estimate advantages A\u0302(st ,at), we write the Q-Prop estimator in terms of advantages to complete the basic derivation,\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(A\u0302(st ,at)\u2212 A\u0304w(st ,at)]+E\u03c1\u03c0 [\u2207aQw(st ,a)|a=\u00b5\u03b8 (st )\u2207\u03b8\u00b5\u03b8 (st)] A\u0304(st ,at) = Q\u0304(st ,at)\u2212E\u03c0\u03b8 [Q\u0304(st ,at)] = \u2207aQw(st ,a)|a=\u00b5\u03b8 (st )(at \u2212\u00b5\u03b8 (st)).\n(8)\nEq. 8 is composed of an analytic gradient through the critic as in Eq. 5 and a residual REINFORCE gradient in Eq. 2. From the above derivation, Q-Prop is simply a Monte Carlo policy gradient estimator with a special form of control variate. The important insight comes from the fact that Qw can be trained using off-policy data as in Eq. 4. Under this setting, Q-Prop is no longer just a Monte Carlo policy gradient method, but more closely resembles an actor-critic method, where the critic can be updated off-policy but the actor is always updated on-policy with an additional REINFORCE correction term so that it remains a Monte Carlo policy gradient method regardless of the parametrization, training method, and performance of the critic. Therefore, Q-Prop can be directly combined with a number of prior techniques from both on-policy methods such as natural policy gradient (Kakade, 2001), trust-region policy optimization (TRPO) (Schulman et al., 2015) and generalized advantage estimation (GAE) (Schulman et al., 2016), and off-policy methods such as DDPG (Lillicrap et al., 2016) and Retrace(\u03bb ) (Munos et al., 2016).\nIntuitively, if the critic Qw approximates Q\u03c0 well, it provides a reliable gradient, reduces the estimator variance, and improves the convergence rate. Interestingly, control variate analysis in the next section shows that this is not the only circumstance where Q-Prop helps reduce variance."}, {"heading": "3.2 CONTROL VARIATE ANALYSIS AND ADAPTIVE Q-PROP", "text": "For Q-Prop to be applied reliably, it is crucial to analyze how the variance of the estimator changes before and after the application of control variate. Following the prior work on control variates (Ross, 2006; Paisley et al., 2012), we first introduce \u03b7(st) to Eq. 8, a weighing variable that modulates the strength of control variate. This additional variable \u03b7(st) does not introduce bias to the estimator.\n\u2207\u03b8 J(\u03b8) =E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(A\u0302(st ,at)\u2212\u03b7(st)A\u0304w(st ,at)] +E\u03c1\u03c0 [\u03b7(st)\u2207aQw(st ,a)|a=\u00b5\u03b8 (st )\u2207\u03b8\u00b5\u03b8 (st)]\n(9)\nThe variance of this estimator is given below, where m = 1...M indexes the dimension of \u03b8 , Var\u2217 = E\u03c1\u03c0 [ \u2211 m Varat (\u2207\u03b8m log\u03c0\u03b8 (at |st)(A\u0302(st ,at)\u2212\u03b7(st)A\u0304(st ,at))) ] . (10)\nIf we choose \u03b7(st) such that Var\u2217 < Var, where Var = E\u03c1\u03c0 [\u2211m Varat (\u2207\u03b8m log\u03c0\u03b8 (at |st)A\u0302(st ,at))] is the original estimator variance measure, then we have managed to reduce the variance. Directly analyzing the above variance measure is nontrivial, for the same reason that computing the optimal baseline is difficult (Weaver & Tao, 2001). In addition, it is often impractical to get multiple action samples from the same state, which prohibits using na\u0131\u0308ve Monte Carlo to estimate the expectations. Instead, we propose a surrogate variance measure, Var = E\u03c1\u03c0 [Varat (A\u0302(st ,at))]. A similar surrogate is also used by prior work on learning state-dependent baseline (Mnih & Gregor, 2014), and the benefit is that the measure becomes more tractable,\nVar\u2217 = E\u03c1\u03c0 [Varat (A\u0302(st ,at)\u2212\u03b7(st)A\u0304(st ,at))] = Var+E\u03c1\u03c0 [\u22122\u03b7(st)Covat (A\u0302(st ,at), A\u0304(st ,at))+\u03b7(st)2Varat (A\u0304(st ,at))].\n(11)\nSince E\u03c0 [A\u0302(st ,at)] = E\u03c0 [A\u0304(st ,at)] = 0, the terms can be simplified as below,\nCovat (A\u0302, A\u0304) = E\u03c0 [A\u0302(st ,at)A\u0304(st ,at)] Varat (A\u0304) = E\u03c0 [A\u0304(st ,at) 2] = \u2207aQw(st ,a)|Ta=\u00b5\u03b8 (st )\u03a3\u03b8 (st)\u2207aQw(st ,a)|a=\u00b5\u03b8 (st ), (12)\nwhere \u03a3\u03b8 (st) is the covariance matrix of the stochastic policy \u03c0\u03b8 . The nice property of Eq. 11 is that Varat (A\u0304) is analytical and Covat (A\u0302, A\u0304) can be estimated with single action sample. Using this estimate, we propose adaptive variants of Q-Prop that regulate the variance of the gradient estimate.\nAdaptive Q-Prop. The optimal state-dependent factor \u03b7(st) can be computed per state, according to \u03b7\u2217(st) = Covat (A\u0302, A\u0304)/Varat (A\u0304). This provides maximum reduction in variance according to Eq. 11. Substituting \u03b7\u2217(st) into Eq. 11, we get Var\u2217 = E\u03c1\u03c0 [(1\u2212\u03c1corr(A\u0302, A\u0304)2)Varat (A\u0302)], where \u03c1corr is the correlation coefficient, which achieves guaranteed variance reduction if at any state A\u0304 is correlated with A\u0302. We call this the fully adaptive Q-Prop method. An important conclusion from this analysis is that, in adaptive Q-Prop, the critic Qw does not necessarily need to be approximating Q\u03c0 well to produce good results. Its Taylor expansion merely needs to be correlated with A\u0302, positively or even negatively. This is in contrast with actor-critic methods, where performance is greatly dependent on the absolute accuracy of the critic\u2019s approximation.\nConservative and Aggressive Q-Prop. In practice, the single-sample estimate of Covat (A\u0302, A\u0304) has high variance itself, and we propose the following two practical implementations of adaptive Q-Prop: (1) \u03b7(st) = 1 if \u02c6Covat (A\u0302, A\u0304)> 0 and \u03b7(st) = 0 if otherwise, and (2) \u03b7(st) = sign( \u02c6Covat (A\u0302, A\u0304)). The first implementation, which we call conservative Q-Prop, can be thought of as a more conservative version of Q-Prop, which effectively disables the control variate for some samples of the states. This is sensible as if A\u0302 and A\u0304 are negatively correlated, it is likely that the critic is very poor. The second variant can correspondingly be termed aggressive Q-Prop, since it makes more liberal use of the control variate."}, {"heading": "3.3 Q-PROP ALGORITHM", "text": "Pseudo-code for the adaptive Q-Prop algorithm is provided in Algorithm 1. It is a mixture of policy gradient and actor-critic. At each iteration, it first rolls out the stochastic policy to collect on-policy\nAlgorithm 1 Adaptive Q-Prop 1: Initialize w for critic Qw, \u03b8 for stochastic policy \u03c0\u03b8 , and replay buffer R\u2190 /0. 2: repeat 3: for e = 1, . . . ,E do . Collect E episodes of on-policy experience using \u03c0\u03b8 4: s0,e \u223c p(s0) 5: for t = 0, . . . ,T \u22121 do 6: at,e \u223c \u03c0\u03b8 (\u00b7|st,e), st+1,e \u223c p(\u00b7|st,e,at,e), rt,e = r(st,e,at,e) 7: Add batch data B = {s0:T,1:E ,a0:T\u22121,1:E ,r0:T\u22121,1:E} to replay buffer R 8: Take E \u00b7T gradient steps on Qw using R and \u03c0\u03b8 9: Fit V\u03c6 (st) using B 10: Compute A\u0302t,e using GAE(\u03bb ) and A\u0304t,e using Eq. 7 11: Set \u03b7t,e based on Section 3.2 12: Compute and center the learning signals lt,e = A\u0302t,e\u2212\u03b7t,eA\u0304t,e 13: Compute \u2207\u03b8 J(\u03b8)\u2248 1ET \u2211e \u2211t \u2207\u03b8 log\u03c0\u03b8 (at,e|st,e)lt,e +\u03b7t,e\u2207aQw(st,e,a)|a=\u00b5\u03b8 (st,e)\u2207\u03b8\u00b5\u03b8 (st,e) 14: Take a gradient step on \u03c0\u03b8 using \u2207\u03b8 J(\u03b8), optionally with a trust-region constraint using B 15: until \u03c0\u03b8 converges.\nsamples, adds the batch to a replay buffer, takes a few gradient steps on the critic, computes A\u0302 and A\u0304, and finally applies a gradient step on the policy \u03c0\u03b8 . In our implementation, the critic Qw is fitted with off-policy TD learning using the same techniques as in DDPG (Lillicrap et al., 2016):\nw = argmin w Est\u223c\u03c1\u03b2 (\u00b7),at\u223c\u03b2 (\u00b7|st )[(r(st ,at)+ \u03b3E\u03c0 [Q \u2032(st+1,at+1)]\u2212Qw(st ,at))2]. (13)\nV\u03c6 is fitted with the same technique in (Schulman et al., 2016). Generalized advantage estimation (GAE) (Schulman et al., 2016) is used to estimate A\u0302. The policy update can be done by any method that utilizes the first-order gradient and possibly the on-policy batch data, which includes trust region policy optimization (TRPO) (Schulman et al., 2015). Importantly, this is just one possible implementation of Q-Prop, and in Appendix C we show a more general form that can interpolate between pure policy gradient and off-policy actor-critic."}, {"heading": "3.4 LIMITATIONS", "text": "A limitation with Q-Prop is that if data collection is very fast, e.g. using fast simulators, the compute time per episode is bound by the critic training at each iteration, and similar to that of DDPG and usually much more than that of TRPO. However, in applications where data collection speed is the bottleneck, there is sufficient time between policy updates to fit Qw well, which can be done asynchronously from the data collection, and the compute time of Q-Prop will be about the same as that of TRPO.\nAnother limitation is the robustness to bad critics. We empirically show that our conservative Q-Prop is more robust than standard Q-Prop and much more robust than pure off-policy actor-critic methods such as DDPG; however, estimating when an off-policy critic is reliable or not is still a fundamental problem that shall be further investigated. We can also alleviate this limitation by adopting more stable off-policy critic learning techniques such as Retrace(\u03bb ) (Munos et al., 2016)."}, {"heading": "4 RELATED WORK", "text": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Weaver & Tao, 2001; Greensmith et al., 2004; Schulman et al., 2016). However, exploration of action-dependent control variates is relatively recent, with most work focusing instead on simpler baselining techniques (Ross, 2006). A subtle exception is compatible feature approximation (Sutton et al., 1999) which can be viewed as a control variate as explained in Appendix B. Another exception is doubly robust estimator in contextual bandits (Dud\u0131\u0301k et al., 2011), which uses a different control variate whose bias cannot be tractably corrected. Control variates were explored recently not in RL but for approximate inference in stochastic models (Paisley et al., 2012), and the closest related work in that domain is the MuProp algorithm (Gu et al., 2016a) which uses a mean-field network as a surrogate for backpropagating a deterministic gradient through stochastic discrete variables. MuProp is not directly applicable to model-free RL because the dynamics are unknown; however, it\ncan be if the dynamics are learned as in model-based RL (Atkeson & Santamaria, 1997; Deisenroth & Rasmussen, 2011). This model-based Q-Prop is itself an interesting direction of research as it effectively corrects bias in model-based learning.\nPart of the benefit of Q-Prop is the ability to use off-policy data to improve on-policy policy gradient methods. Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013). Q-Prop provides a new approach for using off-policy data to reduce variance without introducing further bias.\nLastly, since Q-Prop uses both on-policy policy updates and off-policy critic learning, it can take advantage of prior work along both lines of research. We chose to implement Q-Prop on top of TRPO-GAE primarily for the purpose of enabling a fair comparison in the experiments, but combining Q-Prop with other on-policy update schemes and off-policy critic training methods is an interesting direction for future work. For example, Q-Prop can also be used with other on-policy policy gradient methods such as A3C (Mnih et al., 2016) and off-policy advantage estimation methods such as Retrace(\u03bb ) (Munos et al., 2016), GTD2 (Sutton et al., 2009), emphatic TD (Sutton et al., 2015), and WIS-LSTD (Mahmood et al., 2014)."}, {"heading": "5 EXPERIMENTS", "text": "We evaluated Q-Prop and its variants on continuous control environments from the OpenAI Gym benchmark (Brockman et al., 2016) using the MuJoCo physics simulator (Todorov et al., 2012) as shown in Figure 1. Algorithms are identified by acronyms, followed by a number indicating batch size, except for DDPG, which is a prior online actor-critic algorithm (Lillicrap et al., 2016). \u201cc-\u201d and \u201cv-\u201d denote conservative and aggressive Q-Prop variants as described in Section 3.2. \u201cTR-\u201d denotes trust-region policy optimization (Schulman et al., 2015), while \u201cV-\u201d denotes vanilla policy gradient. For example, \u201cTR-c-Q-Prop-5000\u201d means convervative Q-Prop with the trust-region policy update, and a batch size of 5000. \u201cVPG\u201d and \u201cTRPO\u201d are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016). Unless otherwise stated, all policy gradient methods are implemented with GAE(\u03bb = 0.97) (Schulman et al., 2016). Note that TRPOGAE is currently the state-of-the-art method on most of the OpenAI Gym benchmark tasks, though our experiments show that a well-tuned DDPG implementation sometimes achieves better results. Our algorithm implementations are built on top of the rllab TRPO and DDPG codes from Duan et al. (2016) and available at https://github.com/shaneshixiang/rllabplusplus. Policy and value function architectures and other training details including hyperparameter values are provided in Appendix D."}, {"heading": "5.1 ADAPTIVE Q-PROP", "text": "First, it is useful to identify how reliable each variant of Q-Prop is. In this section, we analyze standard Q-Prop and two adaptive variants, c-Q-Prop and a-Q-Prop, and demonstrate the stability of the method across different batch sizes. Figure 2a shows a comparison of Q-Prop variants with trust-region updates on the HalfCheetah-v1 domain, along with the best performing TRPO hyperparameters. The results are consistent with theory: conservative Q-Prop achieves much more stable performance than the standard and aggressive variants, and all Q-Prop variants significantly outperform TRPO in terms of sample efficiency, e.g. conservative Q-Prop reaches average reward of 4000 using about 10 times less samples than TRPO.\nFigure 2b shows the performance of conservative Q-Prop against TRPO across different batch sizes. Due to high variance in gradient estimates, TRPO typically requires very large batch sizes, e.g. 25000 steps or 25 episodes per update, to perform well. We show that our Q-Prop methods can learn even with just 1 episode per update, and achieves better sample efficiency with small batch sizes. This shows that Q-Prop significantly reduces the variance compared to the prior methods.\nAs we discussed in Section 1, stability is a significant challenge with state-of-the-art deep RL methods, and is very important for being able to reliably use deep RL for real world tasks. In the rest of the experiments, we will use conservative Q-Prop as the main Q-Prop implementation."}, {"heading": "5.2 EVALUATION ACROSS ALGORITHMS", "text": "In this section, we evaluate two versions of conservative Q-Prop, v-c-Q-Prop using vanilla policy gradient and TR-c-Q-Prop using trust-region updates, against other model-free algorithms on the HalfCheetah-v1 domain. Figure 3a shows that c-Q-Prop methods significantly outperform the best TRPO and VPG methods. Even Q-Prop with vanilla policy gradient is comparable to TRPO, confirming the significant benefits from variance reduction. DDPG on the other hand exhibits inconsistent performances. With proper reward scaling, i.e. \u201cDDPG-r0.1\u201d, it outperforms other methods as well as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016). This illustrates the sensitivity of DDPG to hyperparameter settings, while Q-Prop exhibits more stable, monotonic learning behaviors when compared to DDPG. In the next section we show this improved stability allows Q-Prop to outperform DDPG in more complex domains."}, {"heading": "5.3 EVALUATION ACROSS DOMAINS", "text": "Lastly, we evaluate Q-Prop against TRPO and DDPG across multiple domains. While the gym environments are biased toward locomotion, we expect we can achieve similar performance on manipulation tasks such as those in Lillicrap et al. (2016). Table 1 summarizes the results, including the best attained average rewards and the steps to convergence. Q-Prop consistently outperform TRPO in terms of sample complexity and sometimes achieves higher rewards than DDPG in more complex domains. A particularly notable case is shown in Figure 3b, where Q-Prop substantially improves sample efficiency over TRPO on Humanoid-v1 domain, while DDPG cannot find a good solution.\nThe better performance on the more complex domains highlights the importance of stable deep RL algorithms: while costly hyperparameter sweeps may allow even less stable algorithms to perform well on simpler problems, more complex tasks might have such narrow regions of stable hyperparameters that discovering them becomes impractical."}, {"heading": "6 DISCUSSION AND CONCLUSION", "text": "We presented Q-Prop, a policy gradient algorithm that combines reliable, consistent, and potentially unbiased on-policy gradient estimation with a sample-efficient off-policy critic that acts as a control variate. The method provides a large improvement in sample efficiency compared to stateof-the-art policy gradient methods such as TRPO, while outperforming state-of-the-art actor-critic methods on more challenging tasks such as humanoid locomotion. We hope that techniques like these, which combine on-policy Monte Carlo gradient estimation with sample-efficient variance reduction through off-policy critics, will eventually lead to deep reinforcement learning algorithms that are more stable and efficient, and therefore better suited for application to complex real-world learning tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Rocky Duan for sharing and answering questions about rllab code, and Yutian Chen and Laurent Dinh for discussion on control variates. SG and RT were funded by NSERC, Google, and EPSRC grants EP/L000776/1 and EP/M026957/1. ZG was funded by EPSRC grant EP/J012300/1 and the Alan Turing Institute (EP/N510129/1)."}, {"heading": "A Q-PROP ESTIMATOR DERIVATION", "text": "The full derivation of the Q-Prop estimator is shown in Eq. 14. We make use of the following property that is commonly used in baseline derivations:\nEp\u03b8 (x)[\u2207\u03b8 log p\u03b8 (x)] = \u222b\nx \u2207\u03b8 p\u03b8 (x) = \u2207\u03b8 \u222b x p(x) = 0\nThis holds true when f (st ,at) is an arbitrary function differentiable with respect to at and f\u0304 is its first-order Taylor expansion around at = a\u0304t , i.e. f\u0304 (st ,at) = f (st , a\u0304t)+\u2207a f (st ,a)|a=a\u0304t (at \u2212 a\u0304t). Here, \u00b5\u03b8 (st) = E\u03c0 [at ] is the mean of stochastic policy \u03c0\u03b8 . The derivation appears below:\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 f\u0304 (st ,at))]+E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st) f\u0304 (st ,at)] g(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st) f\u0304 (st ,at)]\n= E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)( f (st , a\u0304t)+\u2207a f (st ,a)|a=a\u0304t (at \u2212 a\u0304t))] = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)\u2207a f (st ,a)|a=a\u0304tat ]\n= E\u03c1\u03c0 [\u222b at \u2207\u03b8 \u03c0\u03b8 (at |st)\u2207a f (st ,a)|a=a\u0304tat ]\n= E\u03c1\u03c0 [ \u2207a f (st ,a)|a=a\u0304t \u222b at \u2207\u03b8 \u03c0\u03b8 (at |st)at ] = E\u03c1\u03c0 [\u2207a f (st ,a)|a=a\u0304t \u2207\u03b8E\u03c0 [at ]] = E\u03c1\u03c0 [\u2207a f (st ,a)|a=a\u0304t \u2207\u03b8\u00b5\u03b8 (st)]\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 f\u0304 (st ,at))]+g(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212 f\u0304 (st ,at))]+E\u03c1\u03c0 [\u2207a f (st ,a)|a=a\u0304t \u2207\u03b8\u00b5\u03b8 (st)]\n(14)"}, {"heading": "B CONNECTION BETWEEN Q-PROP AND COMPATIBLE FEATURE APPROXIMATION", "text": "In this section we show that actor-critic with compatible feature approximation is a form of control variate. A critic Qw is compatible (Sutton et al., 1999) if it satisfies (1) Qw(st ,at) = wT \u2207\u03b8 log\u03c0\u03b8 (at |st), i.e. \u2207wQw(st ,at) = \u2207\u03b8 log\u03c0\u03b8 (at |st), and (2) w is fit with objective w = argminw L(w) = argminwE\u03c1\u03c0 ,\u03c0 [(Q\u0302(st ,at)\u2212Qw(st ,at))2], that is fitting Qw on on-policy Monte Carlo returns. Condition (2) implies the following identity,\n\u2207wL = 2E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(Q\u0302(st ,at)\u2212Qw(st ,at))] = 0. (15)\nIn compatible feature approximation, it directly uses Qw as control variate, rather than its Taylor expansion Q\u0304w as in Q-Prop. Using Eq. 15, the Monte Carlo policy gradient is,\n\u2207\u03b8 J(\u03b8) = E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)Qw(st ,at)] = E\u03c1\u03c0 ,\u03c0 [(\u2207\u03b8 log\u03c0\u03b8 (at |st)\u2207\u03b8 log\u03c0\u03b8 (at |st)T )w] = E\u03c1\u03c0 [I(\u03b8 ;st)w],\n(16)\nwhere I(\u03b8 ;st) = E\u03c0\u03b8 [\u2207\u03b8 log\u03c0\u03b8 (at |st)\u2207\u03b8 log\u03c0\u03b8 (at |st)T ] is Fisher\u2019s information matrix. Thus, variance reduction depends on ability to compute or estimate I(\u03b8 ;st) and w effectively."}, {"heading": "C UNIFYING POLICY GRADIENT AND ACTOR-CRITIC", "text": "Q-Prop closely ties together policy gradient and actor-critic algorithms. To analyze this point, we write a generalization of Eq. 9 below, introducing two additional variables \u03b1,\u03c1CR:\n\u2207\u03b8 J(\u03b8) \u221d\u03b1E\u03c1\u03c0 ,\u03c0 [\u2207\u03b8 log\u03c0\u03b8 (at |st)(A\u0302(st ,at)\u2212\u03b7A\u0304w(st ,at)] +\u03b7E\u03c1CR [\u2207aQw(st ,a)|a=\u00b5\u03b8 (st )\u2207\u03b8\u00b5\u03b8 (st)]\n(17)\nEq. 17 enables more analysis where bias generally is introduced only when \u03b1 6= 1 or \u03c1CR 6= \u03c1\u03c0 . Importantly, Eq. 17 covers both policy gradient and deterministic actor-critic algorithm as its special cases. Standard policy gradient is recovered by \u03b7 = 0, and deterministic actor-critic is recovered by \u03b1 = 0 and \u03c1CR = \u03c1\u03b2 . This allows heuristic or automatic methods for dynamically changing these variables through the learning process for optimizing different metrics, e.g. sample efficiency, convergence speed, stability.\nTable 2 summarizes the various edge cases of Eq. 17. For example, since we derive our method from a control variates standpoint, Qw can be any function and the gradient remains almost unbiased (see\nSection 2.1). A natural choice is to use off-policy temporal difference learning to learn the critic Qw corresponding to policy \u03c0 . This enables effectively utilizing off-policy samples without introducing further bias. An interesting alternative to this is to utilize model-based roll-outs to estimate the critic, which resembles MuProp in stochastic neural networks (Gu et al., 2016a). Unlike prior work on using fitted dynamics model to accelerate model-free learning (Gu et al., 2016b), this approach does not introduce bias to the gradient of the original objective."}, {"heading": "D EXPERIMENT DETAILS", "text": "Policy and value function architectures. The network architectures are largely based on the benchmark paper by Duan et al. (2016). For policy gradient methods, the stochastic policy \u03c0\u03b8 (at |st) = N (\u00b5\u03b8 (st),\u03a3\u03b8 ) is a local Gaussian policy with a local state-dependent mean and a global covariance matrix. \u00b5\u03b8 (st) is a neural network with 3 hidden layers of sizes 100-50-25 and tanh nonlinearities at the first 2 layers, and \u03a3\u03b8 is diagonal. For DDPG, the policy is deterministic and has the same architecture as \u00b5\u03b8 except that it has an additional tanh layer at the output. V\u03c6 (st) for baselines and GAE is fit with the same technique by Schulman et al. (2016), a variant of linear regression on Monte Carlo returns with soft-update constraint. For Q-Prop and DDPG, Qw(s,a) is parametrized with a neural network with 2 hidden layers of size 100 and ReLU nonlinearity, where a is included after the first hidden layer.\nTraining details. This section describes parameters of the training algorithms and their hyperparameter search values in {}. The optimal performing hyperparameter results are reported. Policy gradient methods (VPG, TRPO, Q-Prop) used batch sizes of {1000, 5000, 25000} time steps, step sizes of {0.1, 0.01, 0.001} for the trust-region method, and base learning rates of {0.001, 0.0001} with Adam (Kingma & Ba, 2014) for vanilla policy gradient methods. For Q-Prop and DDPG, Qw is learned with the same technique as in DDPG (Lillicrap et al., 2016), using soft target networks with \u03c4 = 0.999, a replay buffer of size 106 steps, a mini-batch size of 64, and a base learning rate of {0.001, 0.0001} with Adam (Kingma & Ba, 2014). For Q-Prop we also tuned the relative ratio of gradient steps on the critic Qw against the number of steps on the policy, in the range {0.1, 0.5, 1.0}, where 0.1 corresponds to 100 critic updates for every policy update if the batch size is 1000. For DDPG, we swept the reward scaling using {0.01,0.1,1.0} as it is sensitive to this parameter."}], "references": [{"title": "Input convex neural networks", "author": ["Brandon Amos", "Lei Xu", "J Zico Kolter"], "venue": "arXiv preprint arXiv:1609.07152,", "citeRegEx": "Amos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amos et al\\.", "year": 2016}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["Christopher G Atkeson", "Juan Carlos Santamaria"], "venue": "In In International Conference on Robotics and Automation. Citeseer,", "citeRegEx": "Atkeson and Santamaria.,? \\Q1997\\E", "shortCiteRegEx": "Atkeson and Santamaria.", "year": 1997}, {"title": "Pilco: A model-based and data-efficient approach to policy search", "author": ["Marc Deisenroth", "Carl E Rasmussen"], "venue": "In Proceedings of the 28th International Conference on machine learning", "citeRegEx": "Deisenroth and Rasmussen.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth and Rasmussen.", "year": 2011}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Doubly robust policy evaluation and learning", "author": ["Miroslav Dud\u0131\u0301k", "John Langford", "Lihong Li"], "venue": "arXiv preprint arXiv:1103.4601,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Tim Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Double q-learning", "author": ["Hado V Hasselt"], "venue": "In Advances in Neural Information Processing Systems, pp. 2613\u20132621,", "citeRegEx": "Hasselt.,? \\Q2010\\E", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "Kakade.,? \\Q2001\\E", "shortCiteRegEx": "Kakade.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deterministic policy gradient algorithms", "author": ["Guy Lever"], "venue": null, "citeRegEx": "Lever.,? \\Q2014\\E", "shortCiteRegEx": "Lever.", "year": 2014}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Levine and Koltun.,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun.", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A Rupam Mahmood", "Hado P van Hasselt", "Richard S Sutton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient offpolicy reinforcement learning", "author": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G Bellemare"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Variational bayesian inference with stochastic search", "author": ["John Paisley", "David Blei", "Michael Jordan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Policy gradient methods for robotics", "author": ["Jan Peters", "Stefan Schaal"], "venue": "In International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Peters and Schaal.,? \\Q2006\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2006}, {"title": "Relative entropy policy search", "author": ["Jan Peters", "Katharina M\u00fclling", "Yasemin Altun"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Doina Precup"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "Precup.,? \\Q2000\\E", "shortCiteRegEx": "Precup.", "year": 2000}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["Richard S Sutton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sutton.,? \\Q1990\\E", "shortCiteRegEx": "Sutton.", "year": 1990}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Richard S Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesv\u00e1ri", "Eric Wiewiora"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["Richard S Sutton", "A Rupam Mahmood", "Martha White"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Bias in natural actor-critic algorithms", "author": ["Philip Thomas"], "venue": "In ICML, pp", "citeRegEx": "Thomas.,? \\Q2014\\E", "shortCiteRegEx": "Thomas.", "year": 2014}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "The optimal reward baseline for gradient-based reinforcement learning", "author": ["Lex Weaver", "Nigel Tao"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Weaver and Tao.,? \\Q2001\\E", "shortCiteRegEx": "Weaver and Tao.", "year": 2001}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "For policy gradient methods, the stochastic policy \u03c0\u03b8 (at |st) = N (\u03bc\u03b8 (st),\u03a3\u03b8 ) is a local Gaussian policy with a local state-dependent mean and a global covariance matrix. \u03bc\u03b8 (st) is a neural network with 3 hidden layers of sizes 100-50-25 and tanh nonlinearities at the first 2 layers", "author": ["Duan"], "venue": null, "citeRegEx": "Duan,? \\Q2016\\E", "shortCiteRegEx": "Duan", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 23, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 13, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 26, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 17, "context": "It has recently been extended to utilize large neural network policies and value functions, and has been shown to be successful in solving a range of difficult problems (Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2016; Silver et al., 2016; Gu et al., 2016b; Mnih et al., 2016).", "startOffset": 169, "endOffset": 293}, {"referenceID": 8, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 16, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 23, "context": "Although a number of recent techniques have sought to alleviate some of these issues (Hasselt, 2010; Mnih et al., 2015; Schulman et al., 2015; 2016), these recent advances still provide only a partial solution to the instability and sample complexity challenges.", "startOffset": 85, "endOffset": 148}, {"referenceID": 23, "context": "Monte Carlo policy gradient methods (Peters & Schaal, 2006; Schulman et al., 2015) are popular on-policy methods that", "startOffset": 36, "endOffset": 82}, {"referenceID": 9, "context": "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing value-based back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.", "startOffset": 198, "endOffset": 233}, {"referenceID": 21, "context": "To cope with high variance gradient estimates and difficult optimization landscapes, a number of techniques have been proposed, including constraining the change in the policy at each gradient step (Kakade, 2001; Peters et al., 2010) and mixing value-based back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al.", "startOffset": 198, "endOffset": 233}, {"referenceID": 23, "context": ", 2010) and mixing value-based back-ups to trade off bias and variance in Monte Carlo return estimates (Schulman et al., 2015).", "startOffset": 103, "endOffset": 126}, {"referenceID": 28, "context": "Offpolicy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.", "startOffset": 38, "endOffset": 119}, {"referenceID": 16, "context": "Offpolicy methods, such as Q-learning (Watkins & Dayan, 1992; Sutton et al., 1999; Mnih et al., 2015; Gu et al., 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al.", "startOffset": 38, "endOffset": 119}, {"referenceID": 11, "context": ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.", "startOffset": 45, "endOffset": 82}, {"referenceID": 13, "context": ", 2016b) and off-policy actor-critic methods (Lever, 2014; Lillicrap et al., 2016), can instead use all samples, including off-policy samples, by adopting temporal difference learning with experience replay.", "startOffset": 45, "endOffset": 82}, {"referenceID": 28, "context": "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 25, "context": "Unlike prior approaches for off-policy learning, which either introduce bias (Sutton et al., 1999; Silver et al., 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 22, "context": ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.", "startOffset": 29, "endOffset": 86}, {"referenceID": 18, "context": ", 2014) or increase variance (Precup, 2000; Levine & Koltun, 2013; Munos et al., 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al.", "startOffset": 29, "endOffset": 86}, {"referenceID": 24, "context": ", 2016), Q-Prop can reduce the variance of gradient estimator without adding bias; unlike prior approaches for critic-based variance reduction (Schulman et al., 2016) which fit the value function on-policy, Q-Prop learns the action-value function off-policy.", "startOffset": 143, "endOffset": 166}, {"referenceID": 23, "context": "We show that Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al.", "startOffset": 157, "endOffset": 186}, {"referenceID": 13, "context": ", 2015; 2016), and improved stability over deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016) across a repertoire of continuous control tasks.", "startOffset": 85, "endOffset": 109}, {"referenceID": 34, "context": "REINFORCE (Williams, 1992) and TRPO (Schulman et al.", "startOffset": 10, "endOffset": 26}, {"referenceID": 23, "context": "REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), and policy gradient with function approximation refers to actor-critic methods (Sutton et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 28, "context": ", 2015), and policy gradient with function approximation refers to actor-critic methods (Sutton et al., 1999) which optimize the policy against a critic, e.", "startOffset": 88, "endOffset": 109}, {"referenceID": 34, "context": "The standard form, known as the REINFORCE algorithm (Williams, 1992), is shown below:", "startOffset": 52, "endOffset": 68}, {"referenceID": 31, "context": "\u03b3 = 1 in the equal for \u03c1\u03c0 , and are therefore biased; in fact, making them unbiased often hurts performance (Thomas, 2014).", "startOffset": 108, "endOffset": 122}, {"referenceID": 22, "context": "Prior attempts use importance sampling to include off-policy trajectories; however, these are known to be difficult scale to highdimensional action spaces because of rapidly degenerating importance weights (Precup, 2000).", "startOffset": 206, "endOffset": 220}, {"referenceID": 28, "context": "Policy gradient methods with function approximation (Sutton et al., 1999), or actor-critic methods, include a policy evaluation step, which often uses temporal difference (TD) learning to fit a critic Qw for the current policy \u03c0(\u03b8), and a policy improvement step which greedily optimizes the policy \u03c0 against the critic estimate Qw.", "startOffset": 52, "endOffset": 73}, {"referenceID": 27, "context": "Significant gains in sample efficiency may be achievable using offpolicy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), typically by means of experience replay for training deep Q networks (Mnih et al.", "startOffset": 152, "endOffset": 187}, {"referenceID": 25, "context": "Significant gains in sample efficiency may be achievable using offpolicy TD learning for the critic, as in Q-learning and deterministic policy gradient (Sutton, 1990; Silver et al., 2014), typically by means of experience replay for training deep Q networks (Mnih et al.", "startOffset": 152, "endOffset": 187}, {"referenceID": 16, "context": ", 2014), typically by means of experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).", "startOffset": 78, "endOffset": 139}, {"referenceID": 13, "context": ", 2014), typically by means of experience replay for training deep Q networks (Mnih et al., 2015; Lillicrap et al., 2016; Gu et al., 2016b).", "startOffset": 78, "endOffset": 139}, {"referenceID": 25, "context": "One particularly relevant example of such a method is the deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016).", "startOffset": 100, "endOffset": 145}, {"referenceID": 13, "context": "One particularly relevant example of such a method is the deep deterministic policy gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016).", "startOffset": 100, "endOffset": 145}, {"referenceID": 13, "context": "Q(\u00b7, \u00b7) is the target network that slowly tracks Qw (Lillicrap et al., 2016).", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).", "startOffset": 131, "endOffset": 192}, {"referenceID": 3, "context": "These properties make DDPG and other analogous off-policy methods significantly more sample-efficient than policy gradient methods (Lillicrap et al., 2016; Gu et al., 2016b; Duan et al., 2016).", "startOffset": 131, "endOffset": 192}, {"referenceID": 19, "context": "By using the deterministic biased estimator as a particular form of control variate (Ross, 2006; Paisley et al., 2012) for the Monte Carlo policy gradient estimator, we can effectively use both types of gradient information to construct a new estimator that in practice exhibits improved sample efficiency through the inclusion of off-policy samples while preserving the stability of on-policy Monte Carlo policy gradient.", "startOffset": 84, "endOffset": 118}, {"referenceID": 9, "context": "Therefore, Q-Prop can be directly combined with a number of prior techniques from both on-policy methods such as natural policy gradient (Kakade, 2001), trust-region policy optimization (TRPO) (Schulman et al.", "startOffset": 137, "endOffset": 151}, {"referenceID": 23, "context": "Therefore, Q-Prop can be directly combined with a number of prior techniques from both on-policy methods such as natural policy gradient (Kakade, 2001), trust-region policy optimization (TRPO) (Schulman et al., 2015) and generalized advantage estimation (GAE) (Schulman et al.", "startOffset": 193, "endOffset": 216}, {"referenceID": 24, "context": ", 2015) and generalized advantage estimation (GAE) (Schulman et al., 2016), and off-policy methods such as DDPG (Lillicrap et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 13, "context": ", 2016), and off-policy methods such as DDPG (Lillicrap et al., 2016) and Retrace(\u03bb ) (Munos et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 19, "context": "Following the prior work on control variates (Ross, 2006; Paisley et al., 2012), we first introduce \u03b7(st) to Eq.", "startOffset": 45, "endOffset": 79}, {"referenceID": 13, "context": "In our implementation, the critic Qw is fitted with off-policy TD learning using the same techniques as in DDPG (Lillicrap et al., 2016): w = argmin w Est\u223c\u03c1\u03b2 (\u00b7),at\u223c\u03b2 (\u00b7|st )[(r(st ,at)+ \u03b3E\u03c0 [Q (st+1,at+1)]\u2212Qw(st ,at))(2)].", "startOffset": 112, "endOffset": 136}, {"referenceID": 24, "context": "(13) V\u03c6 is fitted with the same technique in (Schulman et al., 2016).", "startOffset": 45, "endOffset": 68}, {"referenceID": 24, "context": "Generalized advantage estimation (GAE) (Schulman et al., 2016) is used to estimate \u00c2.", "startOffset": 39, "endOffset": 62}, {"referenceID": 23, "context": "The policy update can be done by any method that utilizes the first-order gradient and possibly the on-policy batch data, which includes trust region policy optimization (TRPO) (Schulman et al., 2015).", "startOffset": 177, "endOffset": 200}, {"referenceID": 18, "context": "We can also alleviate this limitation by adopting more stable off-policy critic learning techniques such as Retrace(\u03bb ) (Munos et al., 2016).", "startOffset": 120, "endOffset": 140}, {"referenceID": 5, "context": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Weaver & Tao, 2001; Greensmith et al., 2004; Schulman et al., 2016).", "startOffset": 105, "endOffset": 173}, {"referenceID": 24, "context": "Variance reduction in policy gradient methods is a long-standing problem with a large body of prior work (Weaver & Tao, 2001; Greensmith et al., 2004; Schulman et al., 2016).", "startOffset": 105, "endOffset": 173}, {"referenceID": 28, "context": "A subtle exception is compatible feature approximation (Sutton et al., 1999) which can be viewed as a control variate as explained in Appendix B.", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": "Another exception is doubly robust estimator in contextual bandits (Dud\u0131\u0301k et al., 2011), which uses a different control variate whose bias cannot be tractably corrected.", "startOffset": 67, "endOffset": 88}, {"referenceID": 19, "context": "Control variates were explored recently not in RL but for approximate inference in stochastic models (Paisley et al., 2012), and the closest related work in that domain is the MuProp algorithm (Gu et al.", "startOffset": 101, "endOffset": 123}, {"referenceID": 28, "context": "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 25, "context": "Prior methods that combine off-policy data with policy gradients either introduce bias (Sutton et al., 1999; Silver et al., 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 22, "context": ", 2014) or use importance weighting, which is known to result in degenerate importance weights in high dimensions, resulting in very high variance (Precup, 2000; Levine & Koltun, 2013).", "startOffset": 147, "endOffset": 184}, {"referenceID": 17, "context": "For example, Q-Prop can also be used with other on-policy policy gradient methods such as A3C (Mnih et al., 2016) and off-policy advantage estimation methods such as Retrace(\u03bb ) (Munos et al.", "startOffset": 94, "endOffset": 113}, {"referenceID": 18, "context": ", 2016) and off-policy advantage estimation methods such as Retrace(\u03bb ) (Munos et al., 2016), GTD2 (Sutton et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 29, "context": ", 2016), GTD2 (Sutton et al., 2009), emphatic TD (Sutton et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 30, "context": ", 2009), emphatic TD (Sutton et al., 2015), and WIS-LSTD (Mahmood et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 3, "context": "Figure 1: Illustrations of OpenAI Gym MuJoCo domains (Brockman et al., 2016; Duan et al., 2016): (a) Ant, (b) HalfCheetah, (c) Hopper, (d) Humanoid, (e) Reacher, (f) Swimmer, (g) Walker.", "startOffset": 53, "endOffset": 95}, {"referenceID": 32, "context": ", 2016) using the MuJoCo physics simulator (Todorov et al., 2012) as shown in Figure 1.", "startOffset": 43, "endOffset": 65}, {"referenceID": 13, "context": "Algorithms are identified by acronyms, followed by a number indicating batch size, except for DDPG, which is a prior online actor-critic algorithm (Lillicrap et al., 2016).", "startOffset": 147, "endOffset": 171}, {"referenceID": 23, "context": "\u201cTR-\u201d denotes trust-region policy optimization (Schulman et al., 2015), while \u201cV-\u201d denotes vanilla policy gradient.", "startOffset": 47, "endOffset": 70}, {"referenceID": 24, "context": "\u201cVPG\u201d and \u201cTRPO\u201d are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 3, "context": "\u201cVPG\u201d and \u201cTRPO\u201d are vanilla policy gradient and trust-region policy optimization respectively (Schulman et al., 2016; Duan et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 3, "context": "1\u201d, it outperforms other methods as well as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 0, "context": "1\u201d, it outperforms other methods as well as the DDPG results reported in prior work (Duan et al., 2016; Amos et al., 2016).", "startOffset": 84, "endOffset": 122}], "year": 2017, "abstractText": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the stateof-the-art on-policy and off-policy methods, on OpenAI Gym\u2019s MuJoCo continuous control environments.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_10"}