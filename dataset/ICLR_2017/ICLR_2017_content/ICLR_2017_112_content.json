{"name": "ICLR_2017_112.pdf", "metadata": {"source": "CRF", "title": "SIGMA-DELTA QUANTIZED NETWORKS", "authors": ["Peter O\u2019Connor", "Max Welling"], "emails": ["p.e.oconnor@uva.nl", "m.welling@uva.nl"], "sections": [{"heading": null, "text": "Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. As a result, it ends up repeatedly doing very similar computations. To put an end to such waste, we introduce SigmaDelta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data."}, {"heading": "1 INTRODUCTION", "text": "For most deep-learning architectures, the amount of computation required to process a sample of input data is independent of the contents of that data.\nNatural data tends to contain a great deal of spatial and temporal redundancy. Researchers have taken advantage of such redundancy to design encoding schemes, like jpeg and mpeg, which introduce small compromises to image fidelity in exchange for substantial savings in the amount of memory required to store images and videos.\nIn neuroscience, it seems clear that that some kind of sparse spatio-temporal coding is going on. Koch et al. (2006) estimate that the human retina transmits 8.75Mbps, which is about the same as compressed 1080p video at 30FPS.\nThus it seems natural to think that perhaps we should be doing this in deep learning. In this paper, we propose a neural network where neurons only communicate discretized changes in their activations to one another. The computational cost of running such a network would be proportional to the amount of change in the input. Neurons send signals when the change in their input accumulates past some threshold, at which point they send a discrete \u201cspike\u201d notifying downstream neurons of the change. Such a system has at least two advantages over the conventional way of doing things.\n1. When extracting features from temporally redundant data, it is much more efficient to communicate the changes in activation than it is to re-process each frame.\n2. When receiving data asynchronously from different sources (e.g. sensors, or nodes in a distributed network) at different rates, it no longer makes sense to have a global network update. We could recompute the network with every new input, reusing the stale inputs from the other sources, but this requires doing a great deal of repeated computation for only small differences in input data. We could keep a history of all inputs and update the network periodically, but then we lose the ability to respond immediately to new inputs. Our approach gets around this ugly tradeoff by allowing for efficient approximate updates of the network given a partial update to the input data. The computational cost of the update is proportional to the effect that the new information has on the network\u2019s state."}, {"heading": "2 RELATED WORK", "text": "This work originated in the study of spiking neural networks, but treads into the territory of discretizing neural nets. The most closely related work is that of Zambrano and Bohte (2016). In this work, the authors describe an Adaptive Sigma-Delta modulation method, in which neurons communicate analog signals to one another by means of a \u201cspike-encoding\u201d mechanism, where a temporal signal is encoded into a sequence of weighted spikes and then approximately decoded as a sum of temporally-shifted exponential kernels. The authors create a scheme for being parsimonious with spikes by allowing adaptive scaling of thresholds, at the cost of sending spikes with real values attached to them, rather than the classic \u201call or nothing\u201d spikes. Their work references a slightly earlier work by Yoon (2016) which reframes common neural models as forms of Asynchronous Sigma-Delta modulation. In a concurrent work, Lee et al. (2016) implement backpropagation in a similar system (but without adaptive threshold scaling), and demonstrate the best-yet performance on MNIST for networks trained with spiking models. This work postdates Diehl et al. (2015), which proposes a scheme for normalizing neuron activations so that a spiking neural network can be optimized for fast classification.\nOur model contrasts with all of the above in that it is time-agnostic. Although we refer to sending \u201ctemporal differences\u201d between neurons, our neurons have no concept of time - their is no \u201cleak\u201d in neuron potential, and our neurons\u2019 behaviour only depends on the order of the inputs. Our work also separates the concepts of nonlinearity and discretization, uses units that communicate differences rather than absolute signal values, and explicitly minimizes an objective function corresponding to computational cost.\nComing from another corner, Courbariaux et al. describe a scheme for binarizing networks with the aim of achieving reductions in the amount of computation and memory required to run neural nets. They introduce a number of tricks for training binarized neural networks - a normally difficult task due to the lack of gradient information. Esser et al. (2016) use a similar binarization scheme to efficiently implement a spiking neural network on the IBM TrueNorth chip. Ardakani et al. (2015) take another approach - to approximate real-valued operations of a neural net with a sequence of stochastic integer operations, and show how these can lead to cheaper computation.\nThese discretization approaches differ from ours in that they do not aim to take advantage of temporal redundancy in data, but rather aim to find ways of saving computation by learning in a low-precision regime. Ideas from these works could be combined with the ideas presented in this paper.\nThe idea of sending quantized temporal differences has been applied to make event-based sensors, such as the Dynamic-Vision Sensor (Lichtsteiner et al., 2008), which quantize changes in analog pixelvoltages and send out pixel-change events asynchronously. The model we propose in this paper could be used to efficiently process the outputs of such sensors.\nFinally, our previous work, (O\u2019Connor and Welling, 2016) develops a method for doing backpropagation with the same type of time-agnostic spiking neurons we use here. In this paper, we do not aim to train the network from scratch, but instead focus on how we can compute efficiently by sending temporal differences between neurons."}, {"heading": "3 THE SIGMA-DELTA NETWORK", "text": "In this Section, we describe how we start with a traditional deep neural network and apply two modifications - temporal-difference communication and rounding - to create the Sigma-Delta network. To explain the network, we follow the Figure 1 from top to bottom, starting with a standard deep network and progressing to our Sigma-Delta network. Here, we will think of the forward pass of a neural network as composition of subfunctions: f(x) = (fL \u25e6 ... \u25e6 f2 \u25e6 f1)(x)."}, {"heading": "3.1 TEMPORAL DIFFERENCE NETWORK", "text": "We now define \u201ctemporal difference\u201d (\u2206T ) and \u201ctemporal integration\u201d (\u03a3T ) modules as follows:\nAlgorithm 1 Temporal Difference (\u2206T ): 1: Internal: ~xlast \u2208 Rd \u2190 ~0 2: Input: ~x \u2208 Rd 3: ~y \u2190 ~x\u2212 ~xlast 4: ~xlast \u2190 ~x 5: Return: ~y \u2208 Rd\nAlgorithm 2 Temporal Integration (\u03a3T ): 1: Internal: ~y \u2208 Rd \u2190 ~0 2: Input: ~x \u2208 Rd 3: ~y \u2190 ~y + ~x 4: Return: ~y \u2208 Rd\nSo that when presented with a sequence of inputs x1, ...xt, \u2206T (xt) = xt \u2212 xt\u22121|x0=0, and \u03a3T (xt) =\u2211t \u03c4=1 x\u03c4 . It should be noted that when we refer to \u201ctemporal differences\u201d, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them. This distinction only matters when dealing with asynchronous inputs such as the Dynamic Vision Sensor, (Lichtsteiner et al., 2008), which are not considered in this paper.\nSince \u03a3T (\u2206T (xt)) = xt, we can insert \u03a3T \u25e6\u2206T pairs into the network without affecting the function. So we can re-express our network function as: f(x) = (fL\u25e6\u03a3T \u25e6\u2206T \u25e6...\u25e6f2\u25e6\u03a3T \u25e6\u2206T \u25e6f1\u25e6\u03a3T \u25e6\u2206T )(x). Now suppose our network consists of alternating linear functions w(x), and nonlinear functions h(x), so that f(x) = (hL \u25e6wL... \u25e6 h2 \u25e6w2 \u25e6 h1 \u25e6w1)(x). As before, we can harmlessly insert our \u03a3T \u25e6\u2206T pairs into the network. But this time, note that for a linear function w(x), the operations (\u03a3T , w,\u2206T ) all commute with one another. That is:\n\u2206T (w(\u03a3T (x))) = w(\u2206T (\u03a3T (x))) = w(x) (1)\nTherefore we can replace all instances of \u2206T \u25e6 w \u25e6 \u03a3T with w, yielding f(x) = (hL \u25e6 \u03a3T \u25e6 wL \u25e6 ... \u25e6 \u2206T \u25e6h2 \u25e6\u03a3T \u25e6w2 \u25e6\u2206T \u25e6h1 \u25e6\u03a3T \u25e6w1 \u25e6\u2206T )(x), which corresponds to the network shown in Figure 1 B. For now this is completely pointless, since we do not change the network function at all, but it will come in handy in the next section, where we discretize the output of the \u2206T modules."}, {"heading": "3.2 DISCRETIZATING THE DELTAS", "text": "When dealing with data that is naturally spatiotemporally redundant, like most video, we expect the output of the \u2206T modules to be a vector with mostly low values, with some peaks corresponding to temporal transitions at certain input positions. We expect the data to have this property not only at the input layer, but even more so at higher layers, which encode higher level features (edges, object parts, class labels), which we would expect to vary more slowly over time than pixel values. If we discretize this \u201cpeaky\u201d vector, we end up with a sparse vector of integers, which can then be used to cheaply communicate the approximate change in state of a layer to its downstream layer(s).\nA sensible approach is to apply rounding before the temporal-difference operation - i.e. round the activation values and then send the temporal differences of these rounded values. It is then easy to show that the network\u2019s function will remain identical to that of the rounding network:\n\u03a3T (w(\u2206T (round(x)))) = w(\u03a3T (\u2206T (round(x)))) = w(round(x)) (2)\nIt\u2019s worth noting that this is equivalent to doing discrete-time Sigma-Delta modulation to quantize the temporal differences - this connection is explained in Appendix A.\nIt follows from this result that our Sigma-Delta network depicted in Figure 1 D computes an identical function to that of the rounding network in Figure 1 C. In other words, the output yt of the Sigma-Delta network is solely dependent on the parameters of the network and the current input xt, and not on any of the previous inputs x1..xt\u22121. The amount of computation required for the update, however, depends on xt\u22121. Specifically, if xt is similar to xt\u22121, the Sigma-Delta network should require less computation to perform an update than the Rounding Network."}, {"heading": "3.3 SPARSE DOT PRODUCT", "text": "Most of the computation in Deep Neural networks is consumed doing matrix multiplications and convolutions. The architecture we propose saves computation by translating the input to these operations into an integer array with a small L1 norm.\nWith sparse, low-magnitude integer input, we can compute the vector-matrix dot product efficiently by decomposing it into a sequence of vector additions. We can see this by decomposing the vector ~x \u2208 Idin into a set of indices \u3008(in, sn) : i \u2208 [1..len(~x)], s \u2208 \u00b11, n = [1..N ]\u3009, such that: ~x = \u2211N n=1 sn~ein , where ein is a one-hot vector with element in hot, and N = |~x|L1 is the total L1 magnitude of the vector. We can then compute the dot-product as a series of additions, as shown in Equation 3.\nu = ~x \u00b7W : W \u2208 Rdin\u00d7dout\n= ( N\u2211 n=1 sn~ein ) \u00b7W = N\u2211 n=1 ~snein \u00b7W = N\u2211 n=1 sn \u00b7Win,\u00b7 (3)\nComputing the dot product this way takes N \u00b7 dout additions. A normal dense dot-product, by comparison, takes din \u00b7 dout multiplications and (din \u2212 1) \u00b7 dout additions. This is where the energy savings come in. Horowitz (2014) estimates that on current 45nm silicon process, a 32-bit floating-point multiplication costs 3.7pJ, vs 0.9pJ for floating-point addition. With integer or fixed-point arithmetic, the difference is even more pronounced, with 3.1pJ for multiplication vs 0.1pJ for addition. This of course ignores the larger cost of processing instructions and moving memory, but gives us an idea of how these operations might compare on optimized hardware. So provided we can approximate the forward pass of a network to a satisfactory degree of precision without doing many more operations than the original network, we can potentially compute much more efficiently."}, {"heading": "3.4 PUTTING IT ALL TOGETHER", "text": "Figure 1 visually summarizes the four types of network we have described. Inserting the temporal sum and difference modules discussed in Section 3.1 leads to the Temporal Difference Network, which\nis functionally identical to the Original Network. Discretizing the output of the temporal difference modules, as discussed in Section 3.2, leads to the Sigma-Delta network. The Sigma-Delta Network is functionally equivalent to the Rounding network, except that it requires less computation per forward pass if it is fed with temporally redundant data."}, {"heading": "4 OPTIMIZING AN EXISTING NETWORK", "text": "In this work, we do not aim to train a quantized networks from scratch, as we did in O\u2019Connor and Welling (2016). Rather, we will take existing pretrained networks and optimize them as Sigma-Delta networks. In in our situation, we have two competing objectives: Error (with respect to a non-quantized forward pass), and Computation: the number of additions performed in a forward pass."}, {"heading": "4.1 RESCALING OUR NEURONS", "text": "We can control the trade-off between these objectives by changing the scale of our discretization. We can thus extend our rounding function by adding a scale k \u2208 R+:\nround(~x, k) \u2261 round(~x \u00b7 k)/k (4)\nThis scale can either be layerwise or unitwise (in which case we have a vector of scales per layer). Higher k values will lead to higher precision, but also more computation, for the reason mentioned in Section 3.2. Note that the final division-by-k is equivalent to scaling the following weight matrix by 1 k ,. So in practice, our network functions become:\nfround(x) = ( hL \u25e6\nwL kL \u25e6 round \u25e6 \u00b7kL \u25e6 ... \u25e6 h1 \u25e6 w1 k1 \u25e6 round \u25e6 \u00b7k1\n) (x) (5)\nf\u03a3\u2206(x) = ( hL \u25e6 \u03a3T \u25e6\nwL kL \u25e6 round \u25e6 \u00b7kL \u25e6\u2206T \u25e6 ... \u25e6 h1 \u25e6 \u03a3T \u25e6 w1 k1 \u25e6 round \u25e6 \u00b7k1 \u25e6\u2206T\n) (x)\n(6)\nFor the Rounding Network and the Sigma-Delta Network, respectively. By adjusting these scales kl, we can affect the tradeoff between computation and error. Note that if we use ReLU activation functions, parameters kl can simply be baked into the parameters of the network (see Appendix C.)"}, {"heading": "4.2 THE ART OF COMPROMISE", "text": "In this section, we aim to find the optimal trade-offs between Error and Computation for the Rounding Network (Network C in Figure 1). We define our loss as follows:\nLerror = D(fround(x), ftrue(x)) (7)\nLcomp = L\u22121\u2211 l=1 |sl|L1dl+1 (8)\nLtotal = Lerror + \u03bbLcomp (9)\nWhere D(a, b) is some scalar distance function (We use KL-divergence for softmax output layers and L2-norm otherwise), fround(x) is the output of the Rounding Network, ftrue(x) is the output of the Original Network. Lcomp is the computational loss, defined as the total number of additions required in a forward pass. Each layer performs |sl|L1dl+1 additions, where sl is the discrete output of the l\u2019th layer, dl+1 is the dimensionality of the (l + 1)\u2019th layer. Finally \u03bb is the tradeoff parameter balancing the importance of the two losses.\nWe aim to use this loss function to optimize our layer-scales, kl to find an optimal tradeoff between accuracy and computation, given the tradeoff parameter \u03bb."}, {"heading": "4.3 DIFFERENTIATING THE UNDIFFERENTIABLE", "text": "We run into an obvious problem: y = round(k \u00b7 x) is not differentiable with respect to our scale, k or our input, x. We get around this by using a similar method to Courbariaux et al., who in turn borrowed it from a lecture by Hinton (2012). That is, on the backward pass, when computing the gradient with respect to the error \u2202Lerror\u2202kl , we simply pass the gradient through the rounding function in layers [l + 1, .., .L], i.e. we say \u2202\u2202xround(x) \u2248 1.\nWhen computing the gradient with respect to the computational cost, \u2202Lcomp\u2202kl , we again just pass the gradient through all rounding operations in the backward pass for layers [l + 1, .., .L]. We found instabilities in training when using the computational loss of higher layers: Lcomp,l\u2032 : l\u2032 \u2208 [l+1, ..., L], to update the scale of layer l. Since we don\u2019t expect this term to have much effect anyway, we choose to only use the gradient of the computational cost in layer l when updating scale kl, i.e., we approximate: \u2202Lcomp \u2202kl \u2248 \u2202Lcomp,l\u2202kl .\nOur scale parameters also must remain in the positive range, and stay well away from zero, where they can cause instability due to the division-by-k (see Equation 5). To handle this, we parametrize our scales in log-space, as \u03bal = log(kl). Our scale-parameter update rule becomes:\n\u2206\u03bal = \u2212\u03b7 ( \u2202Lerror \u2202\u03bal \u2223\u2223\u2223\u2223 pass:[l+1..L] + \u03bb \u2202 \u2202\u03bal |~sl|L1dl+1 \u2223\u2223\u2223\u2223 pass:l ) (10)\nWhere ~sl is the rounded signal from layer l, dl+1 is the \u201cfan-out\u201d (equivalent to the dimension of layer l + 1 in a fully-connected network), and pass : [l + 1..L] indicates that, on the backward pass, we simply pass the gradient through the rounding functions on layers [l + 1..L]."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 TOY PROBLEM: A RANDOM NETWORK", "text": "We start with a very simple toy problem to verify our method. We initialize a 2-layer (100-100-100) ReLU network with random weights using the initialization scheme proposed in Glorot and Bengio (2010), then scaled the weights by ( 1 2 , 8, 1 4 ) . The weight-rescaling does not affect the function of the network but makes it very ill-adapted for discretization (the first layer will be represented too coarsely, causing error; the second too finely, causing wasted computation). We create random input data, and use it to optimize the layer scales according to Equation 10. We verify, by comparing to a large collection of randomly drawn rescalings, that by tuning lambda we land on different places of the Pareto frontier balancing error and computation. Figure 2 shows that this is indeed the case. In this experiment, error and computation are evaluated just on the Rounding network - we test the Sigma-Delta network in the next experiment, which includes temporal data."}, {"heading": "5.2 TEMPORAL-MNIST", "text": "In order to evaluate our network\u2019s ability to save computation on temporal data, we create a dataset that we call \u201cTemporal-MNIST\u201d. This is just a reshuffling of the standard MNIST dataset so that similar frames tend to be nearby, giving the impression of a temporal sequence (see Appendix D for details). The columns of Figure 3 show eight snippets from the Temporal-MNIST dataset.\nWe started our experiment with a conventional ReLU network with layer sizes [784-200-200-10] pretrained on MNIST to a test-accuracy of 97.9%. We then apply the same scale-optimization procedure for the Rounding Network used in the previous experiment to find the optimal rescalings under a range of values for \u03bb. This time, we test the learned scale parameters on both the Rounding Network and the Sigma-Delta network. We do not attempt to directly optimize the scales with respect to the amount of computation in the Sigma-Delta network - we assume that the result should be similar to that for the rounding network, but verifying this is the topic of future work.\nThe results of this experiment can be seen in Figure 4. We see that our discretized networks (Rounding and Sigma-Delta) converge to the error of the original network with fewer computations than are required for a forward pass of the original neural network. Note that the errors of the rounding and\nSigma-Delta networks are identical. This is a consequence of their equivalence, described in Section 3.2. Note also that the errors for all networks are identical between the MNIST and Temporal-MNIST datasets, since for all networks, the prediction function is independent of the order in which inputs are processed. We see that as expected, our Sigma-Delta network does fewer computations than the rounding network on the Temporal-MNIST dataset for the same error, because the update-mechanism of this network takes advantage of the temporal redundancy in the data."}, {"heading": "5.3 A DEEP CONVOLUTIONAL NETWORK ON VIDEO", "text": "Our final experiment is a preliminary exploration into how Sigma Delta networks could perform on natural video data. We start with \u201cVGG 19\u201d - a 19 layer convolutional network, trained to recognise the 1000 ImageNet categories. The network was trained and made public by Simonyan and Zisserman (2014). We take selected videos from the ILSVRC 2015 dataset (Russakovsky et al., 2015), and apply the rescaling method from Section 4.1 to adjust the scales on a per-layer basis. We initially had some difficulty in optimizing the scale parameters of network to a stable point. The network would either fail to reduce computation when it could afford to, or reduce it to the point where the network\u2019s function was so corrupted that error gradients would be meaningless, causing computation loss to win out and activations to drop to zero. A simple solution was to replace the rounding operation in training with addition of uniform random noise \u223c U(\u2212 12 , 1 2 ). This seemed to prevent the network from pushing itself into a regime where all activations become zero. More work is need to understand why the addition of noise is necessary here. Figure 5 shows some preliminary results, which indicate that for video data we can get about 4-10x savings in the amount of computation required, in exchange for a modest loss in computational accuracy."}, {"heading": "6 DISCUSSION", "text": "We have introduced Sigma-Delta Networks, which give us a new way compute the forward pass of a deep neural network. In Sigma-Delta Networks, neurons communicate not by telling other neurons about their current level of activation, but about their change in activation. By discretizing these changes, we end up with very sparse communication between layers. The more similar two consecutive inputs (xt, xt+1) are, the less computation is required to update the network. We show that, while the Sigma-Delta Network\u2019s internal state at time-step t depends on past inputs x1..xt\u22121, the output yt only depends on the current input xt. We show that there is a tradeoff between the accuracy of this network (with respect to the function of a traditional deep net with the same parameters), and the amount of computation required. Finally, we propose a method to jointly optimize error and computation, given a tradeoff parameter \u03bb that indicates how much accuracy we are willing to sacrifice in exchange for fewer computations. We demonstrate that this method substantially reduces the number of computations required to run a deep network on natural, temporally redundant data. However, we observe in our final experiment (Figure 5, bottom) that our assumption that higher-level features would be more temporally stable - and thus require less computation in our Sigma-Delta net - was not true. We suspect that if we were to train the network from scratch on temporal data, we may learn more temporally stable \u201cslow\u201d features, but this is a topic of future work.\nA huge amount of data (eg. video, audio) comes in the form of temporal sequences, and there is an increasingly obvious need to be able to process this data efficiently. There is much to be gained by only doing processing when necessary, based on the contents of the data, and we provide one method for doing that. Further work is needed to determine whether this method would be of use on modern computing hardware, namely GPUs. The problem is that these devices are designed for large, fixed-size array operations, and tend not to be good at taking advantage of sparsity in the data, which requires many random memory accesses to parameters. Fortunately, other devices such as the the IBM TrueNorth (Cassidy et al., 2013) are being designed which keep memory close to processing, and such handle sparse data (and random memory access) much more efficiently.\nThis work opens up an interesting door. In asynchronous, distributed neural networks, a node may receive input from many different nodes asynchronously. Recomputing the function of the network every time a new input signals arrives may be prohibitively expensive. Our scheme deals with this by making the computational cost of an update proportional to the amount of change in the input. The next obvious step is to extend this approach to communicating changes in gradients, which may be helpful in setting up distributed, asynchronous schemes for training neural networks.\nCode for our experiments can be found at: https://github.com/petered/sigma-delta/"}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by Qualcomm, who we\u2019d also like to thank for discussing their past work in the field with us. We\u2019d also like to thank fellow lab members, especially Changyong Oh and Matthias Reisser, for fruitful discussions contributing to this work."}, {"heading": "A DELTA-HERDING PROOF", "text": "Algorithm 3 Herding\n1: Internal: ~\u03c6 \u2208 Rd \u2190 ~0 2: Input: ~xt \u2208 Rd 3: ~\u03c6\u2190 ~\u03c6+ ~xt 4: ~s\u2190 round(~\u03c6) 5: ~\u03c6\u2190 ~\u03c6\u2212 ~s 6: Return: ~s \u2208 Id\nAlgorithm 4 Delta-Herding 1: Internal: ~slast \u2208 Id \u2190 ~0 2: Input: ~xt \u2208 Rd 3: ~s\u2190 round(xt) 4: \u2206~s\u2190 ~s\u2212 ~slast 5: ~slast \u2190 ~s 6: Return: \u2206~s \u2208 Id\nIn previous work (O\u2019Connor and Welling, 2016), we used a quantization scheme which we refer to as herding for brevity and because of its relation to the deterministic sampling scheme in (Welling, 2009), but could otherwise be called Discrete-Time Bidirectional Sigma-Delta Modulation. The procedure is described in Algorithm 3. The input is summed into a potential \u03c6 over time until crossing a quantization threshold (in this case the \u00b1 12 at which the round function changes value), and then resets. Here we prove that Algorithm 4 is equivalent to applying Algorithm 3 to the output of a temporal difference modules. i.e. herd(\u2206t(xt)) = \u2206T (round(xt))\u2200t. First start by observing the following equivalence:\nb = round(a)\u21d4 |a\u2212 b| < 1 2 : b \u2208 I (11)\nWe can apply this to the update rule in Algorithm 3:\nst = round(\u03c6t\u22121 + xt) \u2208 I \u03c6t = (\u03c6t\u22121 + xt)\u2212 st\n(12)\n\u21d2 |\u03c6t| < 1\n2 (13)\nNow, if we unroll the two Equations 12 over time, with initial condition \u03c60 = 0, we see that.\n\u03c6t = t\u2211 \u03c4=1 x\u03c4 \u2212 t\u2211 \u03c4=1 s\u03c4 : \u03c6t \u2208 R, x\u03c4 \u2208 R, s\u03c4 \u2208 I (14)\nUsing Equations 13 and 11, respectively, we can say:\n|\u03c6| = \u2223\u2223\u2223\u2223\u2223 t\u2211\n\u03c4=1\nxt \u2212 t\u2211\n\u03c4=1\ns\u03c4 \u2223\u2223\u2223\u2223\u2223 < 12 (15) \u21d2\nt\u2211 \u03c4=1 s\u03c4 = round\n( t\u2211\n\u03c4=1\nx\u03c4\n) (16)\nWhich can be rearranged to solve for st.\nst = round\n( t\u2211\n\u03c4=0\nx\u03c4\n) \u2212 round ( t\u22121\u2211 \u03c4=0 x\u03c4 ) (17)\nNow if we receive inputs from a \u2206T unit: x\u03c4 = u\u03c4 \u2212 u\u03c4\u22121 with initial condition u0 = 0, then:\n~st = round\n( t\u2211\n\u03c4=0\n(u\u03c4 \u2212 u\u03c4\u22121) ) \u2212 round ( t\u22121\u2211 \u03c4=0 (u\u03c4 \u2212 u\u03c4\u22121) ) (18)\n= round (ut)\u2212 round (ut\u22121) (19) = \u2206T (round(ut)) (20)\nLeaving us with the Delta-Herding algorithm (Algorithm 4).\nTherefore, if we have a linear function w(x), and make use of Equation 1, then we can see that the following is true:\n\u03a3T (w(herd(\u2206T (x)))) = \u03a3T (w(\u2206T (round(x)))) = w(\u03a3T (\u2206T (round(x)))) = w(round(x)) (21)"}, {"heading": "B CALCULATING FLOPS", "text": "When computing the number of operations required for a forward pass, we only account for the matrixproducts/convolutions (which form the bulk of computation in done by a neural network), and not hidden layer activations.\nWe compute the number of operations required for a forward pass of a fully connected network as follows:\nFor the non-discretized network, the number of flops for a single forward pass of a single data point through the network, the flop count is:\nnF lopsdense = L\u22121\u2211 l=0 (dl \u00b7 dl+1 + (dl \u2212 1) \u00b7 dl+1 + dl+1) = 2 L\u22121\u2211 l=0 dl \u00b7 dl+1 (22)\nWhere dl is the dimensionality of layer l (with l = 0 indicating the input layer). The first term counts the number of multiplications, the second the number of additions for reducing the dot-product, and the third the addition of the bias.\nIt can be argued that this is an unfair way to count the number of computations done by the nondiscretized network because of the sparsity of the input layer (due to the zero-background of datasets like MNIST) and the hidden layers (due to ReLU units). Thus we also compute the number of operations for the non-discretized network when factoring in sparsity. The equation is:\nnF lopssparse = L\u22121\u2211 l=0 ( Nl\u2211 i=0 ([al]i 6= 0) \u00b7 dl+1 + ( Nl\u2211 i=0 ([al]i 6= 0)\u2212 1 ) \u00b7 dl+1 + dl+1 )\n= 2 L\u22121\u2211 l=0 Nl\u2211 i=0 ([al]i 6= 0) \u00b7 dl+1\n(23)\nWhere al are the layer activations Nl is the number of units in layer l and ([al]i 6= 0) is 1 if unit i in layer l has nonzero activation and 0 otherwise.\nFor the rounding networks, we count the total absolute value of the discrete activations.\nnF lopsRound = L\u22121\u2211 l=0 ( Nl\u2211 i=0 |[sl]i| \u00b7 dl+1 + dl+1 ) (24)\nWhere sl is the discrete activations of layer l. This corresponds to the number of operations that would be required for doing a dot product with the \u201csequential addition\u201d method described in Section 3.2.\nFinally, the Sigma-Delta network required slightly fewer flops, because the bias only need to be added once (at the beginning), so its cost is amortized.\nnF lops\u03a3\u2206 = L\u22121\u2211 l=0 Nl\u2211 i=0 |[sl]i| \u00b7 dl+1 (25)"}, {"heading": "C BAKING THE SCALES INTO THE PARAMETERS", "text": "In Section 4.1, we mention that we can \u201cbake the scales into the parameters\u201d for ReLU networks. Here we explain that statement.\nSuppose you have a function\nf(x) = k2 \u00b7 h ( x \u00b7 w\nk1\n)\nIf our nonlinearity h is homogeneous (i.e. k \u00b7h(x) = h(k \u00b7x)), as is the case for relu(x) = max(0, x), we can collapse the scales k into the parameters:\nf(x) = k2 \u00b7 relu(x \u00b7 w/k1 + b) (26) = relu (x \u00b7 w \u00b7 k2/k1 + k2 \u00b7 b) (27)\nSo that after training scales, for a given network, we can simply incorporate them into the parameters, as: w\u2032 = w \u00b7 k2/k1, and b\u2032 = k2 \u00b7 b."}, {"heading": "D TEMPORAL MNIST", "text": "The Temporal MNIST dataset is a version of MNIST that is reshuffled so that similar frames end up being nearby. We generate this by iterating through the dataset, keeping a fixed-size buffer of candidates for the next frame. On every iteration, we compare all the candidates to the current frame, and select the closest one. The place that this winning candidate occupied in the buffer is then filled by a new sample from the dataset, and the winning candidate becomes the current frame. The process is repeated until we\u2019ve sorted though all frames in the dataset. Code for generating the dataset can be found at: https://github.com/petered/sigma-delta/blob/master/sigma_ delta/temporal_mnist.py"}, {"heading": "E MNIST RESULTS TABLE", "text": ""}, {"heading": "F HIGH-LEVEL FEATURE STABILITY", "text": "We had initially expected that, when a convolutional network is tasked with processing subsequent frames of video, high-level features would change much more slowly than the pixels and low-level features. This would give a computational advantage to our Sigma-Delta networks, whose computational cost scales with the amount of change in the feature representations. To our surprise, this appeared not to be the case. See the final plot of Figure 5. To verify that this was a property of the original convolutional network (and not somehow related our discretization scheme), we take the same snippet of video used for Figure 5 and measure the inter-frame differences. Figure 6 shows the results of this small experiment, and confirms that our initial belief - that inter-frame differences should become smaller and smaller at higher layers, was not quite correct."}], "references": [{"title": "Vlsi implementation of deep neural network using integral stochastic computing", "author": ["Arash Ardakani", "Fran\u00e7ois Leduc-Primeau", "Naoya Onizawa", "Takahiro Hanyu", "Warren J Gross"], "venue": "arXiv preprint arXiv:1509.08972,", "citeRegEx": "Ardakani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ardakani et al\\.", "year": 2015}, {"title": "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores", "author": ["Andrew S Cassidy", "Paul Merolla", "John V Arthur", "Steve K Esser", "Bryan Jackson", "Rodrigo Alvarez-Icaza", "Pallab Datta", "Jun Sawada", "Theodore M Wong", "Vitaly Feldman"], "venue": "In Neural Networks (IJCNN), The 2013 International Joint Conference on,", "citeRegEx": "Cassidy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cassidy et al\\.", "year": 2013}, {"title": "Fastclassifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["Peter U Diehl", "Daniel Neil", "Jonathan Binas", "Matthew Cook", "Shih-Chii Liu", "Michael Pfeiffer"], "venue": "In 2015 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Diehl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Diehl et al\\.", "year": 2015}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K Esser", "Paul A Merolla", "John V Arthur", "Andrew S Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J Berg", "Jeffrey L McKinstry", "Timothy Melano", "Davis R Barch"], "venue": "arXiv preprint arXiv:1603.08270,", "citeRegEx": "Esser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Neural networks for machine learning. coursera, video lectures", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "computing\u2019s energy problem (and what we can do about it)", "author": ["Mark Horowitz"], "venue": "IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC),", "citeRegEx": "Horowitz.,? \\Q2014\\E", "shortCiteRegEx": "Horowitz.", "year": 2014}, {"title": "How much the eye tells the brain", "author": ["Kristin Koch", "Judith McLean", "Ronen Segev", "Michael A Freed", "Michael J Berry", "Vijay Balasubramanian", "Peter Sterling"], "venue": "Current Biology,", "citeRegEx": "Koch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Koch et al\\.", "year": 2006}, {"title": "Training deep spiking neural networks using backpropagation", "author": ["Jun Haeng Lee", "Tobi Delbruck", "Michael Pfeiffer"], "venue": "arXiv preprint arXiv:1608.08782,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Deep spiking networks", "author": ["Peter O\u2019Connor", "Max Welling"], "venue": "arXiv preprint arXiv:1602.08323,", "citeRegEx": "O.Connor and Welling.,? \\Q2016\\E", "shortCiteRegEx": "O.Connor and Welling.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Herding dynamical weights to learn", "author": ["Max Welling"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Welling.,? \\Q2009\\E", "shortCiteRegEx": "Welling.", "year": 2009}, {"title": "Lif and simplified srm neurons encode signals into spikes via a form of asynchronous pulse sigma-delta modulation", "author": ["Young C Yoon"], "venue": null, "citeRegEx": "Yoon.,? \\Q2016\\E", "shortCiteRegEx": "Yoon.", "year": 2016}, {"title": "Fast and efficient asynchronous neural computation with adapting spiking neural networks", "author": ["Davide Zambrano", "Sander M Bohte"], "venue": "arXiv preprint arXiv:1609.02053,", "citeRegEx": "Zambrano and Bohte.,? \\Q2016\\E", "shortCiteRegEx": "Zambrano and Bohte.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Koch et al. (2006) estimate that the human retina transmits 8.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Finally, our previous work, (O\u2019Connor and Welling, 2016) develops a method for doing backpropagation with the same type of time-agnostic spiking neurons we use here.", "startOffset": 28, "endOffset": 56}, {"referenceID": 6, "context": "The most closely related work is that of Zambrano and Bohte (2016). In this work, the authors describe an Adaptive Sigma-Delta modulation method, in which neurons communicate analog signals to one another by means of a \u201cspike-encoding\u201d mechanism, where a temporal signal is encoded into a sequence of weighted spikes and then approximately decoded as a sum of temporally-shifted exponential kernels.", "startOffset": 41, "endOffset": 67}, {"referenceID": 6, "context": "Their work references a slightly earlier work by Yoon (2016) which reframes common neural models as forms of Asynchronous Sigma-Delta modulation.", "startOffset": 49, "endOffset": 61}, {"referenceID": 5, "context": "In a concurrent work, Lee et al. (2016) implement backpropagation in a similar system (but without adaptive threshold scaling), and demonstrate the best-yet performance on MNIST for networks trained with spiking models.", "startOffset": 22, "endOffset": 40}, {"referenceID": 1, "context": "This work postdates Diehl et al. (2015), which proposes a scheme for normalizing neuron activations so that a spiking neural network can be optimized for fast classification.", "startOffset": 20, "endOffset": 40}, {"referenceID": 1, "context": "This work postdates Diehl et al. (2015), which proposes a scheme for normalizing neuron activations so that a spiking neural network can be optimized for fast classification. Our model contrasts with all of the above in that it is time-agnostic. Although we refer to sending \u201ctemporal differences\u201d between neurons, our neurons have no concept of time - their is no \u201cleak\u201d in neuron potential, and our neurons\u2019 behaviour only depends on the order of the inputs. Our work also separates the concepts of nonlinearity and discretization, uses units that communicate differences rather than absolute signal values, and explicitly minimizes an objective function corresponding to computational cost. Coming from another corner, Courbariaux et al. describe a scheme for binarizing networks with the aim of achieving reductions in the amount of computation and memory required to run neural nets. They introduce a number of tricks for training binarized neural networks - a normally difficult task due to the lack of gradient information. Esser et al. (2016) use a similar binarization scheme to efficiently implement a spiking neural network on the IBM TrueNorth chip.", "startOffset": 20, "endOffset": 1051}, {"referenceID": 0, "context": "Ardakani et al. (2015) take another approach - to approximate real-valued operations of a neural net with a sequence of stochastic integer operations, and show how these can lead to cheaper computation.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Horowitz (2014) estimates that on current 45nm silicon process, a 32-bit floating-point multiplication costs 3.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "4 OPTIMIZING AN EXISTING NETWORK In this work, we do not aim to train a quantized networks from scratch, as we did in O\u2019Connor and Welling (2016). Rather, we will take existing pretrained networks and optimize them as Sigma-Delta networks.", "startOffset": 118, "endOffset": 146}, {"referenceID": 5, "context": ", who in turn borrowed it from a lecture by Hinton (2012). That is, on the backward pass, when computing the gradient with respect to the error \u2202Lerror \u2202kl , we simply pass the gradient through the rounding function in layers [l + 1, .", "startOffset": 44, "endOffset": 58}, {"referenceID": 4, "context": "We initialize a 2-layer (100-100-100) ReLU network with random weights using the initialization scheme proposed in Glorot and Bengio (2010), then scaled the weights by ( 1 2 , 8, 1 4 ) .", "startOffset": 115, "endOffset": 140}, {"referenceID": 6, "context": "In these plots the x-axis is rescaled according to the energy use calculations of Horowitz (2014), assuming the weights and parameters of the network are implemented with 32-bit integer arithmetic.", "startOffset": 82, "endOffset": 98}, {"referenceID": 10, "context": "The network was trained and made public by Simonyan and Zisserman (2014). We take selected videos from the ILSVRC 2015 dataset (Russakovsky et al.", "startOffset": 43, "endOffset": 73}, {"referenceID": 1, "context": "Fortunately, other devices such as the the IBM TrueNorth (Cassidy et al., 2013) are being designed which keep memory close to processing, and such handle sparse data (and random memory access) much more efficiently.", "startOffset": 57, "endOffset": 79}], "year": 2017, "abstractText": "Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. As a result, it ends up repeatedly doing very similar computations. To put an end to such waste, we introduce SigmaDelta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_112"}