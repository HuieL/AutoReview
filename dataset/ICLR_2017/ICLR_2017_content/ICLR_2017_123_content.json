{"name": "ICLR_2017_123.pdf", "metadata": {"source": "CRF", "title": "\u03b2-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK", "authors": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "emails": ["botvinick@google.com", "shakir@google.com", "lerchner@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The difficulty of learning a task for a given machine learning approach can vary significantly depending on the choice of the data representation. Having a representation that is well suited to the particular task and data domain can significantly improve the learning success and robustness of the chosen model (Bengio et al., 2013). It has been suggested that learning a disentangled representation of the generative factors in the data can be useful for a large variety of tasks and domains (Bengio et al., 2013; Ridgeway, 2016). A disentangled representation can be defined as one where single latent units are sensitive to changes in single generative factors, while being relatively invariant to changes in other factors (Bengio et al., 2013). For example, a model trained on a dataset of 3D objects might learn independent latent units sensitive to single independent data generative factors, such as object identity, position, scale, lighting or colour, thus acting as an inverse graphics model (Kulkarni et al., 2015). In a disentangled representation, knowledge about one factor can generalise to novel configurations of other factors. According to Lake et al. (2016), disentangled representations could boost the performance of state-of-the-art AI approaches in situations where they still struggle but where humans excel. Such scenarios include those which require knowledge transfer, where faster learning is achieved by reusing learnt representations for numerous tasks; zero-shot inference, where reasoning about new data is enabled by recombining previously learnt factors; or novelty detection.\nUnsupervised learning of a disentangled posterior distribution over the underlying generative factors of sensory data is a major challenge in AI research (Bengio et al., 2013; Lake et al., 2016). Most previous attempts required a priori knowledge of the number and/or nature of the data generative factors (Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014; Zhu et al., 2014; Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al., 2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016). This is not always feasible in the real world, where the newly initialised learner may be exposed to complex data where no a priori knowledge of the generative factors exists, and little to no supervision for discovering the factors is available. Until recently purely unsupervised\napproaches to disentangled factor learning have not scaled well (Schmidhuber, 1992; Desjardins et al., 2012; Tang et al., 2013; Cohen & Welling, 2014; 2015).\nRecently a scalable unsupervised approach for disentangled factor learning has been developed, called InfoGAN (Chen et al., 2016). InfoGAN extends the generative adversarial network (GAN) (Goodfellow et al., 2014) framework to additionally maximise the mutual information between a subset of the generating noise variables and the output of a recognition network. It has been reported to be capable of discovering at least a subset of data generative factors and of learning a disentangled representation of these factors. The reliance of InfoGAN on the GAN framework, however, comes at the cost of training instability and reduced sample diversity. Furthermore, InfoGAN requires some a priori knowledge of the data, since its performance is sensitive to the choice of the prior distribution and the number of the regularised noise variables. InfoGAN also lacks a principled inference network (although the recognition network can be used as one). The ability to infer the posterior latent distribution from sensory input is important when using the unsupervised model in transfer learning or zero-shot inference scenarios. Hence, while InfoGAN is an important step in the right direction, we believe that further improvements are necessary to achieve a principled way of using unsupervised learning for developing more human-like learning and reasoning in algorithms as described by Lake et al. (2016).\nFinally, there is currently no general method for quantifying the degree of learnt disentanglement. Therefore there is no way to quantitatively compare the degree of disentanglement achieved by different models or when optimising the hyperparameters of a single model.\nIn this paper we attempt to address these issues. We propose \u03b2-VAE, a deep unsupervised generative approach for disentangled factor learning that can automatically discover the independent latent factors of variation in unsupervised data. Our approach is based on the variational autoencoder (VAE) framework (Kingma & Welling, 2014; Rezende et al., 2014), which brings scalability and training stability. While the original VAE work has been shown to achieve limited disentangling performance on simple datasets, such as FreyFaces or MNIST (Kingma & Welling, 2014), disentangling performance does not scale to more complex datasets (e.g. Aubry et al., 2014; Paysan et al., 2009; Liu et al., 2015), prompting the development of more elaborate semi-supervised VAE-based approaches for learning disentangled factors (e.g. Kulkarni et al., 2015; Karaletsos et al., 2016).\nWe propose augmenting the original VAE framework with a single hyperparameter \u03b2 that modulates the learning constraints applied to the model. These constraints impose a limit on the capacity of the latent information channel and control the emphasis on learning statistically independent latent factors. \u03b2-VAE with \u03b2 = 1 corresponds to the original VAE framework (Kingma & Welling, 2014; Rezende et al., 2014). With \u03b2 > 1 the model is pushed to learn a more efficient latent representation of the data, which is disentangled if the data contains at least some underlying factors of variation that are independent. We show that this simple modification allows \u03b2-VAE to significantly improve the degree of disentanglement in learnt latent representations compared to the unmodified VAE framework (Kingma & Welling, 2014; Rezende et al., 2014). Furthermore, we show that \u03b2-VAE achieves state of the art disentangling performance against both the best unsupervised (InfoGAN: Chen et al., 2016) and semi-supervised (DC-IGN: Kulkarni et al., 2015) approaches for disentangled factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry et al., 2014) and faces (Paysan et al., 2009) using qualitative evaluation. Finally, to help quantify the differences, we develop a new measure of disentanglement and show that \u03b2-VAE significantly outperforms all our baselines on this measure (ICA, PCA, VAE Kingma & Ba (2014), DC-IGN Kulkarni et al. (2015), and InfoGAN Chen et al. (2016)).\nOur main contributions are the following: 1) we propose \u03b2-VAE, a new unsupervised approach for learning disentangled representations of independent visual data generative factors; 2) we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models; 3) we demonstrate both qualitatively and quantitatively that our \u03b2-VAE approach achieves state-of-the-art disentanglement performance compared to various baselines on a variety of complex datasets.\n2 \u03b2-VAE FRAMEWORK DERIVATION\nLet D = {X,V,W} be the set that consists of images x \u2208 RN and two sets of ground truth data generative factors: conditionally independent factors v \u2208 RK , where log p(v|x) = \u2211k log p(vk|x); and conditionally dependent factors w \u2208 RH . We assume that the images x are generated by the true world simulator using the corresponding ground truth data generative factors: p(x|v,w) = Sim(v,w).\nWe want to develop an unsupervised deep generative model that, using samples from X only, can learn the joint distribution of the data x and a set of generative latent factors z (z \u2208 RM , where M \u2265 K) such that z can generate the observed data x; that is, p(x|z) \u2248 p(x|v,w) = Sim(v,w). Thus a suitable objective is to maximise the marginal (log-)likelihood of the observed data x in expectation over the whole distribution of latent factors z:\nmax \u03b8\nEp\u03b8(z)[p\u03b8(x|z)] (1)\nFor a given observation x, we describe the inferred posterior configurations of the latent factors z by a probability distribution q\u03c6(z|x). Our aim is to ensure that the inferred latent factors q\u03c6(z|x) capture the generative factors v in a disentangled manner. The conditionally dependent data generative factors w can remain entangled in a separate subset of z that is not used for representing v. In order to encourage this disentangling property in the inferred q\u03c6(z|x), we introduce a constraint over it by trying to match it to a prior p(z) that can both control the capacity of the latent information bottleneck, and embodies the desiderata of statistical independence mentioned above. This can be achieved if we set the prior to be an isotropic unit Gaussian (p(z) = N (0, I)), hence arriving at the constrained optimisation problem in Eq. 2, where specifies the strength of the applied constraint.\nmax \u03c6,\u03b8\nEx\u223cD [ Eq\u03c6(z|x)[log p\u03b8(x|z)] ] subject to DKL(q\u03c6(z|x)||p(z)) < (2)\nRe-writing Eq. 2 as a Lagrangian under the KKT conditions (Kuhn & Tucker, 1951; Karush, 1939), we obtain:\nF(\u03b8, \u03c6, \u03b2;x, z) = Eq\u03c6(z|x)[log p\u03b8(x|z)]\u2212 \u03b2 (DKL(q\u03c6(z|x)||p(z))\u2212 ) (3)\nwhere the KKT multiplier \u03b2 is the regularisation coefficient that constrains the capacity of the latent information channel z and puts implicit independence pressure on the learnt posterior due to the isotropic nature of the Gaussian prior p(z). Since \u03b2, \u2265 0 according to the complementary slackness KKT condition, Eq. 3 can be re-written to arrive at the \u03b2-VAE formulation - as the familiar variational free energy objective function as described by Jordan et al. (1999), but with the addition of the \u03b2 coefficient:\nF(\u03b8, \u03c6, \u03b2;x, z) \u2265 L(\u03b8, \u03c6;x, z, \u03b2) = Eq\u03c6(z|x)[log p\u03b8(x|z)]\u2212 \u03b2 DKL(q\u03c6(z|x)||p(z)) (4)\nVarying \u03b2 changes the degree of applied learning pressure during training, thus encouraging different learnt representations. \u03b2-VAE where \u03b2 = 1 corresponds to the original VAE formulation of (Kingma & Welling, 2014). We postulate that in order to learn disentangled representations of the conditionally independent data generative factors v, it is important to set \u03b2 > 1, thus putting a stronger constraint on the latent bottleneck than in the original VAE formulation of Kingma & Welling (2014). These constraints limit the capacity of z, which, combined with the pressure to maximise the log likelihood of the training data x under the model, should encourage the model to learn the most efficient representation of the data. Since the data x is generated using at least some conditionally independent ground truth factors v, and the DKL term of the \u03b2-VAE objective function encourages conditional independence in q\u03c6(z|x), we hypothesise that higher values of \u03b2 should encourage learning a disentangled representation of v. The extra pressures coming from high \u03b2 values, however, may create a trade-off between reconstruction fidelity and the quality of disentanglement within the learnt latent representations. Disentangled representations emerge when the right balance is found between information preservation (reconstruction cost as regularisation) and latent channel capacity restriction (\u03b2 > 1). The latter can lead to poorer reconstructions due to the loss of high frequency details when passing through a constrained latent bottleneck. Hence, the log likelihood of the data under the learnt model is a poor metric for evaluating disentangling in \u03b2-VAEs. Instead we propose a quantitative metric that directly measures the degree of learnt disentanglement in the latent representation.\nSince our proposed hyperparameter \u03b2 directly affects the degree of learnt disentanglement, we would like to estimate the optimal \u03b2 for learning a disentangled latent representation directly. However, it is not possible to do so. This is because the optimal \u03b2 will depend on the value of in Eq.2. Different datasets and different model architectures will require different optimal values of . However, when optimising \u03b2 in Eq. 4, we are indirectly also optimising for the best disentanglement (see Sec.A.7 for details), and while we can not learn the optimal value of \u03b2 directly, we can instead estimate it using either our proposed disentanglement metric (see Sec. 3) or through visual inspection heuristics.\nz1diff\nzLdiff\nzbdiff = 1\nL\nL\u2211\nl=1\nzldiff\np(y|zbdiff)\nFigure 5: Schematic of the proposed disentanglement metric: over a batch of L samples, each pair of images has a fixed value for one target generative factor y (here y = scale) and differs on all others. A linear classifier is then trained to identify the target factor using the average pairwise difference zbdiff in the latent space over L samples."}, {"heading": "3 DISENTANGLEMENT METRIC", "text": "It is important to be able to quantify the level of disentanglement achieved by different models. Designing a metric for this, however, is not straightforward. We begin by defining the properties that we expect a disentangled representation to have. Then we describe our proposed solution for quantifying the presence of such properties in a learnt representation.\nAs stated above, we assume that the data is generated by a ground truth simulation process which uses a number of data generative factors, some of which are conditionally independent, and we also assume that they are interpretable. For example, the simulator might sample independent factors corresponding to object shape, colour and size to generate an image of a small green apple. Because of the independence property, the simulator can also generate small red apples or big green apples. A representation of the data that is disentangled with respect to these generative factors, i.e. which encodes them in separate latents, would enable robust classification even using very simple linear classifiers (hence providing interpretability). For example, a classifier that learns a decision boundary that relies on object shape would perform as well when other data generative factors, such as size or colour, are varied.\nNote that a representation consisting of independent latents is not necessarily disentangled, according to our desiderata. Independence can readily be achieved by a variety of approaches (such as PCA or ICA) that learn to project the data onto independent bases. Representations learnt by such approaches do not in general align with the data generative factors and hence may lack interpretability. For this reason, a simple cross-correlation calculation between the inferred latents would not suffice as a disentanglement metric.\nOur proposed disentangling metric, therefore, measures both the independence and interpretability (due to the use of a simple classifier) of the inferred latents. To apply our metric, we run inference on a number of images that are generated by fixing the value of one data generative factor while randomly sampling all others. If the independence and interpretability properties hold for the inferred representations, there will be less variance in the inferred latents that correspond to the fixed generative factor. We use a low capacity linear classifier to identify this factor and report the accuracy value as the final disentanglement metric score. Smaller variance in the latents corresponding to the target factor will make the job of this classifier easier, resulting in a higher score under the metric. See Fig. 5 for a representation of the full process.\nMore formally, we start from a dataset D = {X,V,W} as described in Sec. 2, assumed to contain a balanced distribution of ground truth factors (v,w), where images data points are obtained using a ground truth simulator process x \u223c Sim(v,w). We also assume we are given labels identifying a subset of the independent data generative factors v \u2208 V for at least some instances. We then construct a batch of B vectors zbdiff, to be fed as inputs to a linear classifier as follows:\n1. Choose a factor y \u223c Unif [1...K] (e.g. y = scale in Fig. 5).\n2. For a batch of L samples:\n(a) Sample two sets of latent representations, v1,l and v2,l, enforcing [v1,l]k = [v2,l]k if k = y (so that the value of factor k = y is kept fixed). (b) Simulate image x1,l \u223c Sim(v1,l), then infer z1,l = \u00b5(x1,l), using the encoder q(z|x) \u223c N (\u00b5(x), \u03c3(x)). Repeat the process for v2,l.\n(c) Compute the difference zldiff = |z1,l \u2212 z2,l|, the absolute linear difference between the inferred latent representations.\n3. Use the average zbdiff = 1 L \u2211L l=1 z l diff to predict p(y|zbdiff) (again, y = scale in Fig. 5) and\nreport the accuracy of this predictor as disentangement metric score.\nThe classifier\u2019s goal is to predict the index y of the generative factor that was kept fixed for a given zbdiff. The accuracy of this classifier over multiple batches is used as our disentanglement metric score. We choose a linear classifier with low VC-dimension in order to ensure it has no capacity to perform nonlinear disentangling by itself. We take differences of two inferred latent vectors to reduce the variance in the inputs to the classifier, and to reduce the conditional dependence on the inputs x. This ensures that on average\n[ zbdiff ] y < [ zbdiff ] {\\y}. See Equations 5 in Appendix A.4 for more details of\nthe process."}, {"heading": "4 EXPERIMENTS", "text": "In this section we first qualitatively demonstrate that our proposed \u03b2-VAE framework consistently discovers more latent factors and disentangles them in a cleaner fashion that either unmodified VAE (Kingma & Welling, 2014) or state of the art unsupervised (InfoGAN: Chen et al., 2016) and semisupervised (DC-IGN: Kulkarni et al., 2015) solutions for disentangled factor learning on a variety of benchmarks. We then quantify and characterise the differences in disentangled factor learning between our \u03b2-VAE framework and a variety of benchmarks using our proposed new disentangling metric."}, {"heading": "4.1 QUALITATIVE BENCHMARKS", "text": "We trained \u03b2-VAE (see Tbl. 1 for architecture details) on a variety of datasets commonly used to evaluate disentangling performance of models: celebA (Liu et al., 2015), chairs (Aubry et al., 2014) and faces (Paysan et al., 2009). Figures 1-3 provide a qualitative comparison of the disentangling performance of \u03b2-VAE, VAE (\u03b2 = 1) (Kingma & Welling, 2014), InfoGAN (Chen et al., 2016) and DC-IGN (Kulkarni et al., 2015) as appropriate.\nIt can be seen that across all datasets \u03b2-VAE is able to automatically discover and learn to disentangle all of the factors learnt by the semi-supervised DC-IGN (Kulkarni et al., 2015): azimuth (Fig. 3a, Fig. 2a), lighting and elevation (Fig. 3b,c)). Often it acts as a more convincing inverse graphics network than DC-IGN (e.g. Fig. 3a) or InfoGAN (e.g. Fig. 2a, Fig. 1a-c or Fig. 3a). Furthermore, unlike DC-IGN, \u03b2-VAE requires no supervision and hence can learn about extra unlabelled data generative factors that DC-IGN can not learn by design, such as chair width or leg style (Fig. 2b,c). The unsupervised InfoGAN (Chen et al., 2016) approach shares this quality with \u03b2-VAE, and the two frameworks tend to discover overlapping, but not necessarily identical sets of data generative factors. For example, both \u03b2-VAE and InfoGAN (but not DC-IGN) learn about the width of chairs (Fig. 2b). Only \u03b2-VAE, however, learns about the chair leg style (Fig. 2c). It is interesting to note how \u03b2-VAE is able to generate an armchair with a round office chair base, even though such armchairs do not exist in the dataset (or, perhaps, reality). Furthermore, only \u03b2-VAE is able to discover all three factors of variation (chair azimuth, width and leg style) within a single model, while InfoGAN learns to allocate its continuous latent variable to either azimuth or width. InfoGAN sometimes discovers factors that \u03b2-VAE does not precisely disentangle, such as the presence of sunglasses in celebA. \u03b2-VAE does, however, discover numerous extra factors such as skin colour, image saturation, and age/gender that are not reported in the InfoGAN paper (Chen et al., 2016) (Fig. 4). Furthermore, \u03b2-VAE latents tend to learn a smooth continuous transformation over a wider range of factor values than InfoGAN (e.g. rotation over a wider range of angles as shown in Figs. 1-3a).\nOverall \u03b2-VAE tends to consistently and robustly discover more latent factors and learn cleaner disentangled representations of them than either InfoGAN or DC-IGN. This holds even on such challenging datasets as celebA. Furthermore, unlike InfoGAN and DC-IGN, \u03b2-VAE requires no design decisions or assumptions about the data, and is very stable to train.\nWhen compared to the unmodified VAE baseline (\u03b2 = 1) \u03b2-VAE consistently learns significantly more disentangled latent representations. For example, when learning about chairs, VAE entangles chair width with leg style (Fig. 2b). When learning about celebA, VAE entangles azimuth with emotion and gender (Fig. 1a); emotion with hair style, skin colour and identity (Fig. 1b); while the VAE fringe latent also codes for baldness and head size (Fig. 1c). Although VAE performs relatively well on the faces dataset, it still struggles to learn a clean representation of azimuth (Fig. 3a). This, however, suggests that a continuum of disentanglement quality exists, and it can be traversed by varying \u03b2 within the \u03b2-VAE framework. While increasing \u03b2 often leads to better disentanglement, it may come at the cost of blurrier reconstructions and losing representations for some factors, particularly those that correspond to only minor changes in pixel space."}, {"heading": "4.2 QUANTITATIVE BENCHMARKS", "text": "In order to quantitatively compare the disentangling performance of \u03b2-VAE against various baselines, we created a synthetic dataset of 737,280 binary 2D shapes (heart, oval and square) generated from the Cartesian product of the shape and four independent generative factors vk defined in vector graphics: position X (32 values), position Y (32 values), scale (6 values) and rotation (40 values over the 2\u03c0 range). To ensure smooth affine object transforms, each two subsequent values for each factor vk were chosen to ensure minimal differences in pixel space given 64x64 pixel image resolution. This dataset was chosen because it contains no confounding factors apart from its five independent data generative factors (identity, position X, position Y, scale and rotation). This gives us knowledge of the ground truth for comparing the disentangling performance of different models in an objective manner.\nWe used our proposed disentanglement metric (see Sec. 3) to quantitatively compare the ability of \u03b2-VAE to automatically discover and learn a disentangled representation of the data generative factors of the synthetic dataset of 2D shapes described above with that of a number of benchmarks (see Tbl. 1 in Appendix for model architecture details). The table in Fig. 6 (left) reports the classification accuracy of the disentanglement metric for 5,000 test samples. It can be seen that \u03b2-VAE (\u03b2 = 4) significantly outperforms all baselines, such as an untrained VAE and the original VAE formulation of Kingma & Welling (2014) (\u03b2 = 1) with the same architecture as \u03b2-VAE, the top ten PCA or ICA components of the data (see Sec. A.3 for details), or when using the raw pixels directly. \u03b2-VAE also does better than InfoGAN. Remarkably, \u03b2-VAE performs on the same level as DC-IGN despite the latter being semi-supervised and the former wholly unsupervised. Furthermore, \u03b2-VAE achieved similar classification accuracy as the ground truth vectors used for data generation, thus suggesting that it was able to learn a very good disentangled representation of the data generative factors.\nWe also examined qualitatively the representations learnt by \u03b2-VAE, VAE, InfoGAN and DC-IGN on the synthetic dataset of 2D shapes. Fig. 7A demonstrates that after training, \u03b2-VAE with \u03b2 = 4 learnt a good (while not perfect) disentangled representation of the data generative factors, and its decoder learnt to act as a rendering engine. Its performance was comparative to that of DCIGN (Fig. 7C), with the difference that DC-IGN required a priori knowledge about the quantity of the data generative factors, while \u03b2-VAE was able to discover them in an unsupervised manner. The most informative latent units zm of \u03b2-VAE have the highest KL divergence from the unit Gaussian prior (p(z) = N (0, I)), while the uninformative latents have KL divergence close to zero. Fig. 7A demonstrates the selectivity of each latent zm to the independent data generating factors: z\u00b5m = f(vk) \u2200vk \u2208 {vpositionX , vpositionY , vscale, vrotation} (top three rows), where z\u00b5m is the learnt Gaussian mean of latent unit zm. The effect of traversing each latent zm on the resulting reconstructions is shown in the bottom five rows of Fig. 7A. The latents z6 and z2 learnt to encode X and Y coordinates of the objects respectively; unit z1 learnt to encode scale; and units z5 and z7 learnt to encode rotation. The frequency of oscillations in each rotational latent corresponds to the rotational symmetry of the corresponding object (2\u03c0 for heart, \u03c0 for oval and \u03c0/2 for square). Furthermore, the two rotational latents seem to encode cos and sin rotational coordinates, while the positional latents align with the Cartesian axes. While such alignment with intuitive factors for humans is not guaranteed, empirically we found it to be very common. Fig. 7B demonstrates that the unmodified\nVAE baseline (\u03b2 = 1) is not able to disentangle generative factors in the data as well as \u03b2-VAE with appropriate learning pressures. Instead each latent z (apart from z9, which learnt rotation) encodes at least two data generative factors. InfoGAN also achieved a degree of disentangling (see Fig. 7D), particularly for positional factors. However, despite our best efforts to train InfoGAN, we were not able to achieve the same degree of disentangling in other factors, such as rotation, scale and shape. We also found its ability to generate the different shapes in the dataset to be inaccurate and unstable during training, possibly due to reported limitations of the GAN framework, which can struggle to learn the full data distribution and instead will often learn a small subset of its modes (Salimans et al., 2016; Zhao et al., 2016).\nUnderstanding the effects of \u03b2 We hypothesised that constrained optimisation is important for enabling deep unsupervised models to learn disentangled representations of the independent data generative factors (Sec. 2). In the \u03b2-VAE framework this corresponds to tuning the \u03b2 coefficient. One way to view \u03b2 is as a mixing coefficient (see Sec. A.6 for a derivation) for balancing the magnitudes of gradients from the reconstruction and the prior-matching components of the VAE lower bound formulation in Eq. 4 during training. In this context it makes sense to normalise \u03b2 by latent z size m and input x size n in order to compare its different values across different latent layer sizes and different datasets (\u03b2norm = \u03b2MN ). We found that larger latent z layer sizes m require higher constraint pressures (higher \u03b2 values), see Fig. 6 (Right). Furthermore, the relationship of \u03b2 for a given m is characterised by an inverted U curve. When \u03b2 is too low or too high the model learns an entangled latent representation due to either too much or too little capacity in the latent z bottleneck. We find that in general \u03b2 > 1 is necessary to achieve good disentanglement. However if \u03b2 is too high and the resulting capacity of the latent channel is lower than the number of data generative factors, then the learnt representation necessarily has to be entangled (as a low-rank projection of the true data generative factors will compress them in a non-factorial way to still capture the full data distribution well). We also note that VAE reconstruction quality is a poor indicator of learnt disentanglement. Good disentangled representations often lead to blurry reconstructions due to the restricted capacity of the latent information channel z, while entangled representations often result in the sharpest reconstructions. We therefore suggest that one should not necessarily strive for perfect reconstructions when using \u03b2-VAEs as unsupervised feature learners - though it is often possible to find the right \u03b2-VAE architecture and the right value of \u03b2 to have both well disentangled latent representations and good reconstructions.\nWe proposed a principled way of choosing \u03b2 for datasets with at least weak label information. If label information exists for at least a small subset of the independent data generative factors of variation, one can apply the disentanglement metric described in Sec. 3 to approximate the level of learnt disentanglement for various \u03b2 choices during a hyperparameter sweep. When such labelled information is not available, the optimal value of \u03b2 can be found through visual inspection of what\npos. Y pos. X scale rotation rotation\npos. X pos. Y scale rotation object\n0 3\n-3\nA B\npo sit io n sc al e ro ta tio n\nsi ng\nle la\nte nt\nm ea\nn tr\nav er\nsa l\nm ea\nn la\nte nt\nre sp\non se\nle ar nt\nva ria\nnc e\nC 0 1. 5\n-1 .5\nD\npo sit io n sc al e ro ta tio n 0 3\n-3\nFigure 7: A: Representations learnt by a \u03b2-VAE (\u03b2 = 4). Each column represents a latent zi, ordered according to the learnt Gaussian variance (last row). Row 1 (position) shows the mean activation (red represents high values) of each latent zi as a function of all 32x32 locations averaged across objects, rotations and scales. Row 2 and 3 show the mean activation of each unit zi as a function of scale (respectively rotation), averaged across rotations and positions (respectively scales and positions). Square is red, oval is green and heart is blue. Rows 4-8 (second group) show reconstructions resulting from the traversal of each latent zi over three standard deviations around the unit Gaussian prior mean while keeping the remaining 9/10 latent units fixed to the values obtained by running inference on an image from the dataset. B: Similar analysis for VAE (\u03b2 = 1). C: Similar analysis for DCIGN, clamping a single latent each for scale, positions, orientation and 5 for shape. D: Similar analysis for InfoGAN, using 5 continuous latents regularized using the mutual information cost, and 5 additional unconstrained noise latents (not shown).\neffect the traversal of each single latent unit zm has on the generated images (x|z) in pixel space (as shown in Fig. 7 rows 4-8). For the 2D shapes dataset, we have found that the optimal values of \u03b2 as determined by visual inspection match closely the optimal values as determined by the disentanglement metric."}, {"heading": "5 CONCLUSION", "text": "In this paper we have reformulated the standard VAE framework (Kingma & Welling, 2014; Rezende et al., 2014) as a constrained optimisation problem with strong latent capacity constraint and independence prior pressures. By augmenting the lower bound formulation with the \u03b2 coefficient that regulates the strength of such pressures and, as a consequence, the qualitative nature of the representations learnt by the model, we have achieved state of the art results for learning disentangled representations of data generative factors. We have shown that our proposed \u03b2-VAE framework significantly outperforms both qualitatively and quantitatively the original VAE (Kingma & Welling, 2014), as well as state-of-the-art unsupervised (InfoGAN: Chen et al., 2016) and semi-supervised (DC-IGN: Kulkarni et al., 2015) approaches to disentangled factor learning. Furthermore, we have shown that \u03b2-VAE consistently and robustly discovers more factors of variation in the data, and it learns a representation that covers a wider range of factor values and is disentangled more cleanly than other benchmarks, all in a completely unsupervised manner. Unlike InfoGAN and DC-IGN, our approach does not depend on any a priori knowledge about the number or the nature of data generative factors. Our preliminary investigations suggest that the performance of the \u03b2-VAE framework may depend on the sampling density of the data generative factors within a training dataset (see Appendix A.8 for more details). It appears that having more densely sampled data generative factors results in better disentangling performance of \u03b2-VAE, however we leave a more principled investigation of this effect to future work.\n\u03b2-VAE is robust with respect to different architectures, optimisation parameters and datasets, hence requiring few design decisions. Our approach relies on the optimisation of a single hyperparameter \u03b2, which can be found directly through a hyperparameter search if weakly labelled data is available to calculate our new proposed disentangling metric. Alternatively the optimal \u03b2 can be estimated heuristically in purely unsupervised scenarios. Learning an interpretable factorised representation of the independent data generative factors in a completely unsupervised manner is an important precursor for the development of artificial intelligence that understands the world in the same way that humans do (Lake et al., 2016). We believe that using our approach as an unsupervised pretraining stage for supervised or reinforcement learning will produce significant improvements for scenarios such as transfer or fast learning."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "We would like to thank Charles Blundell, Danilo Rezende, Tejas Kulkarni and David Pfau for helpful comments that improved the manuscript."}, {"heading": "A APPENDIX", "text": ""}, {"heading": "A.1 MODEL ARCHITECTURE DETAILS", "text": "A summary of all model architectures used in this paper can be seen in Tbl 1.\nA.2 INFOGAN TRAINING\nTo train the InfoGAN network described in Tbl. 1 on the 2D shapes dataset (Fig. 7), we followed the training paradigm described in Chen et al. (2016) with the following modifications. For the mutual information regularised latent code, we used 5 continuous variables ci sampled uniformly from (\u22121, 1). We used 5 noise variables zi, as we found that using a reduced number of noise variables improved the quality of generated samples for this dataset. To help stabilise training, we used the instance noise trick described in Shi et al. (2016), adding Gaussian noise to the discriminator inputs (0.2 standard deviation on images scaled to [\u22121, 1]). We followed Radford et al. (2015) for the architecture of the convolutional layers, and used batch normalisation in all layers except the last in the generator and the first in the discriminator.\nA.3 ICA AND PCA BASELINES\nIn order to calculate the ICA benchmark, we applied fastICA (Pedregosa et al., 2011) algorithm to the whitened pixel data. Due to memory limitations we had to apply the algorithm to pairwise combinations of the subsets of the dataset corresponding to the transforms of each of the three 2D object identities. We calculated the disentangling metric for all three ICA models trained on each of the three pairwise combinations of 2D objects, before presenting the average of these scores in Fig. 6.\nWe performed PCA on the raw and whitened pixel data. Both approaches resulted in similar disentangling metric scores. Fig. 6 reports the PCA results calculated using whitened pixel data for more direct comparison with the ICA score."}, {"heading": "A.4 DISENTANGLEMENT METRIC DETAILS", "text": "We used a linear classifier to learn the identity of the generative factor that produced zbdiff (see Equations (5) for the process used to obtain samples of zbdiff). We used a fully connected linear\nclassifier to predict p(y|zbdiff), where y is one of four generative factors (position X, position Y, scale and rotation). We used softmax output nonlinearity and a negative log likelihood loss function. The classifier was trained using the Adagrad (Duchi et al., 2011) optimisation algorithm with learning rate of 1e-2 until convergence.\nD = {V \u2208 RK ,W \u2208 RH , X \u2208 RN}, y \u223c Unif [1...K] Repeat for b = 1 . . . B :\nv1,l \u223c p(v), w1,l \u223c p(w), w2,l \u223c p(w), [v2,l]k = {\n[v1,l]k , if k = y \u223c p(vk), otherwise\nx1,l \u223c Sim(v1,l,w1,l), x2,l \u223c Sim(v2,l,w2,l), (5) q(z|x) \u223c N (\u00b5(x), \u03c3(x)), z1,l = \u00b5(x1,l), z2,l = \u00b5(x2,l)\nzldiff = |z1,l \u2212 z2,l|, zbdiff = 1\nL\nL\u2211\nl=1\nzldiff\nAll disentanglement metric score results reported in the paper were calculated in the following manner. Ten replicas of each model with the same hyperparameters were trained using different random seeds to obtain disentangled representations. Each of the ten trained model replicas was evaluated three times using the disentanglement metric score algorithm, each time using a different random seed to initialise the linear classifier. We then discarded the bottom 50% of the thirty resulting scores and reported the remaining results. This was done to control for the outlier results from the few experiments that diverged during training.\nThe results reported in table in Fig. 6 (left) were calculated using the following data. Ground truth uses independent data generating factors v (our dataset did not contain any correlated data generating factors w). PCA and ICA decompositions keep the first ten components (PCA components explain 60.8% of variance). \u03b2-VAE (\u03b2 = 4), VAE (\u03b2 = 1) and VAE untrained have the same fully connected architecture with ten latent units z. InfoGAN uses \u201cinferred\u201d values of the five continuous latents that were regularised with the mutual information objective during training."}, {"heading": "A.5 CLASSIFYING THE GROUND TRUTH DATA GENERATIVE FACTORS VALUES", "text": "In order to further verify the validity of our proposed disentanglement metric we ran an extra quantitative test: we trained a linear classifier to predict the ground truth value of each of the five data generative factors used to generate the 2D shapes dataset. While this test does not measure disentangling directly (since it does not measure independence of the latent representation), a disentangled representation should make such a classification trivial. It can be seen in Table 2 that the representation learnt by \u03b2-VAE is on average the best representation for factor classification across all five factors. It is closely followed by DC-IGN. It is interesting to note that ICA does well only at encoding object identity, while PCA manages to learn a very good representation of object position.\nA.6 INTERPRETING NORMALISED \u03b2\nWe start with the \u03b2-VAE constrained optimisation formulation that we have derived in Sec. 2.\nL(\u03b8, \u03c6;x, z, \u03b2) = Eq\u03c6(z|x)[log p\u03b8(x|z)]\u2212 \u03b2 DKL(q\u03c6(z|x)||p(z)) (6)\nWe make the assumption that every pixel n in x \u2208 RN is conditionally independent given z (Doersch, 2016). The first term of Eq. 6 then becomes:\nEq\u03c6(z|x)[log p\u03b8(x|z)] = Eq\u03c6(z|x)[log \u220f\nn\np\u03b8(xn|z)] = Eq\u03c6(z|x)[ \u2211\nn\nlog p\u03b8(xn|z)] (7)\nDividing both sides of Eq. 6 by N produces:\nL(\u03b8, \u03c6;x, z, \u03b2) \u221d Eq\u03c6(z|x)En[log p\u03b8(xn|z)]\u2212 \u03b2\nN DKL(q\u03c6(z|x)||p(z)) (8)\nWe design \u03b2-VAE to learn conditionally independent factors of variation in the data. Hence we assume conditional independence of every latent zm given x (where m \u2208 1...M , and M is the dimensionality of z). Since our prior p(z) is an isotropic unit Gaussian, we can re-write the second term of Eq. 6 as:\nDKL(q\u03c6(z|x)||p(z)) = \u222b\nz\nq\u03c6(z|x)log q\u03c6(z|x) p(z) = \u2211\nm\n\u222b\nzm\nq\u03c6(zm|x)log q\u03c6(zm|x) p(zm)\n(9)\nMultiplying the second term in Eq. 8 by a factor MM produces:\nL(\u03b8, \u03c6;x, z, \u03b2) \u221d Eq\u03c6(z|x)En[log p\u03b8(xn|z)]\u2212 \u03b2M\nN Em\n\u222b\nzm\n[q\u03c6(zm|x)log q\u03c6(zm|x) p(zm) ]\n= Eq\u03c6(z|x)En[log p\u03b8(xn|z)]\u2212 \u03b2M\nN Em[DKL(q\u03c6(zm|x)||p(zm))]\n(10)\nHence using\n\u03b2norm = \u03b2M\nN\nin Eq. 10 is equivalent to optimising the original \u03b2-VAE formulation from Sec. 2, but with the additional independence assumptions that let us calculate data log likelihood and KL divergence terms in expectation over the individual pixels xn and individual latents zm.\nA.7 RELATIONSHIP BETWEEN \u03b2 AND\nFor a given we can solve the constrained optimisation problem in Eq. 3 (find the optimal (\u03b8\u2217, \u03c6\u2217, \u03b2\u2217), such that \u2206F(\u03b8\u2217, \u03c6\u2217, \u03b2\u2217) = 0). We can then re-write our optimal solution to the original optimisation problem in Eq. 2 as a function of :\nG(\u03b8\u2217( ), \u03c6\u2217( )) = Eq\u03c6\u2217( )(z|x)[log p\u03b8\u2217( )(x|z)] (11) Now \u03b2 can be interpreted as the rate of change of the optimal solution (\u03b8\u2217, \u03c6\u2217) to G when varying the constraint :\n\u03b4G \u03b4 = \u03b2\u2217( ) (12)"}, {"heading": "A.8 DATA CONTINUITY", "text": "We hypothesise that data continuity plays a role in guiding unsupervised models towards learning the correct data manifolds. To test this idea we measure how the degree of learnt disentangling changes with reduced continuity in the 2D shapes dataset. We trained a \u03b2-VAE with \u03b2 = 4 (Figure 7A) on subsamples of the original 2D shapes dataset, where we progressively decreased the generative factor sampling density. Reduction in data continuity negatively correlates with the average pixel wise (Hamming) distance between two consecutive transforms of each object (normalised by the average number of pixels occupied by each of the two adjacent transforms of an object to account for object\nscale). Figure 8 demonstrates that as the continuity in the data reduces, the degree of disentanglement in the learnt representations also drops. This effect holds after additional hyperparameter tuning and can not solely be explained by the decrease in dataset size, since the same VAE can learn disentangled representations from a data subset that preserves data continuity but is approximately 55% of the original size (results not shown).\nA.9 \u03b2-VAE SAMPLES\nSamples from \u03b2-VAE that learnt disentangled (\u03b2 = 4) and entangled (\u03b2 = 1) representations can be seen in Figure 9.\nA.10 EXTRA \u03b2-VAE TRAVERSAL PLOTS\nWe present extra latent traversal plots from \u03b2-VAE that learnt disentangled representations of 3D chairs (Figures 10-11) and CelebA (Figures 12-14) datasets. Here we show traversals from all informative latents from a large number of seed images."}], "references": [{"title": "Disentangling factors of variation via generative", "author": ["G. Desjardins", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Desjardins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Y. Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["1939. D.P. Kingma", "Jimmy Ba"], "venue": "thesis, Univ. of Chicago,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning to disentangle factors", "author": ["2015. Scott Reed", "Kihyuk Sohn", "Yuting Zhang", "Honglak Lee"], "venue": null, "citeRegEx": "Reed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2015}, {"title": "2016), adding Gaussian noise to the discriminator inputs (0.2 standard deviation on images scaled to [\u22121, 1]). We followed Radford et al. (2015) for the architecture of the convolutional layers, and used batch normalisation in all layers except the last in the generator and the first in the discriminator", "author": ["Shi"], "venue": null, "citeRegEx": "Shi,? \\Q2015\\E", "shortCiteRegEx": "Shi", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Having a representation that is well suited to the particular task and data domain can significantly improve the learning success and robustness of the chosen model (Bengio et al., 2013). It has been suggested that learning a disentangled representation of the generative factors in the data can be useful for a large variety of tasks and domains (Bengio et al., 2013; Ridgeway, 2016). A disentangled representation can be defined as one where single latent units are sensitive to changes in single generative factors, while being relatively invariant to changes in other factors (Bengio et al., 2013). For example, a model trained on a dataset of 3D objects might learn independent latent units sensitive to single independent data generative factors, such as object identity, position, scale, lighting or colour, thus acting as an inverse graphics model (Kulkarni et al., 2015). In a disentangled representation, knowledge about one factor can generalise to novel configurations of other factors. According to Lake et al. (2016), disentangled representations could boost the performance of state-of-the-art AI approaches in situations where they still struggle but where humans excel.", "startOffset": 166, "endOffset": 1031}, {"referenceID": 3, "context": "While the original VAE work has been shown to achieve limited disentangling performance on simple datasets, such as FreyFaces or MNIST (Kingma & Welling, 2014), disentangling performance does not scale to more complex datasets (e.g. Aubry et al., 2014; Paysan et al., 2009; Liu et al., 2015), prompting the development of more elaborate semi-supervised VAE-based approaches for learning disentangled factors (e.", "startOffset": 227, "endOffset": 291}, {"referenceID": 3, "context": ", 2015) approaches for disentangled factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 3, "context": ", 2015) approaches for disentangled factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry et al., 2014) and faces (Paysan et al., 2009) using qualitative evaluation. Finally, to help quantify the differences, we develop a new measure of disentanglement and show that \u03b2-VAE significantly outperforms all our baselines on this measure (ICA, PCA, VAE Kingma & Ba (2014), DC-IGN Kulkarni et al.", "startOffset": 103, "endOffset": 413}, {"referenceID": 3, "context": ", 2015) approaches for disentangled factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry et al., 2014) and faces (Paysan et al., 2009) using qualitative evaluation. Finally, to help quantify the differences, we develop a new measure of disentanglement and show that \u03b2-VAE significantly outperforms all our baselines on this measure (ICA, PCA, VAE Kingma & Ba (2014), DC-IGN Kulkarni et al. (2015), and InfoGAN Chen et al.", "startOffset": 103, "endOffset": 444}, {"referenceID": 3, "context": ", 2015) approaches for disentangled factor learning on a number of benchmark datasets, such as CelebA (Liu et al., 2015), chairs (Aubry et al., 2014) and faces (Paysan et al., 2009) using qualitative evaluation. Finally, to help quantify the differences, we develop a new measure of disentanglement and show that \u03b2-VAE significantly outperforms all our baselines on this measure (ICA, PCA, VAE Kingma & Ba (2014), DC-IGN Kulkarni et al. (2015), and InfoGAN Chen et al. (2016)).", "startOffset": 103, "endOffset": 476}, {"referenceID": 3, "context": "1 for architecture details) on a variety of datasets commonly used to evaluate disentangling performance of models: celebA (Liu et al., 2015), chairs (Aubry et al.", "startOffset": 123, "endOffset": 141}], "year": 2017, "abstractText": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce \u03b2-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter \u03b2 that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that \u03b2-VAE with appropriately tuned \u03b2 > 1 qualitatively outperforms VAE (\u03b2 = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, \u03b2-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter \u03b2, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_123"}