{"name": "ICLR_2017_127.pdf", "metadata": {"source": "CRF", "title": "ONLINE BAYESIAN TRANSFER LEARNING FOR SEQUENTIAL DATA MODELING", "authors": ["Priyank Jaini", "Zhitang Chen", "Pablo Carbajal", "Edith Law", "Laura Middleton", "Kayla Regan", "Mike Schaekermann", "George Trimponias", "James Tung", "Pascal Poupart"], "emails": ["pjaini@uwaterloo.ca,", "chenzhitang2@huawei.com,", "pablo@veedata.io,", "edith.law@uwaterloo.ca,", "lmiddlet@uwaterloo.ca,", "kregan@uwaterloo.ca,", "g.trimponias@huawei.com,", "mschaekermann@uwaterloo.ca", "james.tung@uwaterloo.ca", "ppoupart@uwaterloo.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "In several application domains, data instances are produced by a population of individuals that exhibit a variety of different characteristics. For instance, in activity recognition, different individuals might walk or run with different gait patterns. Similarly, in sleep studies, different individuals might exhibit different patterns for the same sleep stages. In telecommunication networks, software applications might generate packet flows between two servers according to different patterns. In such scenarios, it is tempting to treat the population as a homogeneous source of data and to learn a single average model for the population. However, this average model will perform poorly in recognition tasks for individuals that differ significantly from the average. Hence, there is a need for transfer learning techniques that take into account the variations between individuals within a population.\nWe consider the problem of inferring a sequence of hidden states based on a sequence of observations produced by an individual within a population. Our first contribution is an online Bayesian moment matching technique to estimate the parameters of a hidden Markov model (HMM) with observation distributions represented by Gaussian mixture models (GMMs). This approach allows us to learn separate basis models for different individuals based on streaming data. The second contribution is an unsupervised online technique that infers a probability distribution over the basis models that best explain the sequence of observations of a new individual. The classification of hidden states can then be refined in an online fashion based on the individuals that most resemble the new individual. Furthermore, since the basis models are fixed at classification time and we only learn the weight of each model, good classification accuracy can be obtained more quickly as the stream of observations of the new individual are processed. The third contribution of this work is the demonstration of this approach across different real-world applications, which include activity\nrecognition, sleep classification and the prediction of packet flow direction in telecommunication networks.\nThe paper is organized as follows. Section 2 reviews some related work on transfer learning. Section 3 provides some background regarding hidden Markov models Bayesian Moment Matching algorithm Gaussian mixture models. Section 4 describes the proposed online transfer learning technique. Section 5 illustrates the transfer learning technique in three real-world tasks: activity recognition, sleep stage classification and flow direction prediction. Finally, Section 6 concludes the paper and discusses directions for future work."}, {"heading": "2 RELATED WORK", "text": "There is a large literature on transfer learning (Pan & Yang, 2010; Taylor & Stone, 2009; Shao et al., 2015; Cook et al., 2013). Depending on the problem, the input features, the output labels or the distribution over the features and the labels may be different for the source and target domains. In this work, we assume that the same input features are measured and the same output labels are inferred in the source and target domains. The main problem that we consider is subject variability within a population of individuals, which means that different individuals exhibit different distributions over the features and the labels. The problem of subject variability has been studied in several papers. Chieu et al. (2006) describe how to augment conditional random fields with a subject hidden variable to obtain a mixture of conditional random fields that can naturally infer a distribution over the closest subjects in a training population when inferring the activities of a new individual based on physiological data. Rashidi & Cook (2009) proposed a data mining technique with a similarity measure to facilitate the transfer of activity recognition across different people. Chattopadhyay et al. (2011) describe a similarity measure with an intrinsic manifold that preserve the topology of surface electromyography (SEMG) while mitigating distributional differences among individuals. Zhao et al. (2011) proposed a transfer learning technique that starts by training a decision tree to recognize the activities of a user based on smartphone accelerometry. The decision tree is gradually adjusted to a new user by a clustering technique that successively re-weights the training data based on the unlabeled data of the new individual. These approaches mitigate subject variability by various offline transfer learning techniques. In contrast, we propose an online transfer learning technique since the applications that we consider exhibit sequences of observations that arrive in a streaming fashion and therefore require an online technique that can infer the hidden state of each observation as it arrives.\nIn the next section, we describe an online transfer learning technique for hidden Markov models with Gaussian mixture models. The approach learns different transition and emission models for each individual in the training population. Those models are then treated as basis models to speed up the online learning process for new individuals. More specifically, a weighted combination of the basis models is learned for each new individual. This idea is related to boosting techniques for transfer learning (Dai et al., 2007; Yao & Doretto, 2010; Al-Stouhi & Reddy, 2011) that estimate a weighted combination of base classifiers. However, note that we focus on sequence modeling problems where the classes of consecutive data points are correlated while transfer learning by boosting assumes that the data points are identically and independently distributed."}, {"heading": "3 BACKGROUND", "text": "In this section, we give a brief overview of hidden Markov models (HMMs) and review the Bayesian moment matching (BMM) algorithm in detail with an example. We will use both HMMs and BMM subsequently in our transfer learning algorithm described in Section 4."}, {"heading": "3.1 HIDDEN MARKOV MODELS", "text": "In a hidden Markov model (HMM), each observation Xt is associated with a hidden state Yt. The Markov property states that the current state depends only on the previous state. HMMs have been widely used in domains involving sequential data like speech recognition, activity recognition, natural language processing etc. An HMM is represented by two distributions\n\u2022 Transition distribution: The transition distribution models the change in the value of the hidden state over time. The distribution over the current state Yt given that the previous state is Yt\u22121 = j is denoted by \u03b8j = Pr(Yt|Yt\u22121 = j) where \u03b8j = {\u03b81j , ..., \u03b8Nj}, N is the total number of states and \u03b8ij = Pr(Yt = i|Yt\u22121 = j).\n\u2022 Emission distribution: The emission distribution models the effect of the hidden state on the observation Xt at any given time t and is given by Pr(Xt|Yt). In this work, we model the emission distribution as a mixture of Gaussians with M components, i.e., Pr(Xt|Yt = j) = \u2211M i=1 wiN ( Xt;\u00b5ji ,\u03a3 j i\n) In this paper, we will first estimate the parameters of the transition and emission distributions by Bayesian learning from a set of source domains (individuals). Subsequently, we will use these distributions as basis functions when estimating the transition and emission distributions of a target domain in which we wish to predict the hidden state for each observation. Parameter learning of an HMM using Bayesian learning is done by calculating the posterior over the parameters given a prior distribution. Pr ( \u0398,\u03a6, Yt = j|Xt, Yt\u22121 = i ) \u221d Emission distribution\ufe37 \ufe38\ufe38 \ufe37 Pr(Xt|Yt = j) Transition Probability\ufe37 \ufe38\ufe38 \ufe37 Pr(Yt = j|Yt\u22121 = i) Prior for t\u2212 1\ufe37 \ufe38\ufe38 \ufe37 Pr(\u0398,\u03a6, Yt\u22121 = i|X1:t\u22121)\n\u2200j \u2208 {1, 2, ..., N}where \u0398 and \u03a6 parametrize the transition and emission distributions respectively."}, {"heading": "3.2 BAYESIAN MOMENT MATCHING ALGORITHM", "text": "The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example.\nNote that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al. (2011) in the context of topic modeling. BMM was also shown to work better than other online techniques in several papers Rashwan et al. (2016); Hsu & Poupart (2016); Jaini et al. (2016). This is due to the fact that BMM is naturally online and therefore does not require minibatches. In contrast, in order to run in an online fashion Variational Bayes requires mini-batches and a decreasing learning rate, however the size of the mini-batches and the decay procedure for the learning rate require some fine tuning. In general, the use of mini-batches always leads to some information loss since data in previous mini-batches is not accessible. BMM does not suffer from this type of information loss and there is no batch size nor learning rate to fine tune. Hence, we will adapt BMM to transfer learning in this work.\nLet X1:n be a set of d-dimensional i.i.d observations following Pr(X|\u0398) = \u2211M i=1 wiN ( x;\u00b5i,\u039b \u22121) where \u0398 = {(w1,\u00b51,\u039b\u221211 ), (w2,\u00b52,\u039b \u22121 2 ), ...(wM , \u00b5M ,\u039b \u22121 M )} and M is known.\nWe choose the prior as a product of a Dirichlet distribution over the weights w and M NormalWishart distributions corresponding to the parameters (\u00b5,\u039b\u22121) of each Gaussian component. Such a prior forms a conjugate probability pair of the likelihood and is hence desirable. Concretely, P0(\u0398) = Dir(w|\u03b1) \u220fM i=1NW(\u00b5i,\u039bi|\u03b4i, \u03bai,Wi, \u03bdi) where w = (w1, w2, ..., wM ), \u03b1 = (\u03b11, \u03b12, ..., \u03b1M ), W is a symmetric positive definite matrix, \u03ba > 0 is real, \u03b4 \u2208 Rd and \u03bd > d\u2212 1 is real. The posterior P1(\u0398|X1) after observing the first data point X1 is given by\nP1(\u0398|X1) \u221d P0(\u0398) Pr(X1|\u0398)\n\u221d Dir(w|\u03b1) M\u220f i=1 NW(\u00b5i,\u039bi|\u03b4i, \u03bai,Wi, \u03bdi) M\u2211 j=1 wjN ( X1;\u00b5j ,\u039b \u22121 j ) Since a Normal-Wishart distribution is a conjugate prior for a Normal distribution with unknown mean and precision matrix, NW(\u00b5i,\u039bi|\u03b4i, \u03bai,Wi, \u03bdi)N ( X1;\u00b5i,\u039b \u22121 i ) =\ncNW(\u00b5i,\u039bi|\u03b4\u0302i, \u03ba\u0302i, W\u0302i, \u03bd\u0302i) where c is some constant. Similarly,wjDir(w|\u03b11, \u03b12, .., \u03b1j , .., \u03b1M ) = kDir(w1, w2, ..., wM |\u03b11, \u03b12, ..\u03b1\u0302j .., \u03b1M ) where k is some constant. Therefore, P1(\u0398|X1) is\nP1(\u0398|X1) = 1\nZ M\u2211 j=1 ( cjDir(w|\u03b1\u0302j)NW(\u00b5j ,\u039bj |\u03b4\u0302j , \u03ba\u0302j , W\u0302j , \u03bd\u0302j) M\u220f i6=j NW(\u00b5i,\u039bi|\u03b4i, \u03bai,Wi, \u03bdi) )\nwhere \u03b1\u0302j = (\u03b11, \u03b12, .., \u03b1\u0302j , .., \u03b1M ) and Z is the normalization constant. The equation above suggests that the posterior is a mixture of product of distributions where each product component in the summation has the same form as that of the family of distributions of the prior P0(\u0398). It is evident that the terms in the posterior grow by a factor of M for each iteration, which is problematic. The Bayesian moment matching algorithm approximates this mixture P1(\u0398) with a single product of Dirichlet and Normal-Wishart distributions P\u03031(\u0398) by matching all the sufficient moments of P1 with P\u03031 which belongs to the same family of distributions as the prior:\nP\u03031(\u0398) = Dir(w|\u03b11) M\u220f i=1 NW(\u00b5i,\u039bi|\u03b4 1 i , \u03ba 1 i ,W 1 i , \u03bd 1 i )\nWe evaluate the parameters \u03b11, \u03b41i , \u03ba 1 i ,W 1 i , \u03bd 1 i \u2200i \u2208 {1, 2, ..,M} by matching a set of sufficient moments of P\u03031(\u0398) with P1(\u0398). The set of sufficient moments in this case is S = {\u00b5j ,\u00b5j\u00b5Tj ,\u039bj ,\u039b2jkm , wj , w 2 j} \u2200j \u2208 1, 2, ...,M where \u039b2jkm is the (k,m)\nth element of the matrix \u039bj . The expressions for sufficient moments are given by E[g] = \u222b \u0398 gP1(\u0398)d(\u0398). The parameters of P\u03031 can be computed from the following set of equations\nE[wi] = \u03b1i\u2211 j \u03b1j ; E[w2i ] = (\u03b1i)(\u03b1i + 1)(\u2211 j \u03b1j )( 1 + \u2211 j \u03b1j ) E[\u039b] = \u03bdW; V ar(\u039bij) = \u03bd(W2ij + WiiWjj)\nE[\u00b5] = \u03b4; E[(\u00b5\u2212 \u03b4)(\u00b5\u2212 \u03b4)T ] = \u03ba+ 1 \u03ba(\u03bd \u2212 d\u2212 1) W\u22121\nUsing this set of equations, the exact posterior P1(\u0398) can be approximated with P\u03031(\u0398). This posterior will then be the prior for the next iteration and we keep following the steps above iteratively to finally have a distribution P\u0303n(\u0398) after observing a stream of data X1:n. The estimate is \u0398\u0302 = E[P\u0303n(\u0398)]. The exact calculations for the Bayesian Moment Matching algorithm are given in appendix A."}, {"heading": "4 TRANSFER LEARNING USING BMM", "text": "In this section, we first motivate the need for an online transfer learning algorithm for sequential data modeling and then explain in detail the different steps of the algorithm. The complete algorithm is given in Alg. (1)."}, {"heading": "4.1 MOTIVATION", "text": "Several applications produce data instances from a population of individuals that exhibit a variety of different traits. For example, for the task of activity recognition, different individuals will have different gait patterns despite the fact that they are performing the same activity (e.g., walking, running, standing, etc.). Therefore, it is problematic to make predictions in such domains by considering the population to be homogeneous; however, every population will have individuals resembling each other in some characteristics. This suggests that we can use individuals in a population to make predictions about similar individuals by identifying those individuals who closely resemble each other. However, identifying individuals with similar traits is not straightforward. Alternatively, weights can be assigned to each individual in a population based on a target individual (individual on whom predictions are to be made). All those individuals who resemble closely the target individual will receive higher weights than those with dissimilar traits. Subsequently, predictions about the behavior of the target individual will be based mostly on the observed behavior of the similar individuals.\nOur transfer learning algorithm addresses precisely these issues. It has three main steps - first, it learns a model (transition and emission distributions) for each source domain (or individual in a population) that best explains the observations of that source domain. Next, given a target domain (or target individual), it identifies those individuals that closely resemble the target individual by estimating a basis weight associated to each source domain. A higher weight for a source domain implies that the corresponding individual resembles more closely the target individual. Finally, it predicts the hidden states for each observation in the target domain by using the models learned in the source domain and the basis weights that are given to each transition and emission distribution of the source domains. We now explain each step of the algorithm in detail below."}, {"heading": "4.2 SOURCE DOMAIN - TRAINING", "text": "The first step is to learn a model for each source domain in the training data. Suppose that we have labeled sequence data for K different source domains. Let\nY kt = hidden state label at time step t for source domain k\nXkt = feature vector at time step t for source domain k\nLet the sequence of observations be given by Xk1:T = {Xk1 , Xk2 , ..., XkT } and the hidden states be {Y k1 , Y k2 , ..., Y kT } where Y kt \u2208 {1, 2, .., N} \u2200t. Furthermore, let us define\n\u0398kij = Pr(Y k t = i|Y kt\u22121 = j) i.e. the transition probability from state i to state j\nWe denote the transition matrix for the kth source domain with \u0398k. Let the emission distribution be modeled by a mixture of Gaussian with M components. This implies\nPr(Xkt |Y kt = j) = M\u2211 m=1 wkjmN (X k t |\u00b5kjm ,\u03a3 k jm) \u2200j \u2208 {1, 2, .., N}\nOur aim is to learn the parameters characterizing the transition and the emission distribution for each source domain. More precisely, if\n\u03a6k = {\u03c6k1 , \u03c6k2 , ..., \u03c6kN} where \u03c6ki = {(wki1 ,\u00b5 k i1 ,\u03a3 k i1), ...., (w k iM ,\u00b5 k iM ,\u03a3 k iM )}\nthen we want to learn the parameters \u0398k for the transition distribution and \u03a6k for the emission distribution for each source domain k \u2208 {1, 2, ...,K}. Since, we use a hidden Markov model, the update equation at each time step for a source domain k is\nPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) \u221d Emission distribution\ufe37 \ufe38\ufe38 \ufe37 Pr(Xkt |Y kt = j) Transition Probability\ufe37 \ufe38\ufe38 \ufe37 Pr(Y kt = j|Y kt\u22121 = i) Prior for t\u2212 1\ufe37 \ufe38\ufe38 \ufe37 Pr(\u0398k,\u03a6k, Y kt\u22121 = i|Xk1:t\u22121)\n\u2200j \u2208 {1, 2, ..., N} (1)\nThe prior over (\u0398k,\u03a6k) is given by\nPr(\u0398k,\u03a6k) = [ N\u220f i=1 Dir(\u03b8ki |\u03b1ki ) ][ N\u220f j=1 Dir(wkj ;\u03b2 k j ) M\u220f u=1 NW(\u00b5kju ,\u039b k ju ; \u03b4 k ju , \u03ba k ju ,W k ju , v k ju) ] (2)\nAfter substituting the relevant terms in Eq (1), we get\nPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) \u221d M\u2211 m=1 wkjmN (X k t |\u00b5kjm ,\u03a3 k jm)\u03b8 k ji [ N\u220f i=1 Dir(\u03b8ki |\u03b1ki ) ]\n[ N\u220f j=1 Dir(wkj ;\u03b2 k j ) M\u220f u=1 NW(\u00b5kju ,\u039b k ju ; \u03b4 k ju , \u03ba k ju ,W k ju , v k ju) ] \u2200j \u2208 {1, 2, ..., N} (3)\nFurther, \u039bkju = (\u03a3 k ju) \u22121. The prior in Eq (2) can be understood as having the following components\n\u2022 Transition Distribution : Each column of the N \u00d7N transition matrix specifies the probability of making a transition from that column index to another state given by the row index. We define a Dirichlet distribution as a prior over each column of the transition matrix. Hence, \u220fN i=1Dir(\u03b8 k i |\u03b1ki ) is the prior over \u0398k.\n\u2022 Emission Distribution : Dir(wkj ;\u03b2 k j ) \u220fM u=1NW(\u00b5kju ,\u039b k ju ; \u03b4 k ju , \u03ba k ju ,Wkju , v k ju\n) defines a prior over a mixture of Gaussians for hidden state j with M components where the Dirichlet distribution is the prior over the mixture weights and the Normal-Wishart distribution is the prior over the mean and precision matrix of the mixture components. We take a product over j to obtain a prior over all emission distributions.\nThe posterior distribution (Eq (3)) after each observation is a mixture of products of distributions where each component has the same form as the prior distribution since Pr(Xkt |Y kt = j) is a mixture of Gaussians. Therefore, the number of terms in the posterior increases exponentially if we perform exact Bayesian learning. To circumvent this, we use BMM for Gaussian Mixture Models as described in (Jaini et al., 2016; Jaini & Poupart, 2016)3. The complete calculations for learning in the source domain are given in appendix B.\nThe main computation in the learning and updating routine is the calculation of the sufficient set of moments using the Bayesian posterior given in Eq. (9) in appendix B. Let M be the number of components in the mixture model for emission distributions, N the number of hidden states and d the number of features in the data. The computational complexity for updating the parameters in the source domain learning step for each iteration is O(M2N2) for each scalar parameters and is O(M2N2d3) for the parameters of the distribution over the precision matrix because that involves a matrix multiplication step."}, {"heading": "4.3 TARGET DOMAIN - PREDICTION", "text": "The goal is to predict the hidden states for a target individual (or domain) as we observe a sequence of observations. In the previous step, we learned the transition and emission distributions individually for K different sources. These sources can be thought of as individuals in a population. The transition and emission distributions learned from the individual sources form a basis for the transition and emission distributions of the target domain. Specifically, let the transition distribution for the kth source be denoted by g(\u0398k) and emission distribution be denoted by f(\u03a6kj ) for a certain hidden state j. Then, the transition and emission distributions for the target domain is a weighted combination given by\nPr(Yt = j|Yt\u22121 = i) = K\u2211 m=1 \u03bbm Pr(Y m t = j|Y mt\u22121 = i) = K\u2211 m=1 \u03bbmg(\u0398 m ji) (4)\nPr(Xt|Yt = j) = K\u2211 k=1 \u03c0k Pr(X k t |Y kt = j) = K\u2211 k=1 \u03c0kf(\u03a6 k j ) (5)\nWe first need to compute the basis weights \u03bb = (\u03bb1, \u03bb2, ...., \u03bbK) and \u03c0 = (\u03c01, \u03c02, ...., \u03c0K). We estimate (\u03bb,\u03c0) in an unsupervised manner using BMM. We define a Dirichlet prior over \u03bb and \u03c0, i.e. Pr(\u03bb,\u03c0) = Dir(\u03bb;\u03b3)Dir(\u03c0;\u03bd). The posterior after observing a new data point is\nPr ( \u03bb,\u03c0, Yt = j|Xt ) \u221d Pr(Xt|Yt = j) N\u2211 i=1 Pr(Yt = j|Yt\u22121 = i) Pr(\u03bb,\u03c0, Yt\u22121 = i) (6)\n\u221d K\u2211 k=1 \u03c0kf(\u03a6 k j ) N\u2211 i=1 K\u2211 m=1 \u03bbmg(\u0398 m ji)Dir(\u03bb;\u03b3)Dir(\u03c0;\u03bd) (7)\n\u221d K\u2211 k,m N\u2211 i=1 C(i, j, k,m) Dir(\u03c0; \u03bd\u0302)Dir(\u03bb; \u03b3\u0302) (8)\nwhere f(\u03a6kj )g(\u0398 m ji) are known from the source domains, \u03c0kDir(\u03c0;\u03bd) = akDir(\u03c0; \u03bd\u0302), \u03bbmDir(\u03bb;\u03b3) = bmDir(\u03bb; \u03b3\u0302) and C(i, j, k,m) = akbmf(\u03a6kj )g(\u0398 m ji). The exact calculations are given in Appendix C. We approximate the posterior in Eq (8) by projecting it onto a tractable family\nof distributions with the same set of sufficient moments as the posterior using the Bayesian Moment Matching approach. Finally, the estimate of (\u03bb,\u03c0) is the expected value of the final posterior. This completes the learning stage.\nThe transition and emission distributions for the target domain are the weighted combination of transition and emission distributions learned in the source domain respectively. The advantage of this linear combination is to account for heterogeneity in the data. The learning step in the target domain will ensure that only those source domains that resemble closely the target domain are given higher weights. This helps to bias the predictions according to the closest basis models when the population is not homogeneous.\nPredictions can be made in two different manners\n\u2022 Online - initialize the prior over \u03bb and \u03c0 to be uniform. As each new data point is observed in a sequence, a prediction is made based on the mean of the current posterior over \u03bb and \u03c0 and subsequently the posterior is updated based on Eq (8).\n\u2022 Offline - compute the posterior of \u03bb and \u03c0 based on Eq (8) by using the entire sequence of observations of the target individual. Once, the posterior is computed, predict the hidden states for each observation in the sequence based on the mean estimates of the posterior.\nIn Fig. 1, we show the schematic for the proposed online transfer learning algorithm. The figure shows the learning phase for each source domain where the emission and transition distributions are learned using Bayesian Moment Matching technique. After learning in the source domain, we learn the weights of the basis models in the target domain for each new observation and make predictions in an online manner.\nAlgorithm (1) gives the complete algorithm for transfer learning by Bayesian Moment Matching.\nThe target domain step involves two routines :\n\u2022 Update step - In this step, the hyper-parameters (\u03b3,\u03bd) over the weights (\u03bb,\u03c0) are updated. The main computation in this step is the calculation of the set of sufficient moments from the updated Bayesian posterior given in Eq. (8). Hence, the computational complexity of the update step in the target domain for each observation is O(K2N2) where K is the number of source domains and N is the number of hidden states.\n\u2022 Prediction step - In the prediction step, a hidden label is assigned to the observation based on the model obtained from the update step. The main computation is calculation of the likelihood of each hidden state for the observation. The computational complexity of the prediction step is hence O(MKN) where M is the number of components in the mixture model, K is the total number of source domains and N is the number of hidden states.\nAlgorithm 1 Online Transfer Learning by Bayesian Moment Matching 1: Input (Learning): labeled sequence data from multiple domains (individuals) 2: Input (Prediction): unlabeled sequence data from individuals 3: Output: labels for hidden states\nSource Domain - learning transition and emission distribution 4: Input: labeled sequence data from K domains 5: specify # of hidden states : nClass 6: specify # of components in GMM : nComponents 7: procedure LEARNSOURCEHMM(data, nClass, nComponents) 8: for k = 1 : K do 9: Let f(\u0398,\u03a6) be a family of probability distributions with parameters \u03b3 10: Initialize a prior Pk0(\u0398,\u03a6) from f over transition and emission parameters respectively 11: for n = 1 : Dk do . Dk : size of data for kth source domain 12: Compute Pn(\u0398,\u03a6) from Pn\u22121(\u0398,\u03a6) using Eq. 3 13: Using BMM approximate Pn with P\u0303n(\u0398,\u03a6) = f(\u0398,\u03a6|\u03b3) 14: Return : \u0398\u0302 = E\u0398[P\u0303n(\u0398,\u03a6)] 15: Return : \u03a6\u0302 = E\u03a6[P\u0303n(\u0398,\u03a6)] 16: Return : emission and transition distributions for each source Target Domain - learning basis weights for each source domain & prediction 17: Input: unlabeled sequence data 18: procedure PREDICTTARGETDOMAIN(data, sourceDistributions) 19: Let g(\u03bb,\u03c0) = Dir(\u03bb;\u03b3)Dir(\u03c0;\u03bd) be a family of probability distributions 20: Initialize a prior P0(\u03bb,\u03c0) from g with equal weights to each source distribution 21: for n = 1 : D do . D : size of data for target domain 22: Compute Pn(\u0398,\u03a6) from Pn\u22121(\u0398,\u03a6) using Eq. 8 23: Using BMM approximate Pn with P\u0303n(\u03bb,\u03c0) = g(\u03bb,\u03c0) 24: Predict : Y\u0302n = argmaxjPr ( \u03bb,\u03c0, Yn = j|Xn ) using Eq (8)\n25: Return : \u03bb\u0302 = E\u03bb[P\u0303n(\u03bb,\u03c0)] 26: Return : \u03c0\u0302 = E\u03c0[P\u0303n(\u03bb,\u03c0)] 27: Return : prediction Y\u0302n"}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "This section describes experiments on three tasks from different domains - activity recognition, sleep cycle prediction among healthy individuals and patients suffering from Parkinson\u2019s disease and packet flow prediction in telecommunication networks.\nEXPERIMENTAL SETUP\nFor each task, we compare our online transfer learning algorithm to EM (trained by maximum likelihood) and a baseline algorithm (that uses Bayesian moment matching) that both learn a single HMM with mixtures of Gaussians as emissions by treating the population as homogeneous. Furthermore, we conduct experiments using recurrent neural networks (RNNs) due to their popularity in sequence learning.\nThe baseline algorithm uses Bayesian Moment Matching to learn the parameters of the HMM. Concretely, we have data collected from several individuals (or sources) in a population for each task. For transfer learning, we train an HMM with mixture of Gaussian emission distributions for each source (or individual) except the target individual. For the target individual, we estimate a posterior over the basis weights in an online and unsupervised fashion and make online predictions about the hidden states. We compare the performance of our transfer learning algorithm against the EM and baseline algorithms that treat the population as homogeneous, i.e., we train an HMM by combining the data from all the sources except the target individual. Then, using this model, we make online predictions about the hidden states of the target individual.\nWe report the results based on leave-one-out cross validation where the data of a different individual is left out in each round. For each task, we treat every individual as a target individual once. For a fair comparison, the HMM model learned for both the baseline algorithm and the EM algorithm has the same number of components as the HMM model learned by the online transfer learning algorithm.\nRegarding RNNs, we used architectures with as many input nodes as the number of attributes, one hidden layer consisting of long short term memory (LSTM) units (Hochreiter & Schmidhuber, 1997) and one softmax output layer with as many nodes as the number of classes. We use the categorical cross-entropy loss as the cost function. We select LSTM units instead of sigmoid or hyperbolic tangent units due to their popularity and success in sequence learning (Sutskever et al., 2014).\nWe perform grid search to select the best hyper-parameters for each setting. For the training method, we either use Nesterov\u2019s accelerated gradient descent (Nesterov, 1983; Sutskever et al., 2013) with learning rates [0.001,0.01,0.1,0.2] and momentum values [0,0.2,0.4,0.6,0.8,0.9], or rmsprop (Tieleman & Hinton, 2012) having \u03b5 = 10\u22124 and decay factor 0.9 (standard values) with learning rates [0.00005,0.0001,0.0002,0.001] and momentum values [0,0.2,0.4,0.6,0.8,0.9]. The weight decay takes values from [0.001,0.01,0.1], whereas the number of LSTM units in the hidden layer takes the possible values [2,4,6,9,12].\nWe experimented with various architectures before we ended up with the aforementioned values; in particular, architectures with a single hidden layer consistently performed better than multiple layers, possibly because our datasets are not very complex. We train the network by backpropagation through time (bptt) truncated to 20 time steps (Williams & Peng, 1990). The RNNs are trained for a maximum number of 150 epochs, or until convergence is reached. Our implementation is based on the Theano library (Theano Development Team, 2016) in Python.\nFor each task, we run experiments 10 times with each individual taken as target and the rest acting as source domains for training. We report the average percentage accuracy and use the Wilcoxon signed rank test (Wilcoxon, 1950) to compute a p-value and report statistical significance when the p-value is less than 0.05. In the following sections, we discuss the results for each task in detail.\nACTIVITY RECOGNITION\nAs part of an on-going study to promote physical activity, we collected smartphone data with 19 participants and tested our transfer learning algorithm to recognize 5 different kinds of activities: sitting, standing, walking, running and in-a-moving-vehicle. While APIs already exist to automatically recognize walking, running and in-a-moving-vehicle by Android and Apple smartphones, sitting and standing are not available in the standard APIs. Furthermore, our long term goal is to obtain robust recognition algorithms for older adults and individuals with perturbed gait (e.g., due to a stroke). Labeled data was obtained by instructing the 19 participants to walk at varying speeds for 4 min, run for 2 min, stand for 2 min, sit for 2 min and ride a moving vehicle to a destination of their choice. The data collected was segmented in epocs of 1 second where 48 features (means and standard deviations of the 3D accelerometry in each epoch) were computed by the smartphone.\nThe online transfer learning algorithm learned an HMM over 18 individuals which acted as basis models for prediction on the 19th individual. In this manner, we ran experiments for each individual 10 times to get a statistical measure of the results.\nTable (1) compares the average percentage accuracy of prediction for activity recognition with 19 different individuals. It demonstrates that the transfer learning algorithm performed better than the baseline on 15 individuals and in other cases its accuracy was close to the baseline. Furthermore, it is also worth noting that in most cases, the confusion in the algorithm\u2019s prediction was between the following pairs of classes: In a Moving Vehicle\u2014Standing and In a Moving Vehicle\u2014Sitting. This is expected because in most cases the person was either standing/sitting in a bus or sitting in a car. Table (1) also demonstrates the superior performance of online transfer learning algorithm as compared to the EM algorithm. Finally, note the poor performance of RNNs despite the fact that we fine-tuned the architecture to get the best results. RNNs are in theory very expressive. However, they are also notoriously difficult to train and fine-tune due to their non-convexity and vanishing/exploding gradient issues that arise in backpropagation through time. Indeed, in several cases they even underperform all other methods.\nSLEEP STAGE CLASSIFICATION\nSleep disruption can lead to various health issues. Understanding and analyzing sleep patterns, therefore, has great potential to significantly improve the quality of life for both patients and healthy individuals. In both clinical and research settings, the standard tool for quantifying sleep architecture and physiology is polysomnography (PSG), which is the measurement of electroencephalography (EEG), electrooculography (EOG), electromyography (EMG), electrocardiography (ECG), and respiratory function of an individual during sleep. The analysis of sleep architecture is of relevance for the diagnosis of several neurological disorders, e.g., Parkinson\u2019s disease (Peeraully et al., 2012), because neurological anomalies often also reflect in variations of a patient\u2019s sleep patterns.\nTypically, PSG data is divided into 30-second epochs and classified into 5 stages of sleep \u2014 wake (W), rapid eye movement sleep (REM) or one of 3 non-REM sleep stages (N1, N2, and N3) \u2014 based on the visual identification of specific signal features on the EEG, EOG, and EMG channels. Epochs that cannot be distinctly sorted into one of the 5 stages are labeled as Unknown. While it is a valuable clinical and research tool, visual classification of EEG data remains time consuming,\nrequiring up to 2 hours for a highly trained technologist to classify all the epochs within a typical 7- hour PSG recording. Beyond that, inter-scorer agreement rates remain low around 80 (Rosenberg & Van Hout, 2013). High annotation costs and low inter-scorer agreement rates have motivated efforts to develop fully automated approaches for sleep stage classification (Anderer et al., 2005; Jensen et al., 2010; Mal, 2013; Punjabi et al., 2015). However, many of these methods result in generic cross-patient classifiers that fail to reach levels of accuracy and reliability high enough to be adopted in real-world medical settings.\nThe polysomnograms (PSGs) we used for our evaluation were obtained at a clinical neurophysiology laboratory in Toronto (name anonymized) according to the American Academy of Sleep Medicine guidelines using a Grael HD PSG amplifier (Compumedics, Victoria, Australia). We selected recordings from 142 patients obtained between 2009 and 2015. Out of these 142 recordings, 91 were from healthy subjects and 51 were from patients with Parkinson\u2019s disease.\nEach recording was manually scored by a single registered PSG technologist. Recordings were first segmented into fixed-sized windows of 30 second epochs. To reduce complexity and processing time in the feature extraction and manual labeling step, we only retained EEG channel C4-A1, which is deemed especially important for sleep stage classification (Sil, 2007). Channel selection and segmentation resulted in a ground truth data set where each instance was represented by a singlechannel time series of 7680 floating point numbers corresponding to 30 seconds of C4-A1, sampled at 256 Hz. A vector of 26 scalar features was extracted from each epoch. Bao et al. (2011) and Motamedi-Fakhr et al. (2014) give a detailed listing and explanation of all 26 features.\nThe online transfer learning algorithm learned an HMM over 50 individuals chosen at random which acted as basis models for prediction on the target individual. We did not use all 140 individuals for the basis models because it resulted in sources getting sparse weights diluting the effect of heterogeneity. We completed the experiments for each individual 10 times in this manner to get a statistical measure of the results.\nFig. (2) shows the scatter plots of accuracy for our online transfer learning technique and the three baseline algorithms - BMM, EM (maximum likelihood) and RNNs - which treat the data as homogeneous for the sleep stage classification dataset. For each plot, a point above the dotted line indicates higher accuracy of online transfer learning technique as compared to the corresponding baseline algorithm for the target patient. The plots show consistent superior performance of our online transfer learning technique as compared to both baseline algorithms - BMM and EM for all target patients. The online transfer learning technique also performs better on a majority of patients (102 out of 142) as compared to an optimized RNN.\nAll the results are statistically significant under the Wilcoxon signed rank test with p-value < 0.05. More detailed results for comparison of the online transfer learning technique with the three baseline algorithms is given in appendix (D).\nFLOW DIRECTION PREDICTION\nAccurate prediction of future traffic plays an important role in proactive network control. Proactive network control means that if we know the future traffic (including directions and traffic volume), then we have more time to find a better policy for the network routing, priority scheduling as well as rate control in order to maximize network throughput while minimizing transmission delay, packet loss rate, etc.\nBetter understanding the behavior of TCP connections in certain applications can provide important input to automatic application type detection, especially in those scenarios where network traffic is encrypted and DPI (Deep Packet Inspection) is nearly impossible. Different applications can be distinguished by the distinct behavior of their TCP connections, which are well described by the corresponding HMMs.\nWe performed our experiments with a publicly available dataset of real traffic from academic buildings. The dataset consists of packet traces with TCP flows. For our experiments, we only consider three packet sizes and flow size as the features. The hidden labels are the source of generation of the packet, i.e., Server or Client. We divided the dataset into 9 domains with each domain consisting of a number of observation sequences. For the online transfer learning algorithm, we learned an HMM for each of 8 sources that acted as basis models for prediction on the 9th source. We compared the performance of the online transfer learning algorithm with EM and the baseline algorithm which treat the data as homogeneous. Table 2 reports the average (of 10 experimental runs) percentage accuracy for each source. The online transfer learning algorithm performs better than both the baseline and the EM algorithm. The results are statistically significant under the Wilcoxon signed rank test with p-value < 0.05. Furthermore, we compare our method to RNNs. It turns out that for the task of traffic direction prediction, RNNs can actually perform well, unlike for instance the activity recognition dataset. The better performance this time may be due to the simpler structure of the data that consists of a single attribute and a binary class. This is in sharp contrast to the activity recognition dataset whose instances contain 48 attributes and can belong to 5 classes, and is thus harder to train."}, {"heading": "6 CONCLUSION", "text": "In many applications, data is produced by a population of individuals that exhibit a certain degree of variability. Traditionally, machine learning techniques ignore this variability and train a single model under the assumption that the population is homogeneous. While several offline transfer learning techniques have already been proposed to account for population heterogeneity, this work describes the first online transfer learning technique (to our knowledge) that incrementally determines which source models best explain a streaming sequence of observations while predicting the corresponding hidden states. We achieved this by adapting the online Bayesian moment matching algorithm originally developed for mixture models to hidden Markov models. Experimental results confirm\nthe effectiveness of the approach in three real-world applications: activity recognition, sleep stage recognition and flow direction prediction.\nIn the future, this work could be extended in several directions. Since it is not always clear how many basis models should be used and that the observation sequences of target individuals can necessarily be explained by a weighted combination of basis models, it would be interesting to explore techniques that can automatically determine a good number of basis models and that can generate new basis models on the fly when existing ones are insufficient. Furthermore, since recurrent neural networks (RNNs) have been shown to outperform HMMs with GMM emission distributions in some applications such as speech recognition (Graves et al., 2013), it would be interesting to generalize our online transfer learning technique to RNNs."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was funded by grants from the Network for Aging Research at the University of Waterloo, the PROPEL Centre for Population Health Impact at the University of Waterloo, Huawei Noah\u2019s Ark Laboratory in Hong Kong, CIHR (CPG-140200) and NSERC (CHRP 478468-15)."}, {"heading": "A NORMAL-WISHART AND DIRICHLET DISTRIBUTION", "text": "DIRICHLET DISTRIBUTION\nThe Dirichlet distribution is a family of multivariate continuous probability distributions over the interval [0,1]. It is the conjugate prior probability distribution for the multinomial distribution. We next show how the combining happens for a Dirichlet as has been highlighted in (3).\nwmDir(w,\u03b1) = \u0393( \u2211 i \u03b1i)\u220f\ni \u0393(\u03b1i) wm \u220f i w\u03b1ii\nwmDir(w,\u03b1) = \u0393( \u2211 i \u03b1i)\u220f\ni \u0393(\u03b1i) w\u03b1m+1m \u220f i 6=m w\u03b1ii\nwmDir(w,\u03b1) = \u03b1m\u2211 i \u03b1i Dir(w; \u03b1\u0302)\nwhere\n\u03b1\u0302i = { \u03b1i if i 6= m \u03b1i + 1 if i = m\nNORMAL WISHART PRIOR\nThe Normal-Wishart distribution is a conjugate prior of a multivariate Gaussian distribution with unknown mean and precision matrix (Degroot, 1970). It is the combination of a Wishart distribution over the precision matrix and Gaussian distribution over the mean given the precision matrix.\nLet \u00b5 be a d-dimensional vector and \u039b be a symmetric positive definite d\u00d7dmatrix of random variables respectively. Then, a Normal-Wishart distribution over (\u00b5,\u039b) given parameters (\u00b50, \u03ba,W, \u03bd) is such that \u00b5 \u223c Nd ( \u00b5;\u00b50, (\u03ba\u039b) \u22121)where \u03ba > 0 is real, \u00b50 \u2208 Rd and \u039b has a Wishart distribution given as \u039b \u223c W(\u039b; W, \u03bd) where W \u2208 Rd\u00d7d is a positive definite matrix and \u03bd > d \u2212 1 is real. The marginal distribution of \u00b5 is a multivariate t-distribution i.e \u00b5|\u039b \u223c t\u03bd\u2212d+1 ( \u00b5;\u00b50, W \u03ba(\u03bd\u2212d+1) ) . A Normal-Wishart distribution multiplies with a Gaussian with the same mean and precision matrix to give a new Normal-Wishart distribution.\nNd ( y;\u00b5, (\u03ba\u039b)\u22121 ) NW(\u00b5,\u039b;\u00b50, \u03ba,W, \u03bd) = cNW(\u00b5,\u039b;\u00b5\u22170, \u03ba\u2217,W\u2217, \u03bd\u2217)\nwhere\n\u00b5\u22170 = \u03ba\u00b50 + y \u03ba+ 1 \u03ba\u2217 = 1 + \u03ba \u03bd\u2217 = \u03bd + 1\nW\u2217 = W + \u03ba\n\u03ba+ 1 (\u00b50 \u2212 y)(\u00b50 \u2212 y)T\nMOMENT MATCHING\nIn this section we show the system of equations using which the parameters of a product of Dirichlet and Normal-Wishart distribution can be estimated once the set of sufficient moments are known. The set of sufficient moments in this case is S = {\u00b5j ,\u00b5j\u00b5Tj ,\u039bj ,\u039b2jkm , wj , w 2 j} | \u2200j \u2208 1, 2, ...,M} where \u039b2jkm , is the (k,m) th element of the matrix \u039bj .The expressions for the sufficient moments are :\nE[wi] = \u03b1i\u2211 j \u03b1j ; E[w2i ] = (\u03b1i)(\u03b1i + 1)(\u2211 j \u03b1j )( 1 + \u2211 j \u03b1j ) E[\u039b] = \u03bdW; V ar(\u039bij) = \u03bd(W2ij + WiiWjj)\nE[\u00b5] = \u03b4; E[(\u00b5\u2212 \u03b4)(\u00b5\u2212 \u03b4)T ] = \u03ba+ 1 \u03ba(\u03bd \u2212 d\u2212 1) W\u22121\nThe parameters of the approximate posterior P\u0303 can be computed using the equations above in the following manner\n\u03b1i = E[wi] E[wi]\u2212 E[w2i ] E[w2i ]\u2212 E[wi]2 \u03b4 = E[\u00b5]\nWii = V ar(\u039bii)\nE[\u039bii]\nWij = V ar(\u039bij)\nE[\u039bij ]\n\u03bd = E[\u039b]\nW \u03ba = 1\u2212 ( (\u03bd \u2212 d\u2212 1)E[(\u00b5\u2212 \u03b4)(\u00b5\u2212 \u03b4)T ])W )\u22121"}, {"heading": "B SOURCE DOMAIN LEARNING USING BMM", "text": "The update equation at each time step for a source domain k is\nPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) \u221d Emission distribution\ufe37 \ufe38\ufe38 \ufe37 Pr(Xkt |Y kt = j) Transition Probability\ufe37 \ufe38\ufe38 \ufe37 Pr(Y kt = j|Y kt\u22121 = i) Prior for t\u2212 1\ufe37 \ufe38\ufe38 \ufe37 Pr(\u0398k,\u03a6k, Y kt\u22121 = i|Xk1:t\u22121)\n\u2200j \u2208 {1, 2, ..., N}\nThe posterior after inserting all the relevant terms can be written as -\nPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) \u221d M\u2211 m=1 wkjmN (X k t |\u00b5kjm ,\u03a3 k jm)\u03b8 k ji [ N\u220f i=1 Dir(\u03b8ki |\u03b1ki ) ]\n[ N\u220f j=1 Dir(wkj ;\u03b2 k j ) M\u220f u=1 NW(\u00b5kju ,\u039b k ju ; \u03b4 k ju , \u03ba k ju ,W k ju , v k ju) ] \u2200j \u2208 {1, 2, ..., N}\nUsing (A), we can re-write this as\nPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) = 1\nZ M\u2211 m=1 N\u220f u6=i M\u220f u6=m N\u220f i 6=j C(i, j, k,m) [ Dir(\u03b8ki |\u03b1\u0302 k i )Dir(\u03b8 k u|\u03b1ku) ] [ Dir(wkj ; \u03b2\u0302 k j )Dir(w k i ;\u03b2 k i ) ][ NW(\u00b5kjm ,\u039b k jm ; \u03b4\u0302 k m, \u03ba\u0302 k jm , W\u0302 k jm , v\u0302 k jm)NW(\u00b5 k ju ,\u039b k ju ; \u03b4 k ju , \u03ba k ju ,W k ju , v\nk ju) ] (9)\nwhere Z = \u2211 i,j,k,m C(i, j, k,m) is the normalization constant. Eq (9) is a mixture of product of distributions where each component belongs to the same family as the prior distribution. The set of sufficient moments in this case would be\nS = { \u03b8ki , (\u03b8 k i ) 2,wkj , (w k j ) 2, \u00b5kjm , \u00b5 k jm(\u00b5 k jm) T ,\u039bkjm ,\u039b k jm(\u039b k jm) T | \u2200m \u2208 {1, 2, ...,M} }\nThe exact moments can be calculated by E[z] = \u222b\n\u0398,\u03a6\nzPr ( \u0398,\u03a6, Y kt = j|Xkt , Y kt\u22121 = i ) d(\u0398)d(\u03a6) \u2200z \u2208 S\nOnce we know the moments, we can use these moments to estimate the parameters of the approximate distribution using ideas discussed in (3)."}, {"heading": "C TARGET DOMAIN LEARNING USING BMM", "text": "The prior over the weights is\nPr(\u03bb,\u03c0) = Dir(\u03bb;\u03b3)Dir(\u03c0;\u03bd)\nwhere \u03b3 and \u03bd are the hyper-parameters for the Dirichlet distribution. The posterior after each observation is\nPr ( \u03bb,\u03c0, Yt = j|Xt ) \u221d Pr(Xt|Yt = j) N\u2211 i=1 Pr(Yt = j|Yt\u22121 = i) Pr(\u03bb,\u03c0, Yt\u22121) (10)\n\u221d K\u2211 k=1 \u03c0k M\u2211 u=1 N (\u00b5kju ,\u03a3 k ju) N\u2211 i=1 K\u2211 m=1 \u03bbm\u03b8 m ijDir(\u03bb;\u03b3)Dir(\u03c0;\u03bd) (11)\n\u221d K\u2211 k,m N\u2211 i=1 \u03c0kDir(\u03c0;\u03bd)\ufe38 \ufe37\ufe37 \ufe38 combines \u03bbmDir(\u03bb;\u03b3)\ufe38 \ufe37\ufe37 \ufe38 combines M\u2211 u=1 N (\u00b5kju ,\u03a3 k ju)\u03b8 m ij\ufe38 \ufe37\ufe37 \ufe38\nknown\n(12)\n= 1\nZ K\u2211 k,m N\u2211 i=1 C(j, k,m) Dir(\u03c0; \u03bd\u0302)Dir(\u03bb; \u03b3\u0302) (13)\nwhere Z = \u2211 i,j,k,m C(i, j, k,m) is the normalization constant, K is the number of source domains and N is the number of hidden classes.\nNow, we can use the Bayesian Moment Matching algorithm to approximate Eq (8) as a product of two Dirichlets, in the same form as the prior. This posterior will then act as the prior for the next time step. Finally, the values of the weights will be the expected value of each Dirichlet. Let us next see how the combining happens for a Dirichlet.\n\u03bbmDir(\u03bb,\u03b3) = \u03b3m\u2211 i \u03b3i Dir(\u03bb; \u03b3\u0302) (14)\nwhere\n\u03b3\u0302i = { \u03b3i if i 6= m \u03b3i + 1 if i = m\nTherefore C(i,j,k,m) in Eq (8) is\nC(i, j, k,m) = ( \u03b3m\u2211 i \u03b3i )( \u03c0k\u2211 i \u03c0i ) M\u2211 u=1 N (\u00b5kju ,\u03a3 k ju)\u03b8 m ij (15)\nNext, we outline the moment matching step. The set of sufficient moments is given by\nS = {\u03bbi, \u03bb2i , \u03c0i, \u03c02i | \u2200i \u2208 {1, 2, ..,K}}\nE[\u03bbn] = 1\nZ K\u2211 k,m N\u2211 i=1 \u222b \u03bbnC(i, j, k,m)Dir(\u03c0; \u03bd\u0302)Dir(\u03bb; \u03b3\u0302)d(\u03bb)d(\u03c0) (16)\n= 1\nZ K\u2211 k,m N\u2211 i=1 \u222b \u03bbnC(i, j, k,m)Dir(\u03bb; \u03b3\u0302)d(\u03bb) (17)\n= 1\nZ K\u2211 k,m N\u2211 i=1 ( \u03bb\u0302n\u2211 u \u03bb\u0302u ) C(i, j, k,m) (18)\nSimilarly, the second moment can be evaluated as\nE[\u03bb2n] = 1\nZ K\u2211 k,m N\u2211 i=1 \u222b \u03bb2nC(i, j, k,m)Dir(\u03c0; \u03bd\u0302)Dir(\u03bb; \u03b3\u0302)d(\u03bb)d(\u03c0) (19)\n= 1\nZ K\u2211 k,m N\u2211 i=1\n( \u03bb\u0302n(\u03bb\u0302n + 1)\n( \u2211 u \u03bb\u0302u)(1 + \u2211 u \u03bb\u0302u)\n) C(i, j, k,m) (20)\nWe evaluate the moments using the equations above \u2200z \u2208 S Once we have the two moments, we can project the posterior into a family of Dirichlet distributions having the same moments. In this way we can perform the learning of the parameters for the target domain."}, {"heading": "D EXPERIMENT RESULTS : SLEEP STAGE CLASSIFICATION", "text": "Fig. 3, 4 and 5 compare the performance of the online transfer learning algorithm with the baseline algorithm, the EM algorithm and recurrent neural networks (RNNs) respectively.\nFig. 3a compares the average percentage accuracy for our online transfer learning technique and the baseline algorithm and Fig. 4a compares EM and online transfer learning. The blue + signs represent the accuracy of the baseline algorithm and the red o represent the accuracy of the online transfer learning algorithm. The black line is a reference line that passes through the points plotting the accuracy of the online transfer Learning algorithm. The accuracy is plotted against each individual patient. The blue + signs are always below the black line indicating superior performance of the transfer learning algorithm. Fig. 3b and 4b plot the difference between the accuracy of the baseline algorithm and the transfer learning algorithm. In the top plot, the difference in accuracy is for each\npatient corresponding to those shown in Fig. 3a and 4a. In the bottom plot, the difference in accuracy is plotted after sorting. A reference line of 0 is also plotted for the case when there is no difference in performance. The plots suggest that for a majority of patients the transfer learning technique outperforms both the baseline algorithm and EM.\nIn Fig. 5a we compare the performance of the online transfer learning algorithm with RNNs. Fig. 5b plots the difference between the accuracy of RNN and the online transfer learning algorithm. In the top plot, the difference in accuracy is for each patient corresponding to those shown in Fig. 5a. In the bottom plot, the difference in accuracy is plotted after sorting. The figures show that the online transfer learning algorithm outperformed RNNs for a majority of patients (102 out of 142). All the results are statistically significant under the Wilcoxon signed rank test with p-value < 0.05."}], "references": [{"title": "Adaptive boosting for transfer learning using dynamic updates", "author": ["Samir Al-Stouhi", "Chandan K Reddy"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Al.Stouhi and Reddy.,? \\Q2011\\E", "shortCiteRegEx": "Al.Stouhi and Reddy.", "year": 2011}, {"title": "PyEEG: An Open Source Python Module for EEG/MEG Feature Extraction", "author": ["Forrest S Bao", "Xin Liu", "Christina Zhang"], "venue": "Computational Intelligence and Neuroscience,", "citeRegEx": "Bao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2011}, {"title": "Topology preserving domain adaptation for addressing subject based variability in semg signal", "author": ["Rita Chattopadhyay", "Narayanan Chatapuram Krishnan", "Sethuraman Panchanathan"], "venue": "In AAAI Spring Symposium: Computational Physiology, pp", "citeRegEx": "Chattopadhyay et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chattopadhyay et al\\.", "year": 2011}, {"title": "Activity recognition from physiological data using conditional random fields", "author": ["Hai Leong Chieu", "Wee Sun Lee", "Leslie P Kaelbling"], "venue": null, "citeRegEx": "Chieu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2006}, {"title": "Transfer learning for activity recognition: A survey", "author": ["Diane Cook", "Kyle D Feuz", "Narayanan C Krishnan"], "venue": "Knowledge and information systems,", "citeRegEx": "Cook et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2013}, {"title": "Boosting for transfer learning", "author": ["Wenyuan Dai", "Qiang Yang", "Gui-Rong Xue", "Yong Yu"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Dai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2007}, {"title": "Optimal statistical dcisions", "author": ["Morris H. Degroot"], "venue": "URL http://opac.inria.fr/record=", "citeRegEx": "Degroot.,? \\Q1970\\E", "shortCiteRegEx": "Degroot.", "year": 1970}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S Hochreiter", "J Schmidhuber"], "venue": "Neural Comp,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Online bayesian moment matching for topic modeling with unknown number of topics", "author": ["Wei-Shou Hsu", "Pascal Poupart"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Hsu and Poupart.,? \\Q2016\\E", "shortCiteRegEx": "Hsu and Poupart.", "year": 2016}, {"title": "Online and distributed learning of gaussian mixture models by bayesian moment matching", "author": ["Priyank Jaini", "Pascal Poupart"], "venue": "arXiv preprint arXiv:1609.05881,", "citeRegEx": "Jaini and Poupart.,? \\Q2016\\E", "shortCiteRegEx": "Jaini and Poupart.", "year": 2016}, {"title": "Online algorithms for sum-product networks with continuous variables", "author": ["Priyank Jaini", "Abdullah Rashwan", "Han Zhao", "Yue Liu", "Ershad Banijamali", "Zhitang Chen", "Pascal Poupart"], "venue": "In Proceedings of the Eighth International Conference on Probabilistic Graphical Models,", "citeRegEx": "Jaini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaini et al\\.", "year": 2016}, {"title": "Automatic Sleep Scoring in Normals and in Individuals with Neurodegenerative Disorders According to New International Sleep Scoring Criteria", "author": ["Peter S Jensen", "Helge B D Sorensen", "Helle L Leonthin", "Poul Jennum"], "venue": "Journal of Clinical Neurophysiology: Official Publication of the American Electroencephalographic Society,", "citeRegEx": "Jensen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2010}, {"title": "Signal Processing Techniques Applied to Human Sleep EEG Signals - A Review", "author": ["Shayan Motamedi-Fakhr", "Mohamed Moshrefi-Torbati", "Martyn Hill", "Catherine M Hill", "Paul R White"], "venue": "Biomedical Signal Processing and Control,", "citeRegEx": "Motamedi.Fakhr et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Motamedi.Fakhr et al\\.", "year": 2014}, {"title": "A method of solving a convex programming problem with convergence rate O(1/sqr(k))", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Online bayesian learning in probabilistic graphical models using moment matching with applications", "author": ["Farheen Omar"], "venue": null, "citeRegEx": "Omar.,? \\Q2016\\E", "shortCiteRegEx": "Omar.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Sleep and Parkinson\u2019s disease: A review of case-control polysomnography studies", "author": ["Tasneem Peeraully", "Ming-Hui Yong", "Sudhansu Chokroverty", "Eng-King Tan"], "venue": "Movement Disorders,", "citeRegEx": "Peeraully et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peeraully et al\\.", "year": 2012}, {"title": "Computer-Assisted Automated Scoring of Polysomnograms", "author": ["Naresh M Punjabi", "Naima Shifa", "Georg Dorffner", "Susheel Patil", "Grace Pien", "Rashmi N Aurora"], "venue": "Using the Somnolyzer System. Sleep,", "citeRegEx": "Punjabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Punjabi et al\\.", "year": 2015}, {"title": "Transferring learned activities in smart environments", "author": ["Parisa Rashidi", "Diane J Cook"], "venue": "In Intelligent Environments,", "citeRegEx": "Rashidi and Cook.,? \\Q2009\\E", "shortCiteRegEx": "Rashidi and Cook.", "year": 2009}, {"title": "Online and distributed bayesian moment matching for sum-product networks", "author": ["Abdullah Rashwan", "Han Zhao", "Pascal Poupart"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Rashwan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rashwan et al\\.", "year": 2016}, {"title": "The American Academy of Sleep Medicine Inter-scorer Reliability Program: Sleep Stage Scoring", "author": ["Richard S. Rosenberg", "Steven Van Hout"], "venue": "Journal of Clinical Sleep Medicine,", "citeRegEx": "Rosenberg and Hout.,? \\Q2013\\E", "shortCiteRegEx": "Rosenberg and Hout.", "year": 2013}, {"title": "Online model selection based on the variational bayes", "author": ["Masa-Aki Sato"], "venue": "Neural Computation,", "citeRegEx": "Sato.,? \\Q2001\\E", "shortCiteRegEx": "Sato.", "year": 2001}, {"title": "Transfer learning for visual categorization: A survey", "author": ["Ling Shao", "Fan Zhu", "Xuelong Li"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "Shao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["Chong Wang", "John William Paisley", "David M Blei"], "venue": "In AISTATS,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Some rapid approximate statistical procedures", "author": ["Frank Wilcoxon"], "venue": "Annals of the New York Academy of Sciences,", "citeRegEx": "Wilcoxon.,? \\Q1950\\E", "shortCiteRegEx": "Wilcoxon.", "year": 1950}, {"title": "An efficient gradient-based algorithm for online training of recurrent network trajectories", "author": ["R.J. Williams", "J. Peng"], "venue": "Neural Computation,", "citeRegEx": "Williams and Peng.,? \\Q1990\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1990}, {"title": "Boosting for transfer learning with multiple sources", "author": ["Yi Yao", "Gianfranco Doretto"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yao and Doretto.,? \\Q2010\\E", "shortCiteRegEx": "Yao and Doretto.", "year": 2010}, {"title": "Cross-people mobilephone based activity recognition", "author": ["Zhongtang Zhao", "Yiqiang Chen", "Junfa Liu", "Zhiqi Shen", "Mingjie Liu"], "venue": "In Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 23, "context": "There is a large literature on transfer learning (Pan & Yang, 2010; Taylor & Stone, 2009; Shao et al., 2015; Cook et al., 2013).", "startOffset": 49, "endOffset": 127}, {"referenceID": 4, "context": "There is a large literature on transfer learning (Pan & Yang, 2010; Taylor & Stone, 2009; Shao et al., 2015; Cook et al., 2013).", "startOffset": 49, "endOffset": 127}, {"referenceID": 5, "context": "This idea is related to boosting techniques for transfer learning (Dai et al., 2007; Yao & Doretto, 2010; Al-Stouhi & Reddy, 2011) that estimate a weighted combination of base classifiers.", "startOffset": 66, "endOffset": 130}, {"referenceID": 2, "context": "Chieu et al. (2006) describe how to augment conditional random fields with a subject hidden variable to obtain a mixture of conditional random fields that can naturally infer a distribution over the closest subjects in a training population when inferring the activities of a new individual based on physiological data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Chieu et al. (2006) describe how to augment conditional random fields with a subject hidden variable to obtain a mixture of conditional random fields that can naturally infer a distribution over the closest subjects in a training population when inferring the activities of a new individual based on physiological data. Rashidi & Cook (2009) proposed a data mining technique with a similarity measure to facilitate the transfer of activity recognition across different people.", "startOffset": 0, "endOffset": 342}, {"referenceID": 2, "context": "Chattopadhyay et al. (2011) describe a similarity measure with an intrinsic manifold that preserve the topology of surface electromyography (SEMG) while mitigating distributional differences among individuals.", "startOffset": 0, "endOffset": 28}, {"referenceID": 2, "context": "Chattopadhyay et al. (2011) describe a similarity measure with an intrinsic manifold that preserve the topology of surface electromyography (SEMG) while mitigating distributional differences among individuals. Zhao et al. (2011) proposed a transfer learning technique that starts by training a decision tree to recognize the activities of a user based on smartphone accelerometry.", "startOffset": 0, "endOffset": 229}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially.", "startOffset": 150, "endOffset": 170}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al.", "startOffset": 150, "endOffset": 854}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al.", "startOffset": 150, "endOffset": 939}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al. (2011) in the context of topic modeling.", "startOffset": 150, "endOffset": 1001}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al. (2011) in the context of topic modeling. BMM was also shown to work better than other online techniques in several papers Rashwan et al. (2016); Hsu & Poupart (2016); Jaini et al.", "startOffset": 150, "endOffset": 1138}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al. (2011) in the context of topic modeling. BMM was also shown to work better than other online techniques in several papers Rashwan et al. (2016); Hsu & Poupart (2016); Jaini et al.", "startOffset": 150, "endOffset": 1160}, {"referenceID": 11, "context": "2 BAYESIAN MOMENT MATCHING ALGORITHM The Bayesian moment matching (BMM) algorithm for Gaussian Mixture Models was proposed by Jaini & Poupart (2016); Jaini et al. (2016). Exact Bayesian learning of mixture models based on streaming data is intractable because the number of terms in the posterior after observing each observation increases exponentially. BMM circumvents this issue by projecting the distribution of the exact posterior P on a tractable family of distributions P\u0303 by matching a set of sufficient moments. In this section, we give a brief overview of the BMM algorithm with an example. Note that Variational Bayes (VB) and Markov Chain Monte Carlo (MCMC) techniques can also be used for approximate Bayesian learning as an alternative to BMM. However, MCMC is difficult to run in an online fashion. A recent comparison by Omar Omar (2016) showed that BMM achieves better results than online Vational Bayes (oVB) Sato (2001) and Stochastic Variational Inference (SVI) Wang et al. (2011) in the context of topic modeling. BMM was also shown to work better than other online techniques in several papers Rashwan et al. (2016); Hsu & Poupart (2016); Jaini et al. (2016). This is due to the fact that BMM is naturally online and therefore does not require minibatches.", "startOffset": 150, "endOffset": 1181}, {"referenceID": 11, "context": "To circumvent this, we use BMM for Gaussian Mixture Models as described in (Jaini et al., 2016; Jaini & Poupart, 2016)3.", "startOffset": 75, "endOffset": 118}, {"referenceID": 24, "context": "We select LSTM units instead of sigmoid or hyperbolic tangent units due to their popularity and success in sequence learning (Sutskever et al., 2014).", "startOffset": 125, "endOffset": 149}, {"referenceID": 14, "context": "For the training method, we either use Nesterov\u2019s accelerated gradient descent (Nesterov, 1983; Sutskever et al., 2013) with learning rates [0.", "startOffset": 79, "endOffset": 119}, {"referenceID": 25, "context": "For the training method, we either use Nesterov\u2019s accelerated gradient descent (Nesterov, 1983; Sutskever et al., 2013) with learning rates [0.", "startOffset": 79, "endOffset": 119}, {"referenceID": 29, "context": "We report the average percentage accuracy and use the Wilcoxon signed rank test (Wilcoxon, 1950) to compute a p-value and report statistical significance when the p-value is less than 0.", "startOffset": 80, "endOffset": 96}, {"referenceID": 17, "context": ", Parkinson\u2019s disease (Peeraully et al., 2012), because neurological anomalies often also reflect in variations of a patient\u2019s sleep patterns.", "startOffset": 22, "endOffset": 46}, {"referenceID": 12, "context": "High annotation costs and low inter-scorer agreement rates have motivated efforts to develop fully automated approaches for sleep stage classification (Anderer et al., 2005; Jensen et al., 2010; Mal, 2013; Punjabi et al., 2015).", "startOffset": 151, "endOffset": 227}, {"referenceID": 18, "context": "High annotation costs and low inter-scorer agreement rates have motivated efforts to develop fully automated approaches for sleep stage classification (Anderer et al., 2005; Jensen et al., 2010; Mal, 2013; Punjabi et al., 2015).", "startOffset": 151, "endOffset": 227}, {"referenceID": 1, "context": "Bao et al. (2011) and Motamedi-Fakhr et al. (2014) give a detailed listing and explanation of all 26 features.", "startOffset": 0, "endOffset": 51}, {"referenceID": 7, "context": "Furthermore, since recurrent neural networks (RNNs) have been shown to outperform HMMs with GMM emission distributions in some applications such as speech recognition (Graves et al., 2013), it would be interesting to generalize our online transfer learning technique to RNNs.", "startOffset": 167, "endOffset": 188}], "year": 2017, "abstractText": "We consider the problem of inferring a sequence of hidden states associated with a sequence of observations produced by an individual within a population. Instead of learning a single sequence model for the population (which does not account for variations within the population), we learn a set of basis sequence models based on different individuals. The sequence of hidden states for a new individual is inferred in an online fashion by estimating a distribution over the basis models that best explain the sequence of observations of this new individual. We explain how to do this in the context of hidden Markov models with Gaussian mixture models that are learned based on streaming data by online Bayesian moment matching. The resulting transfer learning technique is demonstrated with three real-word applications: activity recognition based on smartphone sensors, sleep classification based on electroencephalography data and the prediction of the direction of future packet flows between a pair of servers in telecommunication networks.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_127"}