{"name": "ICLR_2017_130.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Brendan O\u2019Donoghue", "R\u00e9mi Munos", "Koray Kavukcuoglu"], "emails": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In reinforcement learning an agent explores an environment and through the use of a reward signal learns to optimize its behavior to maximize the expected long-term return. Reinforcement learning has seen success in several areas including robotics (Lin, 1993; Levine et al., 2015), computer games (Mnih et al., 2013; 2015), online advertising (Pednault et al., 2002), board games (Tesauro, 1995; Silver et al., 2016), and many others. For an introduction to reinforcement learning we refer to the classic text by Sutton & Barto (1998). In this paper we consider model-free reinforcement learning, where the state-transition function is not known or learned. There are many different algorithms for model-free reinforcement learning, but most fall into one of two families: action-value fitting and policy gradient techniques.\nAction-value techniques involve fitting a function, called the Q-values, that captures the expected return for taking a particular action at a particular state, and then following a particular policy thereafter. Two alternatives we discuss in this paper are SARSA (Rummery & Niranjan, 1994) and Q-learning (Watkins, 1989), although there are many others. SARSA is an on-policy algorithm whereby the action-value function is fit to the current policy, which is then refined by being mostly greedy with respect to those action-values. On the other hand, Q-learning attempts to find the Qvalues associated with the optimal policy directly and does not fit to the policy that was used to generate the data. Q-learning is an off-policy algorithm that can use data generated by another agent or from a replay buffer of old experience. Under certain conditions both SARSA and Q-learning can be shown to converge to the optimal Q-values, from which we can derive the optimal policy (Sutton, 1988; Bertsekas & Tsitsiklis, 1996).\nIn policy gradient techniques the policy is represented explicitly and we improve the policy by updating the parameters in the direction of the gradient of the performance (Sutton et al., 1999; Silver et al., 2014; Kakade, 2001). Online policy gradient typically requires an estimate of the action-value function of the current policy. For this reason they are often referred to as actor-critic methods, where the actor refers to the policy and the critic to the estimate of the action-value function (Konda & Tsitsiklis, 2003). Vanilla actor-critic methods are on-policy only, although some attempts have been made to extend them to off-policy data (Degris et al., 2012; Levine & Koltun, 2013).\nIn this paper we derive a link between the Q-values induced by a policy and the policy itself when the policy is the fixed point of a regularized policy gradient algorithm (where the gradient vanishes). This connection allows us to derive an estimate of the Q-values from the current policy, which we can refine using off-policy data and Q-learning. We show in the tabular setting that when the regularization penalty is small (the usual case) the resulting policy is close to the policy that would be found without the addition of the Q-learning update. Separately, we show that regularized actor-critic methods can be interpreted as action-value fitting methods, where the Q-values have been parameterized in a particular way. We conclude with some numerical examples that provide empirical evidence of improved data efficiency and stability of PGQL."}, {"heading": "1.1 PRIOR WORK", "text": "Here we highlight various axes along which our work can be compared to others. In this paper we use entropy regularization to ensure exploration in the policy, which is a common practice in policy gradient (Williams & Peng, 1991; Mnih et al., 2016). An alternative is to use KL-divergence instead of entropy as a regularizer, or as a constraint on how much deviation is permitted from a prior policy (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015). Natural policy gradient can also be interpreted as putting a constraint on the KL-divergence at each step of the policy improvement (Amari, 1998; Kakade, 2001; Pascanu & Bengio, 2013). In Sallans & Hinton (2004) the authors use a Boltzmann exploration policy over estimated Q-values which they update using TD-learning. In Heess et al. (2012) this was extended to use an actor-critic algorithm instead of TD-learning, however the two updates were not combined as we have done in this paper. In Azar et al. (2012) the authors develop an algorithm called dynamic policy programming, whereby they apply a Bellman-like update to the action-preferences of a policy, which is similar in spirit to the update we describe here. In Norouzi et al. (2016) the authors augment a maximum likelihood objective with a reward in a supervised learning setting, and develop a connection that resembles the one we develop here between the policy and the Q-values. Other works have attempted to combine on and off-policy learning, primarily using action-value fitting methods (Wang et al., 2013; Hausknecht & Stone, 2016; Lehnert & Precup, 2015), with varying degrees of success. In this paper we establish a connection between actor-critic algorithms and action-value learning algorithms. In particular we show that TD-actor-critic (Konda & Tsitsiklis, 2003) is equivalent to expected-SARSA (Sutton & Barto, 1998, Exercise 6.10) with Boltzmann exploration where the Q-values are decomposed into advantage function and value function. The algorithm we develop extends actor-critic with a Q-learning style update that, due to the decomposition of the Q-values, resembles the update of the dueling architecture (Wang et al., 2016). Recently, the field of deep reinforcement learning, i.e., the use of deep neural networks to represent action-values or a policy, has seen a lot of success (Mnih et al., 2015; 2016; Silver et al., 2016; Riedmiller, 2005; Lillicrap et al., 2015; Van Hasselt et al., 2016). In the examples section we use a neural network with PGQL to play the Atari games suite."}, {"heading": "2 REINFORCEMENT LEARNING", "text": "We consider the infinite horizon, discounted, finite state and action space Markov decision process, with state space S, action space A and rewards at each time period denoted by rt \u2208 R. A policy \u03c0 : S \u00d7 A \u2192 R+ is a mapping from state-action pair to the probability of taking that action at that state, so it must satisfy \u2211 a\u2208A \u03c0(s, a) = 1 for all states s \u2208 S . Any policy \u03c0 induces a probability distribution over visited states, d\u03c0 : S \u2192 R+ (which may depend on the initial state), so the probability of seeing state-action pair (s, a) \u2208 S \u00d7A is d\u03c0(s)\u03c0(s, a). In reinforcement learning an \u2018agent\u2019 interacts with an environment over a number of times steps. At each time step t the agent receives a state st and a reward rt and selects an action at from the policy \u03c0t, at which point the agent moves to the next state st+1 \u223c P (\u00b7, st, at), where P (s\u2032, s, a) is the probability of transitioning from state s to state s\u2032 after taking action a. This continues until the agent encounters a terminal state (after which the process is typically restarted). The goal of the agent is to find a policy \u03c0 that maximizes the expected total discounted return J(\u03c0) = E( \u2211\u221e t=0 \u03b3\ntrt | \u03c0), where the expectation is with respect to the initial state distribution, the state-transition probabilities, and the policy, and where \u03b3 \u2208 (0, 1) is the discount factor that, loosely speaking, controls how much the agent prioritizes long-term versus short-term rewards. Since the agent starts with no knowledge\nof the environment it must continually explore the state space and so will typically use a stochastic policy.\nAction-values. The action-value, or Q-value, of a particular state under policy \u03c0 is the expected total discounted return from taking that action at that state and following \u03c0 thereafter, i.e., Q\u03c0(s, a) = E( \u2211\u221e t=0 \u03b3\ntrt | s0 = s, a0 = a, \u03c0). The value of state s under policy \u03c0 is denoted by V \u03c0(s) = E( \u2211\u221e t=0 \u03b3\ntrt | s0 = s, \u03c0), which is the expected total discounted return of policy \u03c0 from state s. The optimal action-value function is denoted Q? and satisfies Q?(s, a) = max\u03c0 Q\u03c0(s, a) for each (s, a). The policy that achieves the maximum is the optimal policy \u03c0?, with value function V ?. The advantage function is the difference between the action-value and the value function, i.e., A\u03c0(s, a) = Q\u03c0(s, a)\u2212V \u03c0(s), and represents the additional expected reward of taking action a over the average performance of the policy from state s. Since V \u03c0(s) = \u2211 a \u03c0(s, a)Q\n\u03c0(s, a) we have the identity \u2211 a \u03c0(s, a)A\n\u03c0(s, a) = 0, which simply states that the policy \u03c0 has no advantage over itself.\nBellman equation. The Bellman operator T \u03c0 (Bellman, 1957) for policy \u03c0 is defined as\nT \u03c0Q(s, a) = E s\u2032,r,b (r(s, a) + \u03b3Q(s\u2032, b)),\nwhere the expectation is over next state s\u2032 \u223c P (\u00b7, s, a), the reward r(s, a), and the action b from policy \u03c0s\u2032 . The Q-value function for policy \u03c0 is the fixed point of the Bellman operator for \u03c0, i.e., T \u03c0Q\u03c0 = Q\u03c0 . The optimal Bellman operator T ? is defined as\nT ?Q(s, a) = E s\u2032,r (r(s, a) + \u03b3max b Q(s\u2032, b)),\nwhere the expectation is over the next state s\u2032 \u223c P (\u00b7, s, a), and the reward r(s, a). The optimal Q-value function is the fixed point of the optimal Bellman equation, i.e., T ?Q? = Q?. Both the \u03c0-Bellman operator and the optimal Bellman operator are \u03b3-contraction mappings in the sup-norm, i.e., \u2016T Q1\u2212T Q2\u2016\u221e \u2264 \u03b3\u2016Q1\u2212Q2\u2016\u221e, for any Q1, Q2 \u2208 RS\u00d7A. From this fact one can show that the fixed point of each operator is unique, and that value iteration converges, i.e., (T \u03c0)kQ \u2192 Q\u03c0 and (T ?)kQ\u2192 Q? from any initial Q. (Bertsekas, 2005)."}, {"heading": "2.1 ACTION-VALUE LEARNING", "text": "In value based reinforcement learning we approximate the Q-values using a function approximator. We then update the parameters so that the Q-values are as close to the fixed point of a Bellman equation as possible. If we denote by Q(s, a; \u03b8) the approximate Q-values parameterized by \u03b8, then Q-learning updates the Q-values along direction Es,a(T ?Q(s, a; \u03b8)\u2212Q(s, a; \u03b8))\u2207\u03b8Q(s, a; \u03b8) and SARSA updates the Q-values along direction Es,a(T \u03c0Q(s, a; \u03b8)\u2212Q(s, a; \u03b8))\u2207\u03b8Q(s, a; \u03b8). In the online setting the Bellman operator is approximated by sampling and bootstrapping, whereby the Q-values at any state are updated using the Q-values from the next visited state. Exploration is achieved by not always taking the action with the highest Q-value at each time step. One common technique called \u2018epsilon greedy\u2019 is to sample a random action with probability > 0, where starts high and decreases over time. Another popular technique is \u2018Boltzmann exploration\u2019, where the policy is given by the softmax over the Q-values with a temperature T , i.e., \u03c0(s, a) = exp(Q(s, a)/T )/ \u2211 b exp(Q(s, b)/T ), where it is common to decrease the temperature over time."}, {"heading": "2.2 POLICY GRADIENT", "text": "Alternatively, we can parameterize the policy directly and attempt to improve it via gradient ascent on the performance J . The policy gradient theorem (Sutton et al., 1999) states that the gradient of J with respect to the parameters of the policy is given by\n\u2207\u03b8J(\u03c0) = E s,a Q\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a), (1)\nwhere the expectation is over (s, a) with probability d\u03c0(s)\u03c0(s, a). In the original derivation of the policy gradient theorem the expectation is over the discounted distribution of states, i.e., over d\u03c0,s0\u03b3 (s) = \u2211\u221e t=0 \u03b3 tPr{st = s | s0, \u03c0}. However, the gradient update in that case will assign a low\nweight to states that take a long time to reach and can therefore have poor empirical performance. In practice the non-discounted distribution of states is frequently used instead. In certain cases this is equivalent to maximizing the average (i.e., non-discounted) policy performance, even whenQ\u03c0 uses a discount factor (Thomas, 2014). Throughout this paper we will use the non-discounted distribution of states.\nIn the online case it is common to add an entropy regularizer to the gradient in order to prevent the policy becoming deterministic. This ensures that the agent will explore continually. In that case the (batch) update becomes\n\u2206\u03b8 \u221d E s,a Q\u03c0(s, a)\u2207\u03b8 log \u03c0(s, a) + \u03b1E s \u2207\u03b8H\u03c0(s), (2)\nwhere H\u03c0(s) = \u2212 \u2211 a \u03c0(s, a) log \u03c0(s, a) denotes the entropy of policy \u03c0, and \u03b1 > 0 is the regularization penalty parameter. Throughout this paper we will make use of entropy regularization, however many of the results are true for other choices of regularizers with only minor modification, e.g., KL-divergence. Note that equation (2) requires exact knowledge of the Q-values. In practice they can be estimated, e.g., by the sum of discounted rewards along an observed trajectory (Williams, 1992), and the policy gradient will still perform well (Konda & Tsitsiklis, 2003)."}, {"heading": "3 REGULARIZED POLICY GRADIENT ALGORITHM", "text": "In this section we derive a relationship between the policy and the Q-values when using a regularized policy gradient algorithm. This allows us to transform a policy into an estimate of the Q-values. We then show that for small regularization the Q-values induced by the policy at the fixed point of the algorithm have a small Bellman error in the tabular case."}, {"heading": "3.1 TABULAR CASE", "text": "Consider the fixed points of the entropy regularized policy gradient update (2). Let us define f(\u03b8) = Es,aQ \u03c0(s, a)\u2207\u03b8 log \u03c0(s, a) +\u03b1Es\u2207\u03b8H(\u03c0s), and gs(\u03c0) = \u2211 a \u03c0(s, a) for each s. A fixed point is one where we can no longer update \u03b8 in the direction of f(\u03b8) without violating one of the constraints gs(\u03c0) = 1, i.e., where f(\u03b8) is in the span of the vectors {\u2207\u03b8gs(\u03c0)}. In other words, any fixed point must satisfy f(\u03b8) = \u2211 s \u03bbs\u2207\u03b8gs(\u03c0), where for each s the Lagrange multiplier \u03bbs \u2208 R ensures that gs(\u03c0) = 1. Substituting in terms to this equation we obtain\nE s,a\n(Q\u03c0(s, a)\u2212 \u03b1 log \u03c0(s, a)\u2212 cs)\u2207\u03b8 log \u03c0(s, a) = 0, (3)\nwhere we have absorbed all constants into c \u2208 R|S|. Any solution \u03c0 to this equation is strictly positive element-wise since it must lie in the domain of the entropy function. In the tabular case \u03c0 is represented by a single number for each state and action pair and the gradient of the policy with respect to the parameters is the indicator function, i.e.,\u2207\u03b8(t,b)\u03c0(s, a) = 1(t,b)=(s,a). From this we obtain Q\u03c0(s, a) \u2212 \u03b1 log \u03c0(s, a) \u2212 cs = 0 for each s (assuming that the measure d\u03c0(s) > 0). Multiplying by \u03c0(a, s) and summing over a \u2208 A we get cs = \u03b1H\u03c0(s) + V \u03c0(s). Substituting c into equation (3) we have the following formulation for the policy:\n\u03c0(s, a) = exp(A\u03c0(s, a)/\u03b1\u2212H\u03c0(s)), (4)\nfor all s \u2208 S and a \u2208 A. In other words, the policy at the fixed point is a softmax over the advantage function induced by that policy, where the regularization parameter \u03b1 can be interpreted as the temperature. Therefore, we can use the policy to derive an estimate of the Q-values,\nQ\u0303\u03c0(s, a) = A\u0303\u03c0(s, a) + V \u03c0(s) = \u03b1(log \u03c0(s, a) +H\u03c0(s)) + V \u03c0(s). (5)\nWith this we can rewrite the gradient update (2) as\n\u2206\u03b8 \u221d E s,a (Q\u03c0(s, a)\u2212 Q\u0303\u03c0(s, a))\u2207\u03b8 log \u03c0(s, a), (6)\nsince the update is unchanged by per-state constant offsets. When the policy is parameterized as a softmax, i.e., \u03c0(s, a) = exp(W (s, a))/ \u2211 b expW (s, b), the quantity W is sometimes referred to as the action-preferences of the policy (Sutton & Barto, 1998, Chapter 6.6). Equation (4) states that the action preferences are equal to the Q-values scaled by 1/\u03b1, up to an additive per-state constant."}, {"heading": "3.2 GENERAL CASE", "text": "Consider the following optimization problem:\nminimize Es,a(q(s, a)\u2212 \u03b1 log \u03c0(s, a))2 subject to \u2211 a \u03c0(s, a) = 1, s \u2208 S\n(7)\nover variable \u03b8 which parameterizes \u03c0, where we consider both the measure in the expectation and the values q(s, a) to be independent of \u03b8. The optimality condition for this problem is\nE s,a\n(q(s, a)\u2212 \u03b1 log \u03c0(s, a) + cs)\u2207\u03b8 log \u03c0(s, a) = 0,\nwhere c \u2208 R|S| is the Lagrange multiplier associated with the constraint that the policy sum to one at each state. Comparing this to equation (3), we see that if q = Q\u03c0 and the measure in the expectation is the same then they describe the same set of fixed points. This suggests an interpretation of the fixed points of the regularized policy gradient as a regression of the log-policy onto the Q-values. In the general case of using an approximation architecture we can interpret equation (3) as indicating that the error between Q\u03c0 and Q\u0303\u03c0 is orthogonal to \u2207\u03b8i log \u03c0 for each i, and so cannot be reduced further by changing the parameters, at least locally. In this case equation (4) is unlikely to hold at a solution to (3), however with a good approximation architecture it may hold approximately, so that the we can derive an estimate of the Q-values from the policy using equation (5). We will use this estimate of the Q-values in the next section."}, {"heading": "3.3 CONNECTION TO ACTION-VALUE METHODS", "text": "The previous section made a connection between regularized policy gradient and a regression onto the Q-values at the fixed point. In this section we go one step further, showing that actor-critic methods can be interpreted as action-value fitting methods, where the exact method depends on the choice of critic.\nActor-critic methods. Consider an agent using an actor-critic method to learn both a policy \u03c0 and a value function V . At any iteration k, the value function V k has parameters wk, and the policy is of the form\n\u03c0k(s, a) = exp(W k(s, a)/\u03b1)/ \u2211 b exp(W k(s, b)/\u03b1), (8)\nwhere W k is parameterized by \u03b8k and \u03b1 > 0 is the entropy regularization penalty. In this case \u2207\u03b8 log \u03c0k(s, a) = (1/\u03b1)(\u2207\u03b8W k(s, a) \u2212 \u2211 b \u03c0(s, b)\u2207\u03b8W k(s, b)). Using equation (6) the parameters are updated as\n\u2206\u03b8 \u221d E s,a \u03b4ac(\u2207\u03b8W k(s, a)\u2212 \u2211 b \u03c0k(s, b)\u2207\u03b8W k(s, b)), \u2206w \u221d E s,a \u03b4ac\u2207wV k(s) (9)\nwhere \u03b4ac is the critic minus baseline term, which depends on the variant of actor-critic being used (see the remark below).\nAction-value methods. Compare this to the case where an agent is learning Q-values with a dueling architecture (Wang et al., 2016), which at iteration k is given by\nQk(s, a) = Y k(s, a)\u2212 \u2211 b \u00b5(s, b)Y k(s, b) + V k(s),\nwhere \u00b5 is a probability distribution, Y k is parameterized by \u03b8k, V k is parameterized by wk, and the exploration policy is Boltzmann with temperature \u03b1, i.e.,\n\u03c0k(s, a) = exp(Y k(s, a)/\u03b1)/ \u2211 b exp(Y k(s, b)/\u03b1). (10)\nIn action value fitting methods at each iteration the parameters are updated to reduce some error, where the update is given by\n\u2206\u03b8 \u221d E s,a \u03b4av(\u2207\u03b8Y k(s, a)\u2212 \u2211 b \u00b5(s, b)\u2207\u03b8Y k(s, b)), \u2206w \u221d E s,a \u03b4av\u2207wV k(s) (11)\nwhere \u03b4av is the action-value error term and depends on which algorithm is being used (see the remark below).\nEquivalence. The two policies (8) and (10) are identical if W k = Y k for all k. Since X0 and Y 0 can be initialized and parameterized in the same way, and assuming the two value function estimates are initialized and parameterized in the same way, all that remains is to show that the updates in equations (11) and (9) are identical. Comparing the two, and assuming that \u03b4ac = \u03b4av (see remark), we see that the only difference is that the measure is not fixed in (9), but is equal to the current policy and therefore changes after each update. Replacing \u00b5 in (11) with \u03c0k makes the updates identical, in which case W k = Y k at all iterations and the two policies (8) and (10) are always the same. In other words, the slightly modified action-value method is equivalent to an actor-critic policy gradient method, and vice-versa (modulo using the non-discounted distribution of states, as discussed in \u00a72.2). In particular, regularized policy gradient methods can be interpreted as advantage function learning techniques (Baird III, 1993), since at the optimum the quantity W (s, a) \u2212 \u2211 b \u03c0(s, b)W (s, b) = \u03b1(log \u03c0(s, a) + H\n\u03c0(s)) will be equal to the advantage function values in the tabular case.\nRemark. In SARSA (Rummery & Niranjan, 1994) we set \u03b4av = r(s, a) + \u03b3Q(s\u2032, b) \u2212 Q(s, a), where b is the action selected at state s\u2032, which would be equivalent to using a bootstrap critic in equation (6) where Q\u03c0(s, a) = r(s, a) + \u03b3Q\u0303(s\u2032, b). In expected-SARSA (Sutton & Barto, 1998, Exercise 6.10), (Van Seijen et al., 2009)) we take the expectation over the Q-values at the next state, so \u03b4av = r(s, a)+\u03b3V (s\u2032)\u2212Q(s, a). This is equivalent to TD-actor-critic (Konda & Tsitsiklis, 2003) where we use the value function to provide the critic, which is given by Q\u03c0 = r(s, a) + \u03b3V (s\u2032). In Q-learning (Watkins, 1989) \u03b4av = r(s, a) + \u03b3maxbQ(s\u2032, b)\u2212Q(s, a), which would be equivalent to using an optimizing critic that bootstraps using the max Q-value at the next state, i.e., Q\u03c0(s, a) = r(s, a) + \u03b3maxb Q\u0303\n\u03c0(s\u2032, b). In REINFORCE the critic is the Monte Carlo return from that state on, i.e., Q\u03c0(s, a) = ( \u2211\u221e t=0 \u03b3\ntrt | s0 = s, a0 = a). If the return trace is truncated and a bootstrap is performed after n-steps, this is equivalent to n-step SARSA or n-step Q-learning, depending on the form of the bootstrap (Peng & Williams, 1996)."}, {"heading": "3.4 BELLMAN RESIDUAL", "text": "In this section we show that \u2016T ?Q\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 \u2192 0 with decreasing regularization penalty \u03b1, where \u03c0\u03b1 is the policy defined by (4) and Q\u03c0\u03b1 is the corresponding Q-value function, both of which are functions of \u03b1. We shall show that it converges to zero by bounding the sequence below by zero and above with a sequence that converges to zero. First, we have that T ?Q\u03c0\u03b1 \u2265 T \u03c0\u03b1Q\u03c0\u03b1 = Q\u03c0\u03b1 , since T ? is greedy with respect to the Q-values. So T ?Q\u03c0\u03b1 \u2212Q\u03c0\u03b1 \u2265 0. Now, to bound from above we need the fact that \u03c0\u03b1(s, a) = exp(Q\u03c0\u03b1(s, a)/\u03b1)/ \u2211 b exp(Q\n\u03c0\u03b1(s, b)/\u03b1) \u2264 exp((Q\u03c0\u03b1(s, a) \u2212 maxcQ \u03c0\u03b1(s, c))/\u03b1). Using this we have\n0 \u2264 T ?Q\u03c0\u03b1(s, a)\u2212Q\u03c0\u03b1(s, a) = T ?Q\u03c0\u03b1(s, a)\u2212 T \u03c0\u03b1Q\u03c0\u03b1(s, a) = Es\u2032(maxcQ \u03c0\u03b1(s\u2032, c)\u2212 \u2211 b \u03c0\u03b1(s\n\u2032, b)Q\u03c0\u03b1(s\u2032, b)) = Es\u2032 \u2211 b \u03c0\u03b1(s \u2032, b)(maxcQ \u03c0\u03b1(s\u2032, c)\u2212Q\u03c0\u03b1(s\u2032, b))\n\u2264 Es\u2032 \u2211 b exp((Q\n\u03c0\u03b1(s\u2032, b)\u2212Q\u03c0\u03b1(s\u2032, b?))/\u03b1)(maxcQ\u03c0\u03b1(s\u2032, c)\u2212Q\u03c0\u03b1(s\u2032, b)) = Es\u2032 \u2211 b f\u03b1(maxcQ \u03c0\u03b1(s\u2032, c)\u2212Q\u03c0\u03b1(s\u2032, b)),\nwhere we define f\u03b1(x) = x exp(\u2212x/\u03b1). To conclude our proof we use the fact that f\u03b1(x) \u2264 supx f\u03b1(x) = f\u03b1(\u03b1) = \u03b1e \u22121, which yields\n0 \u2264 T ?Q\u03c0\u03b1(s, a)\u2212Q\u03c0\u03b1(s, a) \u2264 |A|\u03b1e\u22121\nfor all (s, a), and so the Bellman residual converges to zero with decreasing \u03b1. In other words, for small enough \u03b1 (which is the regime we are interested in) the Q-values induced by the policy (4) will have a small Bellman residual. Moreover, this implies that lim\u03b1\u21920Q\u03c0\u03b1 = Q?, as one might expect."}, {"heading": "4 PGQL", "text": "In this section we introduce the main contribution of the paper, which is a technique to combine policy gradient with Q-learning. We call our technique \u2018PGQL\u2019, for policy gradient and Q-learning. In the previous section we showed that the Bellman residual is small at the fixed point of a regularized\npolicy gradient algorithm when the regularization penalty is sufficiently small. This suggests adding an auxiliary update where we explicitly attempt to reduce the Bellman residual as estimated from the policy, i.e., a hybrid between policy gradient and Q-learning.\nWe first present the technique in a batch update setting, with a perfect knowledge of Q\u03c0 (i.e., a perfect critic). Later we discuss the practical implementation of the technique in a reinforcement learning setting with function approximation, where the agent generates experience from interacting with the environment and needs to estimate a critic simultaneously with the policy."}, {"heading": "4.1 PGQL UPDATE", "text": "Define the estimate of Q using the policy as\nQ\u0303\u03c0(s, a) = \u03b1(log \u03c0(s, a) +H\u03c0(s)) + V (s), (12)\nwhere V has parameters w and is not necessarily V \u03c0 as it was in equation (5). In (2) it was unnecessary to estimate the constant since the update was invariant to constant offsets, although in practice it is often estimated for use in a variance reduction technique (Williams, 1992; Sutton et al., 1999).\nSince we know that at the fixed point the Bellman residual will be small for small \u03b1, we can consider updating the parameters to reduce the Bellman residual in a fashion similar to Q-learning, i.e.,\n\u2206\u03b8 \u221d E s,a (T ?Q\u0303\u03c0(s, a)\u2212 Q\u0303\u03c0(s, a))\u2207\u03b8 log \u03c0(s, a), \u2206w \u221d E s,a (T ?Q\u0303\u03c0(s, a)\u2212 Q\u0303\u03c0(s, a))\u2207wV (s). (13) This is Q-learning applied to a particular form of the Q-values, and can also be interpreted as an actor-critic algorithm with an optimizing (and therefore biased) critic.\nThe full scheme simply combines two updates to the policy, the regularized policy gradient update (2) and the Q-learning update (13). Assuming we have an architecture that provides a policy \u03c0, a value function estimate V , and an action-value critic Q\u03c0 , then the parameter updates can be written as (suppressing the (s, a) notation)\n\u2206\u03b8 \u221d (1\u2212 \u03b7)Es,a(Q\u03c0 \u2212 Q\u0303\u03c0)\u2207\u03b8 log \u03c0 + \u03b7Es,a(T ?Q\u0303\u03c0 \u2212 Q\u0303\u03c0)\u2207\u03b8 log \u03c0,\n\u2206w \u221d (1\u2212 \u03b7)Es,a(Q\u03c0 \u2212 Q\u0303\u03c0)\u2207wV + \u03b7Es,a(T ?Q\u0303\u03c0 \u2212 Q\u0303\u03c0)\u2207wV, (14)\nhere \u03b7 \u2208 [0, 1] is a weighting parameter that controls how much of each update we apply. In the case where \u03b7 = 0 the above scheme reduces to entropy regularized policy gradient. If \u03b7 = 1 then it becomes a variant of (batch) Q-learning with an architecture similar to the dueling architecture (Wang et al., 2016). Intermediate values of \u03b7 produce a hybrid between the two. Examining the update we see that two error terms are trading off. The first term encourages consistency with critic, and the second term encourages optimality over time. However, since we know that under standard policy gradient the Bellman residual will be small, then it follows that adding a term that reduces that error should not make much difference at the fixed point. That is, the updates should be complementary, pointing in the same general direction, at least far away from a fixed point. This update can also be interpreted as an actor-critic update where the critic is given by a weighted combination of a standard critic and an optimizing critic. Yet another interpretation of the update is a combination of expected-SARSA and Q-learning, where the Q-values are parameterized as the sum of an advantage function and a value function."}, {"heading": "4.2 PRACTICAL IMPLEMENTATION", "text": "The updates presented in (14) are batch updates, with an exact critic Q\u03c0 . In practice we want to run this scheme online, with an estimate of the critic, where we don\u2019t necessarily apply the policy gradient update at the same time or from same data source as the Q-learning update.\nOur proposal scheme is as follows. One or more agents interact with an environment, encountering states and rewards and performing on-policy updates of (shared) parameters using an actor-critic algorithm where both the policy and the critic are being updated online. Each time an agent receives new data from the environment it writes it to a shared replay memory buffer. Periodically a separate learner process samples from the replay buffer and performs a step of Q-learning on the parameters of the policy using (13). This scheme has several advantages. The critic can accumulate the Monte\nCarlo return over many time periods, allowing us to spread the influence of a reward received in the future backwards in time. Furthermore, the replay buffer can be used to store and replay \u2018important\u2019 past experiences by prioritizing those samples (Schaul et al., 2015). The use of the replay buffer can help to reduce problems associated with correlated training data, as generated by an agent exploring an environment where the states are likely to be similar from one time step to the next. Also the use of replay can act as a kind of regularizer, preventing the policy from moving too far from satisfying the Bellman equation, thereby improving stability, in a similar sense to that of a policy \u2018trust-region\u2019 (Schulman et al., 2015). Moreover, by batching up replay samples to update the network we can leverage GPUs to perform the updates quickly, this is in comparison to pure policy gradient techniques which are generally implemented on CPU (Mnih et al., 2016).\nSince we perform Q-learning using samples from a replay buffer that were generated by a old policy we are performing (slightly) off-policy learning. However, Q-learning is known to converge to the optimal Q-values in the off-policy tabular case (under certain conditions) (Sutton & Barto, 1998), and has shown good performance off-policy in the function approximation case (Mnih et al., 2013)."}, {"heading": "4.3 MODIFIED FIXED POINT", "text": "The PGQL updates in equation (14) have modified the fixed point of the algorithm, so the analysis of \u00a73 is no longer valid. Considering the tabular case once again, it is still the case that the policy \u03c0 \u221d exp(Q\u0303\u03c0/\u03b1) as before, where Q\u0303\u03c0 is defined by (12), however where previously the fixed point satisfied Q\u0303\u03c0 = Q\u03c0 , with Q\u03c0 corresponding to the Q-values induced by \u03c0, now we have\nQ\u0303\u03c0 = (1\u2212 \u03b7)Q\u03c0 + \u03b7T ?Q\u0303\u03c0, (15) Or equivalently, if \u03b7 < 1, we have Q\u0303\u03c0 = (1\u2212 \u03b7) \u2211\u221e k=0 \u03b7\nk(T ?)kQ\u03c0 . In the appendix we show that \u2016Q\u0303\u03c0 \u2212 Q\u03c0\u2016 \u2192 0 and that \u2016T ?Q\u03c0 \u2212 Q\u03c0\u2016 \u2192 0 with decreasing \u03b1 in the tabular case. That is, for small \u03b1 the induced Q-values and the Q-values estimated from the policy are close, and we still have the guarantee that in the limit the Q-values are optimal. In other words, we have not perturbed the policy very much by the addition of the auxiliary update."}, {"heading": "5 NUMERICAL EXPERIMENTS", "text": ""}, {"heading": "5.1 GRID WORLD", "text": "In this section we discuss the results of running PGQL on a toy 4 by 6 grid world, as shown in Figure 1a. The agent always begins in the square marked \u2018S\u2019 and the episode continues until it reaches the square marked \u2018T\u2019, upon which it receives a reward of 1. All other times it receives no reward. For this experiment we chose regularization parameter \u03b1 = 0.001 and discount factor \u03b3 = 0.95.\nFigure 1b shows the performance traces of three different agents learning in the grid world, running from the same initial random seed. The lines show the true expected performance of the policy\nfrom the start state, as calculated by value iteration after each update. The blue-line is standard TD-actor-critic (Konda & Tsitsiklis, 2003), where we maintain an estimate of the value function and use that to generate an estimate of the Q-values for use as the critic. The green line is Q-learning where at each step an update is performed using data drawn from a replay buffer of prior experience and where the Q-values are parameterized as in equation (12). The policy is a softmax over the Q-value estimates with temperature \u03b1. The red line is PGQL, which at each step first performs the TD-actor-critic update, then performs the Q-learning update as in (14).\nThe grid world was totally deterministic, so the step size could be large and was chosen to be 1. A step-size any larger than this made the pure actor-critic agent fail to learn, but both PGQL and Q-learning could handle some increase in the step-size, possibly due to the stabilizing effect of using replay.\nIt is clear that PGQL outperforms the other two. At any point along the x-axis the agents have seen the same amount of data, which would indicate that PGQL is more data efficient than either of the vanilla methods since it has the highest performance at practically every point."}, {"heading": "5.2 ATARI", "text": "We tested our algorithm on the full suite of Atari benchmarks (Bellemare et al., 2012), using a neural network to parameterize the policy. In figure 2 we show how a policy network can be augmented with a parameterless additional layer which outputs the Q-value estimate. With the exception of the extra layer, the architecture and parameters were chosen to exactly match the asynchronous advantage actor-critic (A3C) algorithm presented in Mnih et al. (2016), which in turn reused many of the settings from Mnih et al. (2015). Specifically we used the exact same learning rate, number of workers, entropy penalty, bootstrap horizon, and network architecture. This allows a fair comparison between A3C and PGQL, since the only difference is the addition of the Q-learning step. Our technique augmented A3C with the following change: After each actor-learner has accumulated the gradient for the policy update, it performs a single step of Q-learning from replay data as described in equation (13), where the minibatch size was 32 and the Q-learning learning rate was chosen to be 0.5 times the actor-critic learning rate (we mention learning rate ratios rather than choice of \u03b7 in (14) because the updates happen at different frequencies and from different data sources). Each actor-learner thread maintained a replay buffer of the last 100k transitions seen by that thread. We ran the learning for 50 million agent steps (200 million Atari frames), as in (Mnih et al., 2016).\nIn the results we compare against both A3C and a variant of asynchronous deep Q-learning. The changes we made to Q-learning are to make it similar to our method, with some tuning of the hyperparameters for performance. We use the exact same network, the exploration policy is a softmax over the Q-values with a temperature of 0.1, and the Q-values are parameterized as in equation (12) (i.e., similar to the dueling architecture (Wang et al., 2016)), where \u03b1 = 0.1. The Q-value updates are performed every 4 steps with a minibatch of 32 (roughly 5 times more frequently than PGQL). For each method, all games used identical hyper-parameters.\nThe results across all games are given in table 3 in the appendix. All scores have been normalized by subtracting the average score achieved by an agent that takes actions uniformly at random. Each game was tested 5 times per method with the same hyper-parameters but with different ran-\ndom seeds. The scores presented correspond to the best score obtained by any run from a random start evaluation condition (Mnih et al., 2016). Overall, PGQL performed best in 34 games, A3C performed best in 7 games, and Q-learning was best in 10 games. In 6 games two or more methods tied. In tables 1 and 2 we give the mean and median normalized scores as percentage of an expert human normalized score across all games for each tested algorithm from random and human-start conditions respectively. In a human-start condition the agent takes over control of the game from randomly selected human-play starting points, which generally leads to lower performance since the agent may not have found itself in that state during training. In both cases, PGQL has both the highest mean and median, and the median score exceeds 100%, the human performance threshold.\nIt is worth noting that PGQL was the worst performer in only one game, in cases where it was not the outright winner it was generally somewhere in between the performance of the other two algorithms. Figure 3 shows some sample traces of games where PGQL was the best performer. In these cases PGQL has far better data efficiency than the other methods. In figure 4 we show some of the games where PGQL under-performed. In practically every case where PGQL did not perform well it had better data efficiency early on in the learning, but performance saturated or collapsed. We hypothesize that in these cases the policy has reached a local optimum, or over-fit to the early data, and might perform better were the hyper-parameters to be tuned."}, {"heading": "6 CONCLUSIONS", "text": "We have made a connection between the fixed point of regularized policy gradient techniques and the Q-values of the resulting policy. For small regularization (the usual case) we have shown that the Bellman residual of the induced Q-values must be small. This leads us to consider adding an auxiliary update to the policy gradient which is related to the Bellman residual evaluated on a transformation of the policy. This update can be performed off-policy, using stored experience. We call the resulting method \u2018PGQL\u2019, for policy gradient and Q-learning. Empirically, we observe better data efficiency and stability of PGQL when compared to actor-critic or Q-learning alone. We verified the performance of PGQL on a suite of Atari games, where we parameterize the policy using a neural network, and achieved performance exceeding that of both A3C and Q-learning."}, {"heading": "7 ACKNOWLEDGMENTS", "text": "We thank Joseph Modayil for many comments and suggestions on the paper, and Hubert Soyer for help with performance evaluation. We would also like to thank the anonymous reviewers for their constructive feedback."}, {"heading": "A PGQL BELLMAN RESIDUAL", "text": "Here we demonstrate that in the tabular case the Bellman residual of the induced Q-values for the PGQL updates of (14) converges to zero as the temperature \u03b1 decreases, which is the same guarantee as vanilla regularized policy gradient (2). We will use the notation that \u03c0\u03b1 is the policy at the fixed point of PGQL updates (14) for some \u03b1, i.e., \u03c0\u03b1 \u221d exp(Q\u0303\u03c0\u03b1), with induced Q-value function Q\u03c0\u03b1 .\nFirst, note that we can apply the same argument as in \u00a73.4 to show that lim\u03b1\u21920 \u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016 = 0 (the only difference is that we lack the property that Q\u0303\u03c0\u03b1 is the fixed point of T \u03c0\u03b1 ). Secondly, from equation (15) we can write Q\u0303\u03c0\u03b1 \u2212 Q\u03c0\u03b1 = \u03b7(T ?Q\u0303\u03c0\u03b1 \u2212 Q\u03c0\u03b1). Combining these two facts we have\n\u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 = \u03b7\u2016T ?Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 = \u03b7\u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1 + T \u03c0\u03b1Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 \u2264 \u03b7(\u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016+ \u2016T \u03c0\u03b1Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u03c0\u03b1\u2016) \u2264 \u03b7(\u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016+ \u03b3\u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016) \u2264 \u03b7/(1\u2212 \u03b7\u03b3)\u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016,\nand so \u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 \u2192 0 as \u03b1\u2192 0. Using this fact we have\n\u2016T ?Q\u0303\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1\u2016 = \u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1 + T \u03c0\u03b1Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1 +Q\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1\u2016 \u2264 \u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016+ \u2016T \u03c0\u03b1Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u03c0\u03b1\u2016+ \u2016Q\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1\u2016 \u2264 \u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016+ (1 + \u03b3)\u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 < 3/(1\u2212 \u03b7\u03b3)\u2016T ?Q\u0303\u03c0\u03b1 \u2212 T \u03c0\u03b1Q\u0303\u03c0\u03b1\u2016,\nwhich therefore also converges to zero in the limit. Finally we obtain\n\u2016T ?Q\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 = \u2016T ?Q\u03c0\u03b1 \u2212 T ?Q\u0303\u03c0\u03b1 + T ?Q\u0303\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1 + Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 \u2264 \u2016T ?Q\u03c0\u03b1 \u2212 T ?Q\u0303\u03c0\u03b1\u2016+ \u2016T ?Q\u0303\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1\u2016+ \u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016 \u2264 (1 + \u03b3)\u2016Q\u0303\u03c0\u03b1 \u2212Q\u03c0\u03b1\u2016+ \u2016T ?Q\u0303\u03c0\u03b1 \u2212 Q\u0303\u03c0\u03b1\u2016,\nwhich combined with the two previous results implies that lim\u03b1\u21920 \u2016T ?Q\u03c0\u03b1\u2212Q\u03c0\u03b1\u2016 = 0, as before."}, {"heading": "B ATARI SCORES", "text": ""}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Dynamic policy programming", "author": ["Mohammad Gheshlaghi Azar", "Vicen\u00e7 G\u00f3mez", "Hilbert J Kappen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Azar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2012}, {"title": "Covariant policy search", "author": ["J Andrew Bagnell", "Jeff Schneider"], "venue": "In IJCAI,", "citeRegEx": "Bagnell and Schneider.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell and Schneider.", "year": 2003}, {"title": "Advantage updating", "author": ["Leemon C Baird III"], "venue": "Technical Report WL-TR-93-1146, Wright-Patterson Air Force Base Ohio: Wright Laboratory,", "citeRegEx": "III.,? \\Q1993\\E", "shortCiteRegEx": "III.", "year": 1993}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Dynamic programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas.", "year": 2005}, {"title": "Off-policy actor-critic", "author": ["Thomas Degris", "Martha White", "Richard S Sutton"], "venue": null, "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1207.4708,", "citeRegEx": "Fox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2015}, {"title": "On-policy vs. off-policy updates for deep reinforcement learning. Deep Reinforcement Learning: Frontiers and Challenges", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": null, "citeRegEx": "Hausknecht and Stone.,? \\Q2016\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2016}, {"title": "Actor-critic reinforcement learning with energybased policies", "author": ["Nicolas Heess", "David Silver", "Yee Whye Teh"], "venue": "In JMLR: Workshop and Conference Proceedings", "citeRegEx": "Heess et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2012}, {"title": "A natural policy gradient", "author": ["Sham Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade.,? \\Q2001\\E", "shortCiteRegEx": "Kakade.", "year": 2001}, {"title": "On actor-critic algorithms", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Konda and Tsitsiklis.,? \\Q2003\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 2003}, {"title": "Policy gradient methods for off-policy control", "author": ["Lucas Lehnert", "Doina Precup"], "venue": "arXiv preprint arXiv:1512.04105,", "citeRegEx": "Lehnert and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Lehnert and Precup.", "year": 2015}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Levine and Koltun.,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun.", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Long-Ji Lin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin.,? \\Q1993\\E", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu", "Dale Schuurmans"], "venue": "arXiv preprint arXiv:1609.00150,", "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu and Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu and Bengio.", "year": 2013}, {"title": "Sequential cost-sensitive decision making with reinforcement learning", "author": ["Edwin Pednault", "Naoki Abe", "Bianca Zadrozny"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Pednault et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pednault et al\\.", "year": 2002}, {"title": "Incremental multi-step Q-learning", "author": ["Jing Peng", "Ronald J Williams"], "venue": "Machine Learning,", "citeRegEx": "Peng and Williams.,? \\Q1996\\E", "shortCiteRegEx": "Peng and Williams.", "year": 1996}, {"title": "Relative entropy policy search", "author": ["Jan Peters", "Katharina M\u00fclling", "Yasemin Altun"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "On-line Q-learning using connectionist systems", "author": ["Gavin A Rummery", "Mahesan Niranjan"], "venue": null, "citeRegEx": "Rummery and Niranjan.,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan.", "year": 1994}, {"title": "Reinforcement learning with factored states and actions", "author": ["Brian Sallans", "Geoffrey E Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sallans and Hinton.,? \\Q2004\\E", "shortCiteRegEx": "Sallans and Hinton.", "year": 2004}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael Jordan", "Philipp Moritz"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reinforcement Learning: an Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal difference learning and TD-Gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Bias in natural actor-critic algorithms", "author": ["Philip Thomas"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Thomas.,? \\Q2014\\E", "shortCiteRegEx": "Thomas.", "year": 2014}, {"title": "Deep reinforcement learning with double Qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16),", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["Harm Van Seijen", "Hado Van Hasselt", "Shimon Whiteson", "Marco Wiering"], "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Backward q-learning: The combination of sarsa algorithm and q-learning", "author": ["Yin-Hao Wang", "Tzuu-Hseng S Li", "Chih-Jui Lin"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}], "referenceMentions": [{"referenceID": 17, "context": "Reinforcement learning has seen success in several areas including robotics (Lin, 1993; Levine et al., 2015), computer games (Mnih et al.", "startOffset": 76, "endOffset": 108}, {"referenceID": 15, "context": "Reinforcement learning has seen success in several areas including robotics (Lin, 1993; Levine et al., 2015), computer games (Mnih et al.", "startOffset": 76, "endOffset": 108}, {"referenceID": 18, "context": ", 2015), computer games (Mnih et al., 2013; 2015), online advertising (Pednault et al.", "startOffset": 24, "endOffset": 49}, {"referenceID": 23, "context": ", 2013; 2015), online advertising (Pednault et al., 2002), board games (Tesauro, 1995; Silver et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 36, "context": ", 2002), board games (Tesauro, 1995; Silver et al., 2016), and many others.", "startOffset": 21, "endOffset": 57}, {"referenceID": 32, "context": ", 2002), board games (Tesauro, 1995; Silver et al., 2016), and many others.", "startOffset": 21, "endOffset": 57}, {"referenceID": 42, "context": "Two alternatives we discuss in this paper are SARSA (Rummery & Niranjan, 1994) and Q-learning (Watkins, 1989), although there are many others.", "startOffset": 94, "endOffset": 109}, {"referenceID": 34, "context": "Under certain conditions both SARSA and Q-learning can be shown to converge to the optimal Q-values, from which we can derive the optimal policy (Sutton, 1988; Bertsekas & Tsitsiklis, 1996).", "startOffset": 145, "endOffset": 189}, {"referenceID": 35, "context": "In policy gradient techniques the policy is represented explicitly and we improve the policy by updating the parameters in the direction of the gradient of the performance (Sutton et al., 1999; Silver et al., 2014; Kakade, 2001).", "startOffset": 172, "endOffset": 228}, {"referenceID": 31, "context": "In policy gradient techniques the policy is represented explicitly and we improve the policy by updating the parameters in the direction of the gradient of the performance (Sutton et al., 1999; Silver et al., 2014; Kakade, 2001).", "startOffset": 172, "endOffset": 228}, {"referenceID": 11, "context": "In policy gradient techniques the policy is represented explicitly and we improve the policy by updating the parameters in the direction of the gradient of the performance (Sutton et al., 1999; Silver et al., 2014; Kakade, 2001).", "startOffset": 172, "endOffset": 228}, {"referenceID": 7, "context": "Vanilla actor-critic methods are on-policy only, although some attempts have been made to extend them to off-policy data (Degris et al., 2012; Levine & Koltun, 2013).", "startOffset": 121, "endOffset": 165}, {"referenceID": 20, "context": "In this paper we use entropy regularization to ensure exploration in the policy, which is a common practice in policy gradient (Williams & Peng, 1991; Mnih et al., 2016).", "startOffset": 127, "endOffset": 169}, {"referenceID": 25, "context": "An alternative is to use KL-divergence instead of entropy as a regularizer, or as a constraint on how much deviation is permitted from a prior policy (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015).", "startOffset": 150, "endOffset": 239}, {"referenceID": 30, "context": "An alternative is to use KL-divergence instead of entropy as a regularizer, or as a constraint on how much deviation is permitted from a prior policy (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015).", "startOffset": 150, "endOffset": 239}, {"referenceID": 8, "context": "An alternative is to use KL-divergence instead of entropy as a regularizer, or as a constraint on how much deviation is permitted from a prior policy (Bagnell & Schneider, 2003; Peters et al., 2010; Schulman et al., 2015; Fox et al., 2015).", "startOffset": 150, "endOffset": 239}, {"referenceID": 0, "context": "Natural policy gradient can also be interpreted as putting a constraint on the KL-divergence at each step of the policy improvement (Amari, 1998; Kakade, 2001; Pascanu & Bengio, 2013).", "startOffset": 132, "endOffset": 183}, {"referenceID": 11, "context": "Natural policy gradient can also be interpreted as putting a constraint on the KL-divergence at each step of the policy improvement (Amari, 1998; Kakade, 2001; Pascanu & Bengio, 2013).", "startOffset": 132, "endOffset": 183}, {"referenceID": 40, "context": "Other works have attempted to combine on and off-policy learning, primarily using action-value fitting methods (Wang et al., 2013; Hausknecht & Stone, 2016; Lehnert & Precup, 2015), with varying degrees of success.", "startOffset": 111, "endOffset": 180}, {"referenceID": 41, "context": "The algorithm we develop extends actor-critic with a Q-learning style update that, due to the decomposition of the Q-values, resembles the update of the dueling architecture (Wang et al., 2016).", "startOffset": 174, "endOffset": 193}, {"referenceID": 19, "context": ", the use of deep neural networks to represent action-values or a policy, has seen a lot of success (Mnih et al., 2015; 2016; Silver et al., 2016; Riedmiller, 2005; Lillicrap et al., 2015; Van Hasselt et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 32, "context": ", the use of deep neural networks to represent action-values or a policy, has seen a lot of success (Mnih et al., 2015; 2016; Silver et al., 2016; Riedmiller, 2005; Lillicrap et al., 2015; Van Hasselt et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 26, "context": ", the use of deep neural networks to represent action-values or a policy, has seen a lot of success (Mnih et al., 2015; 2016; Silver et al., 2016; Riedmiller, 2005; Lillicrap et al., 2015; Van Hasselt et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 16, "context": ", the use of deep neural networks to represent action-values or a policy, has seen a lot of success (Mnih et al., 2015; 2016; Silver et al., 2016; Riedmiller, 2005; Lillicrap et al., 2015; Van Hasselt et al., 2016).", "startOffset": 100, "endOffset": 214}, {"referenceID": 5, "context": "The Bellman operator T \u03c0 (Bellman, 1957) for policy \u03c0 is defined as T Q(s, a) = E s\u2032,r,b (r(s, a) + \u03b3Q(s\u2032, b)), where the expectation is over next state s\u2032 \u223c P (\u00b7, s, a), the reward r(s, a), and the action b from policy \u03c0s\u2032 .", "startOffset": 25, "endOffset": 40}, {"referenceID": 35, "context": "The policy gradient theorem (Sutton et al., 1999) states that the gradient of J with respect to the parameters of the policy is given by \u2207\u03b8J(\u03c0) = E s,a Q(s, a)\u2207\u03b8 log \u03c0(s, a), (1)", "startOffset": 28, "endOffset": 49}, {"referenceID": 37, "context": ", non-discounted) policy performance, even whenQ uses a discount factor (Thomas, 2014).", "startOffset": 72, "endOffset": 86}, {"referenceID": 43, "context": ", by the sum of discounted rewards along an observed trajectory (Williams, 1992), and the policy gradient will still perform well (Konda & Tsitsiklis, 2003).", "startOffset": 64, "endOffset": 80}, {"referenceID": 41, "context": "Compare this to the case where an agent is learning Q-values with a dueling architecture (Wang et al., 2016), which at iteration k is given by Q(s, a) = Y (s, a)\u2212 \u2211", "startOffset": 89, "endOffset": 108}, {"referenceID": 42, "context": "In Q-learning (Watkins, 1989) \u03b4av = r(s, a) + \u03b3maxbQ(s, b)\u2212Q(s, a), which would be equivalent to using an optimizing critic that bootstraps using the max Q-value at the next state, i.", "startOffset": 14, "endOffset": 29}, {"referenceID": 43, "context": "In (2) it was unnecessary to estimate the constant since the update was invariant to constant offsets, although in practice it is often estimated for use in a variance reduction technique (Williams, 1992; Sutton et al., 1999).", "startOffset": 188, "endOffset": 225}, {"referenceID": 35, "context": "In (2) it was unnecessary to estimate the constant since the update was invariant to constant offsets, although in practice it is often estimated for use in a variance reduction technique (Williams, 1992; Sutton et al., 1999).", "startOffset": 188, "endOffset": 225}, {"referenceID": 41, "context": "If \u03b7 = 1 then it becomes a variant of (batch) Q-learning with an architecture similar to the dueling architecture (Wang et al., 2016).", "startOffset": 114, "endOffset": 133}, {"referenceID": 29, "context": "Furthermore, the replay buffer can be used to store and replay \u2018important\u2019 past experiences by prioritizing those samples (Schaul et al., 2015).", "startOffset": 122, "endOffset": 143}, {"referenceID": 30, "context": "Also the use of replay can act as a kind of regularizer, preventing the policy from moving too far from satisfying the Bellman equation, thereby improving stability, in a similar sense to that of a policy \u2018trust-region\u2019 (Schulman et al., 2015).", "startOffset": 220, "endOffset": 243}, {"referenceID": 20, "context": "Moreover, by batching up replay samples to update the network we can leverage GPUs to perform the updates quickly, this is in comparison to pure policy gradient techniques which are generally implemented on CPU (Mnih et al., 2016).", "startOffset": 211, "endOffset": 230}, {"referenceID": 18, "context": "However, Q-learning is known to converge to the optimal Q-values in the off-policy tabular case (under certain conditions) (Sutton & Barto, 1998), and has shown good performance off-policy in the function approximation case (Mnih et al., 2013).", "startOffset": 224, "endOffset": 243}, {"referenceID": 4, "context": "We tested our algorithm on the full suite of Atari benchmarks (Bellemare et al., 2012), using a neural network to parameterize the policy.", "startOffset": 62, "endOffset": 86}, {"referenceID": 20, "context": "We ran the learning for 50 million agent steps (200 million Atari frames), as in (Mnih et al., 2016).", "startOffset": 81, "endOffset": 100}, {"referenceID": 41, "context": ", similar to the dueling architecture (Wang et al., 2016)), where \u03b1 = 0.", "startOffset": 38, "endOffset": 57}, {"referenceID": 20, "context": "The scores presented correspond to the best score obtained by any run from a random start evaluation condition (Mnih et al., 2016).", "startOffset": 111, "endOffset": 130}], "year": 2017, "abstractText": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_130"}