{"name": "ICLR_2017_167.pdf", "metadata": {"source": "CRF", "title": "STICK-BREAKING VARIATIONAL AUTOENCODERS", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "emails": ["enalisni@uci.edu", "smyth@ics.uci.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep generative models trained via Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014a; Rezende et al., 2014) efficiently couple the expressiveness of deep neural networks with the robustness to uncertainty of probabilistic latent variables. This combination has lead to their success in tasks ranging from image generation (Gregor et al., 2015; Rezende et al., 2016) to semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) to language modeling (Bowman et al., 2016). Various extensions to SGVB have been proposed (Burda et al., 2016; Maal\u00f8e et al., 2016; Salimans et al., 2015), but one conspicuous absence is an extension to Bayesian nonparametric processes. Using SGVB to perform inference for nonparametric distributions is quite attractive. For instance, SGVB allows for a broad class of non-conjugate approximate posteriors and thus has the potential to expand Bayesian nonparametric models beyond the exponential family distributions to which they are usually confined. Moreover, coupling nonparametric processes with neural network inference models equips the networks with automatic model selection properties such as a selfdetermined width, which we explore in this paper.\nWe make progress on this problem by first describing how to use SGVB for posterior inference for the weights of Stick-Breaking processes (Ishwaran & James, 2001). This is not a straightforward task as the Beta distribution, the natural choice for an approximate posterior, does not have the differentiable non-centered parametrization that SGVB requires. We bypass this obstacle by using the little-known Kumaraswamy distribution (Kumaraswamy, 1980).\nUsing the Kumaraswamy as an approximate posterior, we then reformulate two popular deep generative models\u2014the Variational Autoencoder (Kingma & Welling, 2014a) and its semi-supervised variant (model M2 proposed by Kingma et al. (2014))\u2014into their nonparametric analogs. These models perform automatic model selection via an infinite capacity hidden layer that employs as many stick segments (latent variables) as the data requires. We experimentally show that, for datasets of natural images, stick-breaking priors improve upon previously proposed deep generative models by having a latent representation that better preserves class boundaries and provides beneficial regularization for semi-supervised learning."}, {"heading": "2 BACKGROUND", "text": "We begin by reviewing the relevant background material on Variational Autoencoders (Kingma & Welling, 2014a), Stochastic Gradient Variational Bayes (also known as Stochastic Backpropagation) (Kingma & Welling, 2014a; Rezende et al., 2014), and Stick-Breaking Processes (Ishwaran & James, 2001)."}, {"heading": "2.1 VARIATIONAL AUTOENCODERS", "text": "A Variational Autoencoder (VAE) is model comprised of two multilayer perceptrons: one acts as a density network (MacKay & Gibbs, 1999) mapping a latent variable zi to an observed datapoint xi, and the other acts as an inference model (Salimans & Knowles, 2013) performing the reverse mapping from xi to zi. Together the two form a computational pipeline that resembles an unsupervised autoencoder (Hinton & Salakhutdinov, 2006). The generative process can be written mathematically as\nzi \u223c p(z) , xi \u223c p\u03b8(x|zi) (1) where p(z) is the prior and p\u03b8(x|zi) is the density network with parameters \u03b8. The approximate posterior of this generative process, call it q\u03c6(z|xi), is then parametrized by the inference network (with parameters \u03c6). In previous work (Kingma & Welling, 2014a; Rezende & Mohamed, 2015; Burda et al., 2016; Li & Turner, 2016), the prior p(z) and variational posterior have been marginally Gaussian."}, {"heading": "2.2 STOCHASTIC GRADIENT VARIATIONAL BAYES", "text": "The VAE\u2019s generative and variational parameters are estimated by Stochastic Gradient Variational Bayes (SGVB). SGVB is distinguished from classical variational Bayes by it\u2019s use of differentiable Monte Carlo (MC) expectations. To elaborate, consider SGVB\u2019s approximation of the usual evidence lowerbound (ELBO) (Jordan et al., 1999):\nL\u0303(\u03b8,\u03c6;xi) = 1\nS S\u2211 s=1 log p\u03b8(xi|z\u0302i,s)\u2212KL(q\u03c6(zi|xi)||p(z)) (2)\nfor S samples of zi and where KL is the Kullback-Leibler divergence. An essential requirement of SGVB is that the latent variable be represented in a differentiable, non-centered parametrization (DNCP) (Kingma & Welling, 2014b); this is what allows the gradients to be taken through the MC expectation, i.e.:\n\u2202\n\u2202\u03c6 S\u2211 s=1 log p\u03b8(xi|z\u0302i,s) = S\u2211 s=1 \u2202 \u2202z\u0302i,s log p\u03b8(xi|z\u0302i,s) \u2202z\u0302i,s \u2202\u03c6 .\nIn other words, z must have a functional form that deterministically exposes the variational distribution\u2019s parameters and allows the randomness to come from draws from some fixed distribution. Location-scale representations and inverse cumulative distribution functions are two examples of DNCPs. For instance, the VAE\u2019s Gaussian latent variable (with diagonal covariance matrix) is represented as z\u0302i = \u00b5 + \u03c3 where \u223c N(0,1)."}, {"heading": "2.3 STICK-BREAKING PROCESSES", "text": "Lastly, we define stick-breaking processes with the ultimate goal of using their weights for the VAE\u2019s prior p(z). A random measure is referred to as a stick-breaking prior (SBP) (Ishwaran & James, 2001) if it is of the form G(\u00b7) = \u2211\u221e k=1 \u03c0k\u03b4\u03b6k where \u03b4\u03b6k is a discrete measure concentrated at \u03b6k \u223c G0, a draw from the base distribution G0 (Ishwaran & James, 2001). The \u03c0ks are random weights independent of G0, chosen such that 0 \u2264 \u03c0k \u2264 1, and \u2211 k \u03c0k = 1 almost surely. SBPs have been termed as such because of their constructive definition known as the stick-breaking process (Sethuraman, 1994). Mathematically, this definition implies that the weights can be drawn according to the following iterative procedure:\n\u03c0k = { v1 if k = 1 vk \u220f j<k(1\u2212 vj) for k > 1\n(3)\nwhere vk \u223c Beta(\u03b1, \u03b2). When vk \u223c Beta(1, \u03b10), then we have the stick-breaking construction for the Dirichlet Process (Ferguson, 1973). In this case, the name for the joint distribution over the infinite sequence of stick-breaking weights is the Griffiths, Engen and McCloskey distribution with concentration parameter \u03b10 (Pitman, 2002): (\u03c01, \u03c02, . . .) \u223c GEM(\u03b10)."}, {"heading": "3 SGVB FOR GEM RANDOM VARIABLES", "text": "Having covered the relevant background material, we now discuss the first contribution of this paper, using Stochastic Gradient Variational Bayes for the weights of a stick-breaking process. Inference for the random measure G(\u00b7) is an open problem that we leave to future work. We focus on performing inference for just the series of stick-breaking weights, which we will refer to as GEM random variables after their joint distribution."}, {"heading": "3.1 COMPOSITION OF GAMMA RANDOM VARIABLES", "text": "In the original SGVB paper, Kingma & Welling (2014a) suggest representing the Beta distribution as a composition of Gamma random variables by using the fact v \u223c Beta(\u03b1, \u03b2) can be sampled by drawing Gamma variables x \u223c Gamma(\u03b1, 1), y \u223c Gamma(\u03b2, 1) and composing them as v = x/(x + y). However, this representation still does not admit a DNCP as the Gamma distribution does not have one with respect to its shape parameter. Knowles (2015) suggests that when the shape parameter is near zero, the following asymptotic approximation of the inverse CDF is a suitable DNCP:\nF\u22121(u\u0302) \u2248 (u\u0302a\u0393(a)) 1 a\nb (4)\nfor u\u0302 \u223c Uniform(0, 1), shape parameter a, and scale parameter b. This approximation becomes poor as a increases, however, and Knowles recommends a finite difference approximation of the inverse CDF when a \u2265 1."}, {"heading": "3.2 THE KUMARASWAMY DISTRIBUTION", "text": "Another candidate posterior is the little-known Kumaraswamy distribution (Kumaraswamy, 1980). It is a two-parameter continuous distribution also on the unit interval with a density function defined as\nKumaraswamy(x; a, b) = abxa\u22121(1\u2212 xa)b\u22121 (5)\nfor x \u2208 (0, 1) and a, b > 0. In fact, if a = 1 or b = 1 or both, the Kumaraswamy and Beta are equivalent, and for equivalent parameter settings, the Kumaraswamy resembles the Beta albeit with higher entropy. The DNCP we desire is the Kumaraswamy\u2019s closed-form inverse CDF. Samples can be drawn via the inverse transform:\nx \u223c (1\u2212 u 1b ) 1a where u \u223c Uniform(0, 1). (6)\nNot only does the Kumaraswamy make sampling easy, its KL-divergence from the Beta can be closely approximated in closed-form (for ELBO computation)."}, {"heading": "3.2.1 GAUSS-LOGIT PARAMETRIZATION", "text": "Another promising parametrization is inspired by the Probit Stick-Breaking Process (Rodriguez & Dunson, 2011). In a two-step process, we can draw a Gaussian and then use a squashing function to map it on (0, 1):\nv\u0302k = g(\u00b5k + \u03c3k ) (7)\nwhere \u223c N(0, 1). In the Probit SBP, g(\u00b7) is taken to be the Gaussian CDF, and it is chosen as such for posterior sampling considerations. This choice is impractical for our purposes, however, since the Gaussian CDF does not have a closed form. Instead, we use the logistic function g(x) = 1/(1+e\u2212x)."}, {"heading": "4 STICK-BREAKING VARIATIONAL AUTOENCODERS", "text": "Given the discussion above, we now propose the following novel modification to the VAE. Instead of drawing the latent variables from a Gaussian distribution, we draw them from the GEM distribution, making the hidden representation an infinite sequence of stick-breaking weights. We term this model a Stick-Breaking Variational Autoencoder (SB-VAE) and below detail the generative and inference processes implemented in the decoding and encoding models respectively."}, {"heading": "4.1 GENERATIVE PROCESS", "text": "The generative process is nearly identical to previous VAE formulations. The crucial difference is that we draw the latent variable from a stochastic process, the GEM distribution. Mathematically, the hierarchical formulation is written as\n\u03c0i \u223c GEM(\u03b10) , xi \u223c p\u03b8(xi|\u03c0i) (8)\nwhere \u03c0i is the vector of stick-breaking weights and \u03b10 is the concentration parameter of the GEM distribution. The likelihood model p\u03b8(xi|\u03c0i) is a density network just as described in Section 2.1."}, {"heading": "4.2 INFERENCE", "text": "The inference process\u2014how to draw \u03c0i \u223c q\u03c6(\u03c0i|zi)\u2014requires modification from the standard VAE\u2019s in order to sample from the GEM\u2019s stick-breaking construction. Firstly, an inference network computes the parameters of K fraction distributions and samples values vi,k according to one of the parametrizations in Section 3. Next, a linear-time operation composes the stick segments from the sampled fractions:\n\u03c0i = (\u03c0i,1, \u03c0i,2, . . . , \u03c0i,K) = vi,1, vi,2(1\u2212 vi,1), . . . ,K\u22121\u220f j=1 (1\u2212 vi,j)  . (9) The computation path is summarized in Figure 1 (c) with arrows denoting the direction of feedforward computation. The gray blocks represent any deterministic function that can be trained with gradient descent\u2014i.e. one or more neural network layers. Optimization of the SB-VAE is done just as for the VAE, by optimizing Equation 2 w.r.t. \u03c6 and \u03b8. The KL divergence term can be computed (or closely approximated) in closed-form for all three parametrizations under consideration; the Kumaraswamy-to-Beta KL divergence is given in the appendix.\nAn important detail is that the Kth fraction vi,K is always set to one to ensure the stick segments sum to one. This truncation of the variational posterior does not imply that we are using a finite dimensional prior. As explained by Blei & Jordan (2006), the truncation level is a variational parameter and not part of the prior model specification. Truncation-free posteriors have been proposed, but these methods use split-and-merge steps (Hughes et al., 2015) or collapsed Gibbs sampling, both of which are not applicable to the models we consider. Nonetheless, because SGVB imposes few limitations on the inference model, it is possible to have an untruncated posterior. We conducted exploratory experiments using a truncation-free posterior by adding extra variational parameters in an on-line fashion, initializing new weights if more than 1% of the stick remained unbroken. However, we found this made optimization slower without any increase in performance."}, {"heading": "5 SEMI-SUPERVISED MODEL", "text": "We also propose an analogous approach for the semi-supervised relative of the VAE, the M2 model described by Kingma et al. (2014). A second latent variable yi is introduced that represents a class label. Its distribution is the categorical one: q\u03c6(yi|xi) = Cat(y|gy(xi)) where gy is a non-linear function of the inference network. Although y\u2019s distribution is written as independent of z, the two share parameters within the inference network and thus act to regularize one another. We assume the same factorization of the posterior and use the same objectives as in the finite dimensional version (Kingma et al., 2014). Since yi is present for some but not all observations, semi-supervised DGMs need to be trained with different objectives depending on whether the label is present or not. If the label is present, following Kingma et al. (2014) we optimize\nJ\u0303 (\u03b8,\u03c6;xi, yi) = 1\nS S\u2211 s=1 log p\u03b8(xi|\u03c0i,s, yi)\u2212KL(q\u03c6(\u03c0i|xi)||p(\u03c0i;\u03b10)) + log q\u03c6(yi|xi) (10)\nwhere log q\u03c6(yi|xi) is the log-likelihood of the label. And if the label is missing, we optimize\nJ\u0303 (\u03b8,\u03c6;xi) = 1\nS S\u2211 s=1 \u2211 yj q\u03c6(yj |xi) [log p\u03b8(xi|\u03c0i,s, yj)] + H[q\u03c6(y|xi)]\n\u2212KL(q\u03c6(\u03c0i|xi)||p(\u03c0i;\u03b10)) (11)\nwhere H[q\u03c6(yi|xi)] is the entropy of y\u2019s variational distribution."}, {"heading": "6 RELATED WORK", "text": "To the best of our knowledge, neither SGVB nor any of the other recently proposed amortized VI methods (Kingma & Welling, 2014b; Rezende & Mohamed, 2015; Rezende et al., 2014; Tran et al., 2016) have been used in conjunction with BNP priors. There has been work on using nonparametric posterior approximations\u2014in particular, the Variational Gaussian Process (Tran et al., 2016)\u2014but in that work the variational distribution is nonparametric, not the generative model. Moreover, we are not aware of prior work that uses SGVB for Beta (or Beta-like) random variables1.\nIn regards to the autoencoder implementations we describe, they are closely related to the existing work on representation learning with adaptive latent factors\u2014i.e. where the number of latent dimensions grows as the data necessitates. The best known model of this kind is the infinite binary latent feature model defined by the Indian Buffet Process (Ghahramani & Griffiths, 2005); but its discrete latent variables prevent this model from admitting fully differentiable inference. Recent work that is much closer in spirit is the Infinite Restricted Boltzmann Machine (iRBM) (C\u00f4t\u00e9 & Larochelle, 2016), which has gradient-based learning, expands its capacity by adding hidden units, and induces a similar ordering on latent factors. The most significant difference between our SB-VAE and the iRBM is that the latter\u2019s nonparametric behavior arises from a particular definition of the energy function of the Gibbs distribution, not from an infinite dimensional Bayesian prior. Lastly, our training procedure bears some semblance to Nested Dropout (Rippel et al., 2014), which removes all hidden units at an index lower than some threshold index. The SB-VAE can be seen as performing soft nested dropout since the latent variable values decrease as their index increases."}, {"heading": "7 EXPERIMENTS", "text": "We analyze the behavior of the three parametrizations of the SB-VAE and examine how they compare to the Gaussian VAE. We do this by examining their ability to reconstruct the data (i.e. density estimation) and to preserve class structure. Following the original DGM papers (Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014), we performed unsupervised and semi-supervised\n1During preparation of this draft, the work of Ruiz et al. (2016) on the Generalized Reparametrization Gradient was released (on 10/7/16), which can be used for Beta random variables. We plan to compare their technique to our proposed use of the Kumaraswamy in subsequent drafts.\ntasks on the following image datasets: Frey Faces2, MNIST, MNIST+rot, and Street View House Numbers3 (SVHN). MNIST+rot is a dataset we created by combining MNIST and rotated MNIST4 for the purpose of testing the latent representation under the conjecture that the rotated digits should use more latent variables than the non-rotated ones.\nComplete implementation and optimization details can be found in the appendix and code repository5. In all experiments, to best isolate the effects of Gaussian versus stick-breaking latent variables, the same architecture and optimization hyperparameters were used for each model. The only difference was in the prior: p(z) = N(0,1) for Gaussian latent variables and p(v) = Beta(1, \u03b10) (Dirichlet process) for stick-breaking latent variables. We cross-validated the concentration parameter over the range \u03b10 \u2208 {1, 3, 5, 8}. The Gaussian model\u2019s performance potentially could have been improved by cross validating its prior variance. However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative."}, {"heading": "7.1 UNSUPERVISED", "text": "We first performed unsupervised experiments testing each model\u2019s ability to recreate the data as well as preserve class structure (without having access to labels). The inference and generative models both contained one hidden layer of 200 units for Frey Faces and 500 units for MNIST and MNIST+rot. For Frey Faces, the Gauss VAE had a 25 dimensional (factorized) distribution, and we set the truncation level of the SB-VAE also to K = 25, so the SB-VAE could use only as many latent variables as the Gauss VAE. For the MNIST datasets, the latent dimensionality/truncation-level was set at 50. Cross-validation chose \u03b10 = 1 for Frey Faces and \u03b10 = 5 for both MNISTs.\nDensity Estimation. In order to show each model\u2019s optimization progress, Figure 2 (a), (b), and (c) report test expected reconstruction error (i.e. the first term in the ELBO) vs training progress (epochs) for Frey Faces, MNIST, and MNIST+rot respectively. Optimization proceeds much the same in both models except that the SB-VAE learns at a slightly slower pace for all parametrizations. This is not too surprising since the recursive definition of the latent variables likely causes coupled gradients.\nWe compare the final converged models in Table 1, reporting the marginal likelihood of each model via the MC approximation log p(xi) \u2248 log 1S \u2211 s p(xi|z\u0302i,s)p(z\u0302i,s)/q(z\u0302i,s) using 100 samples. The Gaussian VAE has a better likelihood than all stick-breaking implementations (\u223c 96 vs \u223c 98). Between the stick-breaking parametrizations, the Kumaraswamy outperforms both the Gamma and Gauss-Logit on both datasets, which is not surprising given the others\u2019 flaws (i.e. the Gamma is approximate, the Gauss-Logit is restricted). Given this result, we used the Kumaraswamy parametriza-\n2Available at http://www.cs.nyu.edu/~roweis/data.html 3Available at http://ufldl.stanford.edu/housenumbers/ 4Available at http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/MnistVariations 5Theano implementations available at https://github.com/enalisnick/stick-breaking_\ndgms\ntion for all subsequently reported experiments. Note that the likelihoods reported are worse than the ones reported by Burda et al. (2016) because our training set consisted of 50k examples whereas theirs contained 60k (training and validation).\nWe also investigated whether the SB-VAE is using its adaptive capacity in the manner we expect, i.e., the SB-VAE should use a larger latent dimensionality for the rotated images in MNIST+rot than it does for the non-rotated ones. We examined if this is the case by tracking how many \u2018breaks\u2019 it took the model to deconstruct 99% of the stick. On average, the rotated images in the training set were represented by 28.7 dimensions and the non-rotated by 27.4. Furthermore, the rotated images used more latent variables in eight out of ten classes. Although the difference is not as large as we were expecting, it is statistically significant. Moreover, the difference is made smaller by the non-rotated one digits, which use 32 dimensions on average, the most for any class. The non-rotated average decreases to 26.3 when ones are excluded.\nFigure 3 (a) shows MNIST digits drawn from the SB-VAE by sampling from the prior\u2014i.e. vk \u223c Beta(1, 5), and Figure 3 (b) shows Gauss VAE samples for comparison. SB-VAE samples using all fifty dimensions of the truncated posterior are shown in the bottom block. Samples from Dirichlets constrained to a subset of the dimensions are shown in the two columns in order to test that the latent features are concentrating onto lower-dimensional simplices. This is indeed the case: adding a latent variable results in markedly different but still coherent samples. For instance, the second and third dimensions seem to capture the 7-class, the fourth and fifth the 6-class, and the eighth the 5-class. The seventh dimension seems to model notably thick digits.\nDiscriminative Qualities. The discriminative qualities of the models\u2019 latent spaces are assessed by running a k-Nearest Neighbors classifier on (sampled) MNIST latent variables. Results are shown in the table in Figure 4 (a). The SB-VAE exhibits conspicuously better performance than the Gauss VAE at all choices of k, which suggests that although the Gauss VAE converges to a better likelihood, the SB-VAE\u2019s latent space better captures class structure. We also report results for two Gaussian mixture VAEs: Dilokthanakul et al. (2016)\u2019s Gaussian mixture Variational Autoencoder (GMVAE)\nk=3 k=5 k=10 SB-VAE 9.34 8.65 8.90 DLGMM 9.14 8.38 8.42 Gauss VAE 28.4 20.96 15.33 Raw Pixels 2.95 3.12 3.35 GMVAE6 \u2014 8.96 \u2014\n(a) MNIST: Test error for kNN on latent space\nt-SNE EmEeddLng of StLcN-BreaNLng VAE'V Latent SSace\n(b) MNIST SB-VAE\nt-SNE EmEeddLng of StLcN-BreaNLng VAE'V Latent SSace\n(c) MNIST Gauss VAE\nFigure 4: Subfigure (a) shows results of a kNN classifier trained on the latent representations produced by each model. Subfigures (b) and (c) show t-SNE projections of the latent representations learned by the SB-VAE and Gauss VAE respectively.\n(a) Gauss VAE (b) Stick-Breaking VAE\nFigure 5: Sparsity in the latent representation vs sparsity in the decoder network. The Gaussian VAE \u2018turns off\u2019 unused latent dimensions by setting the outgoing weights to zero (in order to dispel the sampled noise). The SB VAE, on the other hand, also has sparse representations but without decay of the associated decoder weights.\nand Nalisnick et al. (2016)\u2019s Deep Latent Gaussian Mixture Model (DLGMM). The GMVAE6 has sixteen mixture components and the DLGMM has five, and hence both have many more parameters than the SB-VAE. Despite the SB-VAE\u2019s lower capacity, we see that its performance is competitive to the mixture VAEs\u2019 (8.65 vs 8.38/8.96).\nThe discriminative qualities of the SB-VAE\u2019s latent space are further supported by Figures 4 (b) and (c). t-SNE was used to embed the Gaussian (c) and stick-breaking (b) latent MNIST representations into two dimensions. Digit classes (denoted by color) in the stick-breaking latent space are clustered with noticeably more cohesion and separation.\nCombating Decoder Pruning. The \u2018component collapsing\u2019 behavior of the variational autoencoder has been well noted (Maal\u00f8e et al., 2016): the model will set to zero the outgoing weights of latent variables that remain near the prior. Figure 5 (a) depicts this phenomenon for the Gauss VAE by plotting the KL divergence from the prior and outgoing decoder weight norm for each latent dimension. We see the weights are only nonzero in the dimensions in which there is posterior deviation. Ostensibly the model receives only sampling noise from the dimensions that remain at the prior, and setting the decoder weights to zero quells this variance. While the behavior of the Gauss VAE is not necessarily improper, all examples are restricted to pass through the same latent variables. A sparse-coded representation\u2014one having few active components per example (like the Gauss VAE) but diversity of activations across examples\u2013would likely be better.\nWe compare the activation patterns against the sparsity of the decoder for the SB-VAE in Figure 5 (b). Since KL-divergence doesn\u2019t directly correspond to sparsity in stick-breaking latent variables like it does for Gaussian ones, the black lines denote the average activation value per dimension. Similarly to (a), blue lines denoted the decoder weight norms, but they had to be down-scaled by a factor of 100 so they could be visualized on the same plot. The SB-VAE does not seem to have any component collapsing, which is not too surprising since the model can set latent variables to zero to deactivate decoder weights without being in the heart of the prior. We conjecture that this increased capacity is\n6The GMVAE\u2019s evaluation is different from performing kNN. Rather, test images are assigned to clusters and whole clusters are given a label. Thus results are not strictly comparable but the ultimate goal of unsupervised MNIST classification is the same.\nthe reason stick-breaking variables demonstrate better discriminative performance in many of our experiments."}, {"heading": "7.2 SEMI-SUPERVISED", "text": "We also performed semi-supervised classification, replicating and extending the experiments in the original semi-supervised DGMs paper (Kingma et al., 2014). We used the MNIST, MNIST+rot, and SVHN datasets and reduced the number of labeled training examples to 10%, 5%, and 1% of the total training set size. Labels were removed completely at random and as a result, class imbalance was all but certainly introduced. Similarly to the unsupervised setting, we compared DGMs with stick-breaking (SB-DGM) and Gaussian (Gauss-DGM) latent variables against one another and a baseline k-Nearest Neighbors classifier (k=5). We used 50 for the latent variable dimensionality / truncation level. The MNIST networks use one hidden layer of 500 hidden units. The MNIST+rot and SVHN networks use four hidden layers of 500 units in each. The last three hidden layers have identity function skip-connections. Cross-validation chose \u03b10 = 5 for MNISTs and \u03b10 = 8 for SVHN.\nQuantitative Evaluation. Table 2 shows percent error on a test set when training with the specified percentage of labeled examples. We see the the SB-DGM performs markedly better across almost all experiments. The Gauss-DGM achieves a superior error rate only on the easiest tasks: MNIST with 10% and 5% of the data labeled."}, {"heading": "8 CONCLUSIONS", "text": "We have described how to employ the Kumaraswamy distribution to extend Stochastic Gradient Variational Bayes to the weights of stick-breaking Bayesian nonparametric priors. Using this development we then defined deep generative models with infinite dimensional latent variables and showed that their latent representations are more discriminative than those of the popular Gaussian variant. Moreover, the only extra computational cost is in assembling the stick segments, a linear operation on the order of the truncation size. Not only are the ideas herein immediately useful as presented, they are an important first-step to integrating black box variational inference and Bayesian nonparametrics, resulting in scalable models that have differentiable control of their capacity. In particular, applying SGVB to full Dirichlet processes with non-trivial base measures is an interesting next step. Furthermore, differentiable stick-breaking has the potential to increase the dynamism and adaptivity of neural networks, a subject of recent interest (Graves, 2016), in a probabilistically principled way."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Many thanks to Marc-Alexandre C\u00f4t\u00e9 and Hugo Larochelle for helpful discussions. This work was supported in part by NSF award number IIS-1320527."}], "references": [{"title": "Variational inference for Dirichlet process mixtures", "author": ["David M Blei", "Michael I Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan.,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan.", "year": 2006}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "An infinite restricted Boltzmann machine", "author": ["Marc-Alexandre C\u00f4t\u00e9", "Hugo Larochelle"], "venue": "Neural Computation,", "citeRegEx": "C\u00f4t\u00e9 and Larochelle.,? \\Q2016\\E", "shortCiteRegEx": "C\u00f4t\u00e9 and Larochelle.", "year": 2016}, {"title": "Deep unsupervised clustering with gaussian mixture variational autoencoders", "author": ["Nat Dilokthanakul", "Pedro Mediano", "Marta Garnelo", "Matthew Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "venue": "ArXiv e-prints,", "citeRegEx": "Dilokthanakul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dilokthanakul et al\\.", "year": 2016}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["Thomas S Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson.", "year": 1973}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Zoubin Ghahramani", "Thomas L Griffiths"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Ghahramani and Griffiths.,? \\Q2005\\E", "shortCiteRegEx": "Ghahramani and Griffiths.", "year": 2005}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Reliable and scalable variational inference for the hierarchical Dirichlet process", "author": ["Michael C Hughes", "Dae Il Kim", "Erik B Sudderth"], "venue": "International Conference on Artificial Intelligence and Statistics (AIStats),", "citeRegEx": "Hughes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hughes et al\\.", "year": 2015}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["Hemant Ishwaran", "Lancelot F James"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ishwaran and James.,? \\Q2001\\E", "shortCiteRegEx": "Ishwaran and James.", "year": 2001}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik Kingma", "Max Welling"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Efficient gradient-based inference through transformations between Bayes nets and neural nets", "author": ["Diederik Kingma", "Max Welling"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Stochastic gradient variational bayes for gamma approximating distributions", "author": ["David A Knowles"], "venue": "ArXiv e-prints,", "citeRegEx": "Knowles.,? \\Q2015\\E", "shortCiteRegEx": "Knowles.", "year": 2015}, {"title": "A generalized probability density function for double-bounded random processes", "author": ["Ponnambalam Kumaraswamy"], "venue": "Journal of Hydrology,", "citeRegEx": "Kumaraswamy.,? \\Q1980\\E", "shortCiteRegEx": "Kumaraswamy.", "year": 1980}, {"title": "Variational inference with renyi divergence", "author": ["Yingzhen Li", "Richard E Turner"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Li and Turner.,? \\Q2016\\E", "shortCiteRegEx": "Li and Turner.", "year": 2016}, {"title": "Auxiliary deep generative models", "author": ["Lars Maal\u00f8e", "Casper Kaae S\u00f8nderby", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Density networks. Statistics and neural networks: advances at the interface", "author": ["David JC MacKay", "Mark N Gibbs"], "venue": null, "citeRegEx": "MacKay and Gibbs.,? \\Q1999\\E", "shortCiteRegEx": "MacKay and Gibbs.", "year": 1999}, {"title": "Approximate inference for deep latent gaussian mixtures", "author": ["Eric Nalisnick", "Lars Hertel", "Padhraic Smyth"], "venue": "NIPS Workshop on Bayesian Deep Learning,", "citeRegEx": "Nalisnick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2016}, {"title": "Combinatorial stochastic processes", "author": ["Jim Pitman"], "venue": "UC Berkeley Technical Report", "citeRegEx": "Pitman.,? \\Q2002\\E", "shortCiteRegEx": "Pitman.", "year": 2002}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Rezende", "Shakir Mohamed"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Oneshot generalization in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "Learning ordered representations with nested dropout", "author": ["Oren Rippel", "Michael A Gelbart", "Ryan P Adams"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Nonparametric bayesian models through probit stick-breaking processes", "author": ["Abel Rodriguez", "David B Dunson"], "venue": "Bayesian analysis,", "citeRegEx": "Rodriguez and Dunson.,? \\Q2011\\E", "shortCiteRegEx": "Rodriguez and Dunson.", "year": 2011}, {"title": "The generalized reparameterization gradient", "author": ["Francisco Ruiz", "Michalis Titsias", "David Blei"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Ruiz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ruiz et al\\.", "year": 2016}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "author": ["Tim Salimans", "David A Knowles"], "venue": "Bayesian Analysis,", "citeRegEx": "Salimans and Knowles.,? \\Q2013\\E", "shortCiteRegEx": "Salimans and Knowles.", "year": 2013}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Tim Salimans", "Diederik Kingma", "Max Welling"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "A constructive definition of Dirichlet priors", "author": ["Jayaram Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman.,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman.", "year": 1994}, {"title": "Variational Gaussian process", "author": ["Dustin Tran", "Rajesh Ranganath", "David M Blei"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "Deep generative models trained via Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014a; Rezende et al., 2014) efficiently couple the expressiveness of deep neural networks with the robustness to uncertainty of probabilistic latent variables.", "startOffset": 80, "endOffset": 127}, {"referenceID": 8, "context": "This combination has lead to their success in tasks ranging from image generation (Gregor et al., 2015; Rezende et al., 2016) to semi-supervised learning (Kingma et al.", "startOffset": 82, "endOffset": 125}, {"referenceID": 26, "context": "This combination has lead to their success in tasks ranging from image generation (Gregor et al., 2015; Rezende et al., 2016) to semi-supervised learning (Kingma et al.", "startOffset": 82, "endOffset": 125}, {"referenceID": 16, "context": ", 2016) to semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) to language modeling (Bowman et al.", "startOffset": 36, "endOffset": 78}, {"referenceID": 20, "context": ", 2016) to semi-supervised learning (Kingma et al., 2014; Maal\u00f8e et al., 2016) to language modeling (Bowman et al.", "startOffset": 36, "endOffset": 78}, {"referenceID": 2, "context": "Various extensions to SGVB have been proposed (Burda et al., 2016; Maal\u00f8e et al., 2016; Salimans et al., 2015), but one conspicuous absence is an extension to Bayesian nonparametric processes.", "startOffset": 46, "endOffset": 110}, {"referenceID": 20, "context": "Various extensions to SGVB have been proposed (Burda et al., 2016; Maal\u00f8e et al., 2016; Salimans et al., 2015), but one conspicuous absence is an extension to Bayesian nonparametric processes.", "startOffset": 46, "endOffset": 110}, {"referenceID": 31, "context": "Various extensions to SGVB have been proposed (Burda et al., 2016; Maal\u00f8e et al., 2016; Salimans et al., 2015), but one conspicuous absence is an extension to Bayesian nonparametric processes.", "startOffset": 46, "endOffset": 110}, {"referenceID": 18, "context": "We bypass this obstacle by using the little-known Kumaraswamy distribution (Kumaraswamy, 1980).", "startOffset": 75, "endOffset": 94}, {"referenceID": 25, "context": "We begin by reviewing the relevant background material on Variational Autoencoders (Kingma & Welling, 2014a), Stochastic Gradient Variational Bayes (also known as Stochastic Backpropagation) (Kingma & Welling, 2014a; Rezende et al., 2014), and Stick-Breaking Processes (Ishwaran & James, 2001).", "startOffset": 191, "endOffset": 238}, {"referenceID": 2, "context": "In previous work (Kingma & Welling, 2014a; Rezende & Mohamed, 2015; Burda et al., 2016; Li & Turner, 2016), the prior p(z) and variational posterior have been marginally Gaussian.", "startOffset": 17, "endOffset": 106}, {"referenceID": 12, "context": "To elaborate, consider SGVB\u2019s approximation of the usual evidence lowerbound (ELBO) (Jordan et al., 1999):", "startOffset": 84, "endOffset": 105}, {"referenceID": 32, "context": "SBPs have been termed as such because of their constructive definition known as the stick-breaking process (Sethuraman, 1994).", "startOffset": 107, "endOffset": 125}, {"referenceID": 5, "context": "When vk \u223c Beta(1, \u03b10), then we have the stick-breaking construction for the Dirichlet Process (Ferguson, 1973).", "startOffset": 94, "endOffset": 110}, {"referenceID": 23, "context": "In this case, the name for the joint distribution over the infinite sequence of stick-breaking weights is the Griffiths, Engen and McCloskey distribution with concentration parameter \u03b10 (Pitman, 2002): (\u03c01, \u03c02, .", "startOffset": 186, "endOffset": 200}, {"referenceID": 18, "context": "Another candidate posterior is the little-known Kumaraswamy distribution (Kumaraswamy, 1980).", "startOffset": 73, "endOffset": 92}, {"referenceID": 10, "context": "Truncation-free posteriors have been proposed, but these methods use split-and-merge steps (Hughes et al., 2015) or collapsed Gibbs sampling, both of which are not applicable to the models we consider.", "startOffset": 91, "endOffset": 112}, {"referenceID": 16, "context": "We assume the same factorization of the posterior and use the same objectives as in the finite dimensional version (Kingma et al., 2014).", "startOffset": 115, "endOffset": 136}, {"referenceID": 25, "context": "To the best of our knowledge, neither SGVB nor any of the other recently proposed amortized VI methods (Kingma & Welling, 2014b; Rezende & Mohamed, 2015; Rezende et al., 2014; Tran et al., 2016) have been used in conjunction with BNP priors.", "startOffset": 103, "endOffset": 194}, {"referenceID": 33, "context": "To the best of our knowledge, neither SGVB nor any of the other recently proposed amortized VI methods (Kingma & Welling, 2014b; Rezende & Mohamed, 2015; Rezende et al., 2014; Tran et al., 2016) have been used in conjunction with BNP priors.", "startOffset": 103, "endOffset": 194}, {"referenceID": 33, "context": "There has been work on using nonparametric posterior approximations\u2014in particular, the Variational Gaussian Process (Tran et al., 2016)\u2014but in that work the variational distribution is nonparametric, not the generative model.", "startOffset": 116, "endOffset": 135}, {"referenceID": 27, "context": "Lastly, our training procedure bears some semblance to Nested Dropout (Rippel et al., 2014), which removes all hidden units at an index lower than some threshold index.", "startOffset": 70, "endOffset": 91}, {"referenceID": 16, "context": "Following the original DGM papers (Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014), we performed unsupervised and semi-supervised (1)During preparation of this draft, the work of Ruiz et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 25, "context": "Following the original DGM papers (Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014), we performed unsupervised and semi-supervised (1)During preparation of this draft, the work of Ruiz et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 1, "context": "However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative.", "startOffset": 70, "endOffset": 203}, {"referenceID": 8, "context": "However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative.", "startOffset": 70, "endOffset": 203}, {"referenceID": 16, "context": "However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative.", "startOffset": 70, "endOffset": 203}, {"referenceID": 25, "context": "However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative.", "startOffset": 70, "endOffset": 203}, {"referenceID": 31, "context": "However, the standard Normal prior is widely used as a default choice (Bowman et al., 2016; Gregor et al., 2015; Kingma et al., 2014; Kingma & Welling, 2014a; Rezende et al., 2014; Salimans et al., 2015), and our goal is to experimentally demonstrate a stick-breaking prior is a competitive alternative.", "startOffset": 70, "endOffset": 203}, {"referenceID": 20, "context": "The \u2018component collapsing\u2019 behavior of the variational autoencoder has been well noted (Maal\u00f8e et al., 2016): the model will set to zero the outgoing weights of latent variables that remain near the prior.", "startOffset": 87, "endOffset": 108}, {"referenceID": 16, "context": "We also performed semi-supervised classification, replicating and extending the experiments in the original semi-supervised DGMs paper (Kingma et al., 2014).", "startOffset": 135, "endOffset": 156}, {"referenceID": 7, "context": "Furthermore, differentiable stick-breaking has the potential to increase the dynamism and adaptivity of neural networks, a subject of recent interest (Graves, 2016), in a probabilistically principled way.", "startOffset": 150, "endOffset": 164}], "year": 2017, "abstractText": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semisupervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "creator": "TeX"}, "id": "ICLR_2017_167"}