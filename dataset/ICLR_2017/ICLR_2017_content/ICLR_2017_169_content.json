{"name": "ICLR_2017_169.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yingzhen Yang", "Jiahui Yu", "Pushmeet Kohli", "Jianchao Yang", "Thomas S. Huang"], "emails": ["jyu79@illinois.edu", "t-huang1@illinois.edu", "pkohli@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The aim of sparse coding is to represent an input vector by a linear combination of a few atoms of a learned dictionary which is usually over-complete, and the coefficients for the atoms are called sparse code. Sparse coding is widely applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation with convincing performance for classification and clustering (Yang et al., 2009; Cheng et al., 2013; Zhang et al., 2013). Suppose the data X = [x1,x2, . . . ,xn] \u2208 IRd\u00d7n lie in the d-dimensional Euclidean space IRd, and the dictionary matrix is D = [D1,D2, . . . ,Dp] \u2208 IRd\u00d7p with each Dk \u2208 IRd (k = 1, . . . , p) being an atom of the dictionary, sparse coding method seeks for the linear sparse representation with respect to the dictionary D for each vector x \u2208X by solving the following convex optimization problem:\nmin D,Z n\u2211 i=1 1 2 \u2016xi \u2212DZi\u201622 + \u03bb\u2016Zi\u20161 s.t. \u2016Dk\u20162 \u2264 c0, k = 1, . . . , p\nwhere \u03bb is a weighting parameter for the `1-norm of z, and c0 is a positive constant that bounds the `2-norm of each dictionary atom. In (Gregor & LeCun, 2010), a feed-forward neural network named Learned Iterative Shrinkage and Thresholding Algorithm (LISTA) is proposed to produce the approximation for sparse coding (1). The architecture of LISTA is illustrated in Figure 1. The LISTA network involves an finite number of stages wherein each stage performs the following operation on the intermediate sparse code:\nz(k+1) = h\u03b8(Wx + Sz (k)), z(0) = 0 (1)\nThis material is based upon work supported by the National Science Foundation under Grant No. 1318971. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\nwhere h\u03b8 is an element-wise shrinkage function defined as\n[h\u03b8(u)]k = sign(uk)(|uk| \u2212 \u03b8)+, k = 1, . . . , p (2)\nand (\u00b7)+ = max{\u00b7, 0} is the positive part of a number. Let f indicate the LISTA network and it generates the approximate sparse code z = f(x,\u0398), where \u0398 = (W,S, \u03b8) collectively denotes the parameters of the LISTA network. Suppose the optimal sparse codes for the training data x1, . . . ,xm are Z\u22171, . . . ,Z\u2217m, then the parameters \u0398 are learned by minimizing the cost function which measures the distance between the predicted approximate sparse codes and the optimal sparse\ncodes: L(\u0398) = 1m m\u2211 i=1 \u2016Z\u2217i \u2212 f(xi,\u0398)\u201622. The optimization is performed by stochastic gradient descent and back-propagation. Inspired by LISTA, a series of previous works have designed neural networks to simulate different forms of linear coding and achieve end-to-end training for different tasks such as image super-resolution (Liu et al., 2016) and hashing (Wang et al., 2016).\nSparse coding is widely used to model high-dimensional data. Based on the formulation of sparse coding (1), it can be observed that the sparse code of each data point is obtained independently when the dictionary is fixed, which ignores the geometric information and manifold structure of the high-dimensional data. In order to obtain the sparse codes that account for the geometric information and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin et al., 2006). Manifold assumption in these methods imposes local smoothness on the sparse codes of nearby data, namely nearby data are encouraged to have similar sparse codes in the sense of `2-distance, and they are termed `2-Regularized Sparse Coding (`2-RSC). In this paper, we propose Support Regularized Sparse Coding (SRSC). Compared to `2-RSC, SRSC captures the locally linear structure of the data manifold by encouraging nearby data to share dictionary atoms. In addition, SRSC enjoys robustness to data noise and preserves freedom in the spare representation of data without constraints on the magnitude of the sparse codes.\nThe remaining parts of the paper are organized as follows. SRSC and its optimization algorithm, together with `2-RSC are introduced in the next section. The theoretical properties of the optimization of SRSC are shown in Section 3, with theoretical guarantee on the obtained sub-optimal solution for each step of the coordinate descent for obtaining the support regularized sparse codes: convergence to the critical point of the objective function and being close to the globally optimal solution. We then show the performance of the SRSC on data clustering, and conclude the paper. We use bold letters for matrices and vectors, and regular lower letter for scalars throughout this paper. The bold letter with superscript indicates the corresponding column of a matrix, and the bold letter with subscript indicates the corresponding element of a matrix or vector. \u2016 \u00b7 \u2016F and \u2016 \u00b7 \u2016p denote the Frobenius norm and the `p-norm."}, {"heading": "2 SUPPORT REGULARIZED SPARSE CODING", "text": ""}, {"heading": "2.1 CAPTURING LOCALLY LINEAR STRUCTURE: SUPPORT REGULARIZED SPARSE CODING", "text": "In this section, we introduce Support Regularized Sparse Coding (SRSC) which is designed to capture the locally linear structure of the data manifold for sparse coding. One of the most important properties of manifold is that it is locally Euclidean, and each data point in the manifold has a neighbourhood that is homeomorphic to a Euclidean space. The success of several manifold learning methods, including LLE (Roweis & Saul, 2000), SMCE (Elhamifar & Vidal, 2011) and Locally Linear Hashing (Irie et al., 2014), is built on exploiting the locally linear structure of manifold. In these methods, the locally linear structure associated with each data point is a linear representation of\nthat point by a set of its nearest neighbors in a nonparametric manner, from which the low-dimensional embedding complying to the manifold structure of the original data is obtained and used for various learning tasks. In the context of sparse coding, the data lie on or close to the subspaces spanned by the dictionary atoms specified by the nonzero elements of the corresponding sparse codes. Inspired by this observation, we propose to capture the locally linear structure of the data manifold for sparse coding by encouraging nearby data to share the atoms of the dictionary, so that nearby data are on or close to the local subspace spanned by the common dictionary atoms (see Figure 2).\nIn order to obtain the sparse codes with locally similar support so as to capture the locally linear structure of the data manifold, we propose Support Regularized Sparse Coding (SRSC), which uses support distance to measure the distance between the sparse codes of nearby data. Given a proper symmetric similarity matrix A, the sparse codes Z that capture the locally linear structure of the manifold minimizes the following support regularization term:\nRA(Z) = 1\n2 n\u2211 i=1 n\u2211 j=1 Aijd(Z i,Zj) (3)\nA is usually the adjacency matrix of K-Nearest-Neighbor (KNN) graph, i.e. Aij = 1 if and only if xi is among the K nearest neighbors of xj or xj is among the K nearest neighbors of xi. Note that KNN is extensively used in the manifold learning literature, such as Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (Belkin & Niyogi, 2003) and Sparse Manifold Clustering and Embedding (SMCE) (Elhamifar & Vidal, 2011), to establish the local neighborhood in the manifold. d indicates the support distance. For two vectors u,v of the same size, their support distance is defined below:\nd(u,v) = |u|\u2211 t=1 (1Iut=0,vt 6=0 + 1Iut 6=0,vt=0) (4)\nwhere 1I is the indicator function. When the support distance between Zi and Zj is small for nonzero Aij , xi and xj choose similar atoms of the dictionary for sparse representation. Therefore, SRSC captures the locally linear structure of the data manifold by encouraging nearby data to share dictionary atoms, wherein the common atoms shared by nearby data serve as the basis of the local subspace.\nThe optimization problem of SRSC is presented below:\nmin D,Z L(D,Z) = n\u2211 i=1 1 2 \u2016xi \u2212DZi\u201622 + \u03bb\u2016Zi\u20161 + \u03b3RA(Z) s.t. \u2016Dk\u20162 \u2264 1, k = 1, . . . , p (5)\nwhere \u03b3 > 0 is the weighting parameter for the support regularization term. Similar to (Lee et al., 2006), problem (5) is optimized alternatingly with respect to the dictionary D and the sparse codes Z respectively with the other variable fixed."}, {"heading": "2.1.1 OPTIMIZING WITH RESPECT TO D WITH FIXED Z", "text": "The optimization with respect to D with fixed Z is a quadratic programming problem:\nmin D\n1 2 \u2016X \u2212DZ\u20162F s.t. \u2016Dk\u20162 \u2264 1, k = 1, . . . , p (6)\nwhich can be solved using Lagrangian dual (Lee et al., 2006)."}, {"heading": "2.1.2 OPTIMIZING WITH RESPECT TO Z WITH FIXED D", "text": "We use coordinate descent to optimize (5) with respect to Z with fixed D:\nmin Z n\u2211 i=1 1 2 \u2016xi \u2212DZi\u201622 + \u03bb\u2016Zi\u20161 + \u03b3RA(Z) (7)\nIn each step of coordinate descent, the optimization is performed over the i-th column of Z, while fixing all the other sparse codes {Zj}j 6=i. For each 1 \u2264 i \u2264 n, the optimization problem for Zi is below:\nmin Zi\nF (Zi) = 1\n2 \u2016xi \u2212DZi\u201622 + \u03bb\u2016Zi\u20161 + \u03b3RA(Zi) (8)\nwhere RA(Zi) = n\u2211 j=1 Aijd(Z i,Zj).\nInspired by recent advances in solving non-convex optimization problems by proximal linearized method (Bolte et al., 2014), proximal gradient descent method (PGD) is used to optimize the nonconvex problem (8). Although the proximal mapping is typically associated with a lower semicontinuous function (Bolte et al., 2014) and it can be verified that RA is not always lower semicontinuous, we can still derive a PGD-styple iterative method to optimize (8). Define GA \u2208 IRp\u00d7n as GAki = n\u2211 j=1 Aij1IZkj=0 \u2212 n\u2211 j=1 Aij1IZkj 6=0 where 1I is the indicator function, then GAki indicates the degree to which Zki is discouraged to be nonzero and it can be verified that 1\nRA(Z i) = p\u2211 k=1 GAki1IZki 6=0 (9)\nSince each indicator function 1IZki 6=0 is lower semicontinuous, RA is lower semicontinuous if GAki \u2265 0 for k = 1, . . . , p. In the following text, we let Q(Zi) = 12\u2016xi \u2212DZ\ni\u201622. The superscript with bracket indicates the iteration number of PGD or the iteration number of the coordinate descent without confusion. The PGD-style iterative method for optimizing (8) is as follows:\nZ\u0303i (t) = Zi (t\u22121) \u2212 1 \u03c4s (D>DZi (t\u22121) \u2212D>xi) (10)\nZki (t) = { arg min v\u2208{uk,0} Hk(v) : uk 6= 0 or uk = 0 and GAki \u2265 0\n\u03b5 : uk = 0 and G A ki < 0\n(11)\nfor k = 1, . . . , p and \u03b5 is any real number such that \u03b5 6= 0 and Hk(\u03b5) \u2264 Hk(Z(t\u22121)ki ). Hk and u are defined below:\nHk(v) = \u03c4s\n2 (v \u2212 Z\u0303(t)ki ) 2 + \u03bb|v|+ \u03b3GAki1Iv 6=0 (12)\nfor v \u2208 IR and each 1 \u2264 k \u2264 p, and\nu = max{|Z\u0303i (t) | \u2212 \u03bb\n\u03c4s , 0} \u25e6 sign(Z\u0303i\n(t) ) (13)\nwhere \u25e6 means element-wise multiplication. Proposition 1 shows that the PGD-style iterative method decreases the value of the objective function in each iteration.\n1RA(Z i) is equal to the right hand side of (9) up to a constant.\nProposition 1. Let the sequence {Zi(t)}t be generated by the PGD-style iterative method with (10) and (11), then the sequence of the objective {F (Zi(t))}t decreases, and the following inequality holds for t \u2265 1:\nF (Zi (t) ) \u2264 F (Zi(t\u22121))\u2212 (\u03c4 \u2212 1)s 2 \u2016Zi(t) \u2212 Zi(t\u22121)\u201622 (14)\nAnd it follows that the sequence {F (Zi(t))}t converges. Remark 1. (10) and (11) in each iteration of the proposed PGD-style iterative method resemble that of the ordinary PGD. (10) performs gradient descent on the differential part, and (11) can be viewed as an approximate solution to the proximal mapping minv\u2208IRp H(v) = \u03c4s 2 \u2016v \u2212 Z\u0303i (t) \u201622 + \u03bb\u2016v\u20161 + \u03b3RA(v). Since RA(Zi) is not always lower semicontinuous, arg minv\u2208IRp H(v) is not guaranteed to exist. One can see a simple example where this happens when uk = 0 and GAki < 0 for some k = 1, . . . , p, and in this case infv\u2208IRHk(v) = \u03c4s 2 (Z\u0303 (t) ki )\n2 + \u03b3GAki but this infinitum can not be achieved.\nIn (10), \u03c4 > 1 is a constant and s is the Lipschitz constant for the gradient of function Q(\u00b7), namely \u2016\u2207Q(y)\u2212\u2207Q(z)\u20162 \u2264 s\u2016y \u2212 z\u20162, \u2200y, z \u2208 IRp (15)\nThe PGD-style iterative method starts from t = 1 and continues until the sequence {F (Zi(t))} converges or maximum iteration number is achieved. When the iterative method converges or terminates for each Zi, the step of coordinate descent for Zi is finished and the optimization algorithm proceeds to optimize other sparse codes.\nAlgorithm 1 Support Regularized Sparse Coding"}, {"heading": "Input:", "text": "The data setX = {xi}ni=1, the parameter \u03bb, \u03b3, maximum iteration number M for the alternating method over D and Z, and maximum iteration number Mz for coordinate descent on Z, maximum iteration number Mp for the PGD-style iterative method on each Zi (i = 1,. . . ,n). and stopping threshold \u03b5. 1: m = 1 2: while m \u2264M do 3: Perform coordinate descent to optimize (7) and obtain Z(m) with fixed D(m\u22121). In i-th (1 \u2264 i \u2264 n) step\nof each iteration of coordinate descent, solve (8) using the PGD-style iterative method (10) and (11) to update Zi in each iteration of the PGD-style iterative method.\n4: Optimize (6) using Lagrangian dual and obtain D(m) with fixed Z(m). 5: if |L(D(m),Z(m))\u2212 L(D(m\u22121),Z(m\u22121))| < \u03b5 then 6: break 7: else 8: m = m+ 1. 9: end if\n10: end while Output: the support regularized sparse codes Z\u0302 when the above iterations converge or maximum iteration\nnumber is achieved."}, {"heading": "TIME COMPLEXITY", "text": "Algorithm 1 describes the algorithm of SRSC. We solve the ordinary sparse coding problem (1) by the online dictionary learning method (Mairal et al., 2009) and use the dictionary and the sparse codes as the initialization D(0) and Z(0) for the alternating method in Algorithm 1. In Algorithm 1, the time complexity of optimization over the sparse codes is O(MMzMpndp2), and time complexity of optimization over the dictionary using Newton\u2019s method to solve the Lagrangian dual problem is O ( M ( np2 + Tnewton(3p 2.807 + 2dp2 + dnp) )) , where Tnewton is the maximum iteration number\nfor Newton\u2019s method. Therefore, the overall time complexity of Algorithm 1 isO ( M ( MzMpndp 2 +\nnp2 + Tnewton(3p 2.807 + 2dp2 + dnp) )) . It should be emphasized that the optimization over the\ndictionary for SRSC has the same efficiency as the efficient sparse coding method (Lee et al., 2006),\nand the optimization over the sparse code of each data point by the PGD-style iterative method (10) and (11) is almost as efficient as the widely used Iterative Shrinkage and Thresholding Algorithm (ISTA) (Daubechies et al., 2004; Beck & Teboulle, 2009). Note that step (10) and (13) are required by both our method and ISTA; compared to ISTA, the extra operations incurred by our PGD-style iterative method (10) and (11) are only the arithmetic operations with time complexity 20p for evaluating the function Hk(\u00b7) defined in (12) for k = 1, . . . , p. More specifically, evaluating the value of function Hk(v) takes 10 arithmetic operations and two evaluations at v = uk and v = 0 are needed. Since a compact dictionary is preferred by the extensive study of the sparse coding and dictionary learning literature and the dictionary size p \u2264 500 is adopted throughout our experiments, our PGD-style iterative method only incurs extra operations of constant time complexity compared to ISTA while learning supported regularized sparse codes. In Section 4, we propose Deep-SRSC as a fast approximation of SRSC with considerable speedup for obtaining the sparse codes of the new data or the test data (see more details in Section 4.1). Furthermore, we conduct the empirical study and show that the parallel coordinate descent method, which updates the codes of a group of P data points in parallel and provides P times speedup over the coordinate descent method used in Section 2.1.2 and Algorithm 1, exhibits almost the same performance as the coordinate descent method for the clustering task on the test set of the CIFAR-10 data. Please refer to the details in the subsection \u201cDeep-SRSC with the Second Test Setting (Referring to the Training Data)\u201d in the Appendix.\n2.2 RELATED WORK: `2 REGULARIZED SPARSE CODING (`2-RSC)\nThe manifold assumption (Belkin et al., 2006) is usually employed by existing regularized sparse coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain the sparse code according to the manifold structure of the data. Interpreting the sparse code of a data point as its embedding, the manifold assumption in the case of sparse coding for most existing methods requires that if two points xi and xj are close in the intrinsic geometry of the submanifold, their corresponding sparse codes Zi and Zj are also expected to be similar to each other in the sense of `2-distance (Zheng et al., 2011; Gao et al., 2013). In other words, z varies smoothly along the geodesics in the intrinsic geometry. Based on the spectral graph theory (Chung, 1997), extensive literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local manifold structure (Belkin et al., 2006; Zheng et al., 2011; Gao et al., 2013).\nThe sparse code Z that captures the local geometric structure of the data in accordance with the manifold assumption by graph Laplacian minimizes the following `2 regularization term, or the Laplacian regularization term:\nR (`2) A (Z) = 1\n2 n\u2211 i=1 n\u2211 j=1 Aij\u2016Zi \u2212 Zj\u201622 (16)\nwhere the `2-norm is used to measure the distance between sparse codes, and A is the same as that in Section 2.1. LA = DA \u2212A is the graph Laplacian associated with the similarity matrix A, the degree matrix DA is a diagonal matrix with each diagonal element being the sum of the\nelements in the corresponding row of A, namely (DA)ii = n\u2211 j=1 Aij . To the best of our knowledge, such `2 regularization is employed by most methods that use graph regularization for sparse coding. Incorporating the `2 regularization term into the optimization problem of sparse coding (1), the formulation of `2 Regularized Sparse Coding (`2-RSC) is presented below\nmin D,Z\nL(` 2)(Z) = n\u2211 i=1 \u2016xi \u2212DZi\u201622 + \u03bb\u2016Zi\u20161 + \u03b3(` 2)R (`2) A (Z) s.t. \u2016D k\u20162 \u2264 1, k = 1, . . . , p (17)\nADVANTAGE OF SRSC OVER `2-RSC\nAlthough `2-RSC imposes the local smoothness on the sparse codes, it does not capture the locally linear structure of the data manifold. By promoting the smoothness on the support of the sparse codes rather than their `2-distance, SRSC encodes the locally linear structure of the manifold in the sparse codes while reserving freedom in the sparse representation of the data with no constraints on the\nmagnitude of the sparse codes. Moreover, as pointed out by (Wang et al., 2015), support regularization offers robustness to noise for sparse coding. In SRSC, all the data consult their neighbors for choosing the dictionary atoms rather than choosing the atoms on their own, and the sparse codes of the noisy data are suppressed since they are forced to choose similar or the same atoms as the nearby clean data instead of choosing the atoms in the interests of representing themselves."}, {"heading": "3 THEORETICAL ANALYSIS", "text": "It can be observed that optimization by coordinate descent over the sparse code in Section 2.1.2 is important for the overall optimization of SRSC, and each step of the coordinate descent (8) is a difficult nonconvex problem and crucial for obtaining the support regularized sparse code, where the nonconvexity comes from the support regularization term RA(Zi) (9). Therefore, the optimization of (8) plays an important role in the overall optimization of SRSC. In the previous section, a PGDstyle iterative method is proposed to decrease the value of the objective in each iteration. In this section, we provide further theoretical analysis on the optimization of problem (8) when GAki \u2265 0 for k = 1, . . . , p. This condition is equivalent to the condition that the support regularization function\nRc(v) , p\u2211 k=1 ck1Ivk 6=0 (18)\nis lower semicontinuous, where c \u2208 IRp is the coefficients and ck = GAki. Under this condition, we prove that the sequence {Zi(t)}t produced by the PGD-style iterative method converges to the suboptimal solution which is a critical point of the objective (8). By connecting the support regularized function to the capped-`1 norm and the nonconvexity analysis of the support regularization term, we present the bound for `2-distance between the sub-optimal solution and the globally optimal solution to (8) in Theorem 1. Note that our analysis is valid for all 1 \u2264 i \u2264 n. We first have the following result that the support regularization function (18) is lower semicontinuous if and only if all the coefficients c are nonnegative. Proposition 2. The support regularization function (18) is lower semicontinuous if and only if all the coefficients c are nonnegative.\nTherefore, if GAki \u2265 0 for k = 1, . . . , p, the support regularization term RA(Zi) is lower semicontinuous with respect to Zi in (9). In this case, the PGD-style iterative method proposed in Section 2.1.2 for each iteration t \u2265 1 becomes\nZ\u0303i (t) = Zi (t\u22121) \u2212 1 \u03c4s (D>DZi (t\u22121) \u2212D>xi) (19)\nZki (t) = arg min\nv\u2208{uk,0} Hk(v), k = 1, . . . , p (20)\nwhich is equivalent to the updates rules in the ordinary proximal gradient descent method. It is worthwhile to mention the meaning of the condition that GAki \u2265 0 for k = 1, . . . , p. For a data point xi, if the number of its neighbors with zero k-th element of the sparse codes is larger than that with nonzero k-th element of the sparse codes, which indicates that the neighbors of xi suggest that a zero k-th element of the sparse code of xi is preferable, then GAki \u2265 0 and GAki quantitatively represents the penalty if the sparse code element Zik is nonzero while the neighbors of xi suggest that Z i k = 0 is preferable. Intuitively, this situation happens when there is conflict between choosing the support of the code solely by the data point itself and the suggestion of its neighbors; if the point is an outlier or suffering from noise, the optimization can help that point make a sensible choice by considering the suggestion of its neighbors. We observe that GAki \u2265 0 for k = 1, . . . , p happens in all the data sets used in this paper.\nIn the following lemma, we show that the sequence {Zi(t)}t generated by (19) and (20) converges to a critical point of F (Zi), denoted by Z\u0302i. Denote by Zi\u2217 the globally optimal solution to the original optimization problem (8). The following lemma also shows that both Z\u0302i and Zi\u2217 are local solutions to the capped-`1 regularized problem (21). Before stating the lemma, the following definitions are introduced which are essential for our analysis.\nDefinition 1. (Critical points) Given the non-convex function f : IRn \u2192 R \u222a {+\u221e} which is a proper and lower semi-continuous function.\n\u2022 for a given x \u2208 domf , its Frechet subdifferential of f at x, denoted by \u2202\u0303f(x), is the set of all vectors u \u2208 IRn which satisfy\nlim sup y 6=x,y\u2192x f(y)\u2212 f(x)\u2212 \u3008u,y \u2212 x\u3009 \u2016y \u2212 x\u2016 \u2265 0\n\u2022 The limiting-subdifferential of f at x \u2208 IRn, denoted by written \u2202f(x), is defined by\n\u2202f(x) = {u \u2208 IRn : \u2203xk \u2192 x, f(xk)\u2192 f(x), u\u0303k \u2208 \u2202\u0303f(xk)\u2192 u}"}, {"heading": "The point x is a critical point of f if 0 \u2208 \u2202f(x).", "text": "Also, we are considering the following capped-`1 regularized problem, which replaces the indicator function in the support regularization term RA(Zi) with the continuous capped-`1 regularization term T:\nmin \u03b2\u2208IRp\nLcapped\u2212`1(\u03b2) = 1\n2 \u2016xi \u2212D\u03b2\u201622 + \u03bb\u2016\u03b2\u20161 + T(\u03b2; b) (21)\nwhere T(\u03b2; b) = p\u2211 k=1 Tk(\u03b2k; b), Tk(t; b) = \u03b3GAki min{|t|,b} b for some b > 0. It can be seen that the\nobjective function of the capped-`1 problem approaches that of (8) when min{|t|,b}b approaches the indicator function 1It6=0 as b\u2192 0+. Define P(\u00b7; b) = \u03bb\u2016 \u00b7 \u20161 + T(\u00b7; b), the location solution to the capped-`1 problem is defined as follows.\nDefinition 2. (Local solution) A vector \u03b2\u0303 is a local solution to the problem (21) if\n\u2016D>(D\u03b2\u0303 \u2212 xi) + P\u0307(\u03b2\u0303; b)\u20162 = 0 (22)\nwhere P\u0307(\u03b2\u0303; b) = [P\u03071(\u03b2\u03031; b), P\u03072(\u03b2\u03032; b), . . . , P\u0307p(\u03b2\u0303p; b)]>, Pk(t; b) = \u03bb|t| + Tk(t; b) for k = 1, . . . , p.\nNote that in the above definition and the following text, P\u0307k(t; b) can be chosen as any value between the right differential \u2202Pk\u2202t (t+; b) (or P\u0307k(t+; b)) and left differential \u2202Pk \u2202t (t\u2212; b) (or P\u0307k(t\u2212; b)) for k = 1, . . . , p.\nDefinition 3. (Degree of Nonconvexity of a Regularizer) For \u03ba \u2265 0 and t \u2208 IR, define\n\u03b8(t, \u03ba) := sup s {\u2212sgn(s\u2212 t)(P\u0307 (s; b)\u2212 P\u0307 (t; b))\u2212 \u03ba|s\u2212 t|}\nas the degree of nonconvexity for function P . If u = (u1, . . . , up)> \u2208 IRp, \u03b8(u, \u03ba) = [\u03b8(u1, \u03ba), . . . , \u03b8(up, \u03ba)]. sgn is the sign function.\nNote that \u03b8(t, \u03ba) = 0 for convex function P .\nLet S\u0302i = supp(Z\u0302i) where supp(\u00b7) indicates the support of a vector, i.e. the indices of its nonzero elements. Denote by Zi\u2217 the globally optimal solution to (8), and S\u2217i = supp(Z i\u2217), then we have\nLemma 1. For any 1 \u2264 i \u2264 n, if GAki \u2265 0 for k = 1, . . . , p, then the sequence {Zi (t)}t generated by (10) and (11) converges to a critical point of F (Zi), which is denoted by Z\u0302i. Moreover, if\n0 < b < min{min j\u2208S\u0302i |Z\u0302ij |, max k/\u2208S\u0302i,GAki 6=0 \u03b3GAki ( \u2202Q \u2202Zi\nk\n|Zi=Z\u0302i \u2212 \u03bb)+ , min j\u2208S\u2217i\n|Zij \u2217|, max\nk/\u2208S\u2217i ,G A ki 6=0 \u03b3GAki ( \u2202Q \u2202Zij |Zi=Zi\u2217 \u2212 \u03bb)+ }\n(23)\n(if the denominator is 0, \u00b70 is defined to be +\u221e in the above inequality), then both Z\u0302 i and Zi\u2217 are\nlocal solutions to the capped-`1 regularized problem (21).\nUsing the degree of nonconvexity of the regularizer P, we have the following theorem showing that the sub-optimal solution Z\u0302i obtained by our PGD-style iterative method can be close to the globally optimal solution to the original problem (8), i.e. Zi\u2217. In the following text, BI indicates a submatrix of B whose columns correspond to the nonzero elements of I, and \u03c3min(\u00b7) indicates the smallest singular value of a matrix. Theorem 1. (Sub-optimal solution is close to the globally optimal solution) For any 1 \u2264 i \u2264 n, let Ei = S\u0302i \u222a S\u2217i . Suppose GAki \u2265 0 for k = 1, . . . , p, DEi is not singular with \u03ba0 , \u03c3min(DEi) > 0, \u03ba20 > \u03ba > 0, and b is chosen according to (23) as in Lemma 1. Let S\u0303i = (S\u0302i \\ S\u2217i )\u222a (S\u2217i \\ S\u0302i) be the symmetric difference between S\u0302i and S\u2217i , then\n\u2016Zi\u2217 \u2212 Z\u0302i\u20162 \u2264 1\n\u03ba20 \u2212 \u03ba (( \u2211 k\u2208S\u0303i\u2229S\u0302i (max{0, \u03b3G A ki b \u2212 \u03ba|Z\u0302ki \u2212 b|})2 + \u2211 k\u2208S\u0303i\\S\u0302i (max{0, \u03b3G A ki b \u2212 \u03bab})2 ) 1 2 + \u2016t\u20162 ) (24)\nwhere t \u2208 IRp, tk = 2\u03bb1IZik\u2217Z\u0302ik<0 + 01IZik\u2217Z\u0302ik>0 for k \u2208 S\u0302i \u2229 S \u2217 i , and tk = 0 for all other k.\nRemark 2. Note that the bound for distance between the sub-optimal solution and the globally optimal solution presented in Theorem 1 does not require typical Restricted Isometry Property (RIP) conditions, e.g. Cands (2008). Also, when \u03b3G A ki\nb \u2212 \u03ba|Z\u0302ki \u2212 b| and \u03b3GAki b \u2212 \u03bab are no greater than\n0 and Zi\u2217 and Z\u0302i has the same sign in the intersection of their support, the sub-optimal solution Z\u0302i is equal to the globally optimal solution. When \u03b3G A ki\nb \u2212 \u03ba|Z\u0302ki \u2212 b| and \u03b3GAki b \u2212 \u03bab are small\npositive numbers and Zi\u2217 and Z\u0302i has similar sign in the intersection of their support, Z\u0302i is close to the globally optimal solution."}, {"heading": "4 DEEP SUPPORT REGULARIZED SPARSE CODING: DEEP-SRSC", "text": "Inspired by the PGD-style iterative method (10) and (11) for SRSC and the LISTA network, we propose Deep Support Regularized Sparse Coding (Deep-SRSC) illustrated in Figure 3, which is a neural network that produces the approximate support regularized sparse codes for SRSC. The goal of Deep-SRSC is to approximate the sparse codes of the input data in a fast way by feeding the data through the Deep-SRSC network, instead of running the iterative optimization algorithm for SRSC in Section 2.1. To achieve this goal, the Deep-SRSC network is trained on the training data by minimizing the squared distance between the predicted codes of the training data by the network and their ground truth codes. The network design of Deep-SRSC is in accordance with the proposed PGD-style iterative method. When W = 1LD >, S = I\u2212 1LD >D where L = \u03c4s, then each stage in the recurrent structure of Deep-SRSC implements one iteration of PGD-style iterative method, i.e. (10) and (11). In Deep-SRSC, W, S and L are to be learned by the network rather than computed from a pre-computed dictionary D, and S is shared over different layers. The min-pooling neuron in Deep-SRSC outputs the result of arg min\nv\u2208{uk,0} Hk(v) or \u03b5, according to the update rule (11). Figure 3\nillustrates Deep-SRSC with 2 layers.\nDenote the training data by x1, . . . ,xm, and let Zsr be the ground truth support regularized sparse codes of the training data which are obtained by the optimization method introduced in Section 2.1. Let fsr be the Deep-SRSC encoder which produces the approximate support regularized sparse code z = fsr(x,\u0398sr), where \u0398sr = (W,S, L) denotes the parameters of Deep-SRSC. Then the parameters of Deep-SRSC are learned by minimizing the cost function which measures the distance between the predicted approximate support regularized sparse codes and the ground truth ones: 1 m m\u2211 i=1 \u2016Zisr \u2212 fsr(xi,\u0398sr)\u201622. Similar to the LISTA network, the optimization is performed by\nstochastic gradient descent and back-propagation. The batch size is set to 1 so as to simulate the coordinate descent method for optimization over the sparse codes in Section 2.1.2. The adjacency matrix of the KNN graph over the training data is required as input for training the network.\nThe approximate codes of the new data, or the test data, are obtained by feeding the new data through the Deep-SRSC network learned on the training data. We provide two test settings below, depending on whether training data are referred to in the test process.\n1) In the first setting where the training data are not referred to, the test data are a group of data points. The test data and the KNN graph over them are fed into the Deep-SRSC network to obtain the approximate codes of the test data. The locally linear manifold structure of the test data is encoded in the KNN graph over the test data. This setting is potentially more suitable for the situation of limited storage where the training data and their codes do not need to be stored in the test process. This setting may not be suitable for the test data that do not reliably reflect the locally linear manifold structure (e.g. in the case of a very small amount of test data), and in this case the second setting below is a better choice.\n2) In the second setting where training data are referred to, the approximate code of each data point is obtained by feeding that point and the KNN graph over that point and the training data into the Deep-SRSC network. The code of each test point is reliably obtained by referring to its nearest neighbors in the training data and this process is independent of the factor that whether the test data reflect the locally linear manifold structure."}, {"heading": "4.1 DEEP-SRSC AS FAST ENCODER", "text": "It should be emphasized that Deep-SRSC is a fast encoder for SRSC when obtaining the codes of the new data (or test data). Each layer of Deep-SRSC resembles one iteration of the PGD-style iterative method (10) and (11), and the computational cost of feeding forward a data point through one layer is the same as that of executing one iteration of the PGD-style iterative method for that point. Therefore, the feed-forward process of obtaining the sparse codes of the new data using `-layer Deep-SRSC is around Mp` times faster than the PGD-style iterative method used in Algorithm 1, where Mp is the maximum iteration number for the PGD-style iterative method. In the experimental results shown in the next section, Deep-SRSC with different number of layers are employed to produce the approximate support regularized sparse codes, and 6-layer Deep-SRSC achieves minimum prediction error. With Mp = 50 throughout our experiments, Deep-SRSC is around 506 \u2248 8.3 times faster than the PGD-style iterative method. Our analysis in this subsection holds for both test settings."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "5.1 CLUSTERING PERFORMANCE", "text": "In this subsection, the superiority of SRSC is demonstrated by its performance in data clustering on various data sets, e.g. USPS handwritten digits data set, COIL-20, COIL-100 and UCI Gesture Phase Segmentation data set. Two measures are used to evaluate the performance of the clustering methods, i.e. the Accuracy (AC) and the Normalized Mutual Information (NMI) (Zheng et al., 2004). SRSC is compared to K-means (KM), Spectral Clustering (SC), Sparse Coding and `2-RSC in Section 2.2. Throughout all the experiments, we set K = 3 for building the adjacency matrix A of KNN graph, dictionary size p = 300 and \u03bb = 0.1 for both `2-RSC and SRSC. We also set \u03b3(`\n2) = 1 which is the suggested default value in (Zheng et al., 2011), and M = Mz = 5 and Mp = 50 in Algorithm 1. The default value of the weight for support regularization term of SRSC is \u03b3 = 0.5. SRSC is implemented by both MATLAB and CUDA C++ with extreme efficiency, and the code is published on GitHub: https://github.com/yingzhenyang/SRSC.\nThe USPS handwritten digits data set is comprised of n = 9298 handwritten images of ten digits from 0 to 9, and each image is of size 16 \u00d7 16 and represented by a 256-dimensional vector. The whole data set is divided into training set of 7291 images and test set of 2007 images. We run Algorithm 1 to obtain the support regularized sparse code Z\u0302, then build a n\u00d7 n similarity matrix Y over all the data. Two similarity measure are employed: the first similarity is the positive part of the inner product of their corresponding sparse codes, namely Yij = max{0, Z\u0302i > Z\u0302j}, the second one is Yij = Aijq > Z\u0302i q Z\u0302j\nwhere qv is a binary vector of the same size as v with element 1 at the indices of nonzero elements of v. The second similarity measure is name the support similarity and it considers the number of common dictionary atoms chosen by the sparse codes. Spectral clustering is performed on the similarity matrix Y to obtain the clustering result of SRSC, and the best performance among the two similarity measures is reported. The same procedure is performed by all the other sparse coding based methods to obtain clustering results. The clustering results of various methods are shown in Table 1.\nCOIL-20 Database has 1440 images of resolution 32 \u00d7 32 for 20 objects, and the background is removed in all images. The dimension of this data is 1024. Its enlarged version, COIL-100 Database, contains 100 objects with 72 images of resolution 32 \u00d7 32 for each object. The images of each object were taken 5 degrees apart when each object was rotated on a turntable. The UCI Gesture Phase Segmentation data set contains the gesture information of three users when they told stories of some comic strips in front of the Microsoft Kinect sensor. We use the processed file provided by the original data consisting of 9873 frames, and the gesture information in each frame is the vectorial velocity and acceleration of left hand, right hand, left wrist, and right wrist, represented by a 32-dimensional vector. The clustering results on these three data sets are shown in Table 2. It can be observed from Table 1 and Table 2 that SRSC always produces better clustering accuracy than other competing methods, due to its capability of capturing the locally linear manifold structure of the data and robustness to noies. In the appendix, we further show the performance of different sparse coding based methods with different dictionary size on COIL-100 data set in Table 5, and investigate the parameter sensitive of SRSC by demonstrating its performance with varying \u03b3 and K in Table 6."}, {"heading": "5.2 APPROXIMATION BY DEEP-SRSC", "text": "In this subsection, Deep-SRSC is employed as a fast encoder to approximate the support regularized sparse codes of SRSC on the USPS data set. Throughout this subsection, we show results using the first test setting introduced in Section 4, i.e. test without referring to the training data. Additional experimental results on the performance of Deep-SRSC with the second test setting, including the application to semi-supervised learning by label propagation (Zhu et al., 2003), are shown in the appendix.\nThe Deep-SRSC network is trained on the training set of the USPS data comprising 7291 images. We adopt three depth settings wherein Deep-SRSC has 1 layer, 2 layers, and 6 layers respectively. We first run SRSC on the training set of USPS data to obtain the dictionary Dsr and the support regularized sparse codes Zsr of the training data. Then the optimization problem (7) is solved by the PGD-style iterative method in Section 2.1.2, whereX is the test data, A is the adjacency matrix of the KNN graph over the test data, to obtain the support regularized sparse codes Zsr,test of the test data with dictionary Dsr. Zsr is used as the ground truth support regularized sparse codes to train Deep-SRSC, and Zsr,test serves as the ground truth codes of the test data. The approximate codes of the test data of the USPS data are obtained by feeding forward them into the Deep-SRSC network together with the KNN graph over the test data, and the prediction error of Deep-SRSC is the average of the squared error between the predicted codes and Zsr,test. Figure 4 illustrates the training error of Deep-SRSC w.r.t. the epoch number for 1 layer, 2 layers, and 6 layers respectively, and Figure 5 in the appendix illustrates the test error of Deep-SRSC. For each depth setting, Deep-SRSC is trained with 300 epoches, and testing is performed for every 5 epoches during training. It can be observed that deeper Deep-SRSC leads to smaller training and test error. Deep-SRSC is implemented with TensorFlow (Abadi et al., 2016). The initial learning rate is set to 10\u22124, and divided by 10 at 100-th epoch and 200-th epoch, so the final learning rate is 10\u22126 upon the termination of the training.\nTable 3 shows the prediction error of Deep-SRSC for different dictionary size p and different number of layers. It can be observed that Deep-SRSC with more layers demonstrates smaller prediction error for the same dictionary size due to its better representation capability, and smaller dictionary size leads to less prediction error for the same number of layers due to the reduced difficulty of representation. Moreover, the codes predicted by 6-layer Deep-SRSC are used to perform clustering on the test data because of its minimum prediction error, with comparison to the performance of sparse coding and `2-RSC shown in Table 4 with respect to different dictionary size. For either sparse coding or `2-RSC, the dictionary is firstly learned on the training data, then the sparse codes of the test data obtained with respect to that dictionary are used to perform clustering on the test set of USPS data. We can see that SRSC together with its approximation, 6-layer Deep-SRSC, achieve the highest accuracy and NMI. In addition, a reasonably large dictionary benefits SRSC, e.g. increasing p from 100 to 300 boosts its accuracy, since the dictionary atoms serve as the basis for the locally linear structures (local subspaces) of the data manifold and a sufficiently large dictionary size is favorable for modeling all such locally linear structures. On the other hand, a too large dictionary (such as p = 500) imposes much difficulty on the optimization which can even hurt the performance of SRSC, `2-RSC and regular sparse coding."}, {"heading": "6 CONCLUSION", "text": "We propose Support Regularized Sparse Coding (SRSC) which captures the locally linear manifold structure of the high-dimensional data for sparse coding and enjoys robustness to noise. SRSC achieves this goal by encouraging nearby data in the manifold to share dictionary atoms. The optimization algorithm of SRSC is presented with theoretical guarantee for the optimization over the sparse codes. In addition, we propose Deep-SRSC, a feed-forward neural network, as a fast encoder to approximate the support regularized sparse codes produce by SRSC. Experimental results demonstrate the effectiveness of SRSC by its application to data clustering, and show that Deep-SRSC renders approximate codes for SRSC with low prediction error. The approximate codes generated by 6-layer Deep-SRSC also deliver compelling empirical performance for data clustering."}, {"heading": "APPENDIX", "text": ""}, {"heading": "PROOFS", "text": "Proof of Proposition 1. Note that u is the optimal solution to the lasso problem arg minv\u2208Rp \u03c4s 2 \u2016v\u2212 Z\u0303(t)ki \u2016 2 2 + \u03bb\u2016v\u20161. Define Tk(v) = \u03c4s2 (v \u2212 Z\u0303 (t) ki ) 2 + \u03bb|v| for v \u2208 IR, then uk = arg minv\u2208IR Tk(v). Since the\ntwo functions Hk(v) and Tk(v) only differ at v = 0, arg minv\u2208{uk,0}Hk(v) is the optimal solution to minv\u2208RHk(v) when uk 6= 0 or uk = 0 and GAki \u2265 0.\nWhen uk = 0 and GAki < 0, when \u03b5\u2192 0 and \u03b5 6= 0, Hk(v)\u2192 GAki and infv\u2208IRHk(v) = \u03c4s2 (Z\u0303 (t) ki ) 2 + \u03b3GAki. Note that the infimum can never be achieved. Since infv\u2208IRHk(v) < Hk(Z (t\u22121) ki ), we can always find \u03b5 6= 0 such that Hk(\u03b5) \u2264 Hk(Z(t\u22121)ki ).\nDefine H(v) = \u03c4s 2 \u2016v \u2212 Z\u0303i (t) \u201622 + \u03bb\u2016v\u20161 + \u03b3RA(v). Based on the above argument, H(Zi (t) ) \u2264 H(Zi (t\u22121) ) which indicates that\n\u03c4s 2 \u2016Zi(t) \u2212 Zi(t\u22121)\u201622 + \u3008Zi (t) \u2212 Zi(t\u22121),\u2207Q(Zi(t\u22121))\u3009 (25)\n+ \u03bb\u2016Zi(t)\u20161 + \u03b3RA(Zi (t) ) \u2264 \u03bb\u2016Zi(t\u22121)\u20161 + \u03b3RA(Zi (t\u22121) ) (26)\nAlso, since s is the Lipschitz constant for the gradient of function Q(\u00b7), we have\nQ(Zi (t) ) \u2264 Q(Zi(t\u22121)) + \u3008Zi(t) \u2212 Zi(t\u22121),\u2207Q(Zi(t\u22121))\u3009+ s 2 \u2016Zi(t) \u2212 Zi(t\u22121)\u201622 (27)\nCombining (25) and (27),\nF (Zi (t) ) \u2264 F (Zi(t\u22121))\u2212 (\u03c4 \u2212 1)s 2 \u2016Zi(t) \u2212 Zi(t\u22121)\u201622\nProof of Lemma 1. We first prove that the sequences {Zi(t)}t is bounded for any 1 \u2264 i \u2264 n. By Proposition 1, the sequence {F (Zi(t))}t decreases, so we have\nF (Zi (t) ) = 1\n2 \u2016xi \u2212DZi (t)\u201622 + \u03bb\u2016Zi (t)\u20161 + \u03b3RA(Zi (t) )\n\u2264 \u2016xi \u2212DZi (0)\u201622 + \u03bb\u2016Zi (0)\u20160 \u2264 1 + RA(Zi (0) )\nfor t \u2265 1. Therefore,\n\u2016Zi(t)\u20161 \u2264 1\n\u03bb \u2016xi \u2212DZi (0)\u201622 + \u03bb\u2016Zi (0)\u20160 \u2264 1 + RA(Zi (0) )\nIt follows that \u2016Zi(t)\u20161 is bounded, and \u2016Zi (t)\u20162 is also bounded. Since GAki \u2265 0 for k = 1, . . . , p and\nthe indicator function 1I\u00b76=0 is semi-algebraic function, RA(\u00b7) is also a semi-algebraic function and lower semicontinuous. Therefore, according to Theorem 1 by Bolte et al. (2014), {Zi(t)}t converges to a critical point of F (Zi), denoted by Z\u0302i.\nLet v\u0302 = D>(DZ\u0302i \u2212 xi) + P\u0307(Z\u0302i; b). For k such that GAki = 0, since Z\u0302i is a critical point of F (Zi), v\u0302k = 0.\nNow we consider the case that GAki 6= 0.\nFor for k \u2208 S\u0302i, since Z\u0302i is a critical point of F (Zi) = 12\u2016xi \u2212 DZ i\u201622 + \u03bb\u2016Zi\u20161 + \u03b3RA(Zi). then \u2202(Q+\u03bb\u2016Zi\u20161) \u2202Zi\nk\n| Zi=Z\u0302i\n= 0 because \u2202RA(Z i)\n\u2202Zi k\n| Zi=Z\u0302i = 0 . Note that mink\u2208S\u0302i |Z\u0302 i k| > b, so \u2202T \u2202Zi\nk\n| Zi=Z\u0302i = 0, and\nit follows that v\u0302k = 0.\nFor k /\u2208 S\u0302i, since dPkdZi k\n(Z\u0302ik+; b) = \u03b3GAki b +\u03bb and dPk dZi\nk\n(Z\u0302ik\u2212; b) = \u2212 \u03b3GAki b \u2212\u03bb, \u03b3G A ki b +\u03bb \u2265 | \u2202Q \u2202Zi\nk\n| Zi=Z\u0302i |, we\ncan choose the k-th element of P\u0307(Z\u0302i; b) such that v\u0302k = 0. Therefore, \u2016v\u0302\u20162 = 0, and Z\u0302i is a local solution to the problem (21).\nNow we prove that Zi\u2217 is also a local solution to (21). Let v\u2217 = D>(DZi\u2217 \u2212 xi) + P\u0307(Zi \u2217 ; b), and Q is defined as before. For k such that GAki = 0, since Z i\u2217 is the globally optimal solution of F (Zi), v\u2217k = 0.\nAgain we consider the case that GAki 6= 0.\nFor k \u2208 S\u2217i , since Zi \u2217 is the globally optimal solution to problem (8), we also have \u2202(Q+\u03bb\u2016Z i\u20161) \u2202Zi\nk\n|Zi=Zi\u2217 = 0.\nIf it is not the case and \u2202(Q+\u03bb\u2016Z i\u20161)\n\u2202Zi k\n|Zi=Zi\u2217 6= 0, then we can change Zik by a small amount in the direction\nof the gradient \u2202(Q+\u03bb\u2016Z i\u20161)\n\u2202Zi k\nat the point Zi = Zi\u2217 while Zik is still nonzero, leading to a smaller value of the\nobjective F (Zi).\nNote that mink\u2208S\u2217i |Z i k \u2217| > b, so \u2202T\n\u2202Zi k\n| Zi=Z\u0302i = 0, and it follows that v\u2217k = 0.\nFor k /\u2208 S\u2217i , since \u03b3GAki b + \u03bb \u2265 maxk/\u2208S\u0302i | \u2202Q\n\u2202Zi k\n|Zi=Zi\u2217 |, we can choose the k-th element of P\u0307(Zi \u2217 ; b) such\nthat v\u2217k = 0. It follows that \u2016v\u2217\u20162 = 0, and Zi \u2217 is also a local solution to the problem (21).\nProof of Theorem 1. According to Lemma 1, both Z\u0302i and Zi\u2217 are local solutions to problem (21). In the following text, let \u03b2I indicates a vector whose elements are those of \u03b2 with indices in I. Let \u2206 = Zi\n\u2217 \u2212 Z\u0302i, \u2206\u0303 = P\u0307(Zi \u2217 )\u2212 P\u0307(Z\u0302i). By Lemma 1, we have\n\u2016D>D\u2206 + \u2206\u0303\u20162 = 0 It follows that\n\u2206>D>D\u2206 + \u2206>\u2206\u0303 \u2264 \u2016\u2206\u20162\u2016D>D\u2206 + \u2206\u0303\u20162 = 0\nAlso, by the proof of Lemma 1, for k \u2208 S\u0302i \u2229 S\u2217i , since (D>D\u2206)k = 2\u03bb1IZi k \u2217Z\u0302i k <0 + 01IZi k \u2217Z\u0302i k >0 we have\n\u2206\u0303k = \u2212(D>D\u2206)k. We now present another property on any nonconvex function P using the degree of nonconvexity in Definition 3: \u03b8(t, \u03ba) := sups{\u2212sgn(s\u2212 t)(P\u0307 (s; b)\u2212 P\u0307 (t; b))\u2212 \u03ba|s\u2212 t|} on the regularizer P. For any s, t \u2208 IR, we have\n\u2212 sgn(s\u2212 t) ( P\u0307 (s; b)\u2212 P\u0307 (t; b) ) \u2212 \u03ba|s\u2212 t| \u2264 \u03b8(t, \u03ba)\nby the definition of \u03b8. It follows that \u03b8(t, \u03ba)|s\u2212 t| \u2265 \u2212(s\u2212 t) ( P\u0307 (s; b)\u2212 P\u0307 (t; b) ) \u2212 \u03ba(s\u2212 t)2\n\u2212 (s\u2212 t) ( P\u0307 (s; b)\u2212 P\u0307 (t; b) ) \u2264 \u03b8(t, \u03ba)|s\u2212 t|+ \u03ba(s\u2212 t)2 (28)\nApplying (28) with P = Pk for k = 1, . . . , p, we have\n\u2206>D>D\u2206 \u2264 \u2212\u2206>\u2206\u0303 = \u2212\u2206>S\u0303i\u2206\u0303S\u0303i \u2212\u2206 > S\u0302i\u2229S\u2217i \u2206\u0303S\u0302i\u2229S\u2217i\n\u2264 |ZiS\u0303i \u2217 \u2212 Z\u0302iS\u0303i | >\u03b8(Z\u0302iS\u0303i , \u03ba) + \u03ba\u2016Z i S\u0303i \u2217 \u2212 Z\u0302iS\u0303i\u2016 2 2 + \u2016\u2206S\u0302i\u2229S\u2217i \u20162\u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162 \u2264 \u2016\u03b8(Z\u0302iS\u0303i , \u03ba)\u20162\u2016Z i S\u0303i \u2217 \u2212 Z\u0302iS\u0303i\u20162 + \u03ba\u2016Z i S\u0303 \u2217 \u2212Z\u0302 i S\u0303\u2016 2 2 + \u2016\u2206\u20162\u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162 \u2264 \u2016\u03b8(Z\u0302iS\u0303i , \u03ba)\u20162\u2016\u2206\u20162 + \u03ba\u2016\u2206\u2016 2 2 + \u2016\u2206\u20162\u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162 (29)\nOn the other hand, \u2206>D>D\u2206 \u2265 \u03ba20\u2016\u2206\u201622. It follows from (29) that\n\u03ba20\u2016\u2206\u201622 \u2264 \u2016\u03b8(Z\u0302iS\u0303i , \u03ba)\u20162\u2016\u2206\u20162 + \u03ba\u2016\u2206\u2016 2 2 + \u2016\u2206\u20162\u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162\nWhen \u2016\u2206\u20162 6= 0, we have\n\u03ba20\u2016\u2206\u20162 \u2264 \u2016\u03b8(Z\u0302iS\u0303i , \u03ba)\u20162 + \u03ba\u2016\u2206\u20162 + \u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162\n\u21d2 \u2016\u2206\u20162 \u2264 \u2016\u03b8(Z\u0302i S\u0303i , \u03ba)\u20162 + \u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162 \u03ba20 \u2212 \u03ba\n(30)\nAccording to the definition of \u03b8, it can be verified that \u03b8(t, \u03ba) = max{0, \u03b3G A ki b \u2212 \u03ba|t \u2212 b|} for |t| > b, and \u03b8(0+, \u03ba) = max{0, \u03b3G A ki b \u2212 \u03bab}. Therefore,\n\u2016\u03b8(Z\u0302iS\u0303i , \u03ba)\u20162 = ( \u2211 k\u2208S\u0303i\u2229S\u0302i (max{0, \u03b3G A ki b \u2212 \u03ba|Z\u0302ki \u2212 b|})2+\n\u2211 k\u2208S\u0303i\\S\u0302i (max{0, \u03b3G A ki b \u2212 \u03bab})2 ) 1 2 (31)\nTherefore,\n\u2016\u2206\u20162 \u2264 1\n\u03ba20 \u2212 \u03ba (( \u2211 k\u2208S\u0303i\u2229S\u0302i (max{0, \u03b3G A ki b \u2212 \u03ba|Z\u0302ki \u2212 b|})2+\n\u2211 k\u2208S\u0303i\\S\u0302i (max{0, \u03b3G A ki b \u2212 \u03bab})2 ) 1 2 + \u2016\u2206\u0303S\u0302i\u2229S\u2217i \u20162 ) (32)\nwhere \u2206\u0303k = \u2212(D>D\u2206)k = \u22122\u03bb1IZi k \u2217Z\u0302i k <0 \u2212 01IZi k \u2217Z\u0302i k >0 for k \u2208 S\u0302i \u2229 S \u2217 i . This proves the result of this\ntheorem."}, {"heading": "MORE EXPERIMENTAL RESULTS", "text": "The test error of Deep-SRSC with different dictionary size, corresponding to Figure 4 showing the training error, is illustrated in Figure 5. We vary the dictionary size and show the clustering results on COIL-100 data set in Table 5, and we can see that SRSC always achieves highest accuracy and NMI with different dictionary size.\nIn addition, we investigate the parameter sensitivity of SRSC, and show in Table 6 the performance change while varying \u03b3, the weight for the support regularization term, and K, the number of nearest neighbors when building the KNN graph for the support regularization term, on the USPS data set. It can be observed that the performance of SRSC is stable over a relatively large range of \u03bb and K. SRSC often has the highest NMI while maintaining a very competitive accuracy."}, {"heading": "DEEP-SRSC WITH THE SECOND TEST SETTING (REFERRING TO THE TRAINING DATA)", "text": "We demonstrate the performance of SRSC and Deep-SRSC with the second test setting (referring to the training data) on clustering and semi-supervised learning. The ground truth code of the each test data point is computed by performing the PGD-style iterative method to solve the problem (8) where xi is the test point, D is Dsr obtained from the training data as in Section 5.2, A is the adjacency matrix of the KNN graph over the test point and the training data. Table 9 shows the prediction error of Deep-SRSC for different dictionary size p and different number of layers on the USPS data, which is comparable to the case of the first test setting in Table 3.\nTwo more data sets are used in this subsection, i.e. MNIST for hand-written digit recognition and CIFAR-10 for image recognition. MNIST is comprised of 60000 training images and 10000 test images of ten digits from 0 to 9, and each image is of size 28\u00d7 28 and represented as a 784-dimensional vector. CIFAR-10 consists of 50000 training images and 10000 testing images in 10 classes, and each image is a color one of size 32\u00d7 32. Using the second test setting, Deep-SRSC is trained on the training set, and the codes of the test set predicted by 6-layer Deep-SRSC are used to perform clustering on the test set for MNIST and CIFAR-10 data, with comparison to other sparse coding based methods. The clustering results are shown in Table 7 and 8 respectively with dictionary size p = 300. We observe that SRSC and Deep-SRSC always achieve the best performance compared to other competing methods. We employ the fast deep neural network named CNN-F (Chatfield et al., 2014) trained on the ILSVRC 2012 data to extract the 4096-dimensional feature vector for each image in the CIFAR-10 data, and all the clustering methods are performed on the extracted features. In addition to the coordinate descent method employed in Section 2.1.2 and Algorithm 1 for the optimization of the sparse codes in SRSC, we further conduct the empirical study showing that the parallel coordinate descent method, which updates the coordinates in parallel for improved efficiency and fits the needs of large-scale data optimization, leads to almost the same results as the coordinate descent method on the CIFAR-10 data. Instead of optimization with respect to the sparse code of a single data point in the coordinate descent method, the parallel coordinate descent method updates the sparse codes of P data points in parallel using the same rule as that in the coordinate descent method in Section 2.1.2 and Algorithm 1. While the parallel coordinate descent method is originally designed for convex problems (Bradley et al., 2011; Richta\u0301rik & Taka\u0301c\u030c, 2016), it demonstrates almost the same empirical performance as the coordinate descent method for the clustering task on the test set of the CIFAR-10 data, with the accuracy of 0.4622 and NMI of 0.3864. P -parallel coordinate descent leads to P times speedup compared to the coordinate descent method. We choose P = 10 and the codes of the training data of CIFAR-10 are learned by the parallel coordinate descent method, and note that the optimization of the codes of the test data are inherently parallelizable due to the nature of the second test setting studied in this subsection.\nMoreover, Table 10 shows the prediction error of Deep-SRSC on the MNIST data and the CIFAR-10 data. It can be observed again that deeper Deep-SRSC network leads to smaller prediction error.\nWe also show the application to semi-supervised learning via label propagation (Zhu et al., 2003), a widely used semi-supervised learning method. Given the data {x1,x2, . . . ,xl,xl+1, . . . ,xn} \u2282 IRd, the first l points {x1,x2, . . . ,xl} are labeled and named the training data, and the remaining n \u2212 l points form the test data for semi-supervised learning. Semi-supervised learning by label propagation aims to predict the labels of the test data by encouraging local smoothness of the labels in accordance with the similarity matrix over the entire data. The performance of label propagation depends on the similarity matrix. For each sparse coding based method, the similarity matrix Y over the entire data is built by the support similarity introduced in Section 5.1: Yij = Aijq\n> Z\u0302i q Z\u0302j , and Zi is the code of data point xi for different sparse coding methods including the 6-layer Deep-SRSC with the second test setting. Label propagation is performed on the similarity matrix Y to obtain the labels of the test data, and the error rate is reported. Note that in the experiment of semi-supervised learning by label propagation, the codes of the test data of each data set are obtained first (e.g. the 10000 test images in the MNIST data). If xi belongs to the test data of a data set, its code is obtained by performing the the corresponding sparse coding optimization with the dictionary learned on the training data of that data set; for SRSC and Deep-SRSC, such optimization also has the KNN graph over the test point xi and the training data as input. With the codes of all the data, the similarity matrix Y over the entire data is constructed. Then, a randomly sampled subset of each class is labeled as the training data, with the other data serving as the test data for semi-supervised learning.\nThe semi-supervised learning results of our methods are compared to that of the Gaussian kernel graph (Gaussian), i.e. the KNN graph with the edge weight set by the Gaussian kernel; Sparse Coding (SC) and `2-RSC; and manifold based similarity adaptation (MBS) by Karasuyama & Mamitsuka (2013), one of the state-of-the-art semi-supervised learning methods based on label propagation. MBS learns the manifold aligned edge similarity by local reconstruction for label propagation.\nThe comparison results of semi-supervised learning by label propagation on the USPS data and the MNIST data are shown in Figure 6 and 7, which illustrate the error rate of label propagation with respect to different number of labeled data points in each class. We can observe from Figure 6 that SRSC and Deep-SRSC with the second test setting lead to superior results on the application to semi-supervised learning, and the performance of SRSC and Deep-SRSC is always the best with respect to different dictionary size. It can also be observed from Figure 6 and 7 that SRSC and Deep-SRSC have very similar performance, revealing the good quality of the fast approximation by Deep-SRSC on the semi-supervised learning task. Furthermore, SRSC and Deep-SRSC significantly outperform other baseline methods with the small number of labeled data points in each class, due to the captured locally linear manifold structure.\nTable 9: Prediction error (average squared error between the predicted codes and the ground truth codes) of Deep-SRSC with different depth and different dictionary size on the test set of the USPS data, using the second test setting\nDictionary Size 1-layer 2-layer 6-layer p = 100 0.05 0.03 0.03 p = 300 0.12 0.08 0.06 p = 500 0.27 0.11 0.09\nTable 10: Prediction error (average squared error between the predicted codes and the ground truth codes) of Deep-SRSC with different depth on the test set of the MNIST data and CIFAR-10 data, using the second test setting\nData Set 1-layer 2-layer 6-layer MNIST 0.15 0.12 0.10\nCIFAR-10 0.16 0.13 0.13"}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vasudevan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2016}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM J. Img. Sci.,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Proximal alternating linearized minimization for nonconvex and nonsmooth problems", "author": ["J\u00e9r\u00f4me Bolte", "Shoham Sabach", "Marc Teboulle"], "venue": "Math. Program.,", "citeRegEx": "Bolte et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bolte et al\\.", "year": 2014}, {"title": "Parallel coordinate descent for l1regularized loss minimization", "author": ["Joseph K. Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "The restricted isometry property and its implications for compressed sensing", "author": ["Emmanuel J. Cands"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "Cands.,? \\Q2008\\E", "shortCiteRegEx": "Cands.", "year": 2008}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Sparse representation and learning in visual recognition: Theory and applications", "author": ["Hong Cheng", "Zicheng Liu", "Lu Yang", "Xuewen Chen"], "venue": "Signal Process.,", "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "Chung.,? \\Q1997\\E", "shortCiteRegEx": "Chung.", "year": 1997}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Sparse manifold clustering and embedding", "author": ["Ehsan Elhamifar", "Ren\u00e9 Vidal"], "venue": "In NIPS, pp", "citeRegEx": "Elhamifar and Vidal.,? \\Q2011\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2011}, {"title": "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "author": ["Shenghua Gao", "Ivor Wai-Hung Tsang", "Liang-Tien Chia"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["Karol Gregor", "Yann LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Gregor and LeCun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Laplacian regularized gaussian mixture model for data clustering", "author": ["Xiaofei He", "Deng Cai", "Yuanlong Shao", "Hujun Bao", "Jiawei Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Locally linear hashing for extracting non-linear manifolds", "author": ["Go Irie", "Zhenguo Li", "Xiao-Ming Wu", "Shih-Fu Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Irie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irie et al\\.", "year": 2014}, {"title": "Manifold-based similarity adaptation for label propagation", "author": ["Masayuki Karasuyama", "Hiroshi Mamitsuka"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Karasuyama and Mamitsuka.,? \\Q2013\\E", "shortCiteRegEx": "Karasuyama and Mamitsuka.", "year": 2013}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Robust single image super-resolution via deep networks with sparse prior", "author": ["Ding Liu", "Zhaowen Wang", "Bihan Wen", "Jianchao Yang", "Wei Han", "Thomas S. Huang"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Gaussian mixture model with local consistency", "author": ["Jialu Liu", "Deng Cai", "Xiaofei He"], "venue": "In AAAI,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Online dictionary learning for sparse coding", "author": ["Julien Mairal", "Francis R. Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2016\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2016}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": "SCIENCE, 290:2323\u20132326,", "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Learning A deep L\u221e encoder for hashing", "author": ["Zhangyang Wang", "Yingzhen Yang", "Shiyu Chang", "Qing Ling", "Thomas S. Huang"], "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Collaborative linear coding for robust image classification", "author": ["Zilei Wang", "Jiashi Feng", "Shuicheng Yan"], "venue": "Int. J. Comput. Vis.,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["Jianchao Yang", "Kai Yu", "Yihong Gong", "Thomas S. Huang"], "venue": "In CVPR,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Low-rank sparse coding for image classification", "author": ["Tianzhu Zhang", "Bernard Ghanem", "Si Liu", "Changsheng Xu", "Narendra Ahuja"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Graph regularized sparse coding for image representation", "author": ["Miao Zheng", "Jiajun Bu", "Chun Chen", "Can Wang", "Lijun Zhang", "Guang Qiu", "Deng Cai"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Zheng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2011}, {"title": "Locality preserving clustering for image database", "author": ["Xin Zheng", "Deng Cai", "Xiaofei He", "Wei-Ying Ma", "Xueyin Lin"], "venue": "In Proceedings of the 12th Annual ACM International Conference on Multimedia,", "citeRegEx": "Zheng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2004}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": "Sparse coding is widely applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation with convincing performance for classification and clustering (Yang et al., 2009; Cheng et al., 2013; Zhang et al., 2013).", "startOffset": 227, "endOffset": 286}, {"referenceID": 8, "context": "Sparse coding is widely applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation with convincing performance for classification and clustering (Yang et al., 2009; Cheng et al., 2013; Zhang et al., 2013).", "startOffset": 227, "endOffset": 286}, {"referenceID": 26, "context": "Sparse coding is widely applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation with convincing performance for classification and clustering (Yang et al., 2009; Cheng et al., 2013; Zhang et al., 2013).", "startOffset": 227, "endOffset": 286}, {"referenceID": 18, "context": "Inspired by LISTA, a series of previous works have designed neural networks to simulate different forms of linear coding and achieve end-to-end training for different tasks such as image super-resolution (Liu et al., 2016) and hashing (Wang et al.", "startOffset": 204, "endOffset": 222}, {"referenceID": 19, "context": "In order to obtain the sparse codes that account for the geometric information and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin et al.", "startOffset": 163, "endOffset": 236}, {"referenceID": 14, "context": "In order to obtain the sparse codes that account for the geometric information and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin et al.", "startOffset": 163, "endOffset": 236}, {"referenceID": 27, "context": "In order to obtain the sparse codes that account for the geometric information and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin et al.", "startOffset": 163, "endOffset": 236}, {"referenceID": 12, "context": "In order to obtain the sparse codes that account for the geometric information and manifold structure of the data, many regularized sparse coding methods, such as (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013), employ manifold assumption (Belkin et al.", "startOffset": 163, "endOffset": 236}, {"referenceID": 3, "context": ", 2013), employ manifold assumption (Belkin et al., 2006).", "startOffset": 36, "endOffset": 57}, {"referenceID": 15, "context": "The success of several manifold learning methods, including LLE (Roweis & Saul, 2000), SMCE (Elhamifar & Vidal, 2011) and Locally Linear Hashing (Irie et al., 2014), is built on exploiting the locally linear structure of manifold.", "startOffset": 145, "endOffset": 164}, {"referenceID": 17, "context": "Similar to (Lee et al., 2006), problem (5) is optimized alternatingly with respect to the dictionary D and the sparse codes Z respectively with the other variable fixed.", "startOffset": 11, "endOffset": 29}, {"referenceID": 17, "context": "which can be solved using Lagrangian dual (Lee et al., 2006).", "startOffset": 42, "endOffset": 60}, {"referenceID": 4, "context": "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method (Bolte et al., 2014), proximal gradient descent method (PGD) is used to optimize the nonconvex problem (8).", "startOffset": 102, "endOffset": 122}, {"referenceID": 4, "context": "Although the proximal mapping is typically associated with a lower semicontinuous function (Bolte et al., 2014) and it can be verified that RA is not always lower semicontinuous, we can still derive a PGD-styple iterative method to optimize (8).", "startOffset": 91, "endOffset": 111}, {"referenceID": 20, "context": "We solve the ordinary sparse coding problem (1) by the online dictionary learning method (Mairal et al., 2009) and use the dictionary and the sparse codes as the initialization D and Z for the alternating method in Algorithm 1.", "startOffset": 89, "endOffset": 110}, {"referenceID": 17, "context": "It should be emphasized that the optimization over the dictionary for SRSC has the same efficiency as the efficient sparse coding method (Lee et al., 2006),", "startOffset": 137, "endOffset": 155}, {"referenceID": 10, "context": "and the optimization over the sparse code of each data point by the PGD-style iterative method (10) and (11) is almost as efficient as the widely used Iterative Shrinkage and Thresholding Algorithm (ISTA) (Daubechies et al., 2004; Beck & Teboulle, 2009).", "startOffset": 205, "endOffset": 253}, {"referenceID": 3, "context": "2 RELATED WORK: `(2) REGULARIZED SPARSE CODING (`(2)-RSC) The manifold assumption (Belkin et al., 2006) is usually employed by existing regularized sparse coding methods (Liu et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 19, "context": ", 2006) is usually employed by existing regularized sparse coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain the sparse code according to the manifold structure of the data.", "startOffset": 74, "endOffset": 147}, {"referenceID": 14, "context": ", 2006) is usually employed by existing regularized sparse coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain the sparse code according to the manifold structure of the data.", "startOffset": 74, "endOffset": 147}, {"referenceID": 27, "context": ", 2006) is usually employed by existing regularized sparse coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain the sparse code according to the manifold structure of the data.", "startOffset": 74, "endOffset": 147}, {"referenceID": 12, "context": ", 2006) is usually employed by existing regularized sparse coding methods (Liu et al., 2010; He et al., 2011; Zheng et al., 2011; Gao et al., 2013) to obtain the sparse code according to the manifold structure of the data.", "startOffset": 74, "endOffset": 147}, {"referenceID": 27, "context": "Interpreting the sparse code of a data point as its embedding, the manifold assumption in the case of sparse coding for most existing methods requires that if two points xi and xj are close in the intrinsic geometry of the submanifold, their corresponding sparse codes Z and Z are also expected to be similar to each other in the sense of `(2)-distance (Zheng et al., 2011; Gao et al., 2013).", "startOffset": 353, "endOffset": 391}, {"referenceID": 12, "context": "Interpreting the sparse code of a data point as its embedding, the manifold assumption in the case of sparse coding for most existing methods requires that if two points xi and xj are close in the intrinsic geometry of the submanifold, their corresponding sparse codes Z and Z are also expected to be similar to each other in the sense of `(2)-distance (Zheng et al., 2011; Gao et al., 2013).", "startOffset": 353, "endOffset": 391}, {"referenceID": 9, "context": "Based on the spectral graph theory (Chung, 1997), extensive literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local manifold structure (Belkin et al.", "startOffset": 35, "endOffset": 48}, {"referenceID": 3, "context": "Based on the spectral graph theory (Chung, 1997), extensive literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local manifold structure (Belkin et al., 2006; Zheng et al., 2011; Gao et al., 2013).", "startOffset": 178, "endOffset": 237}, {"referenceID": 27, "context": "Based on the spectral graph theory (Chung, 1997), extensive literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local manifold structure (Belkin et al., 2006; Zheng et al., 2011; Gao et al., 2013).", "startOffset": 178, "endOffset": 237}, {"referenceID": 12, "context": "Based on the spectral graph theory (Chung, 1997), extensive literature uses graph Laplacian to impose local smoothness of the embedding and preserve the local manifold structure (Belkin et al., 2006; Zheng et al., 2011; Gao et al., 2013).", "startOffset": 178, "endOffset": 237}, {"referenceID": 24, "context": "Moreover, as pointed out by (Wang et al., 2015), support regularization offers robustness to noise for sparse coding.", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": "the Accuracy (AC) and the Normalized Mutual Information (NMI) (Zheng et al., 2004).", "startOffset": 62, "endOffset": 82}, {"referenceID": 27, "context": "We also set \u03b3 (2)) = 1 which is the suggested default value in (Zheng et al., 2011), and M = Mz = 5 and Mp = 50 in Algorithm 1.", "startOffset": 63, "endOffset": 83}, {"referenceID": 29, "context": "Additional experimental results on the performance of Deep-SRSC with the second test setting, including the application to semi-supervised learning by label propagation (Zhu et al., 2003), are shown in the appendix.", "startOffset": 169, "endOffset": 187}], "year": 2017, "abstractText": "Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_169"}