{"name": "ICLR_2017_16.pdf", "metadata": {"source": "CRF", "title": "MAXIMUM ENTROPY FLOW NETWORKS", "authors": ["Gabriel Loaiza-Ganem", "Yuanjun Gao"], "emails": ["gl2480@columbia.edu", "yg2312@columbia.edu", "jpc2181@columbia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The maximum entropy (ME) principle (Jaynes, 1957) states that subject to some given prior knowledge, typically some given list of moment constraints, the distribution that makes minimal additional assumptions \u2013 and is therefore appropriate for a range of applications from hypothesis testing to price forecasting to texture synthesis \u2013 is that which has the largest entropy of any distribution obeying those constraints. First introduced in statistical mechanics by Jaynes (1957), and considered both celebrated and controversial, ME has been extensively applied in areas including natural language processing (Berger et al., 1996), ecology (Phillips et al., 2006), finance (Buchen & Kelly, 1996), computer vision (Zhu et al., 1998), and many more.\nContinuous ME modeling problems typically include certain expectation constraints, and are usually solved by introducing Lagrange multipliers, which under typical assumptions yields an exponential family distribution (also called Gibbs distribution) with natural parameters such that the expectation constraints are obeyed. Unfortunately, fitting ME distributions in even modest dimensions poses significant challenges. First, optimizing the Lagrangian for a Gibbs distribution requires evaluating the normalizing constant, which is in general computationally very costly and error prone. Secondly, in all but the rarest cases, there is no way to draw samples independently and identically from this Gibbs distribution, even if one could derive it. Third, unlike in the discrete case where a number of recent and exciting works have addressed the problem of estimating entropy from discrete-valued data (Jiao et al., 2015; Valiant & Valiant, 2013), estimating differential entropy from data samples remains inefficient and typically biased. These shortcomings are critical and costly, given the common use of ME distributions for generating reference data samples for a null distribution of a test statistic. There is thus ample need for a method that can both solve the ME problem and produce a solution that is easy and fast to sample.\nIn this paper we develop maximum entropy flow networks (MEFN), a stochastic-optimization-based framework and algorithm for fitting continuous maximum entropy models. Two key steps are required. First, conceptually, we replace the idea of maximizing entropy over a density directly with maximizing, over the parameter space of an indexed function family, the entropy of the density induced by mapping a simple distribution (a Gaussian) through that optimized function. Modern\n\u2217These authors contributed equally.\nneural networks, particularly in variational inference (Kingma & Welling, 2013; Rezende & Mohamed, 2015), have successfully employed this same idea to generate complex distributions, and we look to similar technologies. Secondly, unlike most other objectives in this network literature, the entropy objective itself requires evaluation of the target density directly, which is unavailable in most traditional architectures. We overcome this potential issue by learning a smooth, invertible transformation that maps a simple distribution to an (approximate) ME distribution. Recent developments in normalizing flows (Rezende & Mohamed, 2015; Dinh et al., 2016) allow us to avoid biased and computationally inefficient estimators of differential entropy (such as the nearest-neighbor class of estimators like that of Kozachenko-Leonenko; see Berrett et al. (2016)). Our approach avoids calculation of normalizing constants by learning a map with an easy-to-compute Jacobian, yielding tractable probability density computation. The resulting transformation also allows us to reliably generate iid samples from the learned ME distribution. We demonstrate MEFN in detail in examples where we can access ground truth, and then we demonstrate further the ability of MEFN networks in equity option prices fitting and texture synthesis.\nPrimary contributions of this work include: (i) addressing the substantial need for methods to sample ME distributions; (ii) introducing ME problems, and the value of including entropy in a range of generative modeling problems, to the deep learning community; (iii) the novel use of constrained optimization for a deep learning application; and (iv) the application of MEFN to option pricing and texture synthesis, where in the latter we show significant increase in the diversity of synthesized textures (over current state of the art) by using MEFN."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 MAXIMUM ENTROPY MODELING AND GIBBS DISTRIBUTION", "text": "We consider a continuous random variable Z \u2208 Z \u2286 Rd with density p, where p has differential entropy H(p) = \u2212 \u222b p(z) log p(z)dz and support supp(p). The goal of ME modeling is to find, and then be able to easily sample from, the maximum entropy distribution given a set of moment and support constraints, namely the solution to:\np\u2217 = maximize H(p) (1) subject to EZ\u223cp[T (Z)] = 0\nsupp(p) = Z,\nwhere T (z) = (T1(z), ..., Tm(z)) : Z \u2192 Rm is the vector of known (assumed sufficient) statistics, andZ is the given support of the distribution. Under standard regularity conditions, the optimization problem can be solved by Lagrange multipliers, yielding an exponential family p\u2217 of the form:\np\u2217(z) \u221d e\u03b7 >T (z) 1(z \u2208 Z) (2)\nwhere \u03b7 \u2208 Rm is the choice of natural parameters of p\u2217 such that Ep\u2217 [T (Z)] = 0. Despite this simple form, these distributions are only in rare cases tractable from the standpoint of calculating \u03b7, calculating the normalizing constant of p\u2217, and sampling from the resulting distribution. There is extensive literature on finding \u03b7 numerically (Darroch & Ratcliff, 1972; Salakhutdinov et al., 2002; Della Pietra et al., 1997; Dudik et al., 2004; Malouf, 2002; Collins et al., 2002), but doing so requires computing normalizing constants, which poses a challenge even for problems with modest dimensions. Also, even if \u03b7 is correctly found, it is still not trivial to sample from p\u2217. Problemspecific sampling methods (such as importance sampling, MCMC, etc.) have to be designed and used, which is in general challenging (burn-in, mixing time, etc.) and computationally burdensome."}, {"heading": "2.2 NORMALIZING FLOWS", "text": "Following Rezende & Mohamed (2015), we define a normalizing flow as the transformation of a probability density through a sequence of invertible mappings. Normalizing flows provide an elegant way of generating a complicated distribution while maintaining tractable density evaluation. Starting with a simple distribution Z0 \u2208 Rd \u223c p0 (usually taken to be a standard multivariate\nGaussian), and by applying k invertible and smooth functions fi : Rd \u2192 Rd(i = 1, ..., k), the resulting variable Zk = fk \u25e6 fk\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1(Z0) has density:\npk(zk) = p0(f \u22121 1 \u25e6 f \u22121 2 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f \u22121 k (zk)) k\u220f i=1 |det(Ji(zi\u22121))|\u22121, (3)\nwhere Ji is the Jacobian of fi. If the determinant of Ji can be easily computed, pk can be computed efficiently.\nRezende & Mohamed (2015) proposed two specific families of transformations for variational inference, namely planar flows and radial flows, respectively:\nfi(z) = z + uih(w T i z + bi) and fi(z) = z + \u03b2ih(\u03b1i, ri)(z\u2212 z\u2032i), (4)\nwhere bi \u2208 R, ui,wi \u2208 Rd and h is an activation function in the planar case, and where \u03b2i \u2208 R, \u03b1i > 0, z\u2032i \u2208 Rd , h(\u03b1, r) = 1/(\u03b1 + r) and ri = ||z \u2212 z\u2032i|| in the radial. Recently Dinh et al. (2016) proposed a normalizing flow with convolutional, multiscale structure that is suitable for image modeling and has shown promise in density estimation for natural images."}, {"heading": "3 MAXIMUM ENTROPY FLOW NETWORK (MEFN) ALGORITHM", "text": ""}, {"heading": "3.1 FORMULATION", "text": "Instead of solving Equation 2, we propose solving Equation 1 directly by optimizing a transformation that maps a random variable Z0, with simple distribution p0, to the ME distribution. Given a parametric family of normalizing flows F = {f\u03c6, \u03c6 \u2208 Rq}, we denote p\u03c6(z) = p0(f \u22121 \u03c6 (z))|det(J\u03c6(z))|\u22121 as the distribution of the variable f\u03c6(Z0), where J\u03c6 is the Jacobian of f\u03c6. We then rewrite the ME problem as:\n\u03c6\u2217 = maximize H(p\u03c6) (5) subject to EZ0\u223cp0 [T (f\u03c6(Z0))] = 0\nsupp(p\u03c6) = Z.\nWhen p0 is continuous and F is suitably general, the program in Equation 5 recovers the ME distribution p\u03c6 exactly. With a flexible transformation family, the ME distribution can be well approximated. In experiments we found that taking p0 to be a standard multivariate normal distribution achieves good empirical performance. Taking p0 to be a bounded distribution (e.g. uniform distribution) is problematic for learning transformations near the boundary, and heavy tailed distributions (e.g. Cauchy distribution) caused similar trouble due to large numbers of outliers."}, {"heading": "3.2 ALGORITHM", "text": "We solved Equation 5 using the augmented Lagrangian method. Denote R(\u03c6) = E(T (f\u03c6(Z0))), the augmented Lagrangian method uses the following objective:\nL(\u03c6;\u03bb, c) = \u2212H(p\u03c6) + \u03bb>R(\u03c6) + c\n2 ||R(\u03c6)||2 (6)\nwhere \u03bb \u2208 Rm is the Lagrange multiplier and c > 0 is the penalty coefficient. We minimize Equation 6 for a non-decreasing sequence of c and well-chosen \u03bb. As a technical note, the augmented Lagrangian method is guaranteed to converge under some regularity conditions (Bertsekas, 2014). As is usual in neural networks, a proof of these conditions is challenging and not yet available, though intuitive arguments (see Appendix \u00a7A) suggest that most of them should hold. Due to the non rigorous nature of these arguments, we rely on the empirical results of the algorithm to claim that it is indeed solving the optimization problem.\nFor a fixed (\u03bb, c) pair, we optimize L with stochastic gradient descent. Owing to our choice of network and the resulting ability to efficiently calculate the density p\u03c6(z(i)) for any sample point\nAlgorithm 1 Training the MEFN 1: initialize \u03c6 = \u03c60, set c0 > 0 and \u03bb0. 2: for Augmented Lagrangian iteration k = 1, ..., kmax do 3: for SGD iteration i = 1, ..., imax do 4: Sample z(1), ..., z(n) \u223c p0, get transformed variables z(i)\u03c6 = f\u03c6(z(i)), i = 1, ..., n 5: Update \u03c6 by descending its stochastic gradient (using e.g. ADADELTA (Zeiler, 2012)):\n\u2207\u03c6L(\u03c6;\u03bbk, ck) \u2248 1\nn n\u2211 i=1 \u2207\u03c6 log p\u03c6(z(i)\u03c6 ) + 1 n n\u2211 i=1 \u2207\u03c6T (z(i)\u03c6 )\u03bbk + ck 2 n n 2\u2211 i=1 \u2207\u03c6T (z(i)\u03c6 ) \u00b7 2 n n\u2211 i=n\n2 +1\nT (z (i) \u03c6 )\n6: end for 7: Sample z(1), ..., z(n\u0303) \u223c p0, get transformed variables z(i)\u03c6 = f\u03c6(z(i)), i = 1, ..., n\u0303 8: Update \u03bbk+1 = \u03bbk + ck 1n\u0303 \u2211n\u0303 i=1 T (z (i) \u03c6 )\n9: Update ck+1 \u2265 ck (see text for detail) 10: end for\nz(i) (which are easy-to-sample iid draws from the multivariate normal p0), we compute the unbiased estimator of H(p\u03c6) with:\nH(p\u03c6) \u2248 \u2212 1\nn n\u2211 i=1 log p\u03c6(f\u03c6(z (i))) (7)\nR(\u03c6) can also be estimated without bias by taking a sample average of z(i) draws. The resulting optimization procedure is detailed in Algorithm 1, of which step 9 requires some detail: denoting \u03c6k as the resulting \u03c6 after imax SGD iterations at the augmented Lagrangian iteration k, the usual update rule for c (Bertsekas, 2014) is:\nck+1 = { \u03b2ck, if ||R(\u03c6k+1)|| > \u03b3||R(\u03c6k)|| ck, otherwise\n(8)\nwhere \u03b3 \u2208 (0, 1) and \u03b2 > 1. Monte Carlo estimation of R(\u03c6) sometimes caused c to be updated too fast, causing numerical issues. Accordingly, we changed the hard update rule for c to a probabilistic update rule: a hypothesis test is carried out with null hypothesis H0 : E[||R(\u03c6k+1)||] = E[\u03b3||R(\u03c6k)||] and alternative hypothesis H1 : E[||R(\u03c6k+1)||] > E[\u03b3||R(\u03c6k)||]. The p-value p is computed, and ck+1 is updated to \u03b2ck with probability 1 \u2212 p. We used a two-sample t-test to calculate the p-value. What results is a robust and novel algorithm for estimating maximum entropy distributions, while preserving the critical properties of being both easy to calculate densities of particular points, and being trivially able to produce truly iid samples."}, {"heading": "4 EXPERIMENTS", "text": "We first construct an ME problem with a known solution (\u00a74.1), and we analyze the MEFN algorithm with respect to the ground truth and to an approximate Gibbs solution. These examples test the validity of our algorithm and illustrate its performance. \u00a7B and \u00a74.3 then applies the MEFN to a financial data application (predicting equity option values) and texture synthesis, respectively, to illustrate the flexibility and practicality of our algorithm.\nFor \u00a74.1 and \u00a7B, We use 10 layers of planar flow with a final transformation g (specified below) that transforms samples to the specified support, and use with ADADELTA (Zeiler, 2012). For \u00a74.3 we use real NVP structure and use ADAM (Kingma & Ba, 2014) with learning rate = 0.001. For all our experiments, we use imax = 3000, \u03b2 = 4, \u03b3 = 0.25. For \u00a74.1 and \u00a7B we use n = 300, n\u0303 = 1000, kmax = 10; For \u00a74.3 we use n = n\u0303 = 2, kmax = 8."}, {"heading": "4.1 A MAXIMUM ENTROPY PROBLEM WITH KNOWN SOLUTION", "text": "Following the setup of the typical ME problem, suppose we are given a specified support S = {z = (z1, . . . , zd\u22121) : zi \u2265 0 and \u2211d\u22121 k=1 zk \u2264 1} and a set of constraints E[logZk] = \u03bak(k = 1, ..., d),\nwhere Zd = 1\u2212 \u2211d\u22121 k=1 Zk. We then write the maximum entropy program:\np\u2217 = maximize H(p) (9) subject to EZ\u223cp[logZk \u2212 \u03bak] = 0 \u2200k = 1, ..., d\nsupp(p) = S.\nThis is a general ME problem that can be solved via the MEFN. Of course, we have particularly chosen this example because, though it may not obviously appear so, the solution has a standard and tractable form, namely the Dirichlet. This choice allows us to consider a complicated optimization program that happens to have known global optimum, providing a solid test bed for the MEFN (and for the Gibbs approach against which we will compare). Specifically, given a parameter \u03b1 \u2208 Rd, the Dirichlet has density:\np(z1, . . . , zd\u22121) = 1\nB(\u03b1) d\u220f k=1 z\u03b1k\u22121k 1 ((z1, . . . , zd\u22121) \u2208 S) (10)\nwhere B(\u03b1) is the multivariate Beta function, and zd = 1 \u2212 \u2211d\u22121 k=1 zk. Note that this Dirichlet is a distribution on S and not on the (d \u2212 1)-dimensional simplex Sd\u22121 = {(z1, . . . , zd) : zk \u2265 0 and \u2211d k=1 zk = 1} (an often ignored and seemingly unimportant technicality that needs to be correct here to ensure the proper transformation of measure). Connecting this familiar distribution to the ME problem above, we simply have to choose \u03b1 such that \u03bak = \u03c8(\u03b1k)\u2212\u03c8(\u03b10) for k = 1, ..., d, where \u03b10 = \u2211d k=1 \u03b1k and \u03c8 is the digamma function. We then can pose the above ME problem to the MEFN and compare performance against ground truth. Before doing so, we must stipulate the transformation g that maps the Euclidean space of the multivariate normal p0 to the desired support S. Any sensible choice will work well (another point of flexibility for the MEFN); we use the standard transformation:\ng(z1, ..., zd\u22121) = ( ez1\u2211d\u22121\nk=1 e zk + 1\n, ..., ezd\u22121\u2211d\u22121\nk=1 e zk + 1\n)> (11)\nNote that the MEFN outputs vectors in Rd\u22121, and not Rd, because the Dirichlet is specified as a distribution on S (and not on the simplex Sd\u22121). Accordingly, the Jacobian is a square matrix and its determinant can be computed efficiently using the matrix determinant lemma. Here, p0 is set to the (d\u2212 1)-dimensional standard normal.\nWe proceed as follows: We choose \u03b1 and compute the constraints \u03ba1, ..., \u03bad. We run MEFN pretending we do not know \u03b1 or the Dirichlet form. We then take a random sample from the fitted distribution and a random sample from the Dirichlet with parameter \u03b1, and compare the two samples using the maximum mean discrepancy (MMD) kernel two sample test (Gretton et al., 2012), which assesses the fit quality. We take the sample size to be 300 for the two sample kernel test. Figure 1 shows an example of the transformation from normal (left panel) to MEFN (middle panel), and comparing that to the ground truth Dirichlet (right panel). The MEFN and ground truth Dirichlet densities shown in purple match closely, and the samples drawn (red) indeed appear to be iid draws from the same (maximum entropy) distribution in both cases.\nAdditionally, the middle panel of Figure 1 shows an important cautionary tale that foreshadows our texture synthesis results (\u00a74.3). One might suppose that satisfying the moment matching constraints is adequate to produce a distribution which, if not technically the ME distribution, is still interestingly variable. The middle panel shows the failure of this intuition: in dark green, we show a network trained to simply match the moments specified above, and the resulting distribution quite poorly expresses the variability available to a distribution with these constraints, leading to samples that are needlessly similar. Given the substantial interest in using networks to learn implicit generative models (e.g., Mohamed & Lakshminarayanan (2016)), this concern is particularly relevant and highlights the importance of considering entropy.\nFigure 2 quantitatively analyzes these results. In the left panel, for a specific choice of \u03b1 = (1, 2, 3), we show our unbiased entropy estimate of the MEFN distribution p\u03c6 as a function of the number of SGD iterations (red), along with the ground truth maximum entropy H(p\u2217) (green line). Note\nthat the MEFN stabilizes at the correct value (as a stochastic estimator, variance around that value is expected). In the middle panel, we show the distribution of MMD values for the kernel two sample test, as well as the observed statistic for the MEFN (red) and for a randomly chosen Dirichlet distribution (gray; chosen to be close to the true optimum, making a conservative comparison). The MMD test does not reject MEFN as being different from the true ME distribution p\u2217, but it does reject a Dirichlet whose KL to the true p\u2217 is small (see legend). In the right panel, for many different Dirichlets in a small grid around a single true p\u2217, the kernel two sample test statistic is computed, the MMD p-value is calculated, as is the KL to the true distribution. We plot a scatter of these points in grey, and we plot the particular MEFN solution as a red star. We see that for other Dirichlets with similar KL to the true distribution as the MEFN distribution, the p-values seem uniform, meaning that the KL to the true is indeed very small. Again this is conservative, as the grey points have access to the known Dirichlet form, whereas the MEFN considered the entire space (within its network capacity) of S supported distributions. Given this fact, the performance of MEFN is impressive."}, {"heading": "4.2 RISK-NEUTRAL ASSET PRICING", "text": "We illustrate the flexibility and practicality of our algorithm extracting the risk-neutral asset price probability based on option prices, an active and interesting area for ME models. We find that MEFN and the classic Gibbs approach yield comparable performances. Owing to space limitations we have placed these results in Appendix \u00a7B."}, {"heading": "4.3 MODELING IMAGES OF TEXTURES", "text": "Constructing generative models to generate random images with certain texture structure is an important task in computer vision. A line of texture synthesis research proceeds by first extracting a set\nof features that characterizes the target texture and then generate images that match the features. The seminal work of Zhu et al. (1998) proposes constructing texture models under the ME framework, where features (or filters) of the given texture image are adaptively added in the model and a Gibbs distribution whose expected feature matches the target texture is learnt. One major difficulty with the method is that both model learning and image generation involve sampling from a complicated Gibbs distribution. More recent works exploit more complicated features (Portilla & Simoncelli, 2000; Gatys et al., 2015; Ulyanov et al., 2016). Ulyanov et al. (2016) propose the texture net, which uses a texture loss function by using the Gram matrices of the outputs of some convolutional layers of a pre-trained deep neural network for object recognition.\nWhile the use of these complicated features does provide high-quality synthetic texture images, that work focuses exclusively on generating images that match these feature (moments). Importantly, this network focuses only on generating feature-matching images without using the ME framework to promote the diversity of the samples. Doing so can be deeply problematic: in Figure 1 (middle panel), we showed the lack of diversity resulting from only moment matching in that Dirichlet setting, and further we note that the extreme pathology would result in a point mass on the training image \u2013 a global optimum for this objective, but obviously a terrible generative model for synthesizing textures. Ideally, the MEFN will match the moments and promote sample diversity.\nWe applied MEFN to texture synthesis with an RGB representation of the 224 \u00d7 224 pixel images , z \u2208 Z = [0, 1]d, where d = 224 \u00d7 224 \u00d7 3. We follow Ulyanov et al. (2016) (we adapted https://github.com/ProofByConstruction/texture-networks) to create a texture loss measure T (z) : [0, 1]d \u2192 R, and aim to sample a diverse set of images with small moment violation. For the transformation family F we use the real NVP network structure proposed in Dinh et al. (2016) (we adapted https://github.com/taesung89/real-nvp). We use 3 residual blocks with 32 feature maps for each coupling layer and downscale 3 times. For fair comparison, we use the same real NVP structure for both1, implemented in TensorFlow (Abadi et al., 2016).\nAs is shown in top row of figure 3, both methods generate visually pleasing images capturing the texture structure well. The bottom row of Figure 3 shows that texture cost (left panel) is similar for both methods, while MEFN generates figures with much larger entropy than the texture network formulation (middle panel), which is desirable (as previously discussed). The bottom right panel of figure 3 compares the marginal distribution of the RGB values sampled from the networks: we found that MEFN generates a more variable distribution of RGB values than the texture net. Further results are in Appendix \u00a7C.\n1Ulyanov et al. (2016) use a quite different generative network structure, which is not invertible and is therefore infeasible for entropy evaluation, so we replace their generative network by the real NVP structure.\nWe compute in Table 1 the average pairwise Euclidean distance between randomly sampled images (dL2 = meani 6=j\u2016zi \u2212 zj\u201622), and MEFN gives higher dL2 , quantifying diversity across images. We also consider an ANOVA-style analysis to measure the diversity of the images, where we think of the RGB values for the same pixel across multiple images as a group, and compute the within and between group variance. Specifically, denoting zki as the pixel value for a specific pixel k = 1, ..., d for an image i = 1, ...., n. We partition the total sum of square SST = \u2211 i,k(z k i \u2212 z\u0304)2 as the within\ngroup error SSW = \u2211 i,k(z k i \u2212 z\u0304k)2 and between group error SSB = \u2211 i,k n(z\u0304\nk \u2212 z\u0304)2, where z\u0304 and z\u0304k are the mean pixel values across all data and for a specific pixel k. Ideally we want the samples to exhibit large variability across images (large SSW, within a group/pixel) and no structure in the mean image (small SSB, across groups/pixels). Indeed, the MEFN has a larger SSW, implying higher variability around the mean image, a smaller SSB, implying the stationarity of the generated samples, and a larger SST, implying larger total variability also. The MEFN produces images that are conclusively more variable without sacrificing the quality of the texture, implicating the broad utility of ME."}, {"heading": "5 CONCLUSION", "text": "In this paper we propose a general framework for fitting ME models. This approach is novel and has three key features. First, by learning a transformation of a simple distribution rather than the distribution itself, we are able to avoid explicitly computing an intractable normalizing constant for the ME distribution. Second, by combining stochastic optimization with the augmented Lagrangian method, we can fit the model efficiently, allowing us to evaluate the ME density of any point simply and accurately. Third, critically, this construction allows us to trivially sample iid from a ME distribution, extending the utility and efficiency of the ME framework more generally. Also, accuracy equivalent to the classic Gibbs approach is in itself a contribution (owing to these other features). We illustrate the MEFN in both a simulated case with known ground truth and real data examples.\nThere are a few recent works encouraging sample diversity in the setting of texture modeling. Ulyanov et al. (2017) extended Ulyanov et al. (2016) by adding a penalty term using the Kozachenko-Leonenko estimator Kozachenko & Leonenko (1987) of entropy. Their generative network is an arbitrary deep neural network rather than a normalizing flow, which is more flexible but cannot give the probability density of each sample easily so as to compute an unbiased estimator of the entropy. Kozachenko-Leonenko is a biased estimator for entropy and requires a fairly large number of samples to get good performance in high-dimensional settings, hindering the scalability and accuracy of the method; indeed, our choice of normalizing flow networks was driven by these practical issues with Kozachenko-Leonenko. Lu et al. (2016) extended Zhu et al. (1998) by using a more flexible set of filters derived from a pre-trained deep neural networks, and using parallel MCMC chains to learn and sample from the Gibbs distribution. Running parallel MCMC chains results in diverse samples but can be computationally intensive for generating each new sample image. Our MEFN framework enables truly iid sampling with the ease of a feed forward network."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Evan Archer for normalizing flow code, and Xuexin Wei, Christian Andersson Naesseth and Scott Linderman for helpful discussion. This work was supported by a Sloan Fellowship and a McKnight Fellowship (JPC)."}, {"heading": "A AUGMENTED LAGRANGIAN CONDITIONS", "text": "We give a more thorough discussion of the regularity conditions which ensure that the Augmented Lagrangian method will work. The goal of this section is simply to state these conditions and give intuitive arguments about why some should hold in our case, not to attempt to prove that they indeed hold. The conditions (Bertsekas, 2014) are:\n\u2022 There exists a strict local minimum \u03c6\u2217 of the optimization problem of Equation 5: If the function class F is rich enough that it contains a true solver of the maximum entropy problem, then a global optimum exists. Although not rigorous, we would expect that even in the finite expressivity case that a global optimum remains, and indeed, recent theoretical work (Raghu et al., 2016; Poole et al., 2016) has gotten close to proving this.\n\u2022 \u03c6\u2217 is a regular point of the optimization problem, that is, the rows of\u2207\u03c6R(\u03c6\u2217) are linearly independent: Again, this is not formal, but we should not expect this to cause any issues. This clearly depends on the specific form of T , but the condition basically says that there should not be redundant constraints at the optimum, so if T is reasonable this shouldn\u2019t happen.\n\u2022 H(p\u03c6) and R(\u03c6) are twice continuously differentiable on a neighborhood around \u03c6\u2217: This holds by the smoothness of the normalizing flows.\n\u2022 y>\u22072\u03c6L(\u03c6\u2217;\u03bb\u2217, 0)y > 0 for every y 6= 0 such that \u2207\u03c6R(\u03c6\u2217)y = 0, where \u03bb\u2217 is the true Lagrange multiplier: This condition is harder to justify. It would appear it is just asking that the Lagrangian (not the augmented Lagrangian) be strictly convex in feasible directions, but it is actually stronger than this and some simple functions might not satisfy the property. For example, if the function we are optimizing was x4 and we had no constraints, the Lagrangian\u2019s Hessian would be 12x2, which is 0 at x\u2217 = 0 thus not satisfying the condition. Importantly, these conditions are sufficient but not necessary, so even if this doesn\u2019t hold the augmented Lagrangian method might work (it certainly would for x4). Because of this and the nonrigorous justifications of the first two conditions, we left these conditions for the appendix and relied instead on the empirical performance to justify that we are indeed recovering the maximum entropy distribution.\nIf all of these conditions hold, the augmented Lagrangian (for large enough c and \u03bb close enough to \u03bb\u2217) has a unique optimum in a neighborhood around \u03c6\u2217 that is close to \u03c6\u2217 (as \u03bb\u2192 \u03bb\u2217 it converges to \u03c6\u2217) and its hessian at this optimum is positive-definite. Furthermore, \u03bbk \u2192 \u03bb. This implies that gradient descent (with the usual caveats of being started close enough to the solution and with the right steps) will correctly recover \u03c6\u2217 using the augmented Lagrangian method. This of course just guarantees convergence to a local optimum, but if there are no additional assumptions such as convexity, it can be very hard to ensure that it is indeed a global optimum. Some recent research has attempted to explain why optimization algorithms perform so well for neural networks (Choromanska et al., 2015; Kawaguchi, 2016), but we leave such attempts for our case for future research."}, {"heading": "B RISK-NEUTRAL ASSET PRICE", "text": "We extract the risk-neutral asset price probability distribution based on option prices, an active and interesting area for ME models. We give a brief introduction of the problem and refer interested readers to see Buchen & Kelly (1996) for a more detailed explanation. Denoting St as the price of an asset at time t, the buyer of a European call option for the stock that expires at time te with strike price K will receive a payoff of cK = (Ste \u2212 K)+ = max(Ste \u2212 K, 0) at time te. Under the efficient market assumption, the risk-neutral probability distribution for the stock price at time te satisfies: cK = D(te)Eq[(Ste \u2212K)+], (12) where D(te) is the risk-free discount factor and q is the risk-neutral measure. We also have that, under the risk-neutral measure, the current stock price S0 is the discounted expected value of Ste :\nS0 = D(te)Eq(Ste). (13)\nWhen givenm options that expire at time te with strikesK1, ...,Km and prices cK1 , ..., cKm , we get m expectation constraints on q(Ste) from Equation 12, together with Equation 13, we have m + 1 expectation constraints in total. With that partial knowledge we can approximate q(Ste), which is helpful for understanding the market expected volatility and identify mispricing in option markets, etc.\nInferring the risk-neutral density of asset price from a finite number of option prices is an important question in finance and has been studied extensively (Buchen & Kelly, 1996; Borwein et al., 2003; Bondarenko, 2003; Figlewski, 2008). One popular method proposed by Buchen & Kelly (1996) estimates the probability density as the maximum entropy distribution satisfying the expectation constraints and a positivity support constraint by fitting a Gibbs distribution, which results in a piece-wise linear log density:\np(z) \u221d exp { \u03b70z +\nm\u2211 i=1 \u03b7i(z \u2212Ki)+\n} 1 (z \u2265 0) (14)\nand optimize the distribution with numerical methods. Here we compare the performance of the MEFN algorithm with the method proposed in Buchen & Kelly (1996). To enforce the positivity constraint we choose g(z) = eaz+b, where a and b are additional parameters.\nWe collect the closing price of European call options on Nov. 1 2016 for the stock AAPL (Apple inc.) that expires on te = Jun. 16 2017. We usem = 4 of the options with highest trading volume as training data and the rest as testing data. On the left panel of figure 4, we show the fitted risk-neutral density of Ste by MEFN (red line) with that of the fitted Gibbs distribution result (blue line). We find that while the distributions share similar location and variability, the distribution inferred by MEFN is smoother and arguably more plausible. In the middle panel we show a Q-Q plot of the quantiles of the MEFN and Gibbs distributions. We can see that the quantile pairs match the identity closely, which should happen if both methods recovered the exact same distribution. This highlights the effectiveness of MEFN. There does exist a small mismatch in the tails: the distribution inferred by MEFN has slightly heavier tails. This mismatch is difficult to interpret: given that both the Gibbs and MEFN distributions are fit with option price data (and given that one can observe at most one value from the distribution, namely the stock price at expiration), it is fundamentally unclear which distribution is superior, in the sense of better capturing the true ME distribution\u2019s tails. On the right panel we show the fitted option price for the two fitted distributions (for each strike price, we can recover the fitted option price by Equation 12). We noted that the fitted option price and strike price lines for both methods are very similar (they are mostly indiscernible on the right panel of figure 4). We also compare the fitted performance on the test data by computing the root mean square error for the fitted and test data. We observe that the predictive performances for both methods are comparable.\nWe note that for this specific application, there are practical concerns such as the microstructure noise in the data and inefficiency in the market, etc. Applying a pre-processing procedure and incorporating prior assumptions can be helpful for getting a more full-fledged method (see e.g. Figlewski (2008)). Here we mainly focus on illustrating the ability of the MEFN method to approximate the ME distribution for non-typical distributions. Future work for this application includes fitting a riskneutral distribution for multi-dimensional assets by incorporating dependence structure on assets."}, {"heading": "C MODELING IMAGES OF TEXTURES", "text": "We tried our texture modeling approach with many different textures, and although MEFN samples don\u2019t always exhibit more visual diversity than samples obtained from the texture network, they always have more entropy as in figure 3. Figure 5 shows two positive examples, i.e. textures in which samples from MEFN do exhibit higher visual diversity than those from the texture network, as well as a negative example, in which MEFN achieves less visual diversity than the texture network, regardless of the fact that MEFN samples do have larger entropy. We hypothesize that this curious behavior is due to the optimization achieving a local optimum in which the brick boundaries and dark brick locations are not diverse but the entropy within each brick is large. It should also be noted that among the experiments that we ran, this was the only negative example that we got, and that slightly modifying the hyperparameters caused the issue to disappear."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": "Computational linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Efficient multivariate entropy estimation via k-nearest neighbour distances", "author": ["Thomas B Berrett", "Richard J Samworth", "Ming Yuan"], "venue": "arXiv preprint arXiv:1606.00304,", "citeRegEx": "Berrett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Berrett et al\\.", "year": 2016}, {"title": "Constrained optimization and Lagrange multiplier methods", "author": ["Dimitri P Bertsekas"], "venue": "Academic press,", "citeRegEx": "Bertsekas.,? \\Q2014\\E", "shortCiteRegEx": "Bertsekas.", "year": 2014}, {"title": "Estimation of risk-neutral densities using positive convolution approximation", "author": ["Oleg Bondarenko"], "venue": "Journal of Econometrics,", "citeRegEx": "Bondarenko.,? \\Q2003\\E", "shortCiteRegEx": "Bondarenko.", "year": 2003}, {"title": "Probability distributions of assets inferred from option prices via the principle of maximum entropy", "author": ["Jonathan Borwein", "Rustum Choksi", "Pierre Mar\u00e9chal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Borwein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Borwein et al\\.", "year": 2003}, {"title": "The maximum entropy distribution of an asset inferred from option prices", "author": ["Peter W Buchen", "Michael Kelly"], "venue": "Journal of Financial and Quantitative Analysis,", "citeRegEx": "Buchen and Kelly.,? \\Q1996\\E", "shortCiteRegEx": "Buchen and Kelly.", "year": 1996}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Logistic regression, adaboost and bregman distances", "author": ["Michael Collins", "Robert E Schapire", "Yoram Singer"], "venue": "Machine Learning,", "citeRegEx": "Collins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2002}, {"title": "Generalized iterative scaling for log-linear models", "author": ["John N Darroch", "Douglas Ratcliff"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Darroch and Ratcliff.,? \\Q1972\\E", "shortCiteRegEx": "Darroch and Ratcliff.", "year": 1972}, {"title": "Inducing features of random fields", "author": ["Stephen Della Pietra", "Vincent Della Pietra", "John Lafferty"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Performance guarantees for regularized maximum entropy density estimation", "author": ["Miroslav Dudik", "Steven J Phillips", "Robert E Schapire"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Dudik et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2004}, {"title": "Estimating the implied risk neutral density", "author": ["Stephen Figlewski"], "venue": null, "citeRegEx": "Figlewski.,? \\Q2008\\E", "shortCiteRegEx": "Figlewski.", "year": 2008}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "Jaynes.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes.", "year": 1957}, {"title": "Minimax estimation of functionals of discrete distributions", "author": ["Jiantao Jiao", "Kartik Venkat", "Yanjun Han", "Tsachy Weissman"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Jiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiao et al\\.", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Sample estimate of the entropy of a random vector", "author": ["LF Kozachenko", "Nikolai N Leonenko"], "venue": "Problemy Peredachi Informatsii,", "citeRegEx": "Kozachenko and Leonenko.,? \\Q1987\\E", "shortCiteRegEx": "Kozachenko and Leonenko.", "year": 1987}, {"title": "Learning frame models using cnn filters", "author": ["Yang Lu", "Song-chun Zhu", "Ying Nian Wu"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A comparison of algorithms for maximum entropy parameter estimation", "author": ["Robert Malouf"], "venue": "In proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Malouf.,? \\Q2002\\E", "shortCiteRegEx": "Malouf.", "year": 2002}, {"title": "Learning in implicit generative models", "author": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "venue": "arXiv preprint arXiv:1610.03483,", "citeRegEx": "Mohamed and Lakshminarayanan.,? \\Q2016\\E", "shortCiteRegEx": "Mohamed and Lakshminarayanan.", "year": 2016}, {"title": "Maximum entropy modeling of species geographic distributions", "author": ["Steven J Phillips", "Robert P Anderson", "Robert E Schapire"], "venue": "Ecological modelling,", "citeRegEx": "Phillips et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Phillips et al\\.", "year": 2006}, {"title": "Exponential expressivity in deep neural networks through transient chaos", "author": ["Ben Poole", "Subhaneil Lahiri", "Maithreyi Raghu", "Jascha Sohl-Dickstein", "Surya Ganguli"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Poole et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Poole et al\\.", "year": 2016}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Javier Portilla", "Eero P Simoncelli"], "venue": "International journal of computer vision,", "citeRegEx": "Portilla and Simoncelli.,? \\Q2000\\E", "shortCiteRegEx": "Portilla and Simoncelli.", "year": 2000}, {"title": "On the expressive power of deep neural networks", "author": ["Maithra Raghu", "Ben Poole", "Jon Kleinberg", "Surya Ganguli", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "On the convergence of bound optimization algorithms", "author": ["Ruslan Salakhutdinov", "Sam Roweis", "Zoubin Ghahramani"], "venue": "In Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2002}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis", "author": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "venue": null, "citeRegEx": "Ulyanov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2017}, {"title": "Estimating the unseen: improved estimators for entropy and other properties", "author": ["Paul Valiant", "Gregory Valiant"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Valiant and Valiant.,? \\Q2013\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Filters, random fields and maximum entropy (frame): Towards a unified theory for texture modeling", "author": ["Song Chun Zhu", "Yingnian Wu", "David Mumford"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zhu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1998}, {"title": "2016), but we leave such attempts for our case for future research", "author": ["Kawaguchi"], "venue": null, "citeRegEx": "2015 and Kawaguchi,? \\Q2016\\E", "shortCiteRegEx": "2015 and Kawaguchi", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "The maximum entropy (ME) principle (Jaynes, 1957) states that subject to some given prior knowledge, typically some given list of moment constraints, the distribution that makes minimal additional assumptions \u2013 and is therefore appropriate for a range of applications from hypothesis testing to price forecasting to texture synthesis \u2013 is that which has the largest entropy of any distribution obeying those constraints.", "startOffset": 35, "endOffset": 49}, {"referenceID": 1, "context": "First introduced in statistical mechanics by Jaynes (1957), and considered both celebrated and controversial, ME has been extensively applied in areas including natural language processing (Berger et al., 1996), ecology (Phillips et al.", "startOffset": 189, "endOffset": 210}, {"referenceID": 25, "context": ", 1996), ecology (Phillips et al., 2006), finance (Buchen & Kelly, 1996), computer vision (Zhu et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 35, "context": ", 2006), finance (Buchen & Kelly, 1996), computer vision (Zhu et al., 1998), and many more.", "startOffset": 57, "endOffset": 75}, {"referenceID": 17, "context": "Third, unlike in the discrete case where a number of recent and exciting works have addressed the problem of estimating entropy from discrete-valued data (Jiao et al., 2015; Valiant & Valiant, 2013), estimating differential entropy from data samples remains inefficient and typically biased.", "startOffset": 154, "endOffset": 198}, {"referenceID": 11, "context": "Recent developments in normalizing flows (Rezende & Mohamed, 2015; Dinh et al., 2016) allow us to avoid biased and computationally inefficient estimators of differential entropy (such as the nearest-neighbor class of estimators like that of Kozachenko-Leonenko; see Berrett et al.", "startOffset": 41, "endOffset": 85}, {"referenceID": 30, "context": "There is extensive literature on finding \u03b7 numerically (Darroch & Ratcliff, 1972; Salakhutdinov et al., 2002; Della Pietra et al., 1997; Dudik et al., 2004; Malouf, 2002; Collins et al., 2002), but doing so requires computing normalizing constants, which poses a challenge even for problems with modest dimensions.", "startOffset": 55, "endOffset": 192}, {"referenceID": 12, "context": "There is extensive literature on finding \u03b7 numerically (Darroch & Ratcliff, 1972; Salakhutdinov et al., 2002; Della Pietra et al., 1997; Dudik et al., 2004; Malouf, 2002; Collins et al., 2002), but doing so requires computing normalizing constants, which poses a challenge even for problems with modest dimensions.", "startOffset": 55, "endOffset": 192}, {"referenceID": 23, "context": "There is extensive literature on finding \u03b7 numerically (Darroch & Ratcliff, 1972; Salakhutdinov et al., 2002; Della Pietra et al., 1997; Dudik et al., 2004; Malouf, 2002; Collins et al., 2002), but doing so requires computing normalizing constants, which poses a challenge even for problems with modest dimensions.", "startOffset": 55, "endOffset": 192}, {"referenceID": 8, "context": "There is extensive literature on finding \u03b7 numerically (Darroch & Ratcliff, 1972; Salakhutdinov et al., 2002; Della Pietra et al., 1997; Dudik et al., 2004; Malouf, 2002; Collins et al., 2002), but doing so requires computing normalizing constants, which poses a challenge even for problems with modest dimensions.", "startOffset": 55, "endOffset": 192}, {"referenceID": 3, "context": "As a technical note, the augmented Lagrangian method is guaranteed to converge under some regularity conditions (Bertsekas, 2014).", "startOffset": 112, "endOffset": 129}, {"referenceID": 3, "context": "The resulting optimization procedure is detailed in Algorithm 1, of which step 9 requires some detail: denoting \u03c6k as the resulting \u03c6 after imax SGD iterations at the augmented Lagrangian iteration k, the usual update rule for c (Bertsekas, 2014) is:", "startOffset": 229, "endOffset": 246}, {"referenceID": 34, "context": "1 and \u00a7B, We use 10 layers of planar flow with a final transformation g (specified below) that transforms samples to the specified support, and use with ADADELTA (Zeiler, 2012).", "startOffset": 162, "endOffset": 176}, {"referenceID": 15, "context": "We then take a random sample from the fitted distribution and a random sample from the Dirichlet with parameter \u03b1, and compare the two samples using the maximum mean discrepancy (MMD) kernel two sample test (Gretton et al., 2012), which assesses the fit quality.", "startOffset": 207, "endOffset": 229}, {"referenceID": 14, "context": "More recent works exploit more complicated features (Portilla & Simoncelli, 2000; Gatys et al., 2015; Ulyanov et al., 2016).", "startOffset": 52, "endOffset": 123}, {"referenceID": 31, "context": "More recent works exploit more complicated features (Portilla & Simoncelli, 2000; Gatys et al., 2015; Ulyanov et al., 2016).", "startOffset": 52, "endOffset": 123}, {"referenceID": 0, "context": "For fair comparison, we use the same real NVP structure for both1, implemented in TensorFlow (Abadi et al., 2016).", "startOffset": 93, "endOffset": 113}], "year": 2017, "abstractText": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_16"}