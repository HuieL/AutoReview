{"name": "ICLR_2017_171.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["TONAL MUSIC", "Haizi Yu", "Lav R. Varshney"], "emails": ["haiziyu7@illinois.edu", "varshney@illinois.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Forming hierarchical concepts from low-level observations is key to knowledge discovery. In the field of artificial neural networks, deep architectures are employed for machine learning tasks, with the awareness that hierarchical representations are important (Bengio et al., 2013). Rapid progress in deep learning has shown that mapping and representing topical domains through increasingly abstract layers of feature representation is extremely effective. Unfortunately, this layered representation is difficult to interpret or use for teaching people. Consequently, deep learning models are widely used as algorithmic task performers (e.g. AlphaGo), but few act as theorists or pedagogues. In contrast, our goal is to achieve a deeper-level interpretability that explains not just what has been learned (the end results), but also what is being learned at every single stage (the process).\nOn the other hand, music theory studies underlying patterns beneath the music surface. It objectively reveals higher-level invariances that are hidden from the low-level variations. In practice, the development of music theory is an empirical process. Through manual inspection of large corpora of music works, theorists have summarized compositional rules and guidelines (e.g. J. J. Fux, author of Gradus ad Parnassum, the most influential book on Renaissance polyphony), and have devised multi-level analytical methods (e.g. H. Schenker, inventor of Schenkerian analysis) to emphasize the hierarchical structure of music, both of which have become the standard materials taught in today\u2019s music theory classes. The objective and empirical nature of music theory suggests the possibility of an automatic theorist \u2014 statistical techniques that perform hierarchical concept learning \u2014 while its pedagogical purpose requires human interpretability throughout the entire learning process.\nThe book title Gradus ad Parnassum, means \u201cthe path towards Mount Parnassus,\u201d the home of poetry, music, and learning. This paper presents MUS-ROVER II, an extension of our prior work (Yu et al., 2016a;b), to independently retake the path towards Parnassus. The rover acts more as a pathfinder than a generative model (e.g. LSTM), emphasizing the path more than the destination.\nWe compare the paths taken by this improved automatic theorist to paths taken by human theorists (say Fux), studying similarities as well as pros and cons of each. So advantages from both can be jointly taken to maximize the utility in music education and research. In this paper in particular, we highlight the concept hierarchy that one would not get from our prior work, as well as enhanced syllabus personalization that one would not typically get from traditional pedagogy."}, {"heading": "2 MUS-ROVER OVERVIEW", "text": "As the first algorithmic pathfinder in music, MUS-ROVER I introduced a \u201cteacher student\u201d model to extract compositional rules for writing 4-part chorales (Yu et al., 2016a;b). The model is implemented by a self-learning loop between a generative component (student) and a discriminative component (teacher), where both entities cooperate to iterate through the rule-learning process (Figure 1). The student starts as a tabula rasa that picks pitches uniformly at random to form sonorities (a generic term for chord) and sonority progressions. The teacher compares the student\u2019s writing style (represented by a probabilistic model) with the input style (represented by empirical statistics), identifying one feature per iteration that best reveals the gap between the two styles, and making it a rule for the student to update its probabilistic model. As a result, the student becomes less and less random by obeying more and more rules, and thus, approaches the input style. Collecting from its rule-learning traces, MUS-ROVER I successfully recovered many known rules, such as \u201cParallel perfect octaves/fifths are rare\u201d and \u201cTritons are often resolved either inwardly or outwardly\u201d.\nWhat is Inherited from MUS-ROVER I MUS-ROVER II targets the same goal of learning interpretable music concepts. It inherits the self-learning loop, as well as the following design choices.\n(Dataset and Data Representation) We use the same dataset that comprises 370 C scores of Bach\u2019s 4-part chorales. We include only pitches and their durations in a piece\u2019s raw representation, notated as a MIDI matrix whose elements are MIDI numbers for pitches. The matrix preserves the twodimensional chorale texture, with rows corresponding to melodies, and columns to harmonies.\n(Rule Representation) We use the same representation for high-level concepts in terms of rules, unrelated to rules in propositional logic. A (compositional) rule is represented by a feature and its distribution: r = (\u03c6, p\u03c6), which describes likelihoods of feature values. It can also be transformed to a linear equality constraint (A\u03c6pstu = p\u03c6) in the student\u2019s optimization problem (\u0393\u2019s in Figure 1).\n(Student\u2019s Probabilistic Model) We still use n-gram models to represent the student\u2019s style/belief, with words being sonority features, and keep the student\u2019s optimization problem as it was. To reiterate the distinctions to many music n-grams, we never run n-grams in the raw feature space, but only collectively in the high-level feature spaces to prevent overfitting. So, rules are expressed as probabilistic laws that describe either (vertical) sonority features or their (horizontal) progressions.\nWhat is New in MUS-ROVER II We study hierarchies on features, so rules are later presented not just as a linear list, but as hierarchical families and sub-families. In particular, we introduce conceptual hierarchy that is pre-determined by feature maps, and infer informational hierarchy that is post-implied from an information-theoretic perspective. We upgrade the self-learning loop to adaptively select memories in a multi-feature multi-n-gram language model. This is realized by constructing hierarchical filters to filter out conceptual duplicates and informational implications. By further following the information scent spilled by Bayesian surprise (Varshney, 2013), the rover can effectively localize the desired features in the feature universe."}, {"heading": "3 RELATED WORK", "text": "Adversarial or Collaborative MUS-ROVER\u2019s self-learning loop between the teacher (a discriminator) and student (a generator) shares great structural similarity to generative adversarial nets (Goodfellow et al., 2014) and their derivatives (Denton et al., 2015; Makhzani et al., 2015). However, the working mode between the discriminator and generator is different. In current GAN algorithms, the adversarial components are black-boxes to each other, since both are different neural networks that are coupled only end to end. The learned intermediate representation from one model, no matter how expressive or interpretable, is not directly shared with the other. Contrarily in MUSROVER, both models are transparent to each other (also to us): the student directly leverages the rules from the teacher to update its probabilistic model. In this sense, the learning pair in MUSROVER is more collaborative rather than adversarial. Consequently, not only the learned concepts have interpretations individually, but the entire learning trace is an interpretable, cognitive process.\nFurthermore, MUS-ROVER and GAN contrast in the goal of learning and the resulting evaluations. The rover is neither a classifier nor a density estimator, but rather a pure representation learner that outputs high-level concepts and their hierarchies. Training this type of learner in general is challenging due to the lack of a clear objective or target (Bengio et al., 2013), which drives people to consider some end task like classification and use performance on the task to indirectly assess the learned representations. In MUS-ROVER, we introduce information-theoretic criteria to guide the training of the automatic theorist, and in the context of music concept learning, we directly evaluate machine generated rules and hierarchies by comparison to those in existing music theory.\nInterpretable Feature Learning In the neural network community, much has been done to first recover disentangled representations, and then post-hoc interpret the semantics of the learned features. This line of work includes denoising autoencoders (Vincent et al., 2008) and restricted Boltzmann machines (Hinton et al., 2006; Desjardins et al., 2012), ladder network algorithms (Rasmus et al., 2015), as well as more recent GAN models (Radford et al., 2015). In particular, InfoGAN also introduces information-theoretic criteria to augment the standard GAN cost function, and to some extent achieves interpretability for both discrete and continuous latent factors (Chen et al., 2016). However, beyond the end results, the overall learning process of these neural networks are still far away from human-level concept learning (Lake et al., 2015), so not directly instructional to people.\nAutomatic Musicians Music theory and composition form a reciprocal pair, often realized as the complementary cycle of reduction and elaboration (Laitz, 2016) as walks up and down the multilevel music hierarchy. Accordingly, various models have been introduced to automate this up/down walk, including music generation (Cope & Mayer, 1996; Biles, 1994; Simon et al., 2008), analysis (Taube, 1999), or theory evaluation (Rohrmeier & Cross, 2008). In terms of methodologies, we have rule-based systems (Cope, 1987), language models (Google Brain, 2016; Simon et al., 2008), and information-theoretic approaches (Jacoby et al., 2015; Dubnov & Assayag, 2002). However, all of these models leverage domain knowledge (e.g. human-defined chord types, functions, rules) as part of the model inputs. MUS-ROVER takes as input only the raw notations (pitches and durations), and outputs concepts that are comparable to (but also different from) our domain knowledge."}, {"heading": "4 HIERARCHICAL RULE LEARNING", "text": "MUS-ROVER II emphasizes hierarchy induction in learning music representations, and divides the induction process into two stages. In the first stage, we impose conceptual hierarchy as pre-defined structures among candidate features before the self-learning loop. In the second stage, we infer informational hierarchy as post-implied structures through the rule learning loops.\nInterpretable Features A feature is a function that computes a distributed representation of the building blocks that constitute data samples. For Bach\u2019s 4-part chorales, we model every piece (4-row matrix) as a sequence of sonorities (columns). So every sonority is the building block of its composing piece (like a word in a sentence). Then a feature maps a sonority onto some feature space, summarizing an attribute. To formalize, let \u2126 = {R, p1, . . . , pn} be an alphabet that comprises a rest symbol R, and n pitch symbols pi. In addition, the alphabet symbols \u2014 analogous to image pixels \u2014 are manipulable by arithmetic operations, such as plus/minus, modulo, and sort. More precisely, every pi is an integer-valued MIDI number (60 for middle C, granularity 1 for semi-tone), and R is a special character which behaves like a python nan variable. The four coordinates of every sonority p \u2208 \u21264 denote soprano, alto, tenor, and bass, respectively. We define a feature as a surjective function \u03c6 : \u21264 7\u2192 \u03c6(\u21264), and the corresponding feature space by its range. As a first and brutal categorization, we say a feature (space) is raw (or lowest-level) if |\u03c6(\u21264)| = |\u21264|, and high-level if |\u03c6(\u21264)| < |\u21264|. For instance, \u21264 or any permutation of \u21264 is a raw feature space. MUS-ROVER II employs a more systematic way of generating the universe of interpretable features. A (sonority) feature is constructed as the composition of a window and a descriptor. A window is a function that selects parts of the input sonority: wI : \u21264 7\u2192 \u2126|I|, where I is an index set. For instance, w{1,4}(p) = (p1, p4) selects soprano and bass. A descriptor is constructed inductively from a set of basis descriptors B, consisting of atomic arithmetic operations. We currently set B = {order, diff, sort, mod12} (Appendix A.2). We define a descriptor of length k as the composition of k bases: d(k) = bk \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 b1, for all bi \u2208 B, where d(0) is the identity function. We collect the family of all possible windows: W = {wI | I \u2208 2{1,2,3,4}\\{\u2205}}, and the family of all descriptors of length less than or equal to k: D[k] = {d(k\u2032) | 0 \u2264 k\u2032 \u2264 k}, and form the feature universe:\n\u03a6 = {d \u25e6 w | w \u2208W,d \u2208 D[k]}. (1) The fact that every candidate feature in \u03a6 is systematically generated as composition of atomic operators ensures its interpretability, since one can literally read it out step-by-step from the composition.\nFeature-Induced Partition On the one hand, a feature function has all the mathematic specifications to name the corresponding feature and feature values. On the other hand, we only care about the partition of the input domain (\u21264) induced by the feature but not the (superficial) naming of the clusters. In other words, we only identity the sonority clusters whose members are mapped to the same function value, but not the value per se. As a result, we use a partition to refer to the essence of a concept, and the inducing function as a mathematical name to interpret the concept. To formalize, a feature function \u03c6 induces a partition of its domain\nP\u03c6 = { \u03c6\u22121({y}) | y \u2208 \u03c6(\u21264) } . (2)\nGiven a feature universe \u03a6, (2) defines an equivalence relation on \u03a6: \u03c6 P\u223c \u03c6\u2032 if P\u03c6 = P\u03c6\u2032 , which induces the corresponding partition family P\u03a6 as the resulting equivalence classes. For two partitions P,Q \u2208 P\u03a6, we say P is finer than Q (or Q is coarser), written as P Q, if for all p, p\u2032 \u2208 \u21264, p, p\u2032 are in the same cluster under P \u21d2 p, p\u2032 are in the same cluster under Q. We say P is strictly finer, written as P Q, if P Q and Q P . Conceptual Hierarchy Based on the binary relation , we construct the conceptual hierarchy for the partition family P\u03a6, and represent it as a directed acyclic graph (DAG) with nodes being partitions. For any pair of nodes v, v\u2032, v \u2192 v\u2032 if and only if the partition referred by v is (strictly) finer than that referred by v\u2032. The DAG grows from a single source node, which represents the finest partition \u2014 every point in the domain by itself is a cluster \u2014 and extends via the edges to coarser and coarser partitions. In terms of features, we say a feature \u03c6\u2032 is at a higher level than another feature \u03c6, if the induced partitions satisfy P\u03c6 P\u03c6\u2032 . In other words, a higher-level feature induces a coarser partition that ignores lower-level details by merging clusters. One can check that the finest partition (the source node) is indeed induced by a raw feature. We attach an efficient algorithm for pre-computing the conceptual hierarchy in Appendix A.3.\nWe emphasize the necessity of this multi-step process: features\u2192 partitions\u2192 hierarchy (DAG), as opposed to a simple hierarchical clustering (tree). The latter loses many inter-connections due to the tree structure and its greedy manner, and more importantly, the interpretability of the partitions.\nInformational Hierarchy We infer informational hierarchy from a many-to-one relation, called implication, along a rule trace. More formally, let {ri}ki=1 := {(\u03c6i, p\u0302\u03c6i)}ki=1 be the extracted trace\nof rules (in terms of feature and feature distribution) by the kth iteration of the self-learning loop. We say a feature \u03c6 is informationally implied from the trace {ri}ki=1 with tolerance \u03b3 > 0, if\ngap ( p \u3008k\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 ) := D ( p \u3008k\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 ) < \u03b3, and gap ( p \u3008k\u2032\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 ) \u2265 \u03b3,\u2200k\u2032 < k,\nwhere D(\u00b7\u2016\u00b7) is the KL divergence used to characterize the gap of the student\u2019s style (probabilistic model) against Bach\u2019s style (input). One trivial case happens when \u03c6 is extracted as the kth rule, i.e. \u03c6 = \u03c6k, then gap(p \u3008k\u3009 \u03c6\u2032,stu\n\u2225\u2225 p\u0302\u03c6\u2032) = 0 < \u03b3, \u2200\u03c6\u2032 \u2208 {\u03c6\u2032 | P\u03c6 P\u03c6\u2032}, meaning that feature \u03c6, once learned as a rule, informationally implies itself and all its descendants in the conceptual hierarchy. However, what is more interesting is the informational implication from other rules outside the conceptual hierarchy, which is typically hard for humans to \u201ceyeball\u201d.\nOne might question the necessity of conceptual hierarchy since it can be implied in the informational hierarchy. The answer is yes in principle, but no in practice. The main difference is that conceptual hierarchy is pre-computed over the entire feature universe before the loop, which is global, precise, and trace independent. On the contrary, informational hierarchy is trace specific and loose, due to tolerance \u03b3 and the precision of the optimization solver. As a result, informational hierarchy alone tends to lose the big picture and require more post-hoc interpretations, and is unstable in practice.\nHierarchical Filters Beyond their benefits in revealing inter-relational insights among distributed representations, we build hierarchical filters from both conceptual and informational hierarchies, for the purpose of pruning hierarchically entangled features and speeding up feature selection. This upgrades MUS-ROVER II into a more efficient, robust, and cognitive theorist. Recall the skeleton of the teacher\u2019s optimization problem in Figure 1, we flesh it out as follows:\nmaximize \u03c6\u2208\u03a6\ngap ( p \u3008k\u22121\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 )\n(3)\nsubject to H(p\u0302\u03c6) \u2264 \u03b4 (Regularity Condition) \u03c6 /\u2208 C\u3008k\u22121\u3009 := { \u03c6 \u2223\u2223 P\u03c6 P\u03c6\u2032 , \u03c6\u2032 \u2208 \u03a6\u3008k\u22121\u3009 } (Conceptual-Hierarchy Filter)\n\u03c6 /\u2208 I\u3008k\u22121\u3009 := { \u03c6 \u2223\u2223 gap ( p \u3008k\u22121\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 ) < \u03b3 } (Informational-Hierarchy Filter)\nIn the above optimization problem, \u03a6 is the feature universe defined in (1) and \u03c6 \u2208 \u03a6 is the optimization variable whose optimal value is used to form the kth rule: \u03c6k = \u03c6?, rk = (\u03c6?, p\u0302\u03c6?). We decouple the regularity condition from the objective function in our previous work (which was the generalized cultural hole function), and state it separately as the first constraint that requires the Shannon entropy of the feature distribution to be no larger than a given threshold (Pape et al., 2015). The second constraint encodes the filter from conceptual hierarchy, which prunes coarser partitions of the learned features \u03a6\u3008k\u22121\u3009 := {\u03c61, . . . , \u03c6k\u22121}. The third constraint encodes the filter from informational hierarchy, which prunes informationally implied features.\nThere are two hyper-parameters \u03b4 and \u03b3 in the optimization problem (3), whose detailed usage in syllabus customization will be discussed later in Sec. 6. At a high level, we often pre-select \u03b3 before the loop to express a user\u2019s satisfaction level: a smaller \u03b3 signifies a meticulous user who is harder to satisfy; the threshold \u03b4 upper bounds the entropic difficulty of the rules, and is adaptively adjusted through the loop: it starts from a small value (easy rules first), and auto-increases whenever the feasible set of (3) is empty (gradually increases the difficulty when mastering the current level)."}, {"heading": "5 ADAPTIVE MEMORY SELECTION", "text": "MUS-ROVER II considers a continuous range of higher order n-grams (variable memory), and adaptively picks the optimal n based on a balance among multiple criteria. The fact that every ngram is also on multiple high-level feature spaces opens the opportunities for long-term memories without exhausting machine memory, while effectively avoiding overfitting.\nTwo-Dimensional Memory In light of a continuous range of n-grams, say n \u2208 N = {2, 3, . . . }, the feature universe adds another dimension, forming a two-dimensional memory (N\u00d7\u03a6) \u2014 length versus depth \u2014 for the language model (Figure 2: left). The length axis enumerates n-gram orders, with a longer memory corresponding to a larger n; the depth axis enumerates features, with a deeper\nmemory corresponding to a higher-level feature. Every cell in the memory is indexed by two coordinates (n, \u03c6), referring to the feature \u03c6 under the n-gram, and stores the corresponding feature distribution. As a consequence, the rule extraction task involves picking the right feature under the right n-gram, which extends the space of the optimization problem (3) from \u03a6 to N \u00d7 \u03a6. Accordingly, the constraints of (3) jointly forge a mask on top of the 2D memory (Figure 2: right).\nCriteria and Balance We propose three criteria to extract rules from the 2D memory: confidence, regularity, and efficacy. Confidence is quantified by empirical counts: the more relevant examples one sees in Bach\u2019s chorales, the more confident. Regularity is quantified by Shannon entropy of the rule\u2019s feature distribution: a rule is easier to memorize if it is less entropic (Pape et al., 2015). Efficacy is inversely quantified by the gap between the student\u2019s probabilistic model and the rule\u2019s feature distribution: a rule is more effective if it reveals a larger gap. There are tradeoffs among these criteria. For instance, a lower-level feature is usually more effective since it normally reflects larger variations in the gap, but is also unlikely to be regular, thus harder to memorize and generalize. Also a feature under a higher-order n-gram may be both regular and effective, but the number of examples that match the long-term conditionals is likely to be small, reducing confidence.\nAdaptive Selection: Follow the (Bayesian) Surprise The teacher\u2019s optimization problem (3) explicitly expresses the efficacy factor in the objective, and the regularity condition as the first constraint. To further incorporate confidence, we cast the rule\u2019s feature distribution p\u0302\u03c6 in a Bayesian framework rather than a purely empirical framework as in our previous work. We assume the student\u2019s belief with respect to a feature \u03c6 follows a Dirichlet distribution whose expectation is the student\u2019s probabilistic model. In the kth iteration of the self-learning loop, we set the student\u2019s prior belief as the Dirichlet distribution parameterized by the student\u2019s latest probabilistic model:\nprior\u03c6,stu \u223c Dir ( c \u00b7 p\u3008k\u22121\u3009\u03c6,stu ) ,\nwhere c > 0 denotes the strength of the prior. From Bach\u2019s chorales, the teacher inspects the empirical counts q\u03c6 associated with the feature \u03c6 and the relevant n-gram, and computes the student\u2019s posterior belief if \u03c6 were selected as the rule:\nposterior\u03c6,stu \u223c Dir ( q\u03c6 + c \u00b7 p\u3008k\u22121\u3009\u03c6,stu ) .\nThe concentration parameters of the Dirichlet posterior show the balance between empirical counts and the prior. If the total number of empirical counts is small (less confident), the posterior will be smoothed more by the prior, de-emphasizing the empirical distribution from q\u03c6. If we compute\np\u0302\u03c6 \u221d ( q\u03c6 + c \u00b7 p\u3008k\u22121\u3009\u03c6,stu ) in the objective of (3), then\ngap ( p \u3008k\u22121\u3009 \u03c6,stu \u2225\u2225 p\u0302\u03c6 ) = D ( E [ prior\u03c6,stu ] \u2225\u2225 E [ posterior\u03c6,stu ]) . (4)\nThe right side of (4) is closely related to Bayesian surprise (Varshney, 2013), which takes the form of KL divergence from the prior to posterior. If we remove the expectations and switch the roles between the prior and posterior, we get the exact formula for Bayesian surprise. Both functionals\ncapture the idea of comparing the gap between the prior and posterior. Therefore, the efficacy of concept learning is analogous to seeking (informational) surprise in the learning process.\nThe subtlety in (4) where we exchange the prior and posterior, makes a distinction from Bayesian surprise due to the asymmetry of KL divergence. As a brief explanation, adopting (4) as the objective tends to produce rules about what Bach hated to do, while the other way produces what Bach liked to do. So we treat it as a design choice and adopt (4), given that rules are often taught as prohibitions (e.g. \u201cparallel fifths/octaves are bad\u201d, \u201cnever double the tendency tones\u201d). There are more in-depth and information-theoretic discussions on this point (Husza\u0301r, 2015; Palomar & Verdu\u0301, 2008)."}, {"heading": "6 EXPERIMENTS", "text": "MUS-ROVER II\u2019s main use case is to produce personalized syllabi that are roadmaps to learning the input style (customized paths to Mount Parnassus). By substituting the student module, users can join the learning cycle, in which they make hands-on compositions and get iterative feedback from the teacher. Alternatively, for faster experimentation, users make the student their learning puppet, which is personalized by its external parameters. This paper discusses the latter case in detail.\nMath-to-Music Dictionary MUS-ROVER II conceptualizes every rule feature as a partition of the raw space, and uses the inducing function as its mathematical name. To get the meanings of the features, one can simply work out the math, but some of them already have their counterparts as music terminologies. We include a short dictionary of those correspondences in Appendix A.1.\nPace Control and Syllabus Customization We present a simple yet flexible pace control panel to the users of MUS-ROVER II, enabling personalized set-up of their learning puppet. The control panel exposes four knobs: the lower bound, upper bound, and stride of the rule\u2019s entropic difficulty (\u03b4min, \u03b4max, \u03b4stride), as well as the satisfactory gap (\u03b3). These four hyper-parameters together allow the user to personalize the pace and capacity of her learning experience. The entropic difficulty \u03b4 caps the Shannon entropy of a rule\u2019s feature distribution in (3), a surrogate for the complexity (or memorability) of the rule (Pape et al., 2015). It is discretized into a progression staircase from \u03b4min up to \u03b4max, with incremental \u03b4stride. The resulting syllabus starts with \u03b4 = \u03b4min, the entry level difficulty; and ends whenever \u03b4 \u2265 \u03b4max, the maximum difficulty that the user can handle. Anywhere in between, the loop deactivates all rules whose difficulties are beyond current \u03b4, and moves onto the next difficulty level \u03b4 + \u03b4stride if the student\u2019s probabilistic model is \u03b3-close to the input under all currently active rule features.\nTo showcase syllabus customization, we introduce an ambitious user who demands a faster pace and a patient user who prefers a slower one. In practice, one can collectively tune the stride parameter \u03b4stride and the gap parameter \u03b3, with a faster pace corresponding to a larger \u03b4stride (let\u2019s jump directly to the junior year from freshman) and a larger \u03b3 (having an A- is good enough to move onto the next level, why bother having A+). Here we simply fix \u03b4stride, and let \u03b3 control the pace. We illustrate two syllabi in Table 1, which compares the first ten (1-gram) rules in a faster (\u03b3 = 0.5) syllabus and a slower one (\u03b3 = 0.1). Notice the faster syllabus gives the fundamentals that a music student will typically learn in her first-year music theory class, including rules on voice crossing,\npitch class set (scale), intervals, and so on (triads and seventh chords will appear later). It effectively skips the nitty-gritty rules (marked by an asterisk) that are learned in the slower setting. Most of these skipped rules do not have direct counterparts in music theory (such as taking the diff operator twice) and are not important, although occasionally the faster syllabus will skip some rules worth mentioning (such as the second rule in the slower pace, which talks about spacing among soprano, alto, and bass). Setting an appropriate pace for a user is important: a pace that is too fast will miss the whole point of knowledge discovery (jump to the low-level details too fast); a pace that is too slow will bury the important points among unimportant ones (hence, lose the big picture).\nFundamentals: Hierarchical 1-gram Similar to our teaching of music theory, MUS-ROVER II\u2019s proposed syllabus divides into two stages: fundamentals and part writing. The former is under the 1-gram setting, involving knowledge independent of the context; the latter provides online tutoring under multi-n-grams. We begin our experiments with fundamentals, and use them to illustrate the two types of feature hierarchies.\nLet\u2019s take a closer look at the two syllabi in Table 1. The specifications (left) and hierarchies (right) of the four common rules are illustrated in Table 2. The rules\u2019 translations are below the corresponding bar charts, all of which are consistent with our music theory. Extracted from the conceptual hierarchy, the right column lists the partition sub-family sourced at each rule, which is pictorially simplified as a tree by hiding implied edges from its corresponding DAG. Every coarser partition in\na sub-family is indeed a higher-level representation, but has not accumulated sufficient significance to make itself a rule. A partition will never be learned if one of its finer ancestors has been made a rule. Observe that all of the coarser partitions are not typically taught in theory classes.\nMUS-ROVER II measures the student\u2019s progress from many different angles in terms of features. With respect to a feature, the gap between the student and Bach is iteratively recorded to form a trajectory when cycling the loop. Studying the vanishing point of the trajectory reveals the (local) informational hierarchy around the corresponding feature. Taking the second and seventh rule in the slower syllabus for example, we plot their trajectories in Figure 3. Both illustrate a decreasing trend1 for gaps in the corresponding feature spaces. The left figure shows that the second rule is largely but not entirely implied by the first, pointing out the hierarchical structure between the two: the first rule may be considered as the dominant ancestor of the second, which is not conceptually apparent, but informationally implied. On the contrary, the right figure shows that the seventh rule is not predominantly implied by the first, which instead is informationally connected to many other rules. However, one could say that it is probably safe to skip both rules in light of a faster pace, since they will eventually be learned fairly effectively (with small gaps) but indirectly.\nPart Writing: Adaptive n-grams Unlike fundamentals which studies sonority independently along the vertical direction of the chorale texture, rules on part writing (e.g. melodic motion, chord progression) are horizontal, and context-dependent. This naturally results in an online learning framework, in which rule extractions are coupled in the writing process, specific to the realization of a composition (context). Context dependence is captured by the multi-n-gram language model, which further leads to the 2D memory pool of features for rule extraction (Sec. 5). Consider an example of online learning and adaptive memory selection, where we have the beginning of a chorale:\n\u3008s\u3009 \u2192 (60, 55, 52, 36)\u2192 (60, 55, 52, 36)\u2192 (62, 59, 55, 43)\u2192 (62, 59, 55, 43)\u2192 (62, 59, 55, 43), and want to learn the probabilistic model for the next sonority. Instead of starting from scratch, MUS-ROVER II launches the self-learning loop with the ruleset initialized by the fundamentals (incremental learning), and considers the 2D memoryN\u00d7\u03a6, forN = {2, 3, 4, 5}. The first extracted rule is featured by order \u25e6 sort \u25e6 mod12 \u25e6 w{3,4}. The rule is chosen because its corresponding feature has a large confidence level (validated by the large number of matched examples), a small entropy after being smoothed by Bayesian surprise, and reveals a large gap against the Bach\u2019s style. Figure 4 shows the relative performance of this rule (in terms of confidence, regularity, and style gap) to other candidate cells in the 2D memory. Among the top 20 rules for this sonority, 12 are 5-gram, 5 are 4-gram, 3 are 2-gram, showing a long and adaptive dependence to preceding context.\nVisualizing Bach\u2019s Mind With the hierarchical representations in MUS-ROVER II, we are now able to visualize Bach\u2019s music mind step by step via activating nodes in the DAG of rule features\n1Fluctuations on the trajectory are largely incurred by the imperfect solver of the optimization problem.\n(similar to neuron activations in a brain). The hierarchical structure, as well as the additive activation process, is in stark contrast with the linear sequence of rules extracted from our prior work (Appendix A.5). Figure 5 shows a snapshot of the rule-learning status after ten loops, while the student is writing a sonority in the middle of a piece. The visualization makes it clear how earlier independent rules are now self-organized into sub-families, as well as how rules from a new context overwrite those from an old context, emphasizing that music is highly context-dependent."}, {"heading": "7 CONCLUSIONS AND DISCUSSIONS", "text": "Learning hierarchical rules as distributed representations of tonal music has played a central role in music pedagogy for centuries. While our previous work achieved the automation of rule extraction, and to certain level, the interpretability of the rules, this paper yields deeper interpretability that extends to a system of rules and the overall learning process. In summary, it highlights the importance of disentangling the rule features, sorting out their interconnections, and making the concept learning process more dynamic, hierarchical, and cognitive.\nMUS-ROVER is targeted to complement music teaching and learning. For instance, to many music students, learning and applying rules in part-writing is like learning to solve a puzzle (like Sudoku). Rules themselves are quite flexible as opposed to 0-1 derivatives, and may sometimes be contradictory. In addition, due to the limitation of human short-term memory and the difficulty of foreseeing implications, one has to handle a small set of rules at a time in a greedy manner, make some trials, and undo a few steps if no luck. Hence, solving this music puzzle could become a struggle (or maybe interesting): according to personal preferences, one typically begins with a small set of important rules, and via several steps of trial and error, tries one\u2019s best to make the part-writing satisfy a majority of rules, with occasional violations on unimportant ones. On the other hand, a machine is often good at solving and learning from puzzles due to its algorithmic nature. For instance, MUSROVER\u2019s student can take all rules into consideration: load them all at a time as constraints and figure out the global optimum of the optimization problem in only a few hours. The same level of efficiency might take a human student years to achieve.\nWe envision the future of MUS-ROVER as a partner to humans in both music teaching and research, which includes but is not limited to, personalizing the learning experience of a student, as well as suggesting new methodologies to music theorists in analyzing and developing new genres. It also has practical applications: as by-products from the self-learning loop, the teacher can be made into a genre classifier, while the student can be cast into a style synthesizer. We are also eager to study the rover\u2019s partnership beyond the domain of music."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Professor Heinrich Taube, President of Illiac Software, Inc., for providing Harmonia\u2019s MusicXML corpus of Bach\u2019s chorales (https://harmonia.illiacsoftware.com/), as well as his helpful comments and suggestions. This work was supported by the IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), a research collaboration as part of the IBM Cognitive Horizons Network."}, {"heading": "A APPENDIX", "text": ""}, {"heading": "A.1 MATH-TO-MUSIC DICTIONARY", "text": ""}, {"heading": "A.2 ATOMIC ARITHMETIC OPERATORS", "text": "In MUS-ROVER II, we set B = {order, diff, sort, mod12}, where diff(x) = (x2 \u2212 x1, x3 \u2212 x2, \u00b7 \u00b7 \u00b7 ), \u2200x \u2208 \u21262 \u222a \u21263 \u222a \u21264; sort(x) = (x(1), x(2), \u00b7 \u00b7 \u00b7 ), \u2200x \u2208 \u21262 \u222a \u21263 \u222a \u21264; mod12(x) = (mod(x1, 12),mod(x2, 12), \u00b7 \u00b7 \u00b7 ), \u2200x \u2208 \u2126 \u222a \u21262 \u222a \u21263 \u222a \u21264;\nand order(x), similar to argsort, maps x \u2208 \u21262 \u222a \u21263 \u222a \u21264 to a string that specifies the ordering of its elements, e.g. order((60, 55, 52, 52)) = \u201c4=3<2<1\u201d. The numbers in an order string denote the indices of the input vector x."}, {"heading": "A.3 ALGORITHM FOR CONCEPTUAL HIERARCHY", "text": "Input: A family of distinct partitions, represented by a sorted list P = [p1, . . . , pn]: pi 6= pj, for all i 6= j, and |p1| \u2264 . . . \u2264 |pn|;\nOutput: The conceptual hierarchy as a DAG, represented by the n by n adjacency matrix T: T[i, j] = 1 if there is an edge from node i to node j in the DAG;\ninitialize T[i, j] = 0 for all i, j; for i = n : 1 do\nfor j = (i + 1) : n do if T[i, j] == 0 then\nif is coarser(pi, pj) then for k in {k | pj \u227a pk} \u222a {j} do\nT[i, k] = 1; end\nend end\nend end T = Transpose(T);\nAlgorithm 1: Algorithm for computing the conceptual hierarchy"}, {"heading": "A.4 HEURISTICS FOR COMPARING TWO PARTITIONS", "text": "Given two partitions P,Q from the partition family, the function is coarser(P,Q) in Algorithm 1 returns True if P \u227a Q. A brute-force implementation of this function involves studying all (unordered) pairs of elements in the input domain (Hubert & Arabie, 1985), which incurs computational\nburdens if the size of the input domain is large. Therefore, we try to get around this brute-force routine whenever certain heuristic can be used to infer the output of is coarser directly. We propose a few of these heuristics as follows.\nTransitivity Heuristic If P\u03c6 P\u03c6\u2032 and P\u03c6\u2032 P\u03c6\u2032\u2032 , then P\u03c6 P\u03c6\u2032\u2032 . Window Heuristic Let P\u03c6 and P\u03c6\u2032 be two partitions induced by features \u03c6 and \u03c6\u2032, respectively. In addition, \u03c6 and \u03c6\u2032 are generated from the same descriptor that preserves the orders of the inputs\u2019 coordinates, e.g. diff, mod12: \u03c6 = d \u25e6 wI , \u03c6\u2032 = d \u25e6 wI\u2032 . We claim that P\u03c6 P\u03c6\u2032 , if I \u2283 I \u2032 and |\u03c6(\u21264)| > |\u03c6\u2032(\u21264)|. To see why this is the case, pick any x, y \u2208 \u21264 from the same cluster in P\u03c6, then \u03c6(x) = \u03c6(y). Since d preserves the orders of the inputs\u2019 coordinates, and I \u2032 extracts coordinates from I , then \u03c6\u2032(x) = \u03c6\u2032(y), i.e. x, y are in the same cluster in P\u03c6\u2032 . So, by definition, P\u03c6 P\u03c6\u2032 . Since |\u03c6(\u21264)| > |\u03c6\u2032(\u21264)|, P\u03c6 P\u03c6\u2032 . Descriptor Heuristic Let P\u03c6 and P\u03c6\u2032 be two partitions induced by features \u03c6 and \u03c6\u2032, respectively. In addition, \u03c6 and \u03c6\u2032 are generated from the same window:\n\u03c6 = d \u25e6 wI ; \u03c6\u2032 = d\u2032 \u25e6 wI . We claim that P\u03c6 P\u03c6\u2032 , if d\u2032 = b \u25e6 d for some function b and |\u03c6(\u21264)| > |\u03c6\u2032(\u21264)|. To see why this is the case, pick any x, y \u2208 \u21264 from the same cluster in P\u03c6, then \u03c6(x) = \u03c6(y). Since d\u2032 = b \u25e6 d for some b, then \u03c6\u2032 = b \u25e6 d \u25e6 wI = b \u25e6 \u03c6, thus, \u03c6\u2032(x) = \u03c6\u2032(y), i.e. x, y are in the same cluster in P\u03c6\u2032 . So, by definition, P\u03c6 P\u03c6\u2032 . Since |\u03c6(\u21264)| > |\u03c6\u2032(\u21264)|, P\u03c6 P\u03c6\u2032 . Combined Heuristic Combining the above heuristics, one can show that for P\u03c6 and P\u03c6\u2032 where\n\u03c6 = d \u25e6 wI ; \u03c6\u2032 = d\u2032 \u25e6 wI\u2032 , we have P\u03c6 P\u03c6\u2032 , if the following conditions are satisfied:\n1) d, d\u2032 both preserve the orders of the inputs\u2019 coordinates,"}, {"heading": "2) d\u2032 = b \u25e6 d for some b,", "text": "3) I \u2283 I \u2032, 4) |\u03c6(\u21264)| > |\u03c6\u2032(\u21264)|."}, {"heading": "A.5 SAMPLE RULE TRACES FROM MUS-ROVER I", "text": "Table 4 is essentially the same as Table 2 in our previous publication (Yu et al., 2016a), with feature notations following the current fashion. \u03b1 is the pace control parameter that we used in our previous system. No hierarchy was present in any of the three rule traces. For instance, the ordering features were learned as independent rules in a trace, even if they are apparently correlated, e.g. the ordering of w{1,2,3,4} (S,A,T,B) implies the ordering of w{1,4} (S,B)."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "GenJam: A genetic algorithm for generating jazz solos", "author": ["John Biles"], "venue": "In Proc. Int. Comput. Music Conf. (ICMC), pp", "citeRegEx": "Biles.,? \\Q1994\\E", "shortCiteRegEx": "Biles.", "year": 1994}, {"title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "[cs.LG],", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "An expert system for computer-assisted composition", "author": ["David Cope"], "venue": "Comput. Music J.,", "citeRegEx": "Cope.,? \\Q1987\\E", "shortCiteRegEx": "Cope.", "year": 1987}, {"title": "Experiments in Musical Intelligence, volume 12", "author": ["David Cope", "Melanie J. Mayer"], "venue": "AR editions Madison,", "citeRegEx": "Cope and Mayer.,? \\Q1996\\E", "shortCiteRegEx": "Cope and Mayer.", "year": 1996}, {"title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Proc. 29th Annu. Conf. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Disentangling factors of variation via generative entangling", "author": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "venue": "[stat.ML],", "citeRegEx": "Desjardins et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2012}, {"title": "Universal prediction applied to stylistic music generation", "author": ["Shlomo Dubnov", "G\u00e9rard Assayag"], "venue": "Mathematics and Music,", "citeRegEx": "Dubnov and Assayag.,? \\Q2002\\E", "shortCiteRegEx": "Dubnov and Assayag.", "year": 2002}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proc. 28th Annu. Conf. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "How to train your generative models and why does adversarial training work so well", "author": ["Ferenc Husz\u00e1r"], "venue": null, "citeRegEx": "Husz\u00e1r.,? \\Q2015\\E", "shortCiteRegEx": "Husz\u00e1r.", "year": 2015}, {"title": "An information theoretic approach to chord categorization and functional harmony", "author": ["Nori Jacoby", "Naftali Tishby", "Dmitri Tymoczko"], "venue": "J. New Music Res.,", "citeRegEx": "Jacoby et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jacoby et al\\.", "year": 2015}, {"title": "The Complete Musician: an Integrated Approach to Tonal Theory, Analysis, and Listening", "author": ["Steven G. Laitz"], "venue": null, "citeRegEx": "Laitz.,? \\Q2016\\E", "shortCiteRegEx": "Laitz.", "year": 2016}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Adversarial autoencoders", "author": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow"], "venue": "[cs.LG],", "citeRegEx": "Makhzani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makhzani et al\\.", "year": 2015}, {"title": "Complexity measures and concept learning", "author": ["Andreas D. Pape", "Kenneth J. Kurtz", "Hiroki Sayama"], "venue": "J. Math. Psychol.,", "citeRegEx": "Pape et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pape et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "[cs.LG],", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Proc. 29th Annu. Conf. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Statistical properties of tonal harmony in Bach\u2019s chorales", "author": ["Martin Rohrmeier", "Ian Cross"], "venue": "In Proc. 10th Int. Conf. Music Percept. Cogn. (ICMPC),", "citeRegEx": "Rohrmeier and Cross.,? \\Q2008\\E", "shortCiteRegEx": "Rohrmeier and Cross.", "year": 2008}, {"title": "MySong: Automatic accompaniment generation for vocal melodies", "author": ["Ian Simon", "Dan Morris", "Sumit Basu"], "venue": "In Proc. SIGCHI Conf. Hum. Factors Comput. Syst. (CHI", "citeRegEx": "Simon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Simon et al\\.", "year": 2008}, {"title": "Automatic tonal analysis: Toward the implementation of a music theory workbench", "author": ["Heinrich Taube"], "venue": "Comput. Music J.,", "citeRegEx": "Taube.,? \\Q1999\\E", "shortCiteRegEx": "Taube.", "year": 1999}, {"title": "To surprise and inform", "author": ["Lav R. Varshney"], "venue": "In Proc. 2013 IEEE Int. Symp. Inf. Theory, pp", "citeRegEx": "Varshney.,? \\Q2013\\E", "shortCiteRegEx": "Varshney.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proc. 25th Int. Conf. Mach. Learn. (ICML", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "MUS-ROVER: A self-learning system for musical compositional rules", "author": ["Haizi Yu", "Lav R. Varshney", "Guy E. Garnett", "Ranjitha Kumar"], "venue": "In Proc. 4th Int. Workshop Music. Metacreation (MUME 2016),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Learning interpretable musical compositional rules and traces", "author": ["Haizi Yu", "Lav R. Varshney", "Guy E. Garnett", "Ranjitha Kumar"], "venue": "In Proc. 2016 ICML Workshop Hum. Interpret. Mach. Learn. (WHI 2016),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the field of artificial neural networks, deep architectures are employed for machine learning tasks, with the awareness that hierarchical representations are important (Bengio et al., 2013).", "startOffset": 171, "endOffset": 192}, {"referenceID": 21, "context": "By further following the information scent spilled by Bayesian surprise (Varshney, 2013), the rover can effectively localize the desired features in the feature universe.", "startOffset": 72, "endOffset": 88}, {"referenceID": 8, "context": "3 RELATED WORK Adversarial or Collaborative MUS-ROVER\u2019s self-learning loop between the teacher (a discriminator) and student (a generator) shares great structural similarity to generative adversarial nets (Goodfellow et al., 2014) and their derivatives (Denton et al.", "startOffset": 205, "endOffset": 230}, {"referenceID": 0, "context": "Training this type of learner in general is challenging due to the lack of a clear objective or target (Bengio et al., 2013), which drives people to consider some end task like classification and use performance on the task to indirectly assess the learned representations.", "startOffset": 103, "endOffset": 124}, {"referenceID": 22, "context": "This line of work includes denoising autoencoders (Vincent et al., 2008) and restricted Boltzmann machines (Hinton et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 9, "context": ", 2008) and restricted Boltzmann machines (Hinton et al., 2006; Desjardins et al., 2012), ladder network algorithms (Rasmus et al.", "startOffset": 42, "endOffset": 88}, {"referenceID": 6, "context": ", 2008) and restricted Boltzmann machines (Hinton et al., 2006; Desjardins et al., 2012), ladder network algorithms (Rasmus et al.", "startOffset": 42, "endOffset": 88}, {"referenceID": 17, "context": ", 2012), ladder network algorithms (Rasmus et al., 2015), as well as more recent GAN models (Radford et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 16, "context": ", 2015), as well as more recent GAN models (Radford et al., 2015).", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "In particular, InfoGAN also introduces information-theoretic criteria to augment the standard GAN cost function, and to some extent achieves interpretability for both discrete and continuous latent factors (Chen et al., 2016).", "startOffset": 206, "endOffset": 225}, {"referenceID": 13, "context": "However, beyond the end results, the overall learning process of these neural networks are still far away from human-level concept learning (Lake et al., 2015), so not directly instructional to people.", "startOffset": 140, "endOffset": 159}, {"referenceID": 12, "context": "Automatic Musicians Music theory and composition form a reciprocal pair, often realized as the complementary cycle of reduction and elaboration (Laitz, 2016) as walks up and down the multilevel music hierarchy.", "startOffset": 144, "endOffset": 157}, {"referenceID": 1, "context": "Accordingly, various models have been introduced to automate this up/down walk, including music generation (Cope & Mayer, 1996; Biles, 1994; Simon et al., 2008), analysis (Taube, 1999), or theory evaluation (Rohrmeier & Cross, 2008).", "startOffset": 107, "endOffset": 160}, {"referenceID": 19, "context": "Accordingly, various models have been introduced to automate this up/down walk, including music generation (Cope & Mayer, 1996; Biles, 1994; Simon et al., 2008), analysis (Taube, 1999), or theory evaluation (Rohrmeier & Cross, 2008).", "startOffset": 107, "endOffset": 160}, {"referenceID": 20, "context": ", 2008), analysis (Taube, 1999), or theory evaluation (Rohrmeier & Cross, 2008).", "startOffset": 18, "endOffset": 31}, {"referenceID": 3, "context": "In terms of methodologies, we have rule-based systems (Cope, 1987), language models (Google Brain, 2016; Simon et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 19, "context": "In terms of methodologies, we have rule-based systems (Cope, 1987), language models (Google Brain, 2016; Simon et al., 2008), and information-theoretic approaches (Jacoby et al.", "startOffset": 84, "endOffset": 124}, {"referenceID": 11, "context": ", 2008), and information-theoretic approaches (Jacoby et al., 2015; Dubnov & Assayag, 2002).", "startOffset": 46, "endOffset": 91}, {"referenceID": 15, "context": "We decouple the regularity condition from the objective function in our previous work (which was the generalized cultural hole function), and state it separately as the first constraint that requires the Shannon entropy of the feature distribution to be no larger than a given threshold (Pape et al., 2015).", "startOffset": 287, "endOffset": 306}, {"referenceID": 15, "context": "Regularity is quantified by Shannon entropy of the rule\u2019s feature distribution: a rule is easier to memorize if it is less entropic (Pape et al., 2015).", "startOffset": 132, "endOffset": 151}, {"referenceID": 21, "context": "The right side of (4) is closely related to Bayesian surprise (Varshney, 2013), which takes the form of KL divergence from the prior to posterior.", "startOffset": 62, "endOffset": 78}, {"referenceID": 10, "context": "There are more in-depth and information-theoretic discussions on this point (Husz\u00e1r, 2015; Palomar & Verd\u00fa, 2008).", "startOffset": 76, "endOffset": 113}, {"referenceID": 15, "context": "The entropic difficulty \u03b4 caps the Shannon entropy of a rule\u2019s feature distribution in (3), a surrogate for the complexity (or memorability) of the rule (Pape et al., 2015).", "startOffset": 153, "endOffset": 172}], "year": 2017, "abstractText": "Music theory studies the regularity of patterns in music to capture concepts underlying music styles and composers\u2019 decisions. This paper continues the study of building automatic theorists (rovers) to learn and represent music concepts that lead to human interpretable knowledge and further lead to materials for educating people. Our previous work took a first step in algorithmic concept learning of tonal music, studying high-level representations (concepts) of symbolic music (scores) and extracting interpretable rules for composition. This paper further studies the representation hierarchy through the learning process, and supports adaptive 2D memory selection in the resulting language model. This leads to a deeper-level interpretability that expands from individual rules to a dynamic system of rules, making the entire rule learning process more cognitive. The outcome is a new rover, MUS-ROVER II, trained on Bach\u2019s chorales, which outputs customizable syllabi for learning compositional rules. We demonstrate comparable results to our music pedagogy, while also presenting the differences and variations. In addition, we point out the rover\u2019s potential usages in style recognition and synthesis, as well as applications beyond music.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_171"}