{"name": "ICLR_2017_17.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["C. Daniel Freeman", "Joan Bruna"], "emails": ["daniel.freeman@berkeley.edu", "bruna@cims.nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Optimization is a critical component in deep learning, governing its success in different areas of computer vision, speech processing and natural language processing. The prevalent optimization strategy is Stochastic Gradient Descent, invented by Robbins and Munro in the 50s. The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton et al. (2012); Ioffe & Szegedy (2015); Kingma & Ba (2014). This raises a number of theoretical questions as to why neural network optimization does not suffer in practice from poor local minima.\nThe loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a paradigmatic example of a hard, high-dimensional, non-convex problem. Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al. (2016) but merely slowed down. Other notable recent contributions are Kawaguchi (2016), which further develops the spin-glass connection from Choromanska et al. (2015) and resolves the linear case by showing that no poor local minima exist; Sagun et al. (2014) which also\n\u2217Currently on leave from UC Berkeley.\ndiscusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives. Lastly, several papers submitted concurrently and independently of this one deserve note, particularly Swirszcz et al. (2016) which analyzes the explicit criteria under which sigmoid-based neural networks become trapped by poor local minima, as well as Tian (2017), which offers a complementary study of two layer ReLU based networks, and their learning dynamics.\nIn this work, we do not make any linearity assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. The loss surface F (\u03b8) of a given model can be expressed in terms of its level sets \u2126\u03bb, which contain for each energy level \u03bb all parameters \u03b8 yielding a loss smaller or equal than \u03bb. A first question we address concerns the topology of these level sets, i.e. under which conditions they are connected. Connected level sets imply that one can always find a descent direction at each energy level, and therefore that no poor local minima can exist. In absence of nonlinearities, deep (linear) networks have connected level sets Kawaguchi (2016). We first generalize this result to include ridge regression (in the two layer case) and provide an alternative, more direct proof of the general case. We then move to the half-rectified case and show that the topology is intrinsically different and clearly dependent on the interplay between data distribution and model architecture. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.\nBeyond the question of whether the loss contains poor local minima or not, the immediate follow-up question that determines the convergence of algorithms in practice is the local conditioning of the loss surface. It is thus related not to the topology but to the shape or geometry of the level sets. As the energy level decays, one expects the level sets to exhibit more complex irregular structures, which correspond to regions where F (\u03b8) has small curvature. In order to verify this intuition, we introduce an efficient algorithm to estimate the geometric regularity of these level sets by approximating geodesics of each level set starting at two random boundary points. Our algorithm uses dynamic programming and can be efficiently deployed to study mid-scale CNN architectures on MNIST, CIFAR-10 and RNN models on Penn Treebank next word prediction. Our empirical results show that these models have a nearly convex behavior up until their lowest test errors, with a single connected component that becomes more elongated as the energy decays. The rest of the paper is structured as follows. Section 2 presents our theoretical results on the topological connectedness of multilayer networks. Section 3 presents our path discovery algorithm and Section 4 covers the numerical experiments."}, {"heading": "2 TOPOLOGY OF LEVEL SETS", "text": "Let P be a probability measure on a product space X \u00d7Y , where we assume X and Y are Euclidean vector spaces for simplicity. Let {(xi, yi)}i be an iid sample of size L drawn from P defining the training set. We consider the classic empirical risk minimization of the form\nFe(\u03b8) = 1\nL L\u2211 l=1 \u2016\u03a6(xi; \u03b8)\u2212 yi\u20162 + \u03baR(\u03b8) , (1)\nwhere \u03a6(x; \u03b8) encapsulates the feature representation that uses parameters \u03b8 \u2208 RS and R(\u03b8) is a regularization term. In a deep neural network, \u03b8 contains the weights and biases used in all layers. For convenience, in our analysis we will also use the oracle risk minimization:\nFo(\u03b8) = E(X,Y )\u223cP \u2016\u03a6(X; \u03b8)\u2212 Y \u20162 + \u03baR(\u03b8) . (2)\nOur setup considers the case whereR consists on either `1 or `2 norms, as we shall describe below. They correspond to well-known sparse and ridge regularization respectively."}, {"heading": "2.1 POOR LOCAL MINIMA CHARACTERIZATION FROM TOPOLOGICAL CONNECTEDNESS", "text": "We define the level set of F (\u03b8) as\n\u2126F (\u03bb) = {\u03b8 \u2208 RS ; F (\u03b8) \u2264 \u03bb} . (3)\nThe first question we study is the structure of critical points of Fe(\u03b8) and Fo(\u03b8) when \u03a6 is a multilayer neural network. For simplicity, we consider first a strict notion of local minima: \u03b8 \u2208 RS is a strict local minima of F if there is > 0 with F (\u03b8\u2032) > F (\u03b8) for all \u03b8\u2032 \u2208 B(\u03b8, ) and \u03b8\u2032 6= \u03b8. In particular, we are interested to know whether Fe has local minima which are not global minima. This question is answered by knowing whether \u2126F (\u03bb) is connected at each energy level \u03bb: Proposition 2.1. If \u2126F (\u03bb) is connected for all \u03bb then every local minima of F (\u03b8) is a global minima.\nStrict local minima implies that \u2207F (\u03b8) = 0 and HF (\u03b8) 0, but avoids degenerate cases where F is constant along a manifold intersecting \u03b8. In that scenario, if U\u03b8 denotes that manifold, our reasoning immediately implies that if \u2126F (\u03bb) are connected, then for all > 0 there exists \u03b8\u2032 with dist(\u03b8\u2032,U\u03b8) \u2264 and F (\u03b8\u2032) < F (\u03b8). In other words, some element at the boundary of U\u03b8 must be a saddle point. A stronger property that eliminates the risk of gradient descent getting stuck at U\u03b8 is that all elements at the boundary of U\u03b8 are saddle points. This can be guaranteed if one can show that there exists a path connecting any \u03b8 to the lowest energy level such that F is strictly decreasing along it.\nSuch degenerate cases arise in deep linear networks in absence of regularization. If \u03b8 = (W1, . . . ,WK) denotes any parameter value, with N1, . . . NK denoting the hidden layer sizes, and Fk \u2208 GL+Nk(R) are arbitrary elements of the general linear group of invertible Nk \u00d7 Nk matrices with positive determinant, then\nU\u03b8 = {W1F\u221211 , F1W2F \u22121 2 , . . . , FKWK ; Fk \u2208 GL + Nk (R)} .\nIn particular, U\u03b8 has a Lie Group structure. In the half-rectified nonlinear case, the general linear group is replaced by the Lie group of homogeneous invertible matrices Fk = diag(\u03b11, . . . , \u03b1Nk) with \u03b1j > 0.\nThis proposition shows that a sufficient condition to prevent the existence of poor local minima is having connected level sets, but this condition is not necessary: one can have isolated local minima lying at the same energy level. This can be the case in systems that are defined up to a discrete symmetry group, such as multilayer neural networks. However, as we shall see next, this case puts the system in a brittle position, since one needs to be able to account for all the local minima (and there can be exponentially many of them as the parameter dimensionality increases) and verify that their energy is indeed equal."}, {"heading": "2.2 THE LINEAR CASE", "text": "We first consider the particularly simple case where F is a multilayer network defined by\n\u03a6(x; \u03b8) = WK . . .W1x , \u03b8 = (W1, . . . ,WK) . (4)\nand the ridge regression R(\u03b8) = \u2016\u03b8\u20162. This model defines a non-convex (and non-concave) loss Fe(\u03b8). When \u03ba = 0, it has been shown in Saxe et al. (2013) and Kawaguchi (2016) that in this case, every local minima is a global minima. We provide here an alternative proof of that result that uses a somewhat simpler argument and allows for \u03ba > 0 in the case K = 2. Proposition 2.2. Let W1,W2, . . . ,WK be weight matrices of sizes nk \u00d7 nk+1, k < K, and let Fe(\u03b8), Fo(\u03b8) denote the risk minimizations using \u03a6 as in (4). Assume that nj \u2265 min(n1, nK) for j = 2 . . .K \u2212 1. Then \u2126Fe(\u03bb) (and \u2126Fo ) is connected for all \u03bb and all K when \u03ba = 0, and for \u03ba > 0 when K = 2; and therefore there are no poor local minima in these cases. Moreover, any \u03b8 can be connected to the lowest energy level with a strictly decreasing path.\nLet us highlight that this result is slightly complementary than that of Kawaguchi (2016), Theorem 2.3. Whereas we require nj \u2265 min(n1, nK) for j = 2 . . .K \u2212 1 and our analysis does not inform about the order of the saddle points, we do not need full rank assumptions on \u03a3X nor the weights Wk.\nThis result does also highlight a certain mismatch between the picture of having no poor local minima and generalization error. Incorporating regularization drastically changes the topology, and the fact that we are able to show connectedness only in the two-layer case with ridge regression is profound; we conjecture that extending it to deeper models requires a different regularization, perhaps using more general atomic norms Bach (2013). But we now move our interest to the nonlinear case, which is more relevant to our purposes."}, {"heading": "2.3 HALF-RECTIFIED NONLINEAR CASE", "text": "We now study the setting given by\n\u03a6(x; \u03b8) = WK\u03c1WK\u22121\u03c1 . . . \u03c1W1x , \u03b8 = (W1, . . . ,WK) , (5)\nwhere \u03c1(z) = max(0, z). The biases can be implemented by replacing the input vector x with x = (x, 1) and by rebranding each parameter matrix as\nW i = ( Wi bi 0 1 ) ,\nwhere bi contains the biases for each layer. For simplicity, we continue to use Wi and x in the following."}, {"heading": "2.3.1 NONLINEAR MODELS ARE GENERALLY DISCONNECTED", "text": "One may wonder whether the same phenomena of global connectedness also holds in the halfrectified case. A simple motivating counterexample shows that this is not the case in general. Consider a simple setup with X \u2208 R2 drawn from a mixture of two Gaussians N\u22121 and N1, and let Y = (X \u2212 \u00b5Z) \u00b7 Z , where Z is the (hidden) mixture component taking {1,\u22121} values. Let Y\u0302 = \u03a6(X; {W1,W2}) be a single-hidden layer ReLU network, with two hidden units. Let \u03b8A be a configuration that bisects the two mixture components, and let \u03b8B the same configuration, but swapping the bisectrices. One can verify that they can both achieve arbitrarily small risk by letting the covariance of the mixture components go to 0. However, any path that connects \u03b8A to \u03b8B must necessarily pass through a point in which W1 has rank 1, which leads to an estimator with risk at least 1/2.\nIn fact, it is easy to see that this counter-example can be extended to any generic half-rectified architecture, if one is allowed to adversarially design a data distribution. For any given \u03a6(X; \u03b8) with arbitrary architecture and current parameters \u03b8 = (Wi), let P\u03b8 = {A1, . . . ,AS} be the underlying tessellation of the input space given by our current choice of parameters; that is, \u03a6(X; \u03b8) is piece-wise linear and P\u03b8 contains those pieces. Now let X be any arbitrary distribution with density p(x) > 0 for all x \u2208 Rn, for example a Gaussian, and let Y | X d= \u03a6(X; \u03b8) . Since \u03a6 is invariant under a subgroup of permutations \u03b8\u03c3 of its hidden layers, it is easy to see that one can find two parameter values \u03b8A = \u03b8 and \u03b8B = \u03b8\u03c3 such that Fo(\u03b8A) = Fo(\u03b8B) = 0, but any continuous path \u03b3(t) from \u03b8A to \u03b8B will have a different tessellation and therefore won\u2019t satisfy Fo(\u03b3(t)) = 0. Moreover, one can build on this counter-example to show that not only the level sets are disconnected, but also that there exist poor local minima. Let \u03b8\u2032 be a different set of parameters, and Y \u2032 | X d= \u03a6(X; \u03b8\u2032) be a different target distribution. Now consider the data distribution given by the mixture\nX | p(x) , z \u223c Bernoulli(\u03c0) , Y | X, z d= z\u03a6(X; \u03b8) + (1\u2212 z)\u03a6(X; \u03b8\u2032) . By adjusting the mixture component \u03c0 we can clearly change the risk at \u03b8 and \u03b8\u2032 and make them different, but we conjecture that this preserves the status of local minima of \u03b8 and \u03b8\u2032. Appendix E constructs a counter-example numerically.\nThis illustrates an intrinsic difficulty in the optimization landscape if one is after universal guarantees that do not depend upon the data distribution. This difficulty is non-existent in the linear case and not easy to exploit in mean-field approaches such as Choromanska et al. (2015), and shows that in general we should not expect to obtain connected level sets. However, connectedness can be recovered if one is willing to accept a small increase of energy and make some assumptions on the complexity of the regression task. Our main result shows that the amount by which the energy is allowed to increase is upper bounded by a quantity that trades-off model overparametrization and smoothness in the data distribution.\nFor that purpose, we start with a characterization of the oracle loss, and for simplicity let us assume Y \u2208 R and let us first consider the case with a single hidden layer and `1 regularization: R(\u03b8) = \u2016\u03b8\u20161."}, {"heading": "2.3.2 PRELIMINARIES", "text": "Before proving our main result, we need to introduce preliminary notation and results. We first describe the case with a single hidden layer of size m.\nWe define\ne(m) = min W1\u2208Rm\u00d7n,\u2016W1(i)\u20162\u22641,W2\u2208Rm\nE{|\u03a6(X; \u03b8)\u2212 Y |2}+ \u03ba\u2016W2\u20161 . (6)\nto be the oracle risk using m hidden units with norm \u2264 1 and using sparse regression. It is a well known result by Hornik and Cybenko that a single hidden layer is a universal approximator under very mild assumptions, i.e. limm\u2192\u221e e(m) = 0. This result merely states that our statistical setup is consistent, and it should not be surprising to the reader familiar with classic approximation theory. A more interesting question is the rate at which e(m) decays, which depends on the smoothness of the joint density (X,Y ) \u223c P relative to the nonlinear activation family we have chosen. For convenience, we redefine W = W1 and \u03b2 = W2 and Z(W ) = max(0,WX). We also write z(w) = max(0, \u3008w,X\u3009) where (X,Y ) \u223c P and w \u2208 RN is any deterministic vector. Let \u03a3X = EPXXT \u2208 RN\u00d7N be the covariance operator of the random input X . We assume \u2016\u03a3X\u2016 <\u221e. A fundamental property that will be essential to our analysis is that, despite the fact that Z is nonlinear, the quantity [w1, w2]Z := EP {z(w1)z(w2)} is locally equivalent to the linear metric \u3008w1, w2\u3009X = EP {wT1 XXTw2} = \u3008w1,\u03a3Xw2\u3009, and that the linearization error decreases with the angle between w1 and w2. Without loss of generality, we assume here that \u2016w1\u2016 = \u2016w2\u2016 = 1, and we write \u2016w\u20162Z = E{|z(w)|2}. Proposition 2.3. Let \u03b1 = cos\u22121(\u3008w1, w2\u3009) be the angle between unitary vectors w1 and w2 and let wm =\nw1+w2 \u2016w1+w2\u2016 be their unitary bisector. Then\n1 + cos\u03b1\n2 \u2016wm\u20162Z \u2212 2\u2016\u03a3X\u2016\n( 1\u2212 cos\u03b1\n2 + sin2 \u03b1\n) \u2264 [w1, w2]Z \u2264 1 + cos\u03b1\n2 \u2016wm\u20162Z . (7)\nThe term \u2016\u03a3X\u2016 is overly pessimistic: we can replace it by the energy of X projected into the subspace spanned byw1 andw2 (which is bounded by 2\u2016\u03a3X\u2016). When \u03b1 is small, a Taylor expansion of the trigonometric terms reveals that\n2\n3\u2016\u03a3X\u2016 \u3008w1, w2\u3009 =\n2\n3\u2016\u03a3X\u2016 cos\u03b1 =\n2 3\u2016\u03a3X\u2016 (1\u2212 \u03b1\n2\n2 +O(\u03b14))\n\u2264 (1\u2212 \u03b12/4)\u2016wm\u20162Z \u2212 \u2016\u03a3X\u2016(\u03b12/4 + \u03b12) +O(\u03b14) \u2264 [w1, w2]Z +O(\u03b14) ,\nand similarly [w1, w2]Z \u2264 \u3008w1, w2\u3009\u2016wm\u20162Z \u2264 \u2016\u03a3X\u2016\u3008w1, w2\u3009 .\nThe local behavior of parameters w1, w2 on our regression problem is thus equivalent to that of having a linear layer, provided w1 and w2 are sufficiently close to each other. This result can be seen as a spoiler of what is coming: increasing the hidden layer dimensionality m will increase the chances to encounter pairs of vectors w1, w2 with small angle; and with it some hope of approximating the previous linear behavior thanks to the small linearization error.\nIn order to control the connectedness, we need a last definition. Given a hidden layer of size m with current parameters W \u2208 Rn\u00d7m, we define a \u201crobust compressibility\u201d factor as\n\u03b4W (l, \u03b1;m) = min \u2016\u03b3\u20160\u2264l,supi |\u2220(w\u0303i,wi)|\u2264\u03b1\nE{|Y \u2212 \u03b3Z(W\u0303 )|2 + \u03ba\u2016\u03b3\u20161} , (l \u2264 m) . (8)\nThis quantity thus measures how easily one can compress the current hidden layer representation, by keeping only a subset of l its units, but allowing these units to move by a small amount controlled by \u03b1. It is a form of n-width similar to Kolmogorov width Donoho (2006) and is also related to robust sparse coding from Tang et al. (2013); Ekanadham et al. (2011)."}, {"heading": "2.3.3 MAIN RESULT", "text": "Our main result considers now a non-asymptotic scenario given by some fixed size m of the hidden layer. Given two parameter values \u03b8A = (WA1 ,W A 2 ) \u2208 W and \u03b8B = (WB1 ,WB2 ) with Fo(\u03b8 {A,B}) \u2264 \u03bb, we show that there exists a continuous path \u03b3 : [0, 1] \u2192 W connecting \u03b8A and \u03b8B such that its oracle risk is uniformly bounded by max(\u03bb, ), where decreases with model overparametrization.\nTheorem 2.4. For any \u03b8A, \u03b8B \u2208 W and \u03bb \u2208 R satisfying Fo(\u03b8{A,B}) \u2264 \u03bb, there exists a continuous path \u03b3 : [0, 1]\u2192W such that \u03b3(0) = \u03b8A, \u03b3(1) = \u03b8B and\nFo(\u03b3(t)) \u2264 max(\u03bb, ) , with (9)\n= inf l,\u03b1\n( max { e(l),\u03b4WA1 (m, 0;m), \u03b4WA1 (m\u2212 l, \u03b1;m), (10)\n\u03b4WB1 (m, 0;m), \u03b4WB1 (m\u2212 l, \u03b1;m) } + C1\u03b1+O(\u03b1 2) ) , (11)\nwhere C1 is an absolute constant depending only on \u03ba and P .\nSome remarks are in order. First, our regularization term is currently a mix between `2 norm constraints on the first layer and `1 norm constraints on the second layer. We believe this is an artifact of our proof technique, and we conjecture that more general regularizations yield similar results. Next, this result uses the data distribution through the oracle bound e(m) and the covariance term. The extension to empirical risk is accomplished by replacing the probability measure P by the empirical measure P\u0302 = 1L \u2211 l \u03b4 ((x, y)\u2212 (xl, yl)). However, our asymptotic analysis has to be carefully reexamined to take into account and avoid the trivial regime when M outgrows L. A consequence of Theorem 2.4 is that as m increases, the model becomes asymptotically connected, as proven in the following corollary.\nCorollary 2.5. As m increases, the energy gap satisfies = O(m\u2212 1n ) and therefore the level sets become connected at all energy levels.\nThis is consistent with the overparametrization results from Safran & Shamir (2015); Shamir (2016) and the general common knowledge amongst deep learning practitioners. Our next sections explore this question, and refine it by considering not only topological properties but also some rough geometrical measure of the level sets."}, {"heading": "3 GEOMETRY OF LEVEL SETS", "text": ""}, {"heading": "3.1 THE GREEDY ALGORITHM", "text": "The intuition behind our main result is that, for smooth enough loss functions and for sufficient overparameterization, it should be \u201ceasy\u201d to connect two equally powerful models\u2014i.e., two models with Fo\u03b8A,B \u2264 \u03bb. A sensible measure of this ease-of-connectedness is the normalized length of the geodesic connecting one model to the other: |\u03b3A,B(t)|/|\u03b8A \u2212 \u03b8B |. This length represents approximately how far of an excursion one must make in the space of models relative to the euclidean distance between a pair of models. Thus, convex models have a geodesic length of 1, because the geodesic is simply linear interpolation between models, while more non-convex models have geodesic lengths strictly larger than 1.\nBecause calculating the exact geodesic is difficult, we approximate the geodesic paths via a dynamic programming approach we call Dynamic String Sampling. We comment on alternative algorithms in Appendix A.\nFor a pair of models with network parameters \u03b8i, \u03b8j , each with Fe(\u03b8) below a threshold L0, we aim to efficienly generate paths in the space of weights where the empirical loss along the path remains below L0. These paths are continuous curves belonging to \u2126F (\u03bb)\u2013that is, the level sets of the loss function of interest.\nAlgorithm 1 Greedy Dynamic String Sampling 1: L0 \u2190 Threshold below which path will be found 2: \u03a61 \u2190 randomly initialize \u03b81, train \u03a6(xi \u03b81) to L0 3: \u03a62 \u2190 randomly initialize \u03b82, train \u03a6(xi \u03b82) to L0 4: BeadList\u2190(\u03a61,\u03a62) 5: Depth\u2190 0 6: procedure FINDCONNECTION(\u03a61,\u03a62) 7: t\u2217 \u2190 t such that d\u03b3(\u03b81,\u03b82,t)dt \u2223\u2223\u2223\u2223 t = 0 OR t = 0.5\n8: \u03a63 \u2190 train \u03a6(xi; t\u2217\u03b81 + (1\u2212 t\u2217)\u03b82) to L0 9: BeadList\u2190 insert(\u03a63, after \u03a61, BeadList)\n10: MaxError1 \u2190 maxt(Fe(t\u03b83 + (1\u2212 t)\u03b81)) 11: MaxError2 \u2190 maxt(Fe(t\u03b82 + (1\u2212 t)\u03b83)) 12: ifMaxError1 > L0 then return FindConnection(\u03a61,\u03a63) 13: ifMaxError2 > L0 then return FindConnection(\u03a63,\u03a62) 14: Depth\u2190 Depth+1\nThe algorithm recursively builds a string of models in the space of weights which continuously connect \u03b8i to \u03b8j . Models are added and trained until the pairwise linearly interpolated loss, i.e. maxtFe(t\u03b8i + (1 \u2212 t)\u03b8j) for t \u2208 (0, 1), is below the threshold, L0, for every pair of neighboring models on the string. We provide a cartoon of the algorithm in Appendix C."}, {"heading": "3.2 FAILURE CONDITIONS AND PRACTICALITIES", "text": "While the algorithm presented will faithfully certify two models are connected if the algorithm converges, it is worth emphasizing that the algorithm does not guarantee that two models are disconnected if the algorithm fails to converge. In general, the problem of determining if two models are connected can be made arbitrarily difficult by choice of a particularly pathological geometry for the loss function, so we are constrained to heuristic arguments for determining when to stop running the algorithm. Thankfully, in practice, loss function geometries for problems of interest are not intractably difficult to explore. We comment more on diagnosing disconnections more carefully in Appendix E.\nFurther, if the MaxError exceeds L0 for every new recursive branch as the algorithm progresses, the worst case runtime scales as O(exp(Depth)). Empirically, we find that the number of new models added at each depth does grow, but eventually saturates, and falls for a wide variety of models and architectures, so that the typical runtime is closer to O(poly(Depth))\u2014at least up until a critical value of L0.\nTo aid convergence, either of the choices in line 7 of the algorithm works in practice\u2014choosing t\u2217 at a local maximum can provide a modest increase in algorithm runtime, but can be unstable if the the calculated interpolated loss is particularly flat or noisy. t\u2217 = .5 is more stable, but slower. Finally, we find that training \u03a63 to \u03b1L0 for \u03b1 < 1 in line 8 of the algorithm tends to aid convergence without noticeably impacting our numerics. We provide further implementation details in 4."}, {"heading": "4 NUMERICAL EXPERIMENTS", "text": "For our numerical experiments, we calculated normalized geodesic lengths for a variety of regression and classification tasks. In practice, this involved training a pair of randomly initialized models to the desired test loss value/accuracy/perplexity, and then attempting to connect that pair of models via the Dynamic String Sampling algorithm. We also tabulated the average number of \u201cbeads\u201d, or the number intermediate models needed by the algorithm to connect two initial models. For all of the below experiments, the reported losses and accuracies are on a restricted test set. For more complete architecture and implementation details, see our GitHub page.\nThe results are broadly organized by increasing model complexity and task difficulty, from easiest to hardest. Throughout, and remarkably, we were able to easily connect models for every dataset and architecture investigated except the one explicitly constructed counterexample discussed in Appendix E.1. Qualitatively, all of the models exhibit a transition from a highly convex regime at high loss to a non-convex regime at low loss, as demonstrated by the growth of the normalized length as well as the monotonic increase in the number of required \u201cbeads\u201d to form a low-loss connection."}, {"heading": "4.1 POLYNOMIAL REGRESSION", "text": "We studied a 1-4-4-1 fully connected multilayer perceptron style architecture with sigmoid nonlinearities and RMSProp/ADAM optimization. For ease-of-analysis, we restricted the training and test data to be strictly contained in the interval x \u2208 [0, 1] and f(x) \u2208 [0, 1]. The number of required beads, and thus the runtime of the algorithm, grew approximately as a power-law, as demonstrated in Table 1 Fig. 1. We also provide a visualization of a representative connecting path between two models of equivalent power in Appendix D.\nThe cubic regression task exhibits an interesting feature around L0 = .15 in Table 1 Fig. 2, where the normalized length spikes, but the number of required beads remains low. Up until this point, the\ncubic model is strongly convex, so this first spike seems to indicate the onset of non-convex behavior and a concomitant radical change in the geometry of the loss surface for lower loss."}, {"heading": "4.2 CONVOLUTIONAL NEURAL NETWORKS", "text": "To test the algorithm on larger architectures, we ran it on the MNIST hand written digit recognition task as well as the CIFAR10 image recognition task, indicated in Table 1, Figs. 3 and 4. Again, the data exhibits strong qualitative similarity with the previous models: normalized length remains low until a threshold loss value, after which it grows approximately as a power law. Interestingly, the MNIST dataset exhibits very low normalized length, even for models nearly at the state of the art in classification power, in agreement with the folk-understanding that MNIST is highly convex and/or \u201ceasy\u201d. The CIFAR10 dataset, however, exhibits large non-convexity, even at the modest test accuracy of 80%."}, {"heading": "4.3 RECURRENT NEURAL NETWORKS", "text": "To gauge the generalizability of our algorithm, we also applied it to an LSTM architecture for solving the next word prediction task on the PTB dataset, depicted in Table 1 Fig. 5. Noteably, even for a radically different architecture, loss function, and data set, the normalized lengths produced by the DSS algorithm recapitulate the same qualitative features seen in the above datasets\u2014i.e., models can be easily connected at high perplexity, and the normalized length grows at lower and lower perplexity after a threshold value, indicating an onset of increased non-convexity of the loss surface."}, {"heading": "5 DISCUSSION", "text": "We have addressed the problem of characterizing the loss surface of neural networks from the perspective of gradient descent algorithms. We explored two angles \u2013 topological and geometrical aspects \u2013 that build on top of each other.\nOn the one hand, we have presented new theoretical results that quantify the amount of uphill climbing that is required in order to progress to lower energy configurations in single hidden-layer ReLU networks, and proved that this amount converges to zero with overparametrization under mild conditions. On the other hand, we have introduced a dynamic programming algorithm that efficiently approximates geodesics within each level set, providing a tool that not only verifies the connectedness of level sets, but also estimates the geometric regularity of these sets. Thanks to this information, we can quantify how \u2018non-convex\u2019 an optimization problem is, and verify that the optimization of quintessential deep learning tasks \u2013 CIFAR-10 and MNIST classification using CNNs, and next word prediction using LSTMs \u2013 behaves in a nearly convex fashion up until they reach high accuracy levels.\nThat said, there are some limitations to our framework. In particular, we do not address saddle-point issues that can greatly affect the actual convergence of gradient descent methods. There are also a number of open questions; amongst those, in the near future we shall concentrate on:\n\u2022 Extending Theorem 2.4 to the multilayer case. We believe this is within reach, since the main analytic tool we use is that small changes in the parameters result in small changes in the covariance structure of the features. That remains the case in the multilayer case.\n\u2022 Empirical versus Oracle Risk. A big limitation of our theory is that right now it does not inform us on the differences between optimizing the empirical risk versus the oracle risk. Understanding the impact of generalization error and stochastic gradient in the ability to do small uphill climbs is an open line of research.\n\u2022 Influence of symmetry groups. Under appropriate conditions, the presence of discrete symmetry groups does not prevent the loss from being connected, but at the expense of increasing the capacity. An important open question is whether one can improve the asymptotic properties by relaxing connectedness to being connected up to discrete symmetry.\n\u2022 Improving numerics with Hyperplane method. Our current numerical experiments employ a greedy (albeit faster) algorithm to discover connected components and estimate geodesics. We plan to perform experiments using the less greedy algorithm described in Appendix A."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Mark Tygert for pointing out the reference to the -nets and Kolmogorov capacity, and Martin Arjovsky for spotting several bugs in early version of the results. We would also like to thank Maithra Raghu and Jascha Sohl-Dickstein for enlightening discussions, as well as Yasaman Bahri for helpful feedback on an early version of the manuscript. CDF was supported by the NSF Graduate Research Fellowship under Grant DGE-1106400."}, {"heading": "A CONSTRAINED DYNAMIC STRING SAMPLING", "text": "While the algorithm presented in Sec. 3.1 is fast for sufficiently smooth families of loss surfaces with few saddle points, here we present a slightly modified version which, while slower, provides more control over the convergence of the string. We did not use the algorithm presented in this section for our numerical studies.\nInstead of training intermediate models via full SGD to a desired accuracy as in step 8 of the algorithm, intermediate models are be subject to a constraint that ensures they are \u201cclose\u201d to the neighboring models on the string. Specifically, intermediate models are constrained to the unique hyperplane in weightspace equidistant from its two neighbors. This can be further modified by additional regularization terms to control the \u201cspringy-ness\u201d of the string. These heuristics could be chosen to try to more faithfully sample the geodesic between two models.\nIn practice, for a given model on the string, \u03b8i, these two regularizations augment the standard loss by: F\u0303 (\u03b8) = F (\u03b8) + \u03b6(\u2016\u03b8i\u22121\u2212 \u03b8i\u2016+ \u2016\u03b8i+1\u2212 \u03b8i\u2016) +\u03ba\u2016 (\u03b8i\u22121\u2212\u03b8i+1)/2\u2016(\u03b8i\u22121\u2212\u03b8i+1)/2\u2016 \u00b7 (\u03b8i\u2212(\u03b8i\u22121\u2212\u03b8i+1)/2) \u2016(\u03b8i\u2212(\u03b8i\u22121\u2212\u03b8i+1)/2)\u2016\u2016. The \u03b6 regularization term controls the \u201cspringy-ness\u201d of the weightstring, and the \u03ba regularization term controls how far off the hyperplane a new model can deviate.\nBecause adapting DSS to use this constraint is straightforward, here we will describe an alternative \u201cbreadth-first\u201d approach wherein models are trained in parallel until convergence. This alternative approach has the advantage that it will indicate a disconnection between two models \u201csooner\u201d in training. The precise geometry of the loss surface will dictate which approach to use in practice.\nGiven two random models \u03c3i and \u03c3j where |\u03c3i \u2212 \u03c3j | < L0, we aim to follow the evolution of the family of models connecting \u03c3i to \u03c3j . Intuitively, almost every continuous path in the space of random models connecting \u03c3i to \u03c3j has, on average, the same (high) loss. For simplicity, we choose to initialize the string to the linear segment interpolating between these two models. If this entire segment is evolved via gradient descent, the segment will either evolve into a string which is entirely contained in a basin of the loss surface, or some number of points will become fixed at a higher loss. These fixed points are difficult to detect directly, but will be indirectly detected by the persistence of a large interpolated loss between two adjacent models on the string.\nThe algorithm proceeds as follows:\n(0.) Initialize model string to have two models, \u03c3i and \u03c3j .\n1. Begin training all models to the desired loss, keeping the instantaneous loss, L0(t), of all models being trained approximately constant.\n2. If the pairwise interpolated loss between \u03c3n and \u03c3n+1 exceeds L0(t), insert a new model at the maximum of the interpolated loss (or halfway) between these two models.\n3. Repeat steps (1) and (2) until all models (and interpolated errors) are below a threshold loss L0(tfinal) := L0, or until a chosen failure condition (see 3.2)."}, {"heading": "B PROOFS", "text": "B.1 PROOF OF PROPOSITION 2.1\nSuppose that \u03b81 is a local minima and \u03b82 is a global minima, but F (\u03b81) > F (\u03b82). If \u03bb = F (\u03b81), then clearly \u03b81 and \u03b82 both belong to \u2126F (\u03bb). Suppose now that \u2126F (\u03bb) is connected. Then we\ncould find a smooth (i.e. continuous and differentiable) path \u03b3(t) with \u03b3(0) = \u03b81, \u03b3(1) = \u03b82 and F (\u03b3(t)) \u2264 \u03bb = F (\u03b81). But this contradicts the strict local minima status of \u03b81, and therefore \u2126F (\u03bb) cannot be connected .\nB.2 PROOF OF PROPOSITION 2.2\nLet us first consider the case with \u03ba = 0. We proceed by induction over the number of layers K. For K = 1, the loss F (\u03b8) is convex. Let \u03b8A, \u03b8B be two arbitrary points in a level set \u2126\u03bb. Thus F (\u03b8A) \u2264 \u03bb and F (\u03b8B) \u2264 \u03bb. By definition of convexity, a linear path is sufficient in that case to connect \u03b8A and \u03b8B :\nF ((1\u2212 t)\u03b8A + t\u03b8B) \u2264 (1\u2212 t)F (\u03b8A) + tF (\u03b8B) \u2264 \u03bb . Suppose the result is true for K \u2212 1. Let \u03b8A = (WA1 , . . . ,WAK ) and \u03b8B = (WB1 , . . . ,WBK ) with F (\u03b8A) \u2264 \u03bb, F (\u03b8B) \u2264 \u03bb. Since nj \u2265 min(n1, nK) for j = 2 . . .K \u2212 1, we can find k\u2217 = {1,K \u2212 1} such that nk\u2217 \u2265 min(nk\u2217\u22121, nk\u2217+1). For each W1, . . . ,WK , we denote W\u0303j = Wj for j 6= k\u2217, k\u2217 \u2212 1 and W\u0303k\u2217 = Wk\u2217\u22121Wk\u2217 . By induction hypothesis, the loss expressed in terms of \u03b8\u0303 = (W\u03031, . . . , W\u0303K\u22121) is connected between \u03b8\u0303A and \u03b8\u0303B . Let W\u0303k\u2217(t) the corresponding linear path projected in the layer k\u2217. We need to produce a path in the variables Wk\u2217\u22121(t), Wk\u2217(t) such that:\ni Wk\u2217\u22121(0) = WAk\u2217\u22121, Wk\u2217\u22121(1) = W B k\u2217\u22121,\nii Wk\u2217(0) = WAk\u2217 , Wk\u2217(1) = W B k\u2217 ,\niii Wk\u2217(t)Wk\u2217\u22121(t) = W\u0303k\u2217\u22121(t) for t \u2208 (0, 1).\nWe construct it as follows. Let\nWk\u2217(t) = tW B k\u2217 + (1\u2212 t)WAk\u2217 + t(1\u2212 t)V ,\nWk\u2217\u22121(t) = Wk\u2217(t) \u2020W\u0303k\u2217\u22121(t) ,\nwhere Wk\u2217(t)\u2020 = (Wk\u2217(t)TWk\u2217(t))\u22121Wk\u2217(t)T denotes the pseudoinverse and V is a nk\u2217\u22121\u00d7nk\u2217 matrix drawn from a iid distribution. Conditions (i) and (ii) are immediate from the definition, and condition (iii) results from the fact that\nWk\u2217(t)Wk\u2217(t) \u2020 = IN\u2217k ,\nsince W \u2217k (t) has full rank for all t \u2208 (0, 1). Finally, let us prove that the result is also true when K = 2 and \u03ba > 0. We construct the path using the variational properties of atomic norms Bach (2013). When we pick the ridge regression regularization, the corresponding atomic norm is the nuclear norm:\n\u2016X\u2016\u2217 = min UV T=X\n1 2 (\u2016U\u20162 + \u2016V \u20162) .\nThe path is constructed by exploiting the convexity of the variational norm \u2016X\u2016\u2217. Let \u03b8A = (WA1 ,W A 2 ) and \u03b8 B = (WB1 ,W B 2 ), and we define W\u0303 = W1W2. Since W\u0303 {A,B} = W {A,B} 1 W {A,B} 2 , it results that\n\u2016W\u0303 {A,B}\u2016\u2217 \u2264 1 2 (\u2016W {A,B}1 \u20162 + \u2016W {A,B} 2 \u20162) . (12)\nFrom (12) it results that the loss Fo(W1,W2) can be minored by another loss expressed in terms of W\u0303 of the form E{|Y \u2212 W\u0303X|2}+ 2\u03ba\u2016W\u0303\u2016\u2217 , which is convex with respect to W\u0303 . Thus a linear path in W\u0303 from W\u0303A to W\u0303B is guaranteed to be below Fo(\u03b8{A,B}). Let us define\n\u2200 t , W1(t),W2(t) = arg min UV T=W\u0303 (t) (\u2016U\u20162 + \u2016V \u20162) .\nOne can verify that we can first consider a path (\u03b2A1 (s), \u03b2 A 2 (s)) from (W A 1 ,W A 2 ) to (W1(0),W2(0) such that \u2200 s \u03b21(s)\u03b22(s) = W\u0303A and \u2016\u03b21(s)\u20162 + \u2016\u03b22(s)\u20162 decreases ,\nand similarly for (WB1 ,W B 2 ) to (W1(1),W2(1). The path (\u03b2 A {1,2}(s),W{1,2}(t), \u03b2 B {1,2}(s)) satisfies (i-iii) by definition. We also verify that\n\u2016W1(t)\u20162 + \u2016W2(t)\u20162 = 2\u2016W\u0303 (t)\u2016\u2217 \u2264 2(1\u2212 t)\u2016W\u0303 (0)\u2016\u2217 + 2t\u2016W\u0303 (1)\u2016\u2217 \u2264 (1\u2212 t)(\u2016W\u201621(0) + \u2016W\u201622(0)) + t(\u2016W\u201621(1) + \u2016W\u201622(1)) .\nFinally, we verify that the paths we have just created, when applied to \u03b8A arbitrary and \u03b8B = \u03b8\u2217 a global minimum, are strictly decreasing, again by induction. For K = 1, this is again an immediate consequence of convexity. ForK > 1, our inductive construction guarantees that for any 0 < t < 1, the path \u03b8(t) = (Wk(t))k\u2264K satisfies Fo(\u03b8(t)) < Fo(\u03b8A). This concludes the proof .\nB.3 PROOF OF PROPOSITION 2.3\nLet A(w1, w2) = {x \u2208 Rn; \u3008x,w1\u3009 \u2265 0 , \u3008x,w2\u3009 \u2265 0} .\nBy definition, we have\n\u3008w1, w2\u3009Z = E{max(0, \u3008X,w1\u3009) max(0, \u3008X,w2\u3009)} (13)\n= \u222b A(w1,w2) \u3008x,w1\u3009\u3008x,w2\u3009dP (x) , (14)\n= \u222b Q(A(w1,w2)) \u3008Q(x), w1\u3009\u3008Q(x), w2\u3009(dP\u0304 (Q(x))) , (15)\nwhereQ is the orthogonal projection onto the space spanned byw1 andw2 and dP\u0304 (x) = dP\u0304 (x1, x2) is the marginal density on that subspace. Since this projection does not interfere with the rest of the proof, we abuse notation by dropping the Q and still referring to dP (x) as the probability density.\nNow, let r = 12\u2016w1 + w2\u2016 = 1+cos(\u03b1) 2 and d = w2\u2212w1 2 . By construction we have\nw1 = rwm \u2212 d , w2 = rwm + d , and thus \u3008x,w1\u3009\u3008x,w2\u3009 = r2|\u3008x,wm\u3009|2 \u2212 |\u3008x, d\u3009|2 . (16) By denoting C(wm) = {x \u2208 Rn; \u3008x,wm\u3009 \u2265 0}, observe that A(w1, w2) \u2286 C(wm). Let us denote by B = C(wm) \\A(w1, w2) the disjoint complement. It results that\n\u3008w1, w2\u3009Z = \u222b A(w1,w2) \u3008x,w1\u3009\u3008x,w2\u3009dP (x)\n= \u222b C(wm) [r2|\u3008x,wm\u3009|2 \u2212 |\u3008x, d\u3009|2]dP (x)\u2212\nr2 \u222b B |\u3008x,wm\u3009|2dP (x) + \u222b B |\u3008x, d\u3009|2dP (x)\n= r2\u2016wm\u20162Z \u2212 r2 \u222b B\n|\u3008x,wm\u3009|2dP (x)\ufe38 \ufe37\ufe37 \ufe38 E1\n\u2212 \u222b A(w1,w2)\n|\u3008x, d\u3009|2dP (x)\ufe38 \ufe37\ufe37 \ufe38 E2 . (17)\nWe conclude by bounding each error term E1 and E2 separately: 0 \u2264 E1 \u2264 r2| sin(\u03b1)|2 \u222b B \u2016x\u20162dP (x) \u2264 r2| sin(\u03b1)|22\u2016\u03a3X\u2016 , (18)\nsince every point in B by definition has angle greater than \u03c0/2\u2212 \u03b1 from wm. Also, 0 \u2264 E2 \u2264 \u2016d\u20162 \u222b A(w1,w2) \u2016x\u20162dP (x) \u2264 1\u2212 cos(\u03b1) 2 2\u2016\u03a3X\u2016 (19)\nby direct application of Cauchy-Schwartz. The proof is completed by plugging the bounds from (18) and (19) into (17) .\nB.4 PROOF OF THEOREM 2.4\nConsider a generic \u03b1 and l \u2264 m. A path from \u03b8A to \u03b8B will be constructed by concatenating the following paths:\n1. from \u03b8A to \u03b8lA, the best linear predictor using the same first layer as \u03b8A,\n2. from \u03b8lA to \u03b8sA, the best (m\u2212 l)-term approximation using perturbed atoms from \u03b8A, 3. from \u03b8sA to \u03b8\u2217 the oracle l term approximation,\n4. from \u03b8\u2217 to \u03b8sB , the best (m\u2212 l)-term approximation using perturbed atoms from \u03b8B , 5. from \u03b8sB to \u03b8lB , the best linear predictor using the same first layer as \u03b8B ,\n6. from \u03b8lB to \u03b8B .\nThe proof will study the increase in the loss along each subpath and aggregate the resulting increase into a common bound.\nSubpaths (1) and (6) only involve changing the parameters of the second layer while leaving the firstlayer weights fixed, which define a convex loss. Therefore a linear path is sufficient to guarantee that the loss along that path will be upper bounded by \u03bb on the first end and \u03b4WA1 (m, 0,m) on the other end.\nConcerning subpaths (3) and (4), we notice that they can also be constructed using only parameters of the second layer, by observing that one can fit into a single n \u00d7 m parameter matrix both the (m \u2212 l)-term approximation and the oracle l-term approximation. Indeed, let us describe subpath (3) in detail ( subpath (4) is constructed analogously by replacing the role of \u03b8sA with \u03b8sB). Let W\u0303A the first-layer parameter matrix associated with the m \u2212 l-sparse solution \u03b8sA, and let \u03b3A denote its second layer coefficients, which is a m-dimensional vector with at most m \u2212 l non-zero coefficients. LetW\u2217 be the first-layer matrix of the l-term oracle approximation, and \u03b3\u2217 the corresponding second-layer coefficients. Since there are only m \u2212 l columns of W\u0303A that are used, corresponding to the support of \u03b3A, we can consider a path \u03b8\u0304 that replaces the remaining l columns with those from W\u2217 while keeping the second-layer vector \u03b3A fixed. Since the modified columns correspond to zeros in \u03b3A, such paths have constant loss. Call W\u0304 the resulting first-layer matrix, containing both the active m\u2212 l active columns of W\u0303A and the l columns of W\u2217 in the positions determined by the zeros of \u03b3A. Now we can consider the linear subpath that interpolates between \u03b3A and \u03b3\u2217 while keeping the first layer fixed at W\u0304 . Since again this is a linear subpath that only moves second-layer coefficients, it is non-increasing thanks to the convexity of the loss while fixing the first layer. We easily verify that at the end of this linear subpath we are using the oracle l-term approximation, which has loss e(l), and therefore subpath (3) incurs in a loss that is bounded by its extremal values \u03b4WA1 (m\u2212 l, \u03b1,m) and e(l).\nFinally, we need to show how to construct the subpaths (2) and (5), which are the most delicate step since they cannot be bounded using convexity arguments as above. Let W\u0303A be the resulting perturbed first-layer parameter matrix withm\u2212 l sparse coefficients \u03b3A. Let us consider an auxiliary regression of the form W = [WA; W\u0303A] \u2208 Rn\u00d72m . and regression parameters\n\u03b21 = [\u03b21; 0] , \u03b22 = [0; \u03b3A] .\nClearly E{|Y \u2212 \u03b21W |2}+ \u03ba\u2016\u03b21\u20161 = E{|Y \u2212 \u03b21WA|2}+ \u03ba\u2016\u03b21\u20161\nand similarly for \u03b22. By convexity, the augmented linear path \u03b7(t) = (1\u2212 t)\u03b21 + t\u03b22 thus satisfies\n\u2200 t , L(t) = E{|Y \u2212 \u03b7(t)W |2}+ \u03ba\u2016\u03b7(t)\u20161 \u2264 max(L(0), L(1)) .\nLet us now approximate this augmented linear path with a path in terms of first and second layer weights. We consider\n\u03b71(t) = (1\u2212 t)WA + tW\u0303A , and \u03b72(t) = (1\u2212 t)\u03b21 + t\u03b3A .\nWe have that\nFo({\u03b71(t), \u03b72(t)}) = E{|Y \u2212 \u03b72(t)Z(\u03b71(t))|2}+ \u03ba\u2016\u03b72(t)\u20161 (20) \u2264 E{|Y \u2212 \u03b72(t)Z(\u03b71(t))|2}+ \u03ba((1\u2212 t)\u2016\u03b21\u20161 + t\u2016\u03b3A\u20161) = L(t) + E{|Y \u2212 \u03b72(t)Z(\u03b71(t))|2} \u2212 E{|Y \u2212 (1\u2212 t)\u03b21Z(WA)\u2212 t\u03b3AZ(W\u0303A)|2} . (21)\nFinally, we verify that\u2223\u2223\u2223E{|Y \u2212 \u03b72(t)Z(\u03b71(t))|2} \u2212 E{|Y \u2212 (1\u2212 t)\u03b21Z(WA)\u2212 t\u03b3AZ(W\u0303A)|2}\u2223\u2223\u2223 \u2264 (22) \u2264 4\u03b1max(E|Y |2, \u221a E|Y 2|)\u2016\u03a3X\u2016(\u03ba\u22121/2 + \u03b1 \u221a E|Y 2|\u03ba\u22121) + o(\u03b12) .\nIndeed, from Proposition 2.3, and using the fact that \u2200 i \u2264M, t \u2208 [0, 1] , \u2223\u2223\u2220((1\u2212 t)wAi + tw\u0303Ai ;wAi )\u2223\u2223 \u2264 \u03b1 , \u2223\u2223\u2220((1\u2212 t)wAi + tw\u0303Ai ; w\u0303Ai )\u2223\u2223 \u2264 \u03b1\nwe can write (1\u2212 t)\u03b21,iz(wAi ) + t\u03b3A,iz(w\u0303Ai ) d = \u03b72(t)iz(\u03b71(t)i) + ni ,\nwith E{|ni|2} \u2264 4|\u03b72(t)i|2\u2016\u03a3X\u2016\u03b12 + O(\u03b14) and E|ni| \u2264 2|\u03b72(t)i|\u03b1 \u221a \u2016\u03a3X\u2016 using concavity of the moments. Thus\u2223\u2223\u2223E{|Y \u2212 \u03b72(t)Z(\u03b71(t))|2} \u2212 E{|Y \u2212 (1\u2212 t)\u03b21Z(WA)\u2212 t\u03b3AZ(W\u0303A)|2}\u2223\u2223\u2223 \u2264 2E\n{\u2211 i (Y \u2212 \u03b72(t)Z(\u03b71(t)))ni } + E { | \u2211 i ni|2 }\n\u2264 4 ( \u03b1 \u221a E|Y 2|\u2016\u03a3X\u2016\u2016\u03b72\u2016+ \u03b12(\u2016\u03b72\u20161)2\u2016\u03a3X\u2016 ) \u2264 4\u03b1max(1, \u221a E|Y 2|)\u2016\u03a3X\u2016(\u2016\u03b72\u20161 + \u03b1\u2016\u03b72\u201621) + o(\u03b12)\n\u2264 4\u03b1max( \u221a E|Y 2|,E|Y 2|)\u2016\u03a3X\u2016(\u03ba\u22121 + \u03b1 \u221a E|Y 2|\u03ba\u22122) + o(\u03b12) ,\nwhich proves (22).\nWe have just constructed a path from \u03b8A to \u03b8B , in which all subpaths except (2) and (5) have energy maximized at the extrema due to convexity, given respectively by \u03bb, \u03b4W 1A(m, 0,m), \u03b4W 1A(m \u2212 l, \u03b1,m), e(l), \u03b4W 1B (m \u2212 l, \u03b1,m), and \u03b4W 1B (m, 0,m). For the two subpaths (2) and (5), (22) shows that it is sufficient to add the corresponding upper bound to the linear subpath, which is of the form C\u03b1 + o(\u03b12) where C is an explicit constant independent of \u03b8. Since l and \u03b1 are arbitrary, we are free to pick the infimum, which concludes the proof.\nB.5 PROOF OF COROLLARY 2.5\nLet us consider a generic first layer weight matrix W \u2208 Rn\u00d7m. Without loss of generality, we can assume that \u2016wk\u2016 = 1 for all k, since increasing the norm of \u2016wk\u2016within the unit ball has no penalty in the loss, and we can compensate this scaling in the second layer thanks to the homogeneity of the half-rectification. Since this results in an attenuation of these second layer weights, they too are guaranteed not to increase the loss.\nFrom Vershynin (2010) [Lemma 5.2] we verify that the covering number N (Sn\u22121, ) of the Euclidean unit sphere Sn\u22121 satisfies\nN (Sn\u22121, ) \u2264 ( 1 + 2 )n ,\nwhich means that we can cover the unit sphere with an -net of size N (Sn\u22121, ).\nLet 0 < \u03b7 < n\u22121(1 + n\u22121)\u22121, and let us pick, for each m, m = m \u03b7\u22121 n . Let us consider its corresponding -net of size\num = N (Sn\u22121, m) ' ( 1 + 2\nm\n)n ' m1\u2212\u03b7 .\nSince we have m vectors in the unit sphere, it results from the pigeonhole principle that at least one element of the net will be associated with at least vm = mu\u22121m ' m\u03b7 vectors; in other words, we are guaranteed to find amongst our weight vector W a collection Qm of vm ' m\u03b7 vectors that are all at an angle at most 2 m apart. Let us now apply Theorem 2.4 by picking n = vm and \u03b1 = m. We need to see that the terms involved in the bound all converge to 0 as m\u2192\u221e. The contribution of the oracle error e(vm) \u2212 e(m) goes to zero as m \u2192 \u221e by the fact that limm\u2192\u221e e(m) exists (it is a decreasing, positive sequence) and that vm \u2192\u221e. Let us now verify that \u03b4(m \u2212 vm, m,m) also converges to zero. We are going to prune the first layer by removing one by one the vectors in Qm. Removing one of these vectors at a time incurs in an error of the order of m. Indeed, let wk be one of such vectors and let \u03b2\u2032 be the solution of\nmin \u03b2\u2032 E(\u03b2\u2032) = min \u03b2\u2032=(\u03b2f ;\u03b2k)\u2208Rk E{|Y \u2212 \u03b2Tf Z(W\u2212k)\u2212 \u03b2kz(wk)|2}+ \u03ba(\u2016\u03b2f\u20161 + |\u03b2k|) ,\nwhere W\u2212k is a shorthand for the matrix containing the rest of the vectors that have not been discarded yet. Removing the vector wk from the first layer increases the loss by a factor that is upper bounded by E(\u03b2p)\u2212 E(\u03b2), where\n(\u03b2p)j =\n{ \u03b2\u2032j for j < k \u2212 1 ,\n\u03b2\u2032k\u22121 + \u03b2 \u2032 k otherwise.\n,\nsince now \u03b2p is a feasible solution for the pruned first layer.\nLet us finally bound E(\u03b2p)\u2212 E(\u03b2). Since \u2220(wk, wk\u22121) \u2264 m, it results from Proposition 2.3 that\nz(wk) d = z(wk\u22121) + n ,\nwith E{|n|2} \u2264 C\u03b12 for some constantC independent ofm. By redefining p1 = Y \u2212\u03b2Tp Z(W\u2212k)\u2212 1 2n and p2 = 1 2n, we have\nE{|Y \u2212 \u03b2Tp Z(W\u2212k)|2} \u2212 E{|Y \u2212 \u03b2\u2032 T Z(W\u2212k)\u2212 \u03b2kz(wk)|2}\n= E{|p1 + p2|2} \u2212 E{|p1 \u2212 p2|2} = 4E{|p1p2|}\n\u2264 \u221a\u221a\u221a\u221aE{\u2223\u2223\u2223\u2223Y \u2212 \u03b2Tp Z(W\u2212k)\u2212 12n \u2223\u2223\u2223\u22232 }\u221a E{|n|2}\n\u2264 (C + \u03b1)\u03b1 ' m ,\nwhere C only depends on E{|Y |2}. We also verify that \u2016\u03b2p\u20161 \u2264 \u2016\u03b2\u2032\u20161. It results that removing |Qm| of such vectors incurs an increase of the loss at most |Qm| m ' m\u03b7m \u03b7\u22121 n = m\u03b7+\n\u03b7\u22121 n . Since we picked \u03b7 such that \u03b7 + \u03b7\u22121n < 0, this term converges to zero. The\nproof is finished."}, {"heading": "C CARTOON OF ALGORITHM", "text": "Refer to Fig. 2.\nD VISUALIZATION OF CONNECTION\nBecause the weight matrices are anywhere from high to extremely high dimensional, for the purposes of visualization we projected the models on the connecting path into a three dimensionsal subspace. Snapshots of the algorithm in progress for the quadratic regression task are indicated in Fig. 3. This was done by vectorizing all of the weight matrices for all the beads for a given connecting path, and then performing principal component analysis to find the three highest weight projections for the collection of models that define the endpoints of segments for a connecting path\u2014i.e., the\n\u03b8i discussed in the algorithm. We then projected the connecting string of models onto these three directions.\nThe color of the strings was chosen to be representative of the test loss under a log mapping, so that extremely high test loss mapped to red, whereas test loss near the threshold mapped to blue. An animation of the connecting path can be seen on our Github page.\nFinally, projections onto pairs of principal components are indicated by the black curves."}, {"heading": "E A DISCONNECTION", "text": "E.1 A DISCONNECTION\nAs a sanity check for the algorithm, we also applied it to a problem for which we know that it is not possible to connect models of equivalent power by the arguments of section 2.3.1. The input data is 3 points in R2, and the task is to permute the datapoints, i.e. map {x1, x2, x3} \u2192 {x2, x3, x1}. This map requires at least 12 parameters in general for the three linear maps which take xi \u2192 xj for i, j \u2208 {{1, 2}, {2, 3}, {3, 1}}. Our archticture was a 2-3-2 fully connected neural network with a single relu nonlinearity after the hidden layer\u2014a model which clearly has 12 free parameters by construction. The two models we tried to connect were a single model, \u03b8, and a copy of \u03b8 with the first two neurons in the hidden layer permuted, \u03b8\u0303\u03c3 . The algorithm fails to converge when initialized with these two models. We provide a visualization of the string of models produced by the algorithm in Fig. 4.\nIn general, a persistent high interpolated loss between two neighboring beads on the string of models could arise from either a slowly converging, connected pair of models or from a truly disconnected pair of models. \u201cProving\u201d a disconnection at the level of numerical experiments is intractable in general, but a collection of negative results\u2014i.e., failures to converge\u2014are highly suggestive of a true disconnection."}], "references": [{"title": "Convex relaxations of structured matrix factorizations", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1309.3117,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In Proc. AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Recovery of sparse translationinvariant signals with continuous basis pursuit", "author": ["Chaitanya Ekanadham", "Daniel Tranchina", "Eero P Simoncelli"], "venue": "IEEE transactions on signal processing,", "citeRegEx": "Ekanadham et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ekanadham et al\\.", "year": 2011}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Lecture 6a overview of mini\u2013batch gradient descent", "author": ["Geoffrey Hinton", "N Srivastava", "Kevin Swersky"], "venue": "Coursera Class,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "arXiv preprint arXiv:1605.07110,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["Itay Safran", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1511.04210,", "citeRegEx": "Safran and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Safran and Shamir.", "year": 2015}, {"title": "Explorations on high dimensional landscapes", "author": ["Levent Sagun", "V Ugur Guney", "Gerard Ben Arous", "Yann LeCun"], "venue": "arXiv preprint arXiv:1412.6615,", "citeRegEx": "Sagun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Ohad Shamir"], "venue": null, "citeRegEx": "Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Shamir.", "year": 2016}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Daniel Soudry", "Yair Carmon"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "Soudry and Carmon.,? \\Q2016\\E", "shortCiteRegEx": "Soudry and Carmon.", "year": 2016}, {"title": "Local minima in training of neural networks", "author": ["Grzegorz Swirszcz", "Wojciech Marian Czarnecki", "Razvan Pascanu"], "venue": "arXiv preprint arXiv:1611.06310,", "citeRegEx": "Swirszcz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Swirszcz et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton et al.", "startOffset": 188, "endOffset": 208}, {"referenceID": 2, "context": "The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton et al. (2012); Ioffe & Szegedy (2015); Kingma & Ba (2014).", "startOffset": 188, "endOffset": 230}, {"referenceID": 2, "context": "The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton et al. (2012); Ioffe & Szegedy (2015); Kingma & Ba (2014).", "startOffset": 188, "endOffset": 254}, {"referenceID": 2, "context": "The empirical performance of SGD on these models is better than one could expect in generic, arbitrary non-convex loss surfaces, often aided by modifications yielding significant speedups Duchi et al. (2011); Hinton et al. (2012); Ioffe & Szegedy (2015); Kingma & Ba (2014). This raises a number of theoretical questions as to why neural network optimization does not suffer in practice from poor local minima.", "startOffset": 188, "endOffset": 274}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model.", "startOffset": 78, "endOffset": 104}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al.", "startOffset": 78, "endOffset": 395}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al. (2016) but merely slowed down.", "startOffset": 78, "endOffset": 517}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al. (2016) but merely slowed down. Other notable recent contributions are Kawaguchi (2016), which further develops the spin-glass connection from Choromanska et al.", "startOffset": 78, "endOffset": 597}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al. (2016) but merely slowed down. Other notable recent contributions are Kawaguchi (2016), which further develops the spin-glass connection from Choromanska et al. (2015) and resolves the linear case by showing that no poor local minima exist; Sagun et al.", "startOffset": 78, "endOffset": 678}, {"referenceID": 1, "context": "Recent work has explored models from statistical physics such as spin glasses Choromanska et al. (2015), in order to understand the macroscopic properties of the system, but at the expense of strongly simplifying the nonlinear nature of the model. Other authors have advocated that the real danger in high-dimensional setups are saddle points rather than poor local minima Dauphin et al. (2014), although recent results rigorously establish that gradient descent does not get stuck on saddle points Lee et al. (2016) but merely slowed down. Other notable recent contributions are Kawaguchi (2016), which further develops the spin-glass connection from Choromanska et al. (2015) and resolves the linear case by showing that no poor local minima exist; Sagun et al. (2014) which also \u2217Currently on leave from UC Berkeley.", "startOffset": 78, "endOffset": 771}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work.", "startOffset": 247, "endOffset": 272}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives.", "startOffset": 247, "endOffset": 437}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives.", "startOffset": 247, "endOffset": 602}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives. Lastly, several papers submitted concurrently and independently of this one deserve note, particularly Swirszcz et al. (2016) which analyzes the explicit criteria under which sigmoid-based neural networks become trapped by poor local minima, as well as Tian (2017), which offers a complementary study of two layer ReLU based networks, and their learning dynamics.", "startOffset": 247, "endOffset": 808}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives. Lastly, several papers submitted concurrently and independently of this one deserve note, particularly Swirszcz et al. (2016) which analyzes the explicit criteria under which sigmoid-based neural networks become trapped by poor local minima, as well as Tian (2017), which offers a complementary study of two layer ReLU based networks, and their learning dynamics.", "startOffset": 247, "endOffset": 947}, {"referenceID": 6, "context": "discusses the impact of stochastic vs plain gradient, Soudry & Carmon (2016), that studies Empirical Risk Minimization for piecewise multilayer neural networks under overparametrization (which needs to grow with the amount of available data), and Goodfellow et al. (2014), which provided insightful intuitions on the loss surface of large deep learning models and partly motivated our work. Additionally, the work Safran & Shamir (2015) studies some topological properties of homogeneous nonlinear networks and shows how overparametrization acts upon these properties, and the pioneering Shamir (2016) studied the distribution-specific hardness of optimizing non-convex objectives. Lastly, several papers submitted concurrently and independently of this one deserve note, particularly Swirszcz et al. (2016) which analyzes the explicit criteria under which sigmoid-based neural networks become trapped by poor local minima, as well as Tian (2017), which offers a complementary study of two layer ReLU based networks, and their learning dynamics. In this work, we do not make any linearity assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. The loss surface F (\u03b8) of a given model can be expressed in terms of its level sets \u03a9\u03bb, which contain for each energy level \u03bb all parameters \u03b8 yielding a loss smaller or equal than \u03bb. A first question we address concerns the topology of these level sets, i.e. under which conditions they are connected. Connected level sets imply that one can always find a descent direction at each energy level, and therefore that no poor local minima can exist. In absence of nonlinearities, deep (linear) networks have connected level sets Kawaguchi (2016). We first generalize this result to include ridge regression (in the two layer case) and provide an alternative, more direct proof of the general case.", "startOffset": 247, "endOffset": 1761}, {"referenceID": 13, "context": "When \u03ba = 0, it has been shown in Saxe et al. (2013) and Kawaguchi (2016) that in this case, every local minima is a global minima.", "startOffset": 33, "endOffset": 52}, {"referenceID": 9, "context": "(2013) and Kawaguchi (2016) that in this case, every local minima is a global minima.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "Let us highlight that this result is slightly complementary than that of Kawaguchi (2016), Theorem 2.", "startOffset": 73, "endOffset": 90}, {"referenceID": 0, "context": "Incorporating regularization drastically changes the topology, and the fact that we are able to show connectedness only in the two-layer case with ridge regression is profound; we conjecture that extending it to deeper models requires a different regularization, perhaps using more general atomic norms Bach (2013). But we now move our interest to the nonlinear case, which is more relevant to our purposes.", "startOffset": 303, "endOffset": 315}, {"referenceID": 1, "context": "This difficulty is non-existent in the linear case and not easy to exploit in mean-field approaches such as Choromanska et al. (2015), and shows that in general we should not expect to obtain connected level sets.", "startOffset": 108, "endOffset": 134}, {"referenceID": 3, "context": "It is a form of n-width similar to Kolmogorov width Donoho (2006) and is also related to robust sparse coding from Tang et al.", "startOffset": 52, "endOffset": 66}, {"referenceID": 3, "context": "It is a form of n-width similar to Kolmogorov width Donoho (2006) and is also related to robust sparse coding from Tang et al. (2013); Ekanadham et al.", "startOffset": 52, "endOffset": 134}, {"referenceID": 3, "context": "It is a form of n-width similar to Kolmogorov width Donoho (2006) and is also related to robust sparse coding from Tang et al. (2013); Ekanadham et al. (2011).", "startOffset": 52, "endOffset": 159}, {"referenceID": 15, "context": "This is consistent with the overparametrization results from Safran & Shamir (2015); Shamir (2016) and the general common knowledge amongst deep learning practitioners.", "startOffset": 70, "endOffset": 84}, {"referenceID": 15, "context": "This is consistent with the overparametrization results from Safran & Shamir (2015); Shamir (2016) and the general common knowledge amongst deep learning practitioners.", "startOffset": 70, "endOffset": 99}], "year": 2017, "abstractText": "The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of highdimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important folklore facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_17"}