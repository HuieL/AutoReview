{"name": "ICLR_2017_189.pdf", "metadata": {"source": "CRF", "title": "Understanding Trainable Sparse Coding via matrix factorization", "authors": ["Thomas Moreau"], "emails": ["thomas.moreau@cmla.ens-cachan.fr", "joan.bruna@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Feature selection is a crucial point in high dimensional data analysis. Different techniques have been developed to tackle this problem efficiently, and amongst them sparsity has emerged as a leading paradigm. In statistics, the LASSO estimator (Tibshirani, 1996) provides a reliable way to select features and has been extensively studied in the last two decades (Hastie et al. (2015) and references therein). In machine learning and signal processing, sparse coding has made its way into several modern architectures, including large scale computer vision (Coates & Ng, 2011) and biologically inspired models (Cadieu & Olshausen, 2012). Also, Dictionary learning is a generic unsupervised learning method to perform nonlinear dimensionality reduction with efficient computational complexity (Mairal et al., 2009). All these techniques heavily rely on the resolution of `1-regularized least squares.\nThe `1-sparse coding problem is defined as solving, for a given input x \u2208 Rn and dictionary D \u2208 Rn\u00d7m, the following problem:\nz\u2217(x) = arg min z Fx(z) \u2206 =\n1 2 \u2016x\u2212Dz\u20162 + \u03bb\u2016z\u20161 . (1)\nThis problem is convex and can therefore be solved using convex optimization machinery. Proximal splitting methods (Beck & Teboulle, 2009) alternate between the minimization of the smooth and differentiable part using the gradient information and the minimization of the non-differentiable part using a proximal operator (Combettes & Bauschke, 2011). These methods can also be accelerated by considering a momentum term, as it is done in FISTA\n\u2217Work done while appointed at UC Berkeley, Statistics Department (currently on leave)\n(Beck & Teboulle, 2009; Nesterov, 2005). Coordinate descent (Friedman et al., 2007; Osher & Li, 2009) leverages the closed formula that can be derived for optimizing the problem (1) for one coordinate zi given that all the other are fixed. At each step of the algorithm, one coordinate is updated to its optimal value, which yields an inexpensive scheme to perform each step. The choice of the coordinate to update at each step is critical for the performance of the optimization procedure. Least Angle Regression (LARS) (Hesterberg et al., 2008) is another method that computes the whole LASSO regularization path. These algorithms all provide an optimization procedure that leverages the local properties of the cost function iteratively. They can be shown to be optimal among the class of first-order methods for generic convex, non-smooth functions (Bubeck, 2014).\nBut all these results are given in the worst case and do not use the distribution of the considered problem. One can thus wonder whether a more efficient algorithm to solve (1) exists for a fixed dictionary D and generic input x drawn from a certain input data distribution. In Gregor & Le Cun (2010), the authors introduced LISTA, a trained version of ISTA that adapts the parameters of the proximal splitting algorithm to approximate the solution of the LASSO using a finite number of steps. This method exploits the common structure of the problem to learn a better transform than the generic ISTA step. As ISTA is composed of a succession of linear operations and piecewise non linearities, the authors use the neural network framework and the backpropagation to derive an efficient procedure solving the LASSO problem. In Sprechmann et al. (2012), the authors extended LISTA to more generic sparse coding scenarios and showed that adaptive acceleration is possible under general input distributions and sparsity conditions.\nIn this paper, we are interested in the following question: Given a finite computational budget, what is the optimum estimator of the sparse coding? This question belongs to the general topic of computational tradeoffs in statistical inference. Randomized sketches (Alaoui & Mahoney, 2015; Yang et al., 2015) reduce the size of convex problems by projecting expensive kernel operators into random subspaces, and reveal a tradeoff between computational efficiency and statistical accuracy. Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost. More recently, Oymak et al. (2015) provides sharp time-data tradeoffs in the context of linear inverse problems, showing the existence of a phase transition between the number of measurements and the convergence rate of the resulting recovery optimization algorithm. Giryes et al. (2016) builds on this result to produce an analysis of LISTA that describes acceleration in conditions where the iterative procedure has linear convergence rate. Finally, Xin et al. (2016) also studies the capabilities of Deep Neural networks at approximating sparse inference. The authors show that unrolled iterations lead to better approximation if one allows the weights to vary at each layer, contrary to standard splitting algorithms. Whereas their focus is on relaxing the convergence hypothesis of iterative thresholding algorithms, we study a complementary question, namely when is speedup possible, without assuming strongly convex optimization. Their results are consistent with ours, since our analysis also shows that learning shared layer weights is less effective.\nInspired by the LISTA architecture, our mathematical analysis reveals that adaptive acceleration is related to a specific matrix factorization of the Gram matrix of the dictionary B = DTD as B = ATSA\u2212R ,where A is unitary, S is diagonal and the residual is positive semidefinite: R 0. Our factorization balances between near diagonalization by asking that \u2016R\u2016 is small and small perturbation of the `1 norm, i.e. \u2016Az\u20161 \u2212 \u2016z\u20161 is small. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys a convergence rate with improved constants with respect to the non-adaptive version. Moreover, our analysis also shows that acceleration is mostly possible at the beginning of the iterative process, when the current estimate is far from the optimal solution, which is consistent with numerical experiments. We also show that the existence of this factorization is not only sufficient for acceleration, but also necessary. This is shown by constructing dictionaries whose Gram matrix diagonalizes in a basis that is incoherent with the canonical basis, and verifying that LISTA fails in that case to accelerate with respect to ISTA.\nIn our numerical experiments, we design a specialized version of LISTA called FacNet, with more constrained parameters, which is then used as a tool to show that our theoretical analysis captures the acceleration mechanism of LISTA. Our theoretical results can be applied to FacNet and as LISTA is a generalization of this model, it always performs at least as well, showing that the existence of the factorization is a sufficient certificate for acceleration by\nLISTA. Reciprocally, we show that for cases where no acceleration is possible with FacNet, the LISTA model also fail to provide acceleration, linking the two speedup mechanisms. This numerical evidence suggest that the existence of our proposed factorization is sufficient and somewhat necessary for LISTA to show good results.\nThe rest of the paper is structured as follows. Section 2 presents our mathematical analysis and proves the convergence of the adaptive algorithm as a function of the quality of the matrix factorization. Finally, Section 3 presents the generic architectures that will enable the usage of such schemes and the numerical experiments, which validate our analysis over a range of different scenarios."}, {"heading": "2 Accelerating Sparse Coding with Sparse Matrix Factorizations", "text": ""}, {"heading": "2.1 Unitary Proximal Splitting", "text": "In this section we describe our setup for accelerating sparse coding based on the Proximal Splitting method. Let \u2126 \u2282 Rn be the set describing our input data, and D \u2208 Rn\u00d7m be a dictionary, with m > n. We wish to find fast and accurate approximations of the sparse coding z\u2217(x) of any x \u2208 \u2126, defined in (1) For simplicity, we denote B = DTD and y = D\u2020x to rewrite (1) as\nz\u2217(x) = arg min z Fx(z) =\n1 2 (y \u2212 z)TB(y \u2212 z)\ufe38 \ufe37\ufe37 \ufe38\nE(z)\n+\u03bb\u2016z\u20161\ufe38 \ufe37\ufe37 \ufe38 G(z) . (2)\nFor clarity, we will refer to Fx as F and to z \u2217(x) as z\u2217. The classic proximal splitting technique finds z\u2217 as the limit of sequence (zk)k, obtained by successively constructing a surrogate loss Fk(z) of the form\nFk(z) = E(zk) + (zk \u2212 y)TB(z \u2212 zk) + Lk\u2016z \u2212 zk\u201622 + \u03bb\u2016z\u20161 , (3) satisfying Fk(z) \u2265 F (z) for all z \u2208 Rm . Since Fk is separable in each coordinate of z, zk+1 = arg minz Fk(z) can be computed efficiently. This scheme is based on a majoration of the quadratic form (y \u2212 z)TB(y \u2212 z) with an isotropic quadratic form Lk\u2016zk \u2212 z\u201622. The convergence rate of the splitting algorithm is optimized by choosing Lk as the smallest constant satisfying Fk(z) \u2265 F (z), which corresponds to the largest singular value of B. The computation of zk+1 remains separable by replacing the quadratic form LkI by any diagonal form. However, the Gram matrix B = DTD might be poorly approximated via diagonal forms for general dictionaries. Our objective is to accelerate the convergence of this algorithm by finding appropriate factorizations of the matrix B such that\nB \u2248 ATSA , and \u2016Az\u20161 \u2248 \u2016z\u20161 , where A is unitary and S is diagonal positive definite. Given a point zk at iteration k, we can rewrite F (z) as\nF (z) = E(zk) + (zk \u2212 y)TB(z \u2212 zk) +QB(z, zk) , (4)\nwith QB(v, w) := 1\n2 (v \u2212 w)TB(v \u2212 w) + \u03bb\u2016v\u20161 . For any diago-\nnal positive definite matrix S and unitary matrix A, the surrogate loss F\u0303 (z, zk) := E(zk) + (zk \u2212 y)TB(z \u2212 zk) +QS(Az,Azk) can be explicitly minimized, since\narg min z F\u0303 (z, zk) = A T arg min u\n( (zk \u2212 y)TBAT(u\u2212Azk) +QS(u,Azk) ) = AT arg min\nu QS\n( u,Azk \u2212 S\u22121AB(zk \u2212 y) ) (5)\nwhere we use the variable change u = Az. As S is diagonal positive definite, (5) is separable and can be computed easily, using a linear operation followed by a point-wise non linear soft-thresholding. Thus, any couple (A,S) ensures an computationally cheap scheme. The question is then how to factorize B using S and A in an optimal manner, that is, such that the resulting proximal splitting sequence converges as fast as possible to the sparse coding solution."}, {"heading": "2.2 Non-asymptotic Analysis", "text": "We will now establish convergence results based on the previous factorization. These bounds will inform us on how to best choose the factors Ak and Sk in each iteration.\nFor that purpose, let us define \u03b4A(z) = \u03bb ( \u2016Az\u20161 \u2212 \u2016z\u20161 ) , and R = ATSA\u2212B . (6)\nThe quantity \u03b4A(z) thus measures how invariant the `1 norm is to the unitary operator A, whereas R corresponds to the residual of approximating the original Gram matrix B by our factorization ATSA . Given a current estimate zk, we can rewrite\nF\u0303 (z, zk) = F (z) + 1\n2 (z \u2212 zk)TR(z \u2212 zk) + \u03b4A(z) . (7)\nBy imposing that R is a positive semidefinite residual one immediately obtains the following bound.\nProposition 2.1. Suppose that R = ATSA\u2212B is positive definite, and define\nzk+1 = arg min z F\u0303 (z, zk) . (8)\nThen F (zk+1)\u2212 F (z\u2217) \u2264 1\n2 \u2016R\u2016\u2016zk \u2212 z\u2217\u201622+\u03b4A(z\u2217)\u2212 \u03b4A(zk+1) . (9)\nProof. By definition of zk+1 and using the fact that R 0 we have\nF (zk+1)\u2212 F (z\u2217) \u2264 F (zk+1)\u2212 F\u0303 (zk+1, zk) + F\u0303 (z\u2217, zk)\u2212 F (z\u2217)\n= \u22121 2 (zk+1 \u2212 zk)TR(zk+1 \u2212 zk)\u2212 \u03b4A(zk+1) + 1 2 (z\u2217 \u2212 zk)TR(z\u2217 \u2212 zk) + \u03b4A(z\u2217) \u2264 1 2 (z\u2217 \u2212 zk)TR(z\u2217 \u2212 zk) + ( \u03b4A(z \u2217)\u2212 \u03b4A(zk+1) ) .\nwhere the first line results from the definition of zk+1 and the third line makes use of R positiveness.\nThis simple bound reveals that to obtain fast approximations to the sparse coding it is sufficient to find S and A such that \u2016R\u2016 is small and that the `1 commutation term \u03b4A is small. These two conditions will be often in tension: one can always obtain R \u2261 0 by using the Singular Value Decomposition of B = AT0S0A0 and setting A = A0 and S = S0. However, the resulting A0 might introduce large commutation error \u03b4A0 . Similarly, as the\nabsolute value is non-expansive, i.e. \u2223\u2223\u2223|a| \u2212 |b|\u2223\u2223\u2223 \u2264 \u2223\u2223a\u2212 b\u2223\u2223, we have that\n|\u03b4A(z)| = \u03bb \u2223\u2223\u2223\u2016Az\u20161 \u2212 \u2016z\u20161\u2223\u2223\u2223 \u2264 \u03bb\u2016(A\u2212 I)z\u20161 (10)\n\u2264 \u03bb \u221a 2 max(\u2016Az\u20160, \u2016z\u20160) \u00b7 \u2016A\u2212 I\u2016 \u00b7 \u2016z\u20162 ,\nwhere we have used the Cauchy-Schwartz inequality \u2016x\u20161 \u2264 \u221a \u2016x\u20160\u2016x\u20162 in the last equation. In particular, (10) shows that unitary matrices in the neighborhood of I with \u2016A\u2212 I\u2016 small have small `1 commutation error \u03b4A but can be inappropriate to approximate general B matrix.\nThe commutation error also depends upon the sparsity of z and Az . If both z and Az are sparse then the commutation error is reduced, which can be achieved if A is itself a sparse unitary matrix. Moreover, since\n|\u03b4A(z)\u2212 \u03b4A(z\u2032)| \u2264 \u03bb|\u2016z\u20161 \u2212 \u2016z\u2032\u20161|+ \u03bb|\u2016Az\u20161 \u2212 \u2016Az\u2032\u20161|\nand |\u2016z\u20161 \u2212 \u2016z\u2032\u20161| \u2264 \u2016z \u2212 z\u2032\u20161 \u2264 \u221a \u2016z \u2212 z\u2032\u20160\u2016z \u2212 z\u2032\u20162\nit results that \u03b4A is Lipschitz with respect to the Euclidean norm; let us denote by LA(z) its local Lipschitz constant in z, which can be computed using the norm of the subgradient\nin z1. An uniform upper bound for this constant is (1+\u2016A\u20161)\u03bb \u221a m, but it is typically much smaller when z and Az are both sparse. Equation (8) defines an iterative procedure determined by the pairs {(Ak, Sk)}k. The following theorem uses the previous results to compute an upper bound of the resulting sparse coding estimator.\nTheorem 2.2. Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to iteration k, chosen such that Rk = A T kSkAk \u2212B 0. It results that\nF (zk)\u2212 F (z\u2217) \u2264 (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0) + 2LA0(z1)\u2016z\u2217 \u2212 z1\u20162 2k + \u03b1\u2212 \u03b2 2k , (11)\nwith \u03b1 = k\u22121\u2211 i=1 ( 2LAi(zi+1)\u2016z \u2217 \u2212 zi+1\u20162 + (z\u2217 \u2212 zi)T(Ri\u22121 \u2212Ri)(z\u2217 \u2212 zi) ) ,\n\u03b2 = k\u22121\u2211 i=0 (i+ 1) ( (zi+1 \u2212 zi)TRi(zi+1 \u2212 zi) + 2\u03b4Ai(zi+1)\u2212 2\u03b4Ai(zi) ) ,\nwhere LA(z) denote the local lipschitz constant of \u03b4A at z.\nRemarks: If one sets Ak = I and Sk = \u2016B\u2016I for all k \u2265 0, (11) corresponds to the bound of the ISTA algorithm (Beck & Teboulle, 2009).\nWe can specialize the theorem in the case when A0, S0 are chosen to minimize the bound (9) and Ak = I, Sk = \u2016B\u2016I for k \u2265 1. Corollary 2.3. If Ak = I, Sk = \u2016B\u2016I for k \u2265 1 then\nF (zk)\u2212F (z\u2217) \u2264 (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0) + 2LA0(z1)(\u2016z\u2217 \u2212 z1\u2016+ \u2016z1 \u2212 z0\u2016) + (z\u2217 \u2212 z1)TR0(z\u2217 \u2212 z1)T\n2k .\n(12)\nThis corollary shows that by simply replacing the first step of ISTA by the modified proximal step detailed in (5), one can obtain an improved bound at fixed k as soon as\n2\u2016R0\u2016max(\u2016z\u2217\u2212z0\u201622, \u2016z\u2217\u2212z1\u201622)+4LA0(z1) max(\u2016z\u2217\u2212z0\u20162, \u2016z\u2217\u2212z1\u20162) \u2264 \u2016B\u2016\u2016z\u2217\u2212z0\u201622 , which, assuming \u2016z\u2217 \u2212 z0\u20162 \u2265 \u2016z\u2217 \u2212 z1\u20162, translates into\n\u2016R0\u2016+ 2 LA0(z1) \u2016z\u2217 \u2212 z0\u20162 \u2264 \u2016B\u2016 2 . (13)\nMore generally, given a current estimate zk, searching for a factorization (Ak, Sk) will improve the upper bound when\n\u2016Rk\u2016+ 2 LAk(zk+1) \u2016z\u2217 \u2212 zk\u20162 \u2264 \u2016B\u2016 2 . (14)\nWe emphasize that this is not a guarantee of acceleration, since it is based on improving an upper bound. However, it provides a simple picture on the mechanism that makes non-asymptotic acceleration possible."}, {"heading": "2.3 Interpretation", "text": "In this section we analyze the consequences of Theorem 2.2 in the design of fast sparse coding approximations, and provide a possible explanation for the behavior observed numerically."}, {"heading": "2.3.1 \u2018Phase Transition\u201d and Law of Diminishing Returns", "text": "(14) reveals that the optimum matrix factorization in terms of minimizing the upper bound depends upon the current scale of the problem, that is, of the distance \u2016z\u2217 \u2212 zk\u2016. At the beginning of the optimization, when \u2016z\u2217 \u2212 zk\u2016 is large, the bound (14) makes it easier to explore the space of factorizations (A,S) with A further away from the identity. Indeed, the bound tolerates larger increases in LA(zk+1), which is dominated by\nLA(zk+1) \u2264 \u03bb( \u221a \u2016zk+1\u20160 + \u221a \u2016Azk+1\u20160) ,\n1 This quantity exists as \u03b4A is a difference of convex. See proof of ?? in appendices for precisions.\ni.e. the sparsity of both z1 and A0(z1). On the other hand, when we reach intermediate solutions zk such that \u2016z\u2217 \u2212 zk\u2016 is small with respect to LA(zk+1), the upper bound is minimized by choosing factorizations where A is closer and closer to the identity, leading to the non-adaptive regime of standard ISTA (A = Id).\nThis is consistent with the numerical experiments, which show that the gains provided by learned sparse coding methods are mostly concentrated in the first iterations. Once the estimates reach a certain energy level, section 3 shows that LISTA enters a steady state in which the convergence rate matches that of standard ISTA.\nThe natural follow-up question is to determine how many layers of adaptive splitting are sufficient before entering the steady regime of convergence. A conservative estimate of this quantity would require an upper bound of \u2016z\u2217\u2212 zk\u2016 from the energy bound F (zk)\u2212F (z\u2217). Since in general F is convex but not strongly convex, such bound does not exist unless one can assume that F is locally strongly convex (for instance for sufficiently small values of F )."}, {"heading": "2.3.2 Improving the factorization to particular input distributions", "text": "Given an input dataset D = (xi, z(0)i , z\u2217i )i\u2264N , containing examples xi \u2208 R n, initial estimates z (0) i and sparse coding solutions z \u2217 i , the factorization adapted to D is defined as\nmin A,S; ATA=I,ATSA\u2212B 0\n1\nN \u2211 i\u2264N 1 2 (z (0) i \u2212 z \u2217 i ) T(ATSA\u2212B)(z(0)i \u2212 z \u2217 i ) + \u03b4A(z \u2217 i )\u2212 \u03b4A(z1,i) . (15)\nTherefore, adapting the factorization to a particular dataset, as opposed to enforcing it uniformly over a given ball B(z\u2217;R) (where the radius R ensures that the initial value z0 \u2208 B(z\u2217;R)), will always improve the upper bound (9). Studying the gains resulting from the adaptation to the input distribution will be let for future work."}, {"heading": "3 Numerical Experiments", "text": "This section provides numerical arguments to analyse adaptive optimization algorithms and their performances, and relates them to the theoretical properties developed in the previous section. All the experiments were run using Python and Tensorflow. For all the experiments, the training is performed using Adagrad (Duchi et al., 2011). The code to reproduce the figures is available online2."}, {"heading": "3.1 Adaptive Optimization Networks Architectures", "text": "LISTA/LFISTA In Gregor & Le Cun (2010), the authors introduced LISTA, a neural network constructed by considering ISTA as a recurrent neural net. At each step, ISTA performs the following 2-step procedure :\n1. uk+1 = zk \u2212 1\nL DT(Dzk \u2212 x) = (I\u2212\n1\nL DTD)\ufe38 \ufe37\ufe37 \ufe38\nWg\nzk + 1\nL DT\ufe38 \ufe37\ufe37 \ufe38 We x ,\n2. zk+1 = h \u03bb L\n(uk+1) where h\u03b8(u) = sign(u)(|u| \u2212 \u03b8)+ ,  step k of ISTA (16) 2The code can be found at https://github.com/tomMoral/AdaptiveOptim\nThis procedure combines a linear operation to compute uk+1 with an element-wise non linearity. It can be summarized as a recurrent neural network, presented in Figure 1a., with tied weights. The autors in Gregor & Le Cun (2010) considered the architecture \u03a6K\u0398 with parameters \u0398 = (W (k) g ,W (k) e , \u03b8(k))k=1,...K obtained by unfolding K times the recurrent network, as presented in Figure 1b. The layers \u03c6k\u0398 are defined as\nzk+1 = \u03c6 k \u0398(zk) := h\u03b8(Wgzk +Wex) . (17)\nIf W (k) g = I \u2212 D TD L , W (k) e = DT L and \u03b8 (k) = \u03bbL are fixed for all the K layers, the output of this neural net is exactly the vector zK resulting from K steps of ISTA. With LISTA, the parameters \u0398 are learned using back propagation to minimize the cost function:\nf(\u0398) = Ex [ Fx(\u03a6 K \u0398 (x)) ] .\nA similar algorithm can be derived from FISTA, the accelerated version of ISTA to obtain LFISTA (see Figure 5 in Appendix A ). The architecture is very similar to LISTA, now with two memory tapes:\nzk+1 = h\u03b8(Wgzk +Wmzk\u22121 +Wex) .\nFactorization network Our analysis in Section 2 suggests a refactorization of LISTA in more a structured class of parameters. Following the same basic architecture, and using (5), the network FacNet, \u03a8K\u0398 is formed using layers such that:\nzk+1 = \u03c8 k \u0398(zk) := A Th\u03bbS\u22121(Azk \u2212 S\u22121A(DTDzk \u2212DTx)) , (18) with S diagonal and A unitary, the parameters of the k-th layer. The parameters obtained after training such a network with back-propagation can be used with the theory developed in Section 2. Up to the last linear operation AT of the network, this network is a re-parametrization of LISTA in a more constrained parameter space. Thus, LISTA is a generalization of this proposed network and should have performances at least as good as FacNet, for a fixed number of layers.\nThe optimization can also be performed using backpropagation. To enforce the unitary constraints on A(k), the cost function is modified with a penalty:\nf(\u0398) = Ex [ Fx(\u03a8 K \u0398 (x)) ] + \u00b5\nK K\u2211 k=1 \u2225\u2225\u2225\u2225\u2225I\u2212 (A(k))T A(k) \u2225\u2225\u2225\u2225\u2225 2\n2\n, (19)\nwith \u0398 = (A(k), S(k))k=1...K the parameters of the K layers and \u00b5 a scaling factor for the regularization. The resulting matrix A(k) is then projected on the Stiefel Manifold using a SVD to obtain final parameters, coherent with the network structure.\nLinear model Finally, it is important to distinguish the performance gain resulting from choosing a suitable starting point and the acceleration from our model. To highlights the gain obtain by changing the starting point, we considered a linear model with one layer such that zout = A\n(0)x. This model is learned using SGD with the convex cost function f(A(0)) = \u2016(I\u2212DA(0))x\u201622 + \u03bb\u2016A(0)x\u20161 . It computes a tradeoff between starting from the sparsest point 0 and a point with minimal reconstruction error y . Then, we observe the performance of the classical iteration of ISTA using zout as a stating point instead of 0 ."}, {"heading": "3.2 Synthetic problems with known distributions", "text": "Gaussian dictionary In order to disentangle the role of dictionary structure from the role of data distribution structure, the minimization problem is tested using a synthetic generative model with no structure in the weights distribution. First, m atoms di \u2208 Rn are drawn iid from a multivariate Gaussian with mean 0 and covariance In and the dictionary\nD is defined as ( di/\u2016di\u20162 ) i=1...m . The data points are generated from its sparse codes following a Bernoulli-Gaussian model. The coefficients z = (z1, . . . , zm) are constructed with zi = biai, where bi \u223c B(\u03c1) and ai \u223c N (0, \u03c3Im) , where \u03c1 controls the sparsity of the data. The values are set to m=100, n=64 for the dictionary dimension, \u03c1 = 5/m for the sparsity level and \u03c3=10 for the activation coefficient generation parameters. The sparsity\nregularization is set to \u03bb=0.01. The batches used for the training are generated with the model at each step and the cost function is evaluated over a fixed test set, not used in the training.\nFigure 2 displays the cost performance for methods ISTA/FISTA/Linear relatively to their iterations and for methods LISTA/LFISTA/FacNet relatively to the number of layers used to solve our generated problem. Linear has performances comparable to learned methods with the first iteration but a gap appears as the number of layers increases, until a point where it achieves the same performances as non adaptive methods. This highlights that the adaptation is possible in the subsequent layers of the networks, going farther than choosing a suitable starting point for iterative methods. The first layers permit to achieve a large gain over the classical optimization strategy, by leveraging the structure of the problem. This appears even with no structure in the sparsity patterns of input data, in accordance with the results in the previous section. We also observe diminishing returns as the number of layers increases. This results from the phase transition described in Subsubsection 2.3.1, as the last layers behave as ISTA steps and do not speed up the convergence. The 3 learned algorithms are always performing at least as well as their classical counterpart, as it was stated in Theorem 2.2. We also explored the effect of the sparsity level in the training and learning of adaptive networks. In the denser setting, the arbitrage between the `1-norm and the squared error is easier as the solution has a lot of non zero coefficients. Thus in this setting, the approximate method is more precise than in the very sparse setting where the approximation must perform a fine selection of the coefficients. But it also yield lower gain at the beggining as the sparser solution can move faster.\nThere is a small gap between LISTA and FacNet in this setup. This can be explained from the extra constraints on the weights that we impose in the FacNet, which effectively reduce the parameter space by half. Also, we implement the unitary constraints on the matrix A by a soft regularization (see (19)), involving an extra hyper-parameter \u00b5 that also contributes to the small performance gap. In any case, these experiments show that our analysis accounts for most of the acceleration provided by LISTA, as the performance of both methods are similar, up to optimization errors.\nAdversarial dictionary The results from Section 2 show that problems with a gram matrix composed of large eigenvalues associated to non sparse eigenvectors are harder to accelerate. Indeed, it is not possible in this case to find a quasi diagonalization of the matrix B that\ndoes not distort the `1 norm. It is possible to generate such a dictionary using Harmonic Analysis. The Discrete Fourier Transform (DFT) distorts a lot the `1 ball, since a very sparse vector in the temporal space is transformed in widely spread spectrum in the Fourier domain. We can thus design a dictionary for which LISTA and FacNet performances should\nbe degraded. D = ( di/\u2016di\u20162 ) i=1...m is constructed such that dj,k = e \u22122\u03c0ij\u03b6k , with ( \u03b6k ) k\u2264n\nrandomly selected from { 1/m, . . . ,m/2/m } without replacement.\nThe resulting performances are reported in Figure 3. The first layer provides a big gain by changing the starting point of the iterative methods. It realizes an arbitrage of the tradeoff between starting from 0 and starting from y . But the next layers do not yield any extra gain compared to the original ISTA algorithm. After 4 layers, the cost performance of both adaptive methods and ISTA are equivalent. It is clear that in this case, FacNet does not accelerate efficiently the sparse coding, in accordance with our result from Section 2. LISTA also displays poor performances in this setting. This provides further evidence that FacNet and LISTA share the same acceleration mechanism as adversarial dictionaries for FacNet are also adversarial for LISTA."}, {"heading": "3.3 Sparse coding with over complete dictionary on images", "text": "Wavelet encoding for natural images A highly structured dictionary composed of translation invariant Haar wavelets is used to encode 8x8 patches of images from the PASCAL VOC 2008 dataset. The network is used to learn an efficient sparse coder for natural images over this family. 500 images are sampled from dataset to train the encoder. Training batches are obtained by uniformly sampling patches from the training image set to feed the stochastic optimization of the network. The encoder is then tested with 10000 patches sampled from 100 new images from the same dataset.\nLearned dictionary for MNIST To evaluate the performance of LISTA for dictionary learning, LISTA was used to encode MNIST images over an unconstrained dictionary, learned a priori using classical dictionary learning techniques. The dictionary of 100 atoms was learned from 10000 MNIST images in grayscale rescaled to 17x17 using the implementation of Mairal et al. (2009) proposed in scikit-learn, with \u03bb = 0.05. Then, the networks were trained through backpropagation using all the 60000 images from the training set of MNIST. Finally, the perfornance of these encoders were evaluated with the 10000 images of the training set of MNIST.\nThe Figure 4 displays the cost performance of the adaptive procedures compared to nonadaptive algorithms. In both scenario, FacNet has performances comparable to the one of LISTA and their behavior are in accordance with the theory developed in Section 2. The gains become smaller for each added layer and the initial gain is achieved for dictionary either structured or unstructured. The MNIST case presents a much larger gain compare to the experiment with natural images. This results from the difference of structure of the input distribution, as the MNIST digits are much more constrained than patches from natural images and the network is able to leverage it to find a better encoder. In the MNIST case, a network composed of 12 layers is sufficient to achieve performance comparable to ISTA with more than 1000 iterations."}, {"heading": "4 Conclusions", "text": "In this paper we studied the problem of finite computational budget approximation of sparse coding. Inspired by the ability of neural networks to accelerate over splitting methods on the first few iterations, we have studied which properties of the dictionary matrix and the data distribution lead to such acceleration. Our analysis reveals that one can obtain acceleration by finding approximate matrix factorizations of the dictionary which nearly diagonalize its Gram matrix, but whose orthogonal transformations leave approximately invariant the `1 ball. By appropriately balancing these two conditions, we show that the resulting rotated proximal splitting scheme has an upper bound which improves over the ISTA upper bound under appropriate sparsity.\nIn order to relate this specific factorization property to the actual LISTA algorithm, we have introduced a reparametrization of the neural network that specifically computes the factorization, and incidentally provides reduced learning complexity (less parameters) from the original LISTA. Numerical experiments of Section 3 show that such reparametrization recovers the same gains as the original neural network, providing evidence that our theoretical analysis is partially explaining the behavior of the LISTA neural network. Our acceleration scheme is inherently transient, in the sense that once the iterates are sufficiently close to the optimum, the factorization is not effective anymore. This transient effect is also consistent with the performance observed numerically, although the possibility remains open to find alternative models that further exploit the particular structure of the sparse coding. Finally, we provide evidence that successful matrix factorization is not only sufficient but also necessary for acceleration, by showing that Fourier dictionaries are not accelerated.\nDespite these initial results, a lot remains to be understood on the general question of optimal tradeoffs between computational budget and statistical accuracy. Our analysis so far did not take into account any probabilistic consideration (e.g. obtain approximations that hold with high probability or in expectation). Another area of further study is the extension of our analysis to the FISTA case, and more generally to other inference tasks that are currently solved via iterative procedures compatible with neural network parametrizations, such as inference in Graphical Models using Belief Propagation or other ill-posed inverse problems."}, {"heading": "A Learned Fista", "text": "A similar algorithm can be derived from FISTA, the accelerated version of ISTA to obtain LFISTA (see Figure 5 ). The architecture is very similar to LISTA, now with two memory taps: It introduces a momentum term to improve the convergence rate of ISTA as follows:\n1. yk = zk + tk\u22121 \u2212 1\ntk (zk \u2212 zk\u22121) ,\n2. zk+1 = h \u03bb L\n( yk \u2212 1\nL \u2207E(yk)\n) = h \u03bb\nL\n( (I\u2212 1\nL B)yk +\n1 L DTx\n) ,\n3. tk+1 = 1 + \u221a 1 + 4t2k 2 .\nBy substituting the expression for yk into the first equation, we obtain a generic recurrent architecture very similar to LISTA, now with two memory taps, that we denote by LFISTA:\nzk+1 = h\u03b8(W (k) g zk +W (k) m zk\u22121 +W (k) e x) .\nThis model is equivalent to running K-steps of FISTA when its parameters are initialized with\nW (k)g =\n( 1 +\ntk\u22121 \u2212 1 tk\n)( I\u2212 1\nL B\n) ,\nW (k)m =\n( 1\u2212 tk\u22121\ntk\n)( I\u2212 1\nL B\n) ,\nW (k)e = 1\nL DT .\nThe parameters of this new architecture, presented in Figure 5 , are trained analogously as in the LISTA case."}, {"heading": "B Proofs", "text": "Lemma B.1. Suppose that R = ATSA\u2212B is positive definite, and define\nzk+1 = arg min z F\u0303 (z, zk) , and (20)\n\u03b4A(z) = \u2016Az\u20161 \u2212 \u2016z\u20161. Then we have\nF (zk+1)\u2212F (z\u2217) \u2264 1\n2\n( (z\u2217 \u2212 zk)TR(z\u2217 \u2212 zk)\u2212 (z\u2217 \u2212 zk+1)TR(z\u2217 \u2212 zk+1) ) +\u3008\u2202\u03b4A(zk+1), zk+1\u2212z\u2217\u3009 .\n(21)\nProof. We define\nf(t) = F ( tzk+1 + (1\u2212 t)z\u2217 ) , t \u2208 [0, 1] .\nSince F is convex, f is also convex in [0, 1]. Since f(0) = F (z\u2217) is the global minimum, it results that f \u2032(t) is increasing in (0, 1], and hence\nF (zk+1)\u2212 F (z\u2217) = f(1)\u2212 f(0) = \u222b f \u2032(t)dt \u2264 f \u2032(1) ,\nwhere f \u2032(1) is any element of \u2202f(1). Since \u03b4A(z) is a difference of convex functions, its subgradient can be defined as a limit of infimal convolutions Hiriart-Urruty (1991). We have \u2202f(1) = \u3008\u2202F (zk+1), zk+1 \u2212 z\u2217\u3009 , and since \u2202F (z) = \u2202F\u0303 (z, zk)\u2212R(z \u2212 zk)\u2212 \u2202\u03b4A(z) and 0 \u2208 \u2202F\u0303 (zk+1, zk) it results that \u2202F (zk+1) = \u2212R(zk+1 \u2212 zk)\u2212 \u2202\u03b4A(zk+1) , and thus\nF (zk+1)\u2212 F (z\u2217) \u2264 (z\u2217 \u2212 zk+1)TR(zk+1 \u2212 zk) + \u3008\u2202\u03b4A(zk+1), (z\u2217 \u2212 zk+1)\u3009 . (22) (21) is obtained by observing that\n(z\u2217 \u2212 zk+1)TR(zk+1 \u2212 zk) \u2264 1\n2\n( (z\u2217 \u2212 zk)TR(z\u2217 \u2212 zk)\u2212 (z\u2217 \u2212 zk+1)TR(z\u2217 \u2212 zk+1) ) , (23)\nthanks to the fact that R 0.\nTheorem B.2. Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to iteration k, chosen such that Rk = A T kSkAk \u2212B 0. It results that\nF (zk)\u2212 F (z\u2217) \u2264 (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0) + 2\u3008\u2207\u03b4A0(z1), (z\u2217 \u2212 z1)\u3009 2k + \u03b1\u2212 \u03b2 2k , with (24)\n\u03b1 = k\u22121\u2211 n=1 ( 2\u3008\u2207\u03b4An(zn+1), (z\u2217 \u2212 zn+1)\u3009+ (z\u2217 \u2212 zn)T(Rn\u22121 \u2212Rn)(z\u2217 \u2212 zn) ) ,\n\u03b2 = k\u22121\u2211 n=0 (n+ 1) ( (zn+1 \u2212 zn)TRn(zn+1 \u2212 zn) + 2\u03b4An(zn+1)\u2212 2\u03b4An(zn) ) .\nProof: The proof is adapted from (Beck & Teboulle, 2009), Theorem 3.1. From Lemma B.1, we start by using (21) to bound terms of the form F (zn)\u2212 F (z\u2217):\nF (zn)\u2212F (z\u2217) \u2264 \u3008\u2207\u03b4An(zn+1), (z \u2217\u2212zn+1)\u3009+\n1\n2\n( (z\u2217 \u2212 zn)TRn(z\u2217 \u2212 zn)\u2212 (z\u2217 \u2212 zn+1)TRn(z\u2217 \u2212 zn+1) ) .\nAdding these inequalities for n = 0 . . . k \u2212 1 we obtaink\u22121\u2211 n=0 F (zn) \u2212 kF (z\u2217) \u2264 k\u22121\u2211 n=0 \u3008\u2207\u03b4An(zn+1), (z\u2217 \u2212 zn+1)\u3009+ (25)\n+ 1\n2\n( (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0)\u2212 (z\u2217 \u2212 zk)TRk\u22121(z\u2217 \u2212 zk) ) +\n+ 1\n2 k\u22121\u2211 n=1 (z\u2217 \u2212 zn)T(Rn\u22121 \u2212Rn)(z\u2217 \u2212 zn) .\nOn the other hand, we also have\nF (zn)\u2212 F (zn+1) \u2265 F (zn)\u2212 F\u0303 (zn, zn) + F\u0303 (zn+1, zn)\u2212 F (zn+1)\n= \u2212\u03b4An(zn) + \u03b4An(zn+1) + 1\n2 (zn+1 \u2212 zn)TRn(zn+1 \u2212 zn) ,\nwhich results in k\u22121\u2211 n=0 (n+ 1)(F (zn)\u2212 F (zn+1)) \u2265 1 2 k\u22121\u2211 n=0 (n+ 1)(zn+1 \u2212 zn)TRn(zn+1 \u2212 zn) + (26)\n+ k\u22121\u2211 n=0 (n+ 1) ( \u03b4An(zn+1)\u2212 \u03b4An(zn) ) k\u22121\u2211 n=0 F (zn) \u2212 kF (zk) \u2265 k\u22121\u2211 n=0 (n+ 1) ( 1 2 (zn+1 \u2212 zn)TRn(zn+1 \u2212 zn) + \u03b4An(zn+1)\u2212 \u03b4An(zn) ) .\nCombining (25) and (26) we obtain\nF (zk)\u2212 F (z\u2217) \u2264 (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0) + 2\u3008\u2207\u03b4A0(z1), (z\u2217 \u2212 z1)\u3009 2k + \u03b1\u2212 \u03b2 2k (27)\nwith\n\u03b1 = k\u22121\u2211 n=1 ( 2\u3008\u2207\u03b4An(zn+1), (z\u2217 \u2212 zn+1)\u3009+ (z\u2217 \u2212 zn)T(Rn\u22121 \u2212Rn)(z\u2217 \u2212 zn) ) ,\n\u03b2 = k\u22121\u2211 n=0 (n+ 1) ( (zn+1 \u2212 zn)TRn(zn+1 \u2212 zn) + 2\u03b4An(zn+1)\u2212 2\u03b4An(zn) ) .\nCorollary B.3. If Ak = I, Sk = \u2016B\u2016I for k > 0 then F (zk)\u2212F (z\u2217) \u2264 (z\u2217 \u2212 z0)TR0(z\u2217 \u2212 z0) + 2LA0(z1)(\u2016z\u2217 \u2212 z1\u2016+ \u2016z1 \u2212 z0\u2016) + (z\u2217 \u2212 z1)TR0(z\u2217 \u2212 z1)T\n2k .\n(28)\nProof: We verify that in that case, Rn\u22121\u2212Rn \u2261 0 and for n > 1 and \u03b4An \u2261 0 for n > 0 ."}], "references": [{"title": "Computational Trade-offs in Statistical Learning", "author": ["Alekh Agarwal"], "venue": "PhD thesis, University of California, Berkeley,", "citeRegEx": "Agarwal.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal.", "year": 2012}, {"title": "Fast randomized kernel ridge regression with statistical guarantees", "author": ["Ahmed Alaoui", "Michael W Mahoney"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Alaoui and Mahoney.,? \\Q2015\\E", "shortCiteRegEx": "Alaoui and Mahoney.", "year": 2015}, {"title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Theory of convex optimization for machine learning", "author": ["S\u00e9bastien Bubeck"], "venue": null, "citeRegEx": "Bubeck.,? \\Q2014\\E", "shortCiteRegEx": "Bubeck.", "year": 2014}, {"title": "Learning intermediate-level representations of form and motion from natural movies", "author": ["Charles F Cadieu", "Bruno A Olshausen"], "venue": "Neural computation,", "citeRegEx": "Cadieu and Olshausen.,? \\Q2012\\E", "shortCiteRegEx": "Cadieu and Olshausen.", "year": 2012}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces, volume", "author": ["Patrick L Combettes", "Heinz H. Bauschke"], "venue": null, "citeRegEx": "Combettes and Bauschke.,? \\Q2011\\E", "shortCiteRegEx": "Combettes and Bauschke.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Pathwise coordinate optimization", "author": ["Jerome Friedman", "Trevor Hastie", "Holger H\u00f6fling", "Robert Tibshirani"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Tradeoffs between convergence speed and reconstruction accuracy in inverse problems", "author": ["Raja Giryes", "Yonina C Eldar", "Alex M Bronstein", "Guillermo Sapiro"], "venue": null, "citeRegEx": "Giryes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Giryes et al\\.", "year": 2016}, {"title": "Learning Fast Approximations of Sparse Coding", "author": ["Karol Gregor", "Yann Le Cun"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gregor and Cun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and Cun.", "year": 2010}, {"title": "Statistical Learning with Sparsity", "author": ["Trevor Hastie", "Robert Tibshirani", "Martin J. Wainwright"], "venue": "CRC Press,", "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Least angle and 1 penalized regression: A review", "author": ["Tim Hesterberg", "Nam Hee Choi", "Lukas Meier", "Chris Fraley"], "venue": "Statistics Surveys,", "citeRegEx": "Hesterberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hesterberg et al\\.", "year": 2008}, {"title": "How to regularize a difference of convex functions", "author": ["J.B. Hiriart-Urruty"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Hiriart.Urruty.,? \\Q1991\\E", "shortCiteRegEx": "Hiriart.Urruty.", "year": 1991}, {"title": "Online Learning for Matrix Factorization and Sparse Coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Coordinate descent optimization for l1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Stanley Osher", "Yingying Li"], "venue": "Inverse Problems and Imaging,", "citeRegEx": "Osher and Li.,? \\Q2009\\E", "shortCiteRegEx": "Osher and Li.", "year": 2009}, {"title": "Sharp time\u2013data tradeoffs for linear inverse problems", "author": ["Samet Oymak", "Benjamin Recht", "Mahdi Soltanolkotabi"], "venue": null, "citeRegEx": "Oymak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oymak et al\\.", "year": 2015}, {"title": "Learning Efficient Structured Sparse Models", "author": ["Pablo Sprechmann", "Alex Bronstein", "Guillermo Sapiro"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sprechmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2012}, {"title": "Regression Shrinkage and Selection via the Lasso. Journal of the royal statistical society", "author": ["Robert Tibshirani"], "venue": "Series B (methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Maximal sparsity with deep networks", "author": ["Bo Xin", "Yizhou Wang", "Wen Gao", "David Wipf"], "venue": null, "citeRegEx": "Xin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xin et al\\.", "year": 2016}, {"title": "Randomized sketches for kernels: Fast and optimal non-parametric regression", "author": ["Yun Yang", "Mert Pilanci", "Martin J Wainwright"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "In statistics, the LASSO estimator (Tibshirani, 1996) provides a reliable way to select features and has been extensively studied in the last two decades (Hastie et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 15, "context": "Also, Dictionary learning is a generic unsupervised learning method to perform nonlinear dimensionality reduction with efficient computational complexity (Mairal et al., 2009).", "startOffset": 154, "endOffset": 175}, {"referenceID": 12, "context": "In statistics, the LASSO estimator (Tibshirani, 1996) provides a reliable way to select features and has been extensively studied in the last two decades (Hastie et al. (2015) and references therein).", "startOffset": 155, "endOffset": 176}, {"referenceID": 9, "context": "Coordinate descent (Friedman et al., 2007; Osher & Li, 2009) leverages the closed formula that can be derived for optimizing the problem (1) for one coordinate zi given that all the other are fixed.", "startOffset": 19, "endOffset": 60}, {"referenceID": 13, "context": "Least Angle Regression (LARS) (Hesterberg et al., 2008) is another method that computes the whole LASSO regularization path.", "startOffset": 30, "endOffset": 55}, {"referenceID": 3, "context": "They can be shown to be optimal among the class of first-order methods for generic convex, non-smooth functions (Bubeck, 2014).", "startOffset": 112, "endOffset": 126}, {"referenceID": 22, "context": "Randomized sketches (Alaoui & Mahoney, 2015; Yang et al., 2015) reduce the size of convex problems by projecting expensive kernel operators into random subspaces, and reveal a tradeoff between computational efficiency and statistical accuracy.", "startOffset": 20, "endOffset": 63}, {"referenceID": 2, "context": "They can be shown to be optimal among the class of first-order methods for generic convex, non-smooth functions (Bubeck, 2014). But all these results are given in the worst case and do not use the distribution of the considered problem. One can thus wonder whether a more efficient algorithm to solve (1) exists for a fixed dictionary D and generic input x drawn from a certain input data distribution. In Gregor & Le Cun (2010), the authors introduced LISTA, a trained version of ISTA that adapts the parameters of the proximal splitting algorithm to approximate the solution of the LASSO using a finite number of steps.", "startOffset": 113, "endOffset": 429}, {"referenceID": 2, "context": "They can be shown to be optimal among the class of first-order methods for generic convex, non-smooth functions (Bubeck, 2014). But all these results are given in the worst case and do not use the distribution of the considered problem. One can thus wonder whether a more efficient algorithm to solve (1) exists for a fixed dictionary D and generic input x drawn from a certain input data distribution. In Gregor & Le Cun (2010), the authors introduced LISTA, a trained version of ISTA that adapts the parameters of the proximal splitting algorithm to approximate the solution of the LASSO using a finite number of steps. This method exploits the common structure of the problem to learn a better transform than the generic ISTA step. As ISTA is composed of a succession of linear operations and piecewise non linearities, the authors use the neural network framework and the backpropagation to derive an efficient procedure solving the LASSO problem. In Sprechmann et al. (2012), the authors extended LISTA to more generic sparse coding scenarios and showed that adaptive acceleration is possible under general input distributions and sparsity conditions.", "startOffset": 113, "endOffset": 980}, {"referenceID": 0, "context": "Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost.", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost.", "startOffset": 0, "endOffset": 151}, {"referenceID": 0, "context": "Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost. More recently, Oymak et al. (2015) provides sharp time-data tradeoffs in the context of linear inverse problems, showing the existence of a phase transition between the number of measurements and the convergence rate of the resulting recovery optimization algorithm.", "startOffset": 0, "endOffset": 304}, {"referenceID": 0, "context": "Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost. More recently, Oymak et al. (2015) provides sharp time-data tradeoffs in the context of linear inverse problems, showing the existence of a phase transition between the number of measurements and the convergence rate of the resulting recovery optimization algorithm. Giryes et al. (2016) builds on this result to produce an analysis of LISTA that describes acceleration in conditions where the iterative procedure has linear convergence rate.", "startOffset": 0, "endOffset": 557}, {"referenceID": 0, "context": "Agarwal (2012) provides several theoretical results on perfoming inference under various computational constraints, and Chandrasekaran & Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoffs between accuracy and computational cost. More recently, Oymak et al. (2015) provides sharp time-data tradeoffs in the context of linear inverse problems, showing the existence of a phase transition between the number of measurements and the convergence rate of the resulting recovery optimization algorithm. Giryes et al. (2016) builds on this result to produce an analysis of LISTA that describes acceleration in conditions where the iterative procedure has linear convergence rate. Finally, Xin et al. (2016) also studies the capabilities of Deep Neural networks at approximating sparse inference.", "startOffset": 0, "endOffset": 739}, {"referenceID": 8, "context": "For all the experiments, the training is performed using Adagrad (Duchi et al., 2011).", "startOffset": 65, "endOffset": 85}, {"referenceID": 15, "context": "The dictionary of 100 atoms was learned from 10000 MNIST images in grayscale rescaled to 17x17 using the implementation of Mairal et al. (2009) proposed in scikit-learn, with \u03bb = 0.", "startOffset": 123, "endOffset": 144}], "year": 2017, "abstractText": "Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods are optimal in the class of first-order methods for non-smooth, convex functions. However, they do not exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks, coined LISTA, was proposed in Gregor & Le Cun (2010), which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the `1 ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_189"}