{"name": "ICLR_2017_192.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shiyu Liang"], "emails": ["sliang26@illinois.edu", "rsrikant@illinois.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural networks have drawn significant interest from the machine learning community, especially due to their recent empirical successes (see the surveys (Bengio, 2009)). Neural networks are used to build state-of-art systems in various applications such as image recognition, speech recognition, natural language process and others (see, Krizhevsky et al. 2012; Goodfellow et al. 2013; Wan et al. 2013, for example). The result that neural networks are universal approximators is one of the theoretical results most frequently cited to justify the use of neural networks in these applications. Numerous results have shown the universal approximation property of neural networks in approximations of different function classes, (see, e.g., Cybenko 1989; Hornik et al. 1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015).\nAll these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau & Bengio (2011) have shown that there exist deep sum-product networks which cannot be approximated by shallow sum-product networks unless they use an exponentially larger amount of units or neurons. Montufar et al. (2014) have shown that the number of linear region increases exponentially with the number of layers in the neural network. Telgarsky (2016) has established such a result for neural networks, which is the subject of this paper. Eldan & Shamir (2015) have shown that, to approximate a specific function, a two-layer network requires an exponential number of neurons in the input dimension, while a three-layer network requires a polynomial number of neurons. These recent papers demonstrate the power of deep networks by showing that depth can lead to an exponential reduction in the number of neurons required, for specific functions or specific neural networks. Our goal here is different: we are interested in function approximation specifically\nand would like to show that for a given upper bound on the approximation error, shallow networks require exponentially more neurons than deep networks for a large class of functions.\nThe multilayer neural networks considered in this paper are allowed to use either rectifier linear units (ReLU) or binary step units (BSU), or any combination of the two. The main contributions of this paper are\n\u2022 We have shown that, for \u03b5-approximation of functions with enough piecewise smoothness, a multilayer neural network which uses \u0398(log(1/\u03b5)) layers only needs O(poly log(1/\u03b5)) neurons, while \u2126(poly(1/\u03b5)) neurons are required by neural networks with o(log(1/\u03b5)) layers. In other words, shallow networks require exponentially more neurons than a deep network to achieve the level of accuracy for function approximation.\n\u2022 We have shown that for all differentiable and strongly convex functions, multilayer neural networks need \u2126(log(1/\u03b5)) neurons to achieve an \u03b5-approximation. Thus, our results for deep networks are tight.\nThe outline of this paper is as follows. In Section 2, we present necessary definitions and the problem statement. In Section 3, we present upper bounds on network size, while the lower bound is provided in Section 4. Conclusions are presented in Section 5. Around the same time that our paper was uploaded in arxiv, a similar paper was also uploaded in arXiv by Yarotsky (2016). The results in the two papers are similar in spirit, but the details and the general approach are substantially different."}, {"heading": "2 PRELIMINARIES AND PROBLEM STATEMENT", "text": "In this section, we present definitions on feedforward neural networks and formally present the problem statement."}, {"heading": "2.1 FEEDFORWARD NEURAL NETWORKS", "text": "A feedforward neural network is composed of layers of computational units and defines a unique function f\u0303 : Rd \u2192 R. Let L denote the number of hidden layers, Nl denote the number of units of layer l, N = \u2211L l=1 Nl denote the size of the neural network, vector x = (x\n(1), ..., x(d)) denote the input of neural network, zlj denote the output of the jth unit in layer l, w l i,j denote the weight of the edge connecting unit i in layer l and unit j in layer l + 1, blj denote the bias of the unit j in layer l. Then outputs between layers of the feedforward neural network can be characterized by following iterations:\nzl+1j = \u03c3 ( Nl\u2211 i=1 wli,jz l i + b l+1 j ) , l \u2208 [L\u2212 1], j \u2208 [Nl+1],\nwith\ninput layer: z1j = \u03c3\n( d\u2211\ni=1\nw0i,jx (i) + b1j ) , j \u2208 [N1],\noutput layer: f\u0303(x) = \u03c3 ( NL\u2211 i=1 wLi,jz L i + b L+1 j ) .\nHere, \u03c3(\u00b7) denotes the activation function and [n] denotes the index set [n] = {1, ..., n}. In this paper, we only consider two important types of activation functions:\n\u2022 Rectifier linear unit: \u03c3(x) = max{0, x}, x \u2208 R. \u2022 Binary step unit: \u03c3(x) = I{x \u2265 0}, x \u2208 R.\nWe call the number of layers and the number of neurons in the network as the depth and the size of the feedforward neural network, respectively. We use the set F(N,L) to denote the function set containing all feedforward neural networks of depth L, size N and composed of a combination\nof rectifier linear units (ReLUs) and binary step units. We say one feedforward neural network is deeper than the other network if and only if it has a larger depth. Through this paper, the terms feedforward neural network and multilayer neural network are used interchangeably."}, {"heading": "2.2 PROBLEM STATEMENT", "text": "In this paper, we focus on bounds on the size of the feedforward neural network function approximation. Given a function f , our goal is to understand whether a multilayer neural network f\u0303 of depth L and size N exists such that it solves\nmin f\u0303\u2208F(N,L)\n\u2225f \u2212 f\u0303\u2225 \u2264 \u03b5. (1)\nSpecifically, we aim to answer the following questions:\n1 Does there exists L(\u03b5) and N(\u03b5) such that (1) is satisfied? We will refer to such L(\u03b5) and N(\u03b5) as upper bounds on the depth and size of the required neural network.\n2 Given a fixed depth L, what is the minimum value of N such that (1) is satisfied? We will refer to such an N as a lower bound on the size of a neural network of a given depth L.\nThe first question asks what depth and size are sufficient to guarantee an \u03b5-approximation. The second question asks, for a fixed depth, what is the minimum size of a neural network required to guarantee an \u03b5-approximation. Obviously, tight bounds in the answers to these two questions provide tight bounds on the network size and depth required for function approximation. Besides, solutions to these two questions together can be further used to answer the following question. If a deeper neural network of size Nd and a shallower neural network of size Ns are used to approximate the same function with the same error, then how fast does the ratio Nd/Ns decay to zero as the error decays to zero?"}, {"heading": "3 UPPER BOUNDS ON FUNCTION APPROXIMATIONS", "text": "In this section, we present upper bounds on the size of the multilayer neural network which are sufficient for function approximation. Before stating the results, some notations and terminology deserve further explanation. First, the upper bound on the network size represents the number of neurons required at most for approximating a given function with a certain error. Secondly, the notion of the approximation is the L\u221e distance: for two functions f and g, the L\u221e distance between these two function is the maximum point-wise disagreement over the cube [0, 1]d."}, {"heading": "3.1 APPROXIMATION OF UNIVARIATE FUNCTIONS", "text": "In this subsection, we present all results on approximating univariate functions. We first present a theorem on the size of the network for approximating a simple quadratic function. As part of the proof, we present the structure of the multilayer feedforward neural network used and show how the neural network parameters are chosen. Results on approximating general functions can be found in Theorem 2 and 4.\nTheorem 1. For function f(x) = x2, x \u2208 [0, 1], there exists a multilayer neural network f\u0303(x) with O ( log 1\u03b5 ) layers, O ( log 1\u03b5 ) binary step units and O ( log 1\u03b5 ) rectifier linear units such that |f(x)\u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1].\nProof. The proof is composed of three parts. For any x \u2208 [0, 1], we first use the multilayer neural network to approximate x by its finite binary expansion \u2211n i=0 xi 2i . We then construct a 2-layer neural\nnetwork to implement function f (\u2211n\ni=0 xi 2i\n) .\nFor each x \u2208 [0, 1], x can be denoted by its binary expansion x = \u2211\u221e\ni=0 xi 2i , where xi \u2208 {0, 1} for\nall i \u2265 0. It is straightforward to see that the n-layer neural network shown in Figure 1 can be used to find x0, ..., xn.\nNext, we implement the function f\u0303(x) = f (\u2211n\ni=0 xi 2i\n) by a two-layer neural network. Since\nf(x) = x2, we then rewrite f\u0303(x) as follows:\nf\u0303(x) =\n( n\u2211\ni=0\nxi 2i\n)2 =\nn\u2211 i=0 xi \u00b7  1 2i n\u2211 j=0 xj 2j  = n\u2211 i=0 max 0, 2(xi \u2212 1) + 1 2i n\u2211 j=0 xj 2j  . The third equality follows from the fact that xi \u2208 {0, 1} for all i. Therefore, the function f\u0303(x) can be implemented by a multilayer network containing a deep structure shown in Figure 1 and another hidden layer with n rectifier linear units. This multilayer neural network has O(n) layers, O(n) binary step units and O(n) rectifier linear units. Finally, we consider the approximation error of this multilayer neural network,\n|f(x)\u2212 f\u0303(x)| = \u2223\u2223\u2223\u2223\u2223\u2223x2 \u2212 ( n\u2211 i=0 xi 2i )2\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2 \u2223\u2223\u2223\u2223\u2223x\u2212 n\u2211 i=0 xi 2i \u2223\u2223\u2223\u2223\u2223 = 2 \u2223\u2223\u2223\u2223\u2223 \u221e\u2211 i=n+1 xi 2i \u2223\u2223\u2223\u2223\u2223 \u2264 12n\u22121 . Therefore, in order to achieve \u03b5-approximation error, one should choose n = \u2308 log2 1 \u03b5 \u2309 + 1. In\nsummary, the deep neural network has O ( log 1\u03b5 ) layers, O ( log 1\u03b5 ) binary step units and O ( log ( 1 \u03b5 )) rectifier linear units.\nNext, a theorem on the size of the network for approximating general polynomials is given as follows. Theorem 2. For polynomials f(x) = \u2211p i=0 aix i, x \u2208 [0, 1] and \u2211p\ni=1 |ai| \u2264 1, there exists a multilayer neural network f\u0303(x) with O ( p+ log p\u03b5 ) layers, O ( log p\u03b5 ) binary step units and O ( p log p\u03b5 ) rectifier linear units such that |f(x)\u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1].\nProof. The proof is composed of three parts. We first use the deep structure shown in Figure 1 to find the n-bit binary expansion \u2211n i=0 aix\ni of x. Then we construct a multilayer network to approximate polynomials gi(x) = xi, i = 1, ..., p. Finally, we analyze the approximation error.\nUsing the same deep structure shown in Figure 1, we could find the binary expansion sequence {x0, ..., xn}. In this step, we used n binary steps units in total. Now we rewrite gm+1( \u2211n i=0 xi 2n ),\ngm+1\n( n\u2211\ni=0\nxi 2i\n) =\nn\u2211 j=0\n[ xj \u00b7 1\n2j gm\n( n\u2211\ni=0\nxi 2i\n)] =\nn\u2211 j=0 max\n[ 2(xj \u2212 1) + 1\n2j gm\n( n\u2211\ni=0\nxi 2i\n) , 0 ] .\n(2) Clearly, the equation (2) defines iterations between the outputs of neighbor layers. Therefore, the deep neural network shown in Figure 2 can be used to implement the iteration given by (2). Further, to implement this network, one should use O(p) layers with O(pn) rectifier linear units in total. We now define the output of the multilayer neural network as f\u0303(x) = \u2211p i=0 aigi (\u2211n j=0 xj 2j ) . For this multilayer network, the approximation error is\n|f(x)\u2212 f\u0303(x)| = \u2223\u2223\u2223\u2223\u2223\u2223 p\u2211\ni=0\naigi  n\u2211 j=0 xj 2j \u2212 p\u2211 i=0 aix i \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 p\u2211 i=0 |ai| \u00b7 \u2223\u2223\u2223\u2223\u2223\u2223gi  n\u2211 j=0 xj 2j \u2212 xi \u2223\u2223\u2223\u2223\u2223\u2223  \u2264 p 2n\u22121\nThis indicates, to achieve \u03b5-approximation error, one should choose n = \u2308 log p\u03b5 \u2309 + 1. Besides, since we used O(n + p) layers with O(n) binary step units and O(pn) rectifier linear units in total, this multilayer neural network thus has O ( p+ log p\u03b5 ) layers, O ( log p\u03b5 ) binary step units and\nO ( p log p\u03b5 ) rectifier linear units.\nIn Theorem 2, we have shown an upper bound on the size of multilayer neural network for approximating polynomials. We can easily observe that the number of neurons in network grows as p log p with respect to p, the degree of the polynomial. We note that both Andoni et al. (2014) and Barron (1993) showed the sizes of the networks grow exponentially with respect to p if only 3-layer neural networks are allowed to be used in approximating polynomials.\nBesides, every function f with p + 1 continuous derivatives on a bounded set can be approximated easily with a polynomial with degree p. This is shown by the following well known result of Lagrangian interpolation. By this result, we could further generalize Theorem 2. The proof can be found in the reference (Gil et al., 2007). Lemma 3 (Lagrangian interpolation at Chebyshev points). If a function f is defined at points z0, ..., zn, zi = cos((i+ 1/2)\u03c0/(n+ 1)), i \u2208 [n], there exists a polynomial of degree not more than n such that Pn(zi) = f(zi), i = 0, ..., n. This polynomial is given by Pn(x) = \u2211n i=0 f(zi)Li(x) where Li(x) = \u03c0n+1(x)\n(x\u2212zi)\u03c0\u2032n+1(zi) and \u03c0n+1(x) =\n\u220fn j=0(x \u2212 zj). Additionally, if f is continuous on\n[\u22121, 1] and n+ 1 times differentiable in (\u22121, 1), then\n\u2225Rn\u2225 = \u2225f \u2212 Pn\u2225 \u2264 1\n2n(n+ 1)! \u2225\u2225\u2225f (n+1)\u2225\u2225\u2225 , where f (n)(x) is the derivative of f of the nth order and the norm \u2225f\u2225 is the l\u221e norm \u2225f\u2225 = maxx\u2208[\u22121,1] f(x).\nThen the upper bound on the network size for approximating more general functions follows directly from Theorem 2 and Lemma 3. Theorem 4. Assume that function f is continuous on [0, 1] and \u2308 log 2\u03b5 \u2309 + 1 times differentiable in\n(0, 1). Let f (n) denote the derivative of f of nth order and \u2225f\u2225 = maxx\u2208[0,1] f(x). If \u2225\u2225f (n)\u2225\u2225 \u2264 n!\nholds for all n \u2208 [\u2308 log 2\u03b5 \u2309 + 1 ] , then there exists a deep neural network f\u0303 with O ( log 1\u03b5 ) layers,\nO ( log 1\u03b5 ) binary step units, O (( log 1\u03b5 )2) rectifier linear units such that \u2225\u2225\u2225f \u2212 f\u0303\u2225\u2225\u2225 \u2264 \u03b5. Proof. Let N = \u2308 log 2\u03b5 \u2309 . From Lemma 3, it follows that there exists polynomial PN of degree N such that for any x \u2208 [0, 1],\n|f(x)\u2212 PN (x)| \u2264 \u2225\u2225f (N+1)\u2225\u2225 2N (N + 1)! \u2264 1 2N .\nLet x0, ..., xN denote the first N + 1 bits of the binary expansion of x and define f\u0303(x) = PN (\u2211N i=0 xi 2N ) . In the following, we first analyze the approximation error of f\u0303 and next\nshow the implementation of this function. Let x\u0303 = \u2211N\ni=0 xi 2i . The error can now be upper bounded\nby\n|f(x)\u2212 f\u0303(x)| = |f(x)\u2212 PN (x\u0303)| \u2264 |f(x)\u2212 f (x\u0303)|+ |f (x\u0303)\u2212 PN (x\u0303)| \u2264 \u2225\u2225\u2225f (1)\u2225\u2225\u2225 \u00b7 \u2223\u2223\u2223\u2223\u2223x\u2212 N\u2211 i=0 xi 2i \u2223\u2223\u2223\u2223\u2223+ 12N \u2264 12N + 12N \u2264 \u03b5 In the following, we describe the implementation of f\u0303 by a multilayer neural network. Since PN is a polynomial of degree N , function f\u0303 can be rewritten as\nf\u0303(x) = PN ( N\u2211 i=0 xi 2i ) = N\u2211 n=0 cngn ( N\u2211 i=0 xi 2i ) for some coefficients c0, ..., cN and gn = xn, n \u2208 [N ]. Hence, the multilayer neural network shown in the Figure 2 can be used to implement f\u0303(x). Notice that the network uses O(N) layers with O(N) binary step units in total to decode x0,...,xN and O(N) layers with O(N2) rectifier linear units in total to construct the polynomial PN . Substituting N = \u2308 log 2\u03b5 \u2309 , we have proved the theorem.\nRemark: Note that, to implement the architecture in Figure 2 using the definition of a feedforward neural network in Section 2, we need the gi, i \u2208 [p] at the output. This can be accomplished by using O(p2) additional ReLUs. Since p = O(log(1/\u03b5)), this doesn\u2019t change the order result in Theorem 4.\nTheorem 4 shows that any function f with enough smoothness can be approximated by a multilayer neural network containing polylog ( 1 \u03b5 ) neurons with \u03b5 error. Further, Theorem 4 can be used to show that for functions h1,...,hk with enough smoothness, then linear combinations, multiplications and compositions of these functions can as well be approximated by multilayer neural networks containing polylog ( 1 \u03b5 ) neurons with \u03b5 error. Specific results are given in the following corollaries. Corollary 5 (Function addition). Suppose that all functions h1, ..., hk satisfy the conditions in Theorem 4, and the vector \u03b2 \u2208 {\u03c9 \u2208 Rk : \u2225\u03c9\u22251 = 1}, then for the linear combination f = \u2211k i=1 \u03b2ihi, there exists a deep neural network f\u0303 with O ( log 1\u03b5 ) layers, O ( log 1\u03b5 ) binary\nstep units, O (( log 1\u03b5 )2) rectifier linear units such that |f(x)\u2212 f\u0303 | \u2264 \u03b5, \u2200x \u2208 [0, 1].\nRemark: Clearly, Corollary 5 follows directly from the fact that the linear combination f satisfies the conditions in Theorem 4 if all the functions h1,...,hk satisfy those conditions. We note here that the upper bound on the network size for approximating linear combinations is independent of k, the number of component functions. Corollary 6 (Function multiplication). Suppose that all functions h1,...,hk are continuous on [0, 1] and \u2308 4k log2 4k + 4k + 2 log2 2 \u03b5 \u2309 + 1 times differentiable in (0, 1). If \u2225h(n)i \u2225 \u2264 n! holds for all\ni \u2208 [k] and n \u2208 [\u2308 4k log2 4k + 4k + 2 log2 2 \u03b5 \u2309 + 1 ] then for the multiplication f = \u220fk\ni=1 hi, there exists a multilayer neural network f\u0303 with O ( k log k + log 1\u03b5 ) layers, O ( k log k + log 1\u03b5 ) bi-\nnary step units and O ( (k log k)2 + ( log 1\u03b5 )2) rectifier linear units such that |f(x) \u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1]. Corollary 7 (Function composition). Suppose that all functions h1, ..., hk : [0, 1] \u2192 [0, 1] satisfy the conditions in Theorem 4, then for the composition f = h1 \u25e6 h2 \u25e6 ... \u25e6 hk, there exists a multilayer neural network f\u0303 with O ( k log k log 1\u03b5 + log k ( log 1\u03b5 )2) layers,\nO ( k log k log 1\u03b5 + log k ( log 1\u03b5 )2) binary step units and O ( k2 ( log 1\u03b5 )2 + ( log 1\u03b5 )4) rectifier lin-\near units such that |f(x)\u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1].\nRemark: Proofs of Corollary 6 and 7 can be found in the appendix. We observe that different from the case of linear combinations, the upper bound on the network size grows as k2 log2 k in the case of function multiplications and grows as k2 ( log 1\u03b5 )2 in the case of function compositions where k is the number of component functions.\nIn this subsection, we have shown a polylog ( 1 \u03b5 ) upper bound on the network size for \u03b5approximation of both univariate polynomials and general univariate functions with enough smoothness. Besides, we have shown that linear combinations, multiplications and compositions of univariate functions with enough smoothness can as well be approximated with \u03b5 error by a multilayer neural network of size polylog ( 1 \u03b5 ) . In the next subsection, we will show the upper bound on the network size for approximating multivariate functions."}, {"heading": "3.2 APPROXIMATION OF MULTIVARIATE FUNCTIONS", "text": "In this subsection, we present all results on approximating multivariate functions. We first present a theorem on the upper bound on the neural network size for approximating a product of multivariate linear functions. We next present a theorem on the upper bound on the neural network size for approximating general multivariate polynomial functions. Finally, similar to the results in the univariate case, we present the upper bound on the neural network size for approximating the linear combination, the multiplication and the composition of multivariate functions with enough smoothness. Theorem 8. Let W = {w \u2208 Rd : \u2225w\u22251 = 1}. For f(x) = \u220fp i=1 ( wTi x ) , x \u2208 [0, 1]d and\nwi \u2208 W , i = 1, ..., p, there exists a deep neural network f\u0303(x) with O ( p+ log pd\u03b5 ) layers and\nO ( log pd\u03b5 ) binary step units and O ( pd log pd\u03b5 ) rectifier linear units such that |f(x)\u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1]d.\nTheorem 8 shows an upper bound on the network size for \u03b5-approximation of a product of multivariate linear functions. Furthermore, since any general multivariate polynomial can be viewed as a linear combination of products, the result on general multivariate polynomials directly follows from Theorem 8. Theorem 9. Let the multi-index vector \u03b1 = (\u03b11, ..., \u03b1d), the norm |\u03b1| = \u03b11+...+\u03b1d, the coefficient"}, {"heading": "C\u03b1 = C\u03b11...\u03b1d , the input vector x = (x", "text": "(1), ..., x(d)) and the multinomial x\u03b1 = x(1) \u03b11 ...x(d)\n\u03b1d . For positive integer p and polynomial f(x) = \u2211 \u03b1:|\u03b1|\u2264p C\u03b1x \u03b1, x \u2208 [0, 1]d and \u2211 \u03b1:|\u03b1|\u2264p |C\u03b1| \u2264 1,\nthere exists a deep neural network f\u0303(x) of depth O ( p+ log dp\u03b5 ) and size N(d, p, \u03b5) such that |f(x)\u2212 f(x\u0303)| \u2264 \u03b5, where\nN(d, p, \u03b5) = p2 ( p+ d\u2212 1 d\u2212 1 ) log pd \u03b5 .\nRemark: The proof is given in the appendix. By further analyzing the results on the network size, we obtain the following results: (a) fixing degree p, N(d, \u03b5) = O ( dp+1 log d\u03b5 ) as d \u2192 \u221e and\n(b) fixing input dimension d, N(p, \u03b5) = O ( pd log p\u03b5 ) as p \u2192 \u221e. Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size dp/\u03b52. Andoni et al. (2014) showed that one could use the gradient descent to train a 3-layer neural network of size d2p/\u03b52 to approximate any multivariate polynomial. However, Theorem 9 shows that the deep neural network could reduce the network size from O (1/\u03b5) to O ( log 1\u03b5 ) for the same \u03b5 error. Besides, for a fixed input dimension d, the size of the 3-layer neural network used by Andoni et al. (2014) and Barron (1993) grows exponentially with respect to the degree p. However, the size of the deep neural network shown in Theorem 9 grows only polynomially with respect to the degree. Therefore, the deep neural network could reduce the network size from O(exp(p)) to O(poly(p)) when the degree p becomes large. Theorem 9 shows an upper bound on the network size for approximating multivariate polynomials. Further, by combining Theorem 4 and Corollary 7, we could obtain an upper bound on the network size for approximating more general functions. The results are shown in the following corollary. Corollary 10. Assume that all univariate functions h1, ..., hk : [0, 1] \u2192 [0, 1], k \u2265 1, satisfy the conditions in Theorem 4. Assume that the multivariate polynomial l(x) : [0, 1]d \u2192 [0, 1] is of degree p. For composition f = h1 \u25e6 h2 \u25e6 ... \u25e6 hk \u25e6 l(x), there exists a multilayer neural network f\u0303 of depth O ( p+ log d+ k log k log 1\u03b5 + log k ( log 1\u03b5 )2) and of size N(k, p, d, \u03b5) such that |f\u0303(x)\u2212f(x)| \u2264 \u03b5 for \u2200x \u2208 [0, 1]d, where\nN(k, p, d, \u03b5) = O ( p2 ( p+ d\u2212 1 d\u2212 1 ) log pd \u03b5 + k2 ( log 1 \u03b5 )2 + ( log 1 \u03b5 )4) .\nRemark: Corollary 10 shows an upper bound on network size for approximating compositions of multivariate polynomials and general univariate functions. The upper bound can be loose due to the assumption that l(x) is a general multivariate polynomials of degree p. For some specific cases, the upper bound can be much smaller. We present two specific examples in the Appendix H and I.\nIn this subsection, we have shown that a similar polylog ( 1 \u03b5 ) upper bound on the network size for \u03b5-approximation of general multivariate polynomials and functions which are compositions of univariate functions and multivariate polynomials.\nThe results in this section can be used to find a multilayer neural network of size polylog ( 1 \u03b5 ) which provides an approximation error of at most \u03b5. In the next section, we will present lower bounds on the network size for approximating both univariate and multivariate functions. The lower bound together with the upper bound shows a tight bound on the network size required for function approximations.\nWhile we have presented results in both the univariate and multivariate cases for smooth functions, the results automatically extend to functions that are piecewise smooth, with a finite number of pieces. In other words, if the domain of the function is partitioned into regions, and the function is sufficiently smooth (in the sense described in the Theorems and Corollaries earlier) in each of the regions, then the results essentially remain unchanged except for an additional factor which will depend on the number of regions in the domain."}, {"heading": "4 LOWER BOUNDS ON FUNCTION APPROXIMATIONS", "text": "In this section, we present lower bounds on the network size in function for certain classes of functions. Next, by combining the lower bounds and the upper bounds shown in the previous section, we could analytically show the advantages of deeper neural networks over shallower ones. The theorem below is inspired by a similar result (DasGupta & Schnitger, 1993) for univariate quadratic functions, where it is stated without a proof. Here we show that the result extends to general multivariate strongly convex functions. Theorem 11. Assume function f : [0, 1]d \u2192 R is differentiable and strongly convex with parameter \u00b5. Assume the multilayer neural network f\u0303 is composed of rectifier linear units and binary step units. If |f(x)\u2212 f\u0303(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1]d, then the network size N \u2265 log2 ( \u00b5 16\u03b5 ) .\nRemark: The proof is in the Appendix F. Theorem 11 shows that every strongly convex function cannot be approximated with error \u03b5 by any multilayer neural network with rectifier linear units and binary step units and of size smaller than log2(\u00b5/\u03b5) \u2212 4. Theorem 11 together with Theorem 1 directly shows that to approximate quadratic function f(x) = x2 with error \u03b5, the network size should be of order \u0398 ( log 1\u03b5 ) . Further, by combining Theorem 11 and Theorem 4, we could analytically show the benefits of deeper neural networks. The result is given in the following corollary. Corollary 12. Assume that univariate function f satisfies conditions in both Theorem 4 and Theorem 11. If a neural network f\u0303s is of depth Ls = o ( log 1\u03b5 ) , size Ns and |f(x) \u2212 f\u0303s(x)| \u2264 \u03b5,\nfor \u2200x \u2208 [0, 1], then there exists a deeper neural network f\u0303d(x) of depth \u0398 ( log 1\u03b5 ) , size Nd = O(L2s log 2 Ns) such that |f(x)\u2212 f\u0303d(x)| \u2264 \u03b5, \u2200x \u2208 [0, 1].\nRemarks: (i) The strong convexity requirement can be relaxed: the result obviously holds if the function is strongly concave and it also holds if the function consists of pieces which are strongly convex or strongly concave. (ii) Corollary 12 shows that in the approximation of the same function, the size of the deep neural network Ns is only of polynomially logarithmic order of the size of the shallow neural network Nd, i.e., Nd = O(polylog(Ns)). Similar results can be obtained for multivariate functions on the type considered in Section 3.2."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we have shown that an exponentially large number of neurons are needed for function approximation using shallow networks, when compared to deep networks. The results are established for a large class of smooth univariate and multivariate functions. Our results are established for the case of feedforward neural networks with ReLUs and binary step units."}, {"heading": "ACKNOWLEDGMENTS", "text": "The research reported here was supported by NSF Grants CIF 14-09106, ECCS 16-09370, and ARO Grant W911NF-16-1-0259."}, {"heading": "APPENDIX A PROOF OF COROLLARY 5", "text": "Proof. By Theorem 4, for each hi, i = 1, ..., k, there exists a multilayer neural network h\u0303i such that |hi(x)\u2212 h\u0303(x)| \u2264 \u03b5 for any x \u2208 [0, 1]. Let\nf\u0303(x) = k\u2211 i=1 \u03b2ih\u0303i(x).\nThen the approximation error is upper bounded by\n|f(x)\u2212 f\u0303(x)| = \u2223\u2223\u2223\u2223\u2223 k\u2211\ni=1\n\u03b2ihi(x) \u2223\u2223\u2223\u2223\u2223 \u2264 k\u2211\ni=1\n|\u03b2i| \u00b7 |hi(x)\u2212 h\u0303(x)| = \u03b5.\nNow we compute the size of the multilayer neural network f\u0303 . Let N = \u2308 log 2\u03b5 \u2309 and \u2211N i=0 xi 2i be the binary expansion of x. Since h\u0303i(x) has a form of\nh\u0303i(x) = N\u2211 j=0 cijgj ( N\u2211 i=0 xi 2i ) ,\nwhere gj(x) = xj , then f\u0303 should has a form of\nf\u0303(x) = k\u2211 i=1 \u03b2i  N\u2211 j=0 cijgj ( N\u2211 i=0 xi 2i ) and can be further rewritten as\nf\u0303(x) = N\u2211 j=0\n[( k\u2211\ni=1\ncij\u03b2i ) \u00b7 gj ( N\u2211 i=0 xi 2i )] \u225c N\u2211 j=0 c\u2032jgj ( N\u2211 i=0 xi 2i ) ,\nwhere c\u2032j = \u2211 i cij\u03b2i. Therefore, f\u0303 can be implemented by a multilayer neural network shown in\nFigure 2 and this network has at most O ( log 1\u03b5 ) layers, O ( log 1\u03b5 ) binary step units, O (( log 1\u03b5 )2) rectifier linear units."}, {"heading": "APPENDIX B PROOF OF COROLLARY 6", "text": "Proof. Since f(x) = h1(x)h2(x)...hk(x), then the derivative of f of order n is\nf (n) = \u2211\n\u03b11+...+\u03b1k=n \u03b11\u22650,...,\u03b1k\u22650\nn!\n\u03b11!\u03b12!...\u03b1k! h (\u03b11) 1 h (\u03b12) 2 ...h (\u03b1k) k .\nBy the assumption that \u2225\u2225\u2225h(\u03b1i)i \u2225\u2225\u2225 \u2264 \u03b1i! holds for i = 1, ..., k, then we have\u2225\u2225\u2225f (n)\u2225\u2225\u2225 \u2264 \u2211\n\u03b11+...+\u03b1k=n \u03b11\u22650,...,\u03b1k\u22650\nn!\n\u03b11!\u03b12!...\u03b1k!\n\u2225\u2225\u2225h(\u03b11)1 h(\u03b12)2 ...h(\u03b1k)k \u2225\u2225\u2225 \u2264 (n+ k \u2212 1k \u2212 1 ) n!.\nThen from Theorem 4, it follows that there exists a polynomial of PN degree N that \u2225RN\u2225 = \u2225f \u2212 PN\u2225 \u2264 \u2225\u2225f (N+1)\u2225\u2225 (N + 1)!2N \u2264 1 2N ( N + k k \u2212 1 ) .\nSince( N + k k \u2212 1 ) \u2264 (N + k) N+k (k \u2212 1)k\u22121(N + 1)N+1 = ( N + k k \u2212 1 )k\u22121( 1 + k \u2212 1 N + 1 )N+1 \u2264 ( e(N + k) k \u2212 1 )k\u22121\nthen the error has an upper bound of\n\u2225RN\u2225 \u2264 (eN)k\n2N \u2264 22k+k log2 N\u2212N . (3)\nSince we need to bound \u2225RN\u2225 \u2264 \u03b5\n2 ,\nthen we need to choose N such that\nN \u2265 k log2 N + 2k + log2 2\n\u03b5 .\nThus, N can be chosen such that\nN \u2265 2k log2 N and N \u2265 4k + 2 log2 2\n\u03b5 .\nFurther, function l(x) = x/ log2 x is monotonically increasing on [e,\u221e) and\nl(4k log2 4k) = 4k log2 4k log2 4k + log2 log2 4k \u2265 4k log2 4k log2 4k + log2 4k = 2k.\nTherefore, to suffice the inequality (3), one should should choose\nN \u2265 4k log2 4k + 4k + 2 log2 2\n\u03b5 . Since N = \u2308 4k log2 4k + 4k + 2 log2 2 \u03b5 \u2309 by assumptions, then there exists a polynomial PN of degree N such that \u2225f \u2212 PN\u2225 \u2264 \u03b5\n2 . Let \u2211N\ni=0 xi 2i denote the binary expansion of x and let\nf\u0303(x) = PN ( N\u2211 i=0 xi 2i ) .\nThe approximation error is\n|f\u0303(x)\u2212 f(x)| \u2264 \u2223\u2223\u2223\u2223\u2223f(x)\u2212 f ( N\u2211 i=0 xi 2i )\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223f ( N\u2211 i=0 xi 2i ) \u2212 PN ( N\u2211 i=0 xi 2i )\u2223\u2223\u2223\u2223\u2223 \u2264 \u2225f(1)\u2225\n\u2223\u2223\u2223\u2223\u2223x\u2212 N\u2211 i=0 xi 2i \u2223\u2223\u2223\u2223\u2223+ \u03b52 \u2264 \u03b5 Further, function f\u0303 can be implemented by a multilayer neural network shown in Figure 2 and this network has at most O(N) layers, O(N) binary step units and O(N2) rectifier linear units."}, {"heading": "APPENDIX C PROOF OF COROLLARY 7", "text": "Proof. We prove this theorem by induction. Define function Fm = h1 \u25e6 ... \u25e6 hm, m = 1, ..., k. Let T1(m) log3 3m \u03b5 , T2(m) log3 3m \u03b5 and T3(m) ( log3 3m \u03b5 )2 denote the number of layers, the number of binary step units and the number of rectifier linear units required at most for \u03b5-approximation of Fm, respectively. By Theorem 4, for m = 1, there exists a multilayer neural network F\u03031 with at most T1(1) log3 3 \u03b5 layers, T2(1) log3 3 \u03b5 binary step units and T3(1) ( log3 3 \u03b5 )2 rectifier linear units such that |F1(x)\u2212 F\u03031(x)| \u2264 \u03b5, for x \u2208 [0, 1]. Now we consider the cases for 2 \u2264 m \u2264 k. We assume for Fm\u22121, there exists a multilayer neural network F\u0303m\u22121 with not more than T1(m\u2212 1) log3 3 m \u03b5 layers, T2(m\u2212 1) log3 3m \u03b5 binary step units\nand T3(m\u2212 1) ( log3 3m\n\u03b5\n)2 rectifier linear units such that\n|Fm\u22121(x)\u2212 F\u0303m\u22121(x)| \u2264 \u03b5\n3 , for x \u2208 [0, 1].\nFurther we assume the derivative of Fm\u22121 has an upper bound \u2225\u2225F \u2032m\u22121\u2225\u2225 \u2264 1. Then for Fm, since Fm(x) can be rewritten as Fm(x) = Fm\u22121(hm(x)), and there exists a multilayer neural network h\u0303m with at most T1(1) log3 3 \u03b5 layers, T2(1) log3 3 \u03b5 binary\nstep units and T3(1) ( log3 3 \u03b5 )2 rectifier linear units such that\n|hm(x)\u2212 h\u0303m(x)| \u2264 \u03b5\n3 , for x \u2208 [0, 1], and \u2225\u2225\u2225h\u0303m\u2225\u2225\u2225 \u2264 (1 + \u03b5/3). Then for cascaded multilayer neural network F\u0303m = F\u0303m\u22121 \u25e6 ( 11+\u03b5/3 h\u0303m), we have\n\u2225 Fm \u2212 F\u0303m \u2225 = \u2225\u2225\u2225\u2225\u2225Fm\u22121(hm)\u2212 F\u0303m\u22121 ( h\u0303m 1 + \u03b5/3 )\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225Fm\u22121(hm)\u2212 Fm\u22121 ( h\u0303m 1 + \u03b5/3 )\u2225\u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u2225Fm\u22121 ( h\u0303m 1 + \u03b5/3 ) \u2212 F\u0303m\u22121 ( h\u0303m 1 + \u03b5/3\n)\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225F \u2032m\u22121\u2225\u2225 \u00b7 \u2225\u2225\u2225\u2225\u2225hm \u2212 h\u0303m1 + \u03b5/3 \u2225\u2225\u2225\u2225\u2225+ \u03b53\n\u2264 \u2225\u2225F \u2032m\u22121\u2225\u2225 \u00b7 \u2225\u2225\u2225hm \u2212 h\u0303m\u2225\u2225\u2225+ \u2225\u2225F \u2032m\u22121\u2225\u2225 \u00b7 \u2225\u2225\u2225\u2225 \u03b5/31 + \u03b5/3 h\u0303m \u2225\u2225\u2225\u2225+ \u03b53 \u2264 \u03b5\n3 +\n\u03b5 3 + \u03b5 3 = \u03b5\nIn addition, the derivative of Fm can be upper bounded by \u2225F \u2032m\u2225 \u2264 \u2225\u2225F \u2032m\u22121\u2225\u2225 \u00b7 \u2225h\u2032m\u2225 = 1. Since the multilayer neural network F\u0303m is constructed by cascading multilayer neural networks F\u0303m\u22121 and h\u0303m, then the iterations for T1, T2 and T3 are\nT1(m) log3 3m\n\u03b5 =T1(m\u2212 1) log3\n3m\n\u03b5 + T1(1) log3\n3 \u03b5 , (4)\nT2(m) log3 3m\n\u03b5 =T2(m\u2212 1) log3\n3m\n\u03b5 + T2(1) log3\n3 \u03b5 , (5)\nT3(m) ( log3 3m\n\u03b5\n)2 =T3(m\u2212 1) ( log3 3m\n\u03b5\n)2 + T3(1) ( log3 3\n\u03b5\n)2 . (6)\nFrom iterations (4) and (5), we could have for 2 \u2264 m \u2264 k,\nT1(m) = T1(m\u2212 1) + T1(1) 1 + log3(1/\u03b5)\nm+ log3(1/\u03b5) \u2264 T1(m\u2212 1) + T1(1)\n1 + log3(1/\u03b5)\nm\nT2(m) = T2(m\u2212 1) + T2(1) 1 + log3(1/\u03b5)\nm+ log3(1/\u03b5) \u2264 T2(m\u2212 1) + T2(1)\n1 + log3(1/\u03b5)\nm\nand thus\nT1(k) = O ( log k log 1\n\u03b5\n) , T2(k) = O ( log k log 1\n\u03b5\n) .\nFrom the iteration (6), we have for 2 \u2264 m \u2264 k, T3(m) = T3(m\u2212 1) + T3(1) ( 1 + log3(1/\u03b5)\nm+ log3(1/\u03b5)\n)2 \u2264 T3(m\u2212 1) + (1 + log3(1/\u03b5)) 3\nm2 ,\nand thus\nT3(k) = O (( log 1\n\u03b5\n)2) .\nTherefore, to approximate f = Fk, we need at most O ( k log k log 1\u03b5 + log k ( log 1\u03b5 )2) layers,\nO ( k log k log 1\u03b5 + log k ( log 1\u03b5 )2) binary step units and O ( k2 ( log 1\u03b5 )2 + ( log 1\u03b5 )4) rectifier linear units."}, {"heading": "APPENDIX D PROOF OF THEOREM 8", "text": "Proof. The proof is composed of two parts. As before, we first use the deep structure shown in Figure 1 to find the binary expansion of x and next use a multilayer neural network to approximate the polynomial.\nLet x = (x(1), ..., x(d)) and wi = (wi1, ..., wid). We could now use the deep structure shown in Figure 1 to find the binary expansion for each x(k), k \u2208 [d]. Let x\u0303(k) = \u2211n\nr=0 x(k)r 2r denote the binary\nexpansion of x(k), where x(k)r is the rth bit in the binary expansion of x(k). Obviously, to decode all the n-bit binary expansions of all x(k), k \u2208 [d], we need a multilayer neural network with n layers and dn binary units in total. Besides, we let x\u0303 = (x\u0303(1), ..., x\u0303(d)). Now we define\nf\u0303(x) = f(x\u0303) = p\u220f i=1\n( d\u2211\nk=1\nwikx\u0303 (k) ) .\nWe further define\ngl(x\u0303) = l\u220f i=1\n( d\u2211\nk=1\nwikx\u0303 (k) ) .\nSince for l = 1, ..., p\u2212 1,\ngl(x\u0303) = l\u220f i=1\n( d\u2211\nk=1\nwikx\u0303 (k) ) \u2264\nl\u220f i=1 \u2225wi\u22251 = 1,\nthen we can rewrite gl+1(x\u0303), l = 1, ..., p\u2212 1 into\ngl+1(x\u0303) = l+1\u220f i=1\n( d\u2211\nk=1\nwikx\u0303 (k) ) =\nd\u2211 k=1 [ w(l+1)kx\u0303 (k) \u00b7 gl(x\u0303) ] = d\u2211 k=1 { w(l+1)k n\u2211 r=0 [ x(k)r \u00b7 gl(x\u0303) 2r ]}\n= d\u2211 k=1\n{ w(l+1)k\nn\u2211 r=0 max [ 2(x(k)r \u2212 1) + gl(x\u0303) 2r , 0 ]} (7)\nObviously, equation (7) defines a relationship between the outputs of neighbor layers and thus can be used to implement the multilayer neural network. In this implementation, we need dn rectifier linear units in each layer and thus dnp rectifier linear units. Therefore, to implement function f\u0303(x), we need p+ n layers, dn binary step units and dnp rectifier linear units in total.\nIn the rest of proof, we consider the approximation error. Since for k = 1, ..., d and \u2200x \u2208 [0, 1]d,\u2223\u2223\u2223\u2223\u2202f(x)\u2202x(k) \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 p\u2211\nj=1 wjk \u00b7 p\u220f i=1,i\u0338=j ( wTi x )\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 p\u2211 j=1 |wjk| \u2264 p,\nthen\n|f(x)\u2212 f\u0303(x)| = |f(x)\u2212 f(x\u0303)| \u2264 \u2225\u2207f\u22252 \u00b7 \u2225x\u2212 x\u0303\u22252 \u2264 pd\n2n .\nBy choosing n = \u2308 log2 pd \u03b5 \u2309 , we have\n|f(x)\u2212 f(x\u0303)| \u2264 \u03b5.\nSince we use nd binary step units to convert the input to binary form and dnp neurons in function approximation, we thus use O ( d log pd\u03b5 ) binary step units and O ( pd log pd\u03b5 ) rectifier linear units in total. In addition, since we have used n layers to convert the input to binary form and p layers in the function approximation section of the network, the whole deep structure has O ( p+ log pd\u03b5\n) layers."}, {"heading": "APPENDIX E PROOF OF THEOREM 9", "text": "Proof. For each multinomial function g with multi-index \u03b1, g\u03b1(x) = x\u03b1, it follows from Theorem 4 that there exists a deep neural network g\u0303\u03b1 of size O ( |\u03b1| log |\u03b1|d\u03b5 ) and depth\nO ( |\u03b1|+ log |\u03b1|d\u03b5 ) such that |g\u03b1(x)\u2212 g\u0303\u03b1(x)| \u2264 \u03b5. Let the deep neural network be\nf\u0303(x) = \u2211\n\u03b1:|\u03b1|\u2264p\nC\u03b1g\u0303\u03b1(x),\nand thus |f(x)\u2212 f\u0303(x)| \u2264 \u2211 \u03b1:|\u03b1|\u2264p |C\u03b1| \u00b7 |g\u03b1(x)\u2212 g\u0303\u03b1(x)| = \u03b5.\nSince the total number of multinomial is upper bounded by\np ( p+ d\u2212 1 d\u2212 1 ) ,\nthe size of deep neural network is thus upper bounded by p2 ( p+ d\u2212 1 d\u2212 1 ) log pd \u03b5 . (8)\nIf the dimension of the input d is fixed, then (8) is has the order of p2 ( p+ d\u2212 1 d\u2212 1 ) log pd \u03b5 = O ( (ep) d+1 log pd \u03b5 ) , p \u2192 \u221e\nwhile if the degree p is fixed, then (8) is has the order of p2 ( p+ d\u2212 1 d\u2212 1 ) log pd \u03b5 = O ( p2 (ed) p log pd \u03b5 ) , d \u2192 \u221e."}, {"heading": "APPENDIX F PROOF OF THEOREM 11", "text": "Proof. We first prove the univariate case d = 1. The proof is composed of two parts. We say the function g(x) has a break point at x = z if g is discontinuous at z or its derivative g\u2032 is discontinuous at z. We first present the lower bound on the number of break points M(\u03b5) that the multilayer neural network f\u0303 should have for \u03b5-approximation of function f with error \u03b5. We next relate the number of break points M(\u03b5) to the network depth L and the size N .\nNow we calculate the lower bound on M(\u03b5). We first define 4 points x0, x1 = x0 + 2 \u221a\n\u03c1\u03b5/\u00b5, x2 = x1 + 2 \u221a \u03c1\u03b5/\u00b5 and x3 = x2 + 2 \u221a \u03c1\u03b5/\u00b5, \u2200\u03c1 > 1. We assume\n0 \u2264 x0 < x1 < x2 < x3 \u2264 1.\nWe now prove that if multilayer neural network f\u0303 has no break point in [x1, x2], then f\u0303 should have a break point in [x0, x1] and a break point in [x2, x3]. We prove this by contradiction. We assume the neural network f\u0303 has no break points in the interval [x0, x3]. Since f\u0303 is constructed by rectifier linear units and binary step units and has no break points in the interval [x0, x3], then f\u0303 should be a linear function in the interval [x0, x3], i.e., f\u0303(x) = ax + b, x \u2208 [x0, x3] for some a and b. By assumption, since f\u0303 approximates f with error at most \u03b5 everywhere in [0, 1], then\n|f(x1)\u2212 ax1 \u2212 b| \u2264 \u03b5 and |f(x2)\u2212 ax2 \u2212 b| \u2264 \u03b5.\nThen we have f(x2)\u2212 f(x1)\u2212 2\u03b5\nx2 \u2212 x1 \u2264 a \u2264 f(x2)\u2212 f(x1) + 2\u03b5 x2 \u2212 x1 .\nBy strong convexity of f , f(x2)\u2212 f(x1)\nx2 \u2212 x1 +\n\u00b5 2 (x2 \u2212 x1) \u2264 f \u2032(x2).\nBesides, since \u03c1 > 1 and \u00b5\n2 (x2 \u2212 x1) =\n\u221a \u03c1\u00b5\u03b5 = 2\u03c1\u03b5\nx2 \u2212 x1 >\n2\u03b5\nx2 \u2212 x1 ,\nthen a \u2264 f \u2032(x2). (9) Similarly, we can obtain a \u2265 f \u2032(x1). By our assumption that f\u0303 = ax+ b, x \u2208 [x0, x3], then f(x3)\u2212 f\u0303(x3) = f(x3)\u2212 ax3 \u2212 b\n= f(x3)\u2212 f(x2)\u2212 a(x3 \u2212 x2) + f(x2)\u2212 ax2 \u2212 b \u2265 f \u2032(x2)(x3 \u2212 x2) + \u00b5\n2 (x3 \u2212 x2)2 \u2212 a(x3 \u2212 x2)\u2212 \u03b5\n= (f \u2032(x2)\u2212 a)(x3 \u2212 x2) + \u00b5\n2\n( 2 \u221a \u03c1\u03b5/\u00b5 )2 \u2212 \u03b5\n\u2265 (2\u03c1\u2212 1)\u03b5 > \u03b5 The first inequality follows from strong convexity of f and f(x2) \u2212 ax2 \u2212 b \u2265 \u03b5. The second inequality follows from the inequality (9). Therefore, this leads to the contradiction. Thus there exists a break point in the interval [x2, x3]. Similarly, we could prove there exists a break point in the interval [x0, x1]. These indicate that to achieve \u03b5-approximation in [0, 1], the multilayer neural network f\u0303 should have at least \u2308 1 4 \u221a \u00b5 \u03c1\u03b5 \u2309 break points in [0, 1]. Therefore,\nM(\u03b5) \u2265 \u2308 1\n4\n\u221a \u00b5\n\u03c1\u03b5\n\u2309 , \u2200\u03c1 > 1.\nFurther, Telgarsky (2016) has shown that the maximum number of break points that a multilayer neural network of depth L and size N could have is (N/L)L. Thus, L and N should satisfy\n(N/L)L >\n\u2308 1\n4\n\u221a \u00b5\n\u03c1\u03b5\n\u2309 , \u2200\u03c1 > 1.\nTherefore, we have\nN \u2265 L ( \u00b5 16\u03b5 ) 1 2L .\nBesides, let m = N/L. Since each layer in network should have at least 2 neurons, i.e., m \u2265 2, then\nN \u2265 m 2 log2 m log2 ( \u00b5 16\u03b5 ) \u2265 log2 ( \u00b5 16\u03b5 ) .\nNow we consider the multivariate case d > 1. Assume input vector to be x = (x1, ..., x(d)). We now fix x(2), ..., x(d) and define two univariate functions\ng(y) = f(y, x(2), ..., x(d)), and g\u0303(y) = f\u0303(y, x(2), ..., x(d)). By assumption, g(y) is a strongly convex function with parameter \u00b5 and for all y \u2208 [0, 1], |g(y)\u2212 g\u0303(y)| \u2264 \u03b5. Therefore, by results in the univariate case, we should have\nN \u2265 L ( \u00b5 16\u03b5 ) 1 2L and N \u2265 log2 ( \u00b5 16\u03b5 ) . (10)\nNow we have proved the theorem.\nRemark: We make the following remarks about the lower bound in the theorem.\n(1) if the depth L is fixed, as in shallow networks, the number of neurons required is \u2126 ( (1/\u03b5) 1 2L ) .\n(2) if we are allowed to choose L optimally to minimize the lower bound, we will choose L = 12 log( \u00b5 16\u03b5 ) and thus the lower bound will become \u2126(log 1 \u03b5 ), closed to the O(log 2 1 \u03b5 )\nupper bound shown in Theorem 4."}, {"heading": "APPENDIX G PROOF OF COROLLARY 12", "text": "Proof. From Theorem 4, it follows that there exists a deep neural network f\u0303d of depth Ld = \u0398 ( log 1\u03b5 ) and size\nNd \u2264 c ( log 1\n\u03b5\n)2 (11)\nfor some constant c > 0 such that \u2225f\u0303d \u2212 f\u2225 \u2264 \u03b5. From the equation (10) in the proof of Theorem 11, it follows that for all shallow neural networks f\u0303s of depth Ls and\n\u2225\u2225\u2225f\u0303s \u2212 f\u2225\u2225\u2225 \u2264 \u03b5, their sizes should satisfy Ns \u2265 Ls ( \u00b5 16\u03b5 ) 1 2Ls ,\nwhich is equivalent to\nlogNs \u2265 logLs + 1 2Ls log ( \u00b5 16\u03b5 ) . (12)\nSubstituting for log ( 1 \u03b5 ) from (12) to (11), we have\nNd = O(L2s log 2 Ns).\nBy definition, a shallow neural network has a small number of layers, i.e., Ls. Thus, the size of the deep neural network is O(log2 Ns). This means Nd \u226a Ns."}, {"heading": "APPENDIX H PROOF OF COROLLARY 13", "text": "Corollary 13 (Gaussian function). For Gaussian function f(x) = f(x(1), ..., x(d)) = e\u2212 \u2211d i=1(x (i))2/2, x \u2208 [0, 1]d, there exists a deep neural network f\u0303(x) with O ( log d\u03b5 ) lay-\ners, O ( d log d\u03b5 ) binary step units and O ( d log d\u03b5 + ( log 1\u03b5 )2) rectifier linear units such that\n|f\u0303(x)\u2212 f(x)| \u2264 \u03b5 for \u2200x \u2208 [0, 1]d.\nProof. It follows from the Theorem 4 that there exists d multilayer neural networks g\u03031(x (1)), ..., g\u0303d(x (d)) with O ( log d\u03b5 ) layers and O ( d log d\u03b5 ) binary step units and O ( d log d\u03b5 ) rectifier linear units in total such that\u2223\u2223\u2223\u2223\u2223x(1) 2 + ...+ x(d) 2 2 \u2212 g\u03031(x (1)) + ...+ g\u0303d(x (d)) 2\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b52 . (13) Besides, from Theorem 4, it follows that there exists a deep neural network f\u0302 with O ( log 1\u03b5 ) layers\nO ( log 1\u03b5 ) binary step units and O (( log 1\u03b5 )2) such that\n|e\u2212dx \u2212 f\u0302(x)| \u2264 \u03b5 2 , \u2200x \u2208 [0, 1].\nLet x = (g\u03031(x(1)) + ...+ g\u0303d(x(d)))/2d, then we have\u2223\u2223\u2223\u2223\u2223e\u2212(\u2211di=1 g\u0303i(x(i)))/2 \u2212 f\u0302 (\u2211d i=1 g\u0303i(x (i)) 2 )\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b52 . (14) Let the deep neural network\nf\u0303(x) = f\u0302\n( g\u03031(x (1)) + ...+ g\u0303d(x (d))\n2\n) .\nBy inequalities (13) and (14), the the approximation error is upper bounded by\n|f(x)\u2212 f\u0303(x)| = \u2223\u2223\u2223\u2223\u2223e\u2212(\u2211di=1 x(i))/2 \u2212 f\u0302 (\u2211d i=1 g\u0303i(x (i)) 2 )\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223e\u2212(\u2211di=1 x(i))/2 \u2212 e\u2212(\u2211di=1 g\u0303i(x(i)))/2\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223e\u2212(\u2211di=1 g\u0303i(x(i)))/2 \u2212 f\u0302 (\u2211d i=1 g\u0303i(x (i)) 2\n)\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5\n2 +\n\u03b5 2 = \u03b5.\nNow the deep neural network has O ( log d\u03b5 ) layers, O ( d log d\u03b5 ) binary step units and\nO ( d log d\u03b5 + ( log 1\u03b5 )2) rectifier linear units.\nAPPENDIX I PROOF OF COROLLARY 14\nCorollary 14 (Ridge function). If f(x) = g(aTx) for some direction a \u2208 Rd with \u2225a\u22251 = 1, a \u2ab0 0, x \u2208 [0, 1]d and some univariate function g satisfying conditions in Theorem 4, then there exists a multilayer neural network f\u0303 with O ( log 1\u03b5 ) layers, O ( log 1\u03b5 ) binary step units and O (( log 1\u03b5 )2) rectifier linear units such that |f(x)\u2212 f\u0303(x)| \u2264 \u03b5 for \u2200x \u2208 [0, 1]d.\nProof. Let t = aTx. Since \u2225a\u22251 = 1, a \u2ab0 0 and x \u2208 [0, 1]d, then 0 \u2264 t \u2264 1. Then from Theorem 4, it follows that then there exists a multilayer neural network g\u0303 with O ( log 1\u03b5 ) layers,\nO ( log 1\u03b5 ) binary step units and O (( log 1\u03b5 )2) rectifier linear units such that\n|g(t)\u2212 g\u0303(t)| \u2264 \u03b5, \u2200t \u2208 [0, 1].\nIf we define the deep network f\u0303 as f\u0303(x) = g\u0303(t),\nthen the approximation error of f\u0303 is\n|f(x)\u2212 f\u0303(x)| = |g(t)\u2212 g\u0303(t)| \u2264 \u03b5.\nNow we have proved the corollary."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "In ICML,", "citeRegEx": "Andoni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2014}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information theory,", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Approximation by ridge functions and neural networks with one hidden layer", "author": ["C.K. Chui", "X. Li"], "venue": "Journal of Approximation Theory,", "citeRegEx": "Chui and Li.,? \\Q1992\\E", "shortCiteRegEx": "Chui and Li.", "year": 1992}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "The power of approximating: a comparison of activation functions", "author": ["B. DasGupta", "G. Schnitger"], "venue": "In NIPS,", "citeRegEx": "DasGupta and Schnitger.,? \\Q1993\\E", "shortCiteRegEx": "DasGupta and Schnitger.", "year": 1993}, {"title": "Shallow vs. deep sum-product networks", "author": ["O. Delalleau", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["R. Eldan", "O. Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K.I. Funahashi"], "venue": "Neural networks,", "citeRegEx": "Funahashi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi.", "year": 1989}, {"title": "Numerical methods for special functions", "author": ["A. Gil", "J. Segura", "N.M. Temme"], "venue": null, "citeRegEx": "Gil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gil et al\\.", "year": 2007}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Hornik.,? \\Q1991\\E", "shortCiteRegEx": "Hornik.", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Notes on hierarchical splines, dclns and i-theory", "author": ["T. Poggio", "L. Rosasco", "A. Shashua", "N. Cohen", "F. Anselmi"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "Poggio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Error bounds for approximations with deep ReLU networks", "author": ["D. Yarotsky"], "venue": "arXiv preprint arXiv:1610.01145,", "citeRegEx": "Yarotsky.,? \\Q2016\\E", "shortCiteRegEx": "Yarotsky.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Neural networks have drawn significant interest from the machine learning community, especially due to their recent empirical successes (see the surveys (Bengio, 2009)).", "startOffset": 153, "endOffset": 167}, {"referenceID": 11, "context": "Numerous results have shown the universal approximation property of neural networks in approximations of different function classes, (see, e.g., Cybenko 1989; Hornik et al. 1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015).", "startOffset": 133, "endOffset": 256}, {"referenceID": 14, "context": "Numerous results have shown the universal approximation property of neural networks in approximations of different function classes, (see, e.g., Cybenko 1989; Hornik et al. 1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015).", "startOffset": 133, "endOffset": 256}, {"referenceID": 1, "context": "1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau & Bengio (2011) have shown that there exist deep sum-product networks which cannot be approximated by shallow sum-product networks unless they use an exponentially larger amount of units or neurons.", "startOffset": 51, "endOffset": 412}, {"referenceID": 1, "context": "1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau & Bengio (2011) have shown that there exist deep sum-product networks which cannot be approximated by shallow sum-product networks unless they use an exponentially larger amount of units or neurons. Montufar et al. (2014) have shown that the number of linear region increases exponentially with the number of layers in the neural network.", "startOffset": 51, "endOffset": 618}, {"referenceID": 1, "context": "1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau & Bengio (2011) have shown that there exist deep sum-product networks which cannot be approximated by shallow sum-product networks unless they use an exponentially larger amount of units or neurons. Montufar et al. (2014) have shown that the number of linear region increases exponentially with the number of layers in the neural network. Telgarsky (2016) has established such a result for neural networks, which is the subject of this paper.", "startOffset": 51, "endOffset": 752}, {"referenceID": 1, "context": "1989; Funahashi 1989; Hornik 1991; Chui & Li 1992; Barron 1993; Poggio et al. 2015). All these results and many others provide upper bounds on the network size and assert that small approximation error can be achieved if the network size is sufficiently large. More recently, there has been much interest in understanding the approximation capabilities of deep versus shallow networks. Delalleau & Bengio (2011) have shown that there exist deep sum-product networks which cannot be approximated by shallow sum-product networks unless they use an exponentially larger amount of units or neurons. Montufar et al. (2014) have shown that the number of linear region increases exponentially with the number of layers in the neural network. Telgarsky (2016) has established such a result for neural networks, which is the subject of this paper. Eldan & Shamir (2015) have shown that, to approximate a specific function, a two-layer network requires an exponential number of neurons in the input dimension, while a three-layer network requires a polynomial number of neurons.", "startOffset": 51, "endOffset": 861}, {"referenceID": 17, "context": "Around the same time that our paper was uploaded in arxiv, a similar paper was also uploaded in arXiv by Yarotsky (2016). The results in the two papers are similar in spirit, but the details and the general approach are substantially different.", "startOffset": 105, "endOffset": 121}, {"referenceID": 9, "context": "The proof can be found in the reference (Gil et al., 2007).", "startOffset": 40, "endOffset": 58}, {"referenceID": 0, "context": "We note that both Andoni et al. (2014) and Barron (1993) showed the sizes of the networks grow exponentially with respect to p if only 3-layer neural networks are allowed to be used in approximating polynomials.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "We note that both Andoni et al. (2014) and Barron (1993) showed the sizes of the networks grow exponentially with respect to p if only 3-layer neural networks are allowed to be used in approximating polynomials.", "startOffset": 18, "endOffset": 57}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993).", "startOffset": 75, "endOffset": 96}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5(2).", "startOffset": 75, "endOffset": 114}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5(2).", "startOffset": 75, "endOffset": 129}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5(2). Andoni et al. (2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5(2) to approximate any multivariate polynomial.", "startOffset": 75, "endOffset": 293}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5(2). Andoni et al. (2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5(2) to approximate any multivariate polynomial. However, Theorem 9 shows that the deep neural network could reduce the network size from O (1/\u03b5) to O ( log 1\u03b5 ) for the same \u03b5 error. Besides, for a fixed input dimension d, the size of the 3-layer neural network used by Andoni et al. (2014) and Barron (1993) grows exponentially with respect to the degree p.", "startOffset": 75, "endOffset": 676}, {"referenceID": 0, "context": "Similar results on approximating multivariate polynomials were obtained by Andoni et al. (2014) and Barron (1993). Barron (1993) showed that on can use a 3-layer neural network to approximate any multivariate polynomial with degree p, dimension d and network size d/\u03b5(2). Andoni et al. (2014) showed that one could use the gradient descent to train a 3-layer neural network of size d/\u03b5(2) to approximate any multivariate polynomial. However, Theorem 9 shows that the deep neural network could reduce the network size from O (1/\u03b5) to O ( log 1\u03b5 ) for the same \u03b5 error. Besides, for a fixed input dimension d, the size of the 3-layer neural network used by Andoni et al. (2014) and Barron (1993) grows exponentially with respect to the degree p.", "startOffset": 75, "endOffset": 694}], "year": 2017, "abstractText": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of \u03b5 uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on \u03b5) require \u03a9(poly(1/\u03b5)) neurons while deep networks (i.e., networks whose depth grows with 1/\u03b5) require O(polylog(1/\u03b5)) neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_192"}