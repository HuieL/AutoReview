{"name": "ICLR_2017_194.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Andrew Brock", "Theodore Lim"], "emails": ["j.m.ritchie}@hw.ac.uk", "Nick.Weston@renishaw.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Editing photos typically involves some form of manipulating individual pixels, and achieving desirable results often requires significant user expertise. Given a sufficiently powerful image model, however, a user could quickly make large, photorealistic changes with ease by instead interacting with the model\u2019s controls. Two recent advances, the Variational Autoencoder (VAE)(Kingma & Welling, 2014) and Generative Adversarial Network (GAN)(Goodfellow et al., 2014), have shown great promise for use in modeling the complex, high-dimensional distributions of natural images, but significant challenges remain before these models can be used as general-purpose image editors.\nVAEs are probabilistic graphical models that learn to maximize a variational lower bound on the likelihood of the data by projecting into a learned latent space, then reconstructing samples from that space. GANs learn a generative model by training one network, the \"discriminator,\" to distinguish between real and generated data, while simultaneously training a second network, the \"generator,\" to transform a noise vector into samples which the discriminator cannot distinguish from real data. Both approaches can be used to generate and interpolate between images by operating in a low-dimensional learned latent space, but each comes with its own set of benefits and drawbacks.\nVAEs have stable training dynamics, but tend to produce images that discard high-frequency details when trained using maximum likelihood. Using the intermediate activations of a pre-trained discriminative neural network as features for comparing reconstructions to originals (Lamb et al., 2016) mollifies this effect, but requires labels in order to train the discriminative network in a supervised fashion.\nBy contrast, GANs have unstable and often oscillatory training dynamics, but produce images with sharp, photorealistic features. Basic GANs lack an inference mechanism, though techniques to train an inference network (Dumoulin et al., 2016) (Donahue et al., 2016) have recently been developed, as well as a hybridization that uses the VAE\u2019s inference network (Larsen et al., 2015).\nTwo key issues arise when attempting to use a latent-variable generative model to manipulate natural images. First, producing acceptable edits requires that the model be able to achieve close-to-exact reconstructions by inferring latents, or else the model\u2019s output will not match the original image. This simultaneously necessitates an inference mechanism (or inference-by-optimization) and careful\ndesign of the model architecture, as there is a tradeoff between reconstruction accuracy and learned feature quality that varies with the size of the information bottleneck.\nSecond, achieving a specific desired edit requires that the user be able to manipulate the model\u2019s latent variables in an interpretable way. Typically, this would require that the model\u2019s latent space be augmented during training and testing with a set of labeled attributes, such that interpolating along a latent such as \"not smiling/smiling\" produces a specific change. In the fully unsupervised setting, however, such semantically meaningful output features are generally controlled by an entangled set of latents which cannot be directly manipulated.\nIn this paper, we present the Neural Photo Editor, an interface that handles both of these issues, enabling a user to make large, coherent changes to the output of unsupervised generative models by indirectly manipulating the latent vector with a \"contextual paintbrush.\" By applying a simple interpolating mask, we enable this same exploration for existing photos despite reconstruction errors.\nComplementary to the Neural Photo Editor, we develop techniques to improve on common design tradeoffs in generative models. Our model, the Introspective Adversarial Network (IAN), is a hybridization of the VAE and GAN that leverages the power of the adversarial objective while maintaining the VAE\u2019s efficient inference mechanism, improving upon previous VAE/GAN hybrids both in parametric efficiency and output quality. We employ a novel convolutional block based on dilated convolutions (Yu & Koltun, 2016) to efficiently increase the network\u2019s receptive field, and Orthogonal Regularization, a novel weight regularizer.\nWe demonstrate the qualitative sampling, reconstructing, and interpolating ability of the IAN on CelebA (Liu et al., 2015), SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky & Hinton, 2009), and Imagenet (Russakovsky et al., 2015), and quantitatively demonstrate its inference capabilities with competitive performance on the semi-supervised SVHN classification task. Further quantitative experiments on CIFAR-100 (Krizhevsky & Hinton, 2009) verify the generality of our dilated convolution blocks and Orthogonal Regularization."}, {"heading": "2 NEURAL PHOTO EDITING", "text": "We present an interface, shown in Figure 1, that turns a coarse user input into a refined, photorealistic image edit by indirectly manipulating the latent space with a \"contextual paintbrush.\" The key idea is simple: a user selects a paint brush size and color (as with a typical image editor) and paints on the output image. Instead of changing individual pixels, the interface backpropagates the difference between the local image patch and the requested color, and takes a gradient descent step in the latent space to minimize that difference. This step results in globally coherent changes that are semantically meaningful in the context of the requested color change. Given an output image X\u0302 and\na user requested color Xuser, the change in latent values is \u2212d||Xuser\u2212X\u0302||2dZ , evaluated at the current paintbrush location each time a user requests an edit.\nFor example, if a user has an image of a person with light skin, dark hair, and a widow\u2019s peak, by painting a dark color on the forehead, the system will automatically add hair in the requested area. Similarly, if a user has a photo of a person with a closed-mouth smile, the user can produce a toothy grin by painting bright white over the target\u2019s mouth.\nThis technique enables exploration of samples generated by the network, but fails when applied directly to existing photos, as it relies on the manipulated image being completely controlled by the latent variables, and reconstructions are usually imperfect. We circumvent this issue by introducing a simple masking technique that transfers edits from a reconstruction back to the original image.\nWe take the output image to be a sum of the reconstruction, and a masked combination of the requested pixel-wise changes and the reconstruction error:\nY = X\u0302 +M\u2206 + (1\u2212M)(X \u2212 X\u0302) (1)\nWhereX is the original image, X\u0302 is the model\u2019s reconstruction ofX , and \u2206 is the difference between the modified reconstruction and X\u0302 . The mask M is the channel-wise mean of the absolute value of \u2206, smoothed with a Gaussian filter g and truncated pointwise to be between 0 and 1:\nM = min(g( \u00af|\u2206|), 1) (2)\nThe mask is designed to allow changes to the reconstruction to show through based on their magnitude. This relaxes the accuracy constraints by requiring that the reconstruction be feature-aligned rather than pixel-perfect, as only modifications to the reconstruction are applied to the original image. As long as the reconstruction is close enough and interpolations are smooth and plausible, the system will successfully transfer edits.\nA visualization of the masking technique is shown in Figure 2. This method adds minimal computational cost to the underlying latent space exploration and produces convincing changes of features including hair color and style, skin tone, and facial expression. A video of the interface in action is available online.1"}, {"heading": "3 INTROSPECTIVE ADVERSARIAL NETWORKS", "text": "Complementary to the Neural Photo Editor, we introduce the Introspective Adversarial Network (IAN), a novel hybridization of the VAE and GAN motivated by the need for an image model with photorealistic outputs that achieves high-quality reconstructions without loss of representational power. There is typically a design tradeoff between these two goals related to the size of the latent space: a higher-dimensional latent space (i.e. a wider representational bottleneck) tends to learn less descriptive features, but produces higher quality reconstructions.\nWe thus seek techniques to improve the capacity of the latent space without increasing its dimensionality. Similar to VAE/GAN (Larsen et al., 2015), we use the decoder network of the autoencoder as the generator network of the GAN, but instead of training a separate discriminator network, we combine the encoder and discriminator into a single network. Central to the IAN is the idea that features learned by a discriminatively trained network tend to be more expressive those learned by an encoder network trained via maximum likelihood (i.e. more useful on semi-supervised tasks), and thus better suited for inference. As the Neural Photo Editor relies on high-quality reconstructions, the inference capacity of the underlying model is critical. Accordingly, we use the discriminator of the GAN, D, as a feature extractor for an inference subnetwork, E, which is implemented as a fully-connected layer on top of the final convolutional layer of the discriminator. We infer latent values Z \u223c E(X) = q(Z|X) for reconstruction and sample random values Z \u223c p(Z) from a standard normal for random image generation using the generator network, G.\nSimilar to VAE/GAN and DeePSiM (Dosovitskiy & Brox, 2016), we use three distinct loss functions:\n\u2022 Limg, the L1 pixel-wise reconstruction loss, which we prefer to the L2 reconstruction loss for its higher average gradient.\n\u2022 Lfeature, the feature-wise reconstruction loss, evaluated as the L2 difference between the original and reconstruction in the space of the hidden layers of the discriminator.\n\u2022 Ladv, the ternary adversarial loss, a modification of the adversarial loss that forces the discriminator to label a sample as real, generated, or reconstructed (as opposed to a binary real vs. generated label).\nIncluding the VAE\u2019s KL divergence between the inferred latents E(X) and the prior p(Z), the loss function for the generator and encoder network is thus:\nLE,G = \u03bbadvLGadv + \u03bbimgLimg + \u03bbfeatureLfeature +DKL(E(X)||p(Z)) (3)\n1https://www.youtube.com/watch?v=FDELBFSeqQs\nWhere the \u03bb terms weight the relative importance of each loss. We set \u03bbimg to 3 and leave the other terms at 1. The discriminator is updated solely using the ternary adversarial loss. During each training step, the generator produces reconstructions G(E(X)) (using the standard VAE reparameterization trick) from data X and random samples G(Z), while the discriminator observes X as well as the reconstructions and random samples, and both networks are simultaneously updated."}, {"heading": "3.1 FEATURE-WISE LOSS", "text": "We compare reconstructions using the intermediate activations, f(G(E(X))), of all convolutional layers of the discriminator, mirroring the perceptual losses of Discriminative Regularization (Lamb et al., 2016), VAE/GAN (Larsen et al., 2015), and DeepSiM (Dosovitskiy & Brox, 2016). We note that Feature Matching (Salimans et al., 2016) is designed to operate in a similar fashion, but without the guidance of an inference mechanism to match latent values Z to particular values of f(G(Z)). We find that using this loss to complement the pixel-wise difference results in sharper reconstructions that better preserve higher frequency features and edges."}, {"heading": "3.2 TERNARY ADVERSARIAL LOSS", "text": "The standard GAN discriminator network is trained using an implicit label source (real vs fake); noting the success of augmenting the discriminator\u2019s objective with supervised labels (Odena et al., 2016), we seek additional sources of implicit labels, in the hopes of achieving similar improvements. The ternary loss provides an additional source of supervision to the discriminator by asking it to determine if a sample is real, generated, or a reconstruction, while the generator\u2019s goal is still to have the discriminator assign a high \"real\" probability to both samples and reconstructions. We thus modify the discriminator to have three output units with a softmax nonlinearity, and train it to minimize the categorical cross-entropy:\nLDadv = \u2212log(Dreal(X))\u2212 log(Dgenerated(G(Z)))\u2212 log(Dreconstructed(G(E(X)))) (4)\nWhere each D term in Equation 4 indicates the discriminator output unit assigned to each label class. The generator is trained to produce outputs that maximize the probability of the label \"real\" being assigned by the discriminator by minimizing LGadv:\nLGadv = \u2212log(Dreal(G(Z)))\u2212 log(Dreal(G(E(X))) (5)\nWe posit that this loss helps maintain the balance of power early in training by preventing the discriminator from learning a small subset of features (e.g. artifacts in the generator\u2019s output) that distinguish real and generated samples, reducing the range of useful features the generator can learn from the discriminator. We also find that this loss leads to higher sample quality, perhaps because the additional source of supervision leads to the discriminator ultimately learning a richer feature space."}, {"heading": "3.3 ARCHITECTURE", "text": "Our model has the same basic structure as DCGAN (Radford et al., 2015), augmented with Multiscale Dilated Convolution (MDC) blocks in the generator, and Minibatch Discrimination (Salimans et al., 2016) in the discriminator. As in (Radford et al., 2015), we use Batch Normalization (Ioffe & Szegedy, 2015) and Adam (Kingma & Ba, 2014) in both networks. All of our code is publicly available.2"}, {"heading": "3.4 MULTISCALE DILATED CONVOLUTION BLOCKS", "text": "We propose a novel Inception-style (Szegedy et al., 2016) convolutional block motivated by the ideas that image features naturally occur at multiple scales, that a network\u2019s expressivity is proportional to the range of functions it can represent divided by its total number of parameters, and by the desire to efficiently expand a network\u2019s receptive field. The Multiscale Dilated Convolution (MDC) block applies a single FxF filter at multiple dilation factors, then performs a weighted elementwise sum\n2https://github.com/ajbrock/Neural-Photo-Editor\nof each dilated filter\u2019s output, allowing the network to simultaneously learn a set of features and the relevant scales at which those features occur with a minimal increase in parameters. This also rapidly expands the network\u2019s receptive field without requiring an increase in depth or the number of parameters. Dilated convolutions have previously been successfully applied in semantic segmentation (Yu & Koltun, 2016), and a similar scheme, minus the parameter sharing, is proposed in (Chen et al., 2016).\nAs shown in Figure 4(a), each block is parameterized by a bank of N FxF filters W , applied with S factors of dilation, and a set of N*S scalars k, which relatively weight the output of each filter at each scale. This is naturally and efficiently implemented by reparameterizing a sparsely populated F+(S-1)*(F-1) filterbank as displayed in Figure 4(b). We propose two variants: Standard MDC, where the filter weights are tied to a base W , and Full-Rank MDC, where filters are given the sparse layout of Figure 4(b) but the weights are not tied. Selecting Standard versus Full-Rank MDC blocks allows for a design tradeoff between parametric efficiency and model flexibility. In our architecture, we replace the hidden layers of the generator with Standard MDC blocks, using F=5 and D=2; we specify MDC blocks by their base filter size and their maximum dilation factor (e.g. 5d2)."}, {"heading": "3.5 ORTHOGONAL REGULARIZATION", "text": "Orthogonality is a desirable quality in ConvNet filters, partially because multiplication by an orthogonal matrix leaves the norm of the original matrix unchanged. This property is valuable in deep or recurrent networks, where repeated matrix multiplication can result in signals vanishing or exploding. We note the success of initializing weights with orthogonal matrices (Saxe et al., 2014), and posit that maintaining orthogonality throughout training is also desirable. To this end, we propose a simple weight regularization technique, Orthogonal Regularization, that encourages weights to be orthogonal by pushing them towards the nearest orthogonal manifold. We augment our objective with the cost:\nLortho = \u03a3(|WWT \u2212 I|) (6)\nWhere \u03a3 indicates a sum across all filter banks, W is a filter bank, and I is the identity matrix."}, {"heading": "4 RELATED WORK", "text": "Our architecture builds directly off of previous VAE/GAN hybrids (Larsen et al., 2015) (Dosovitskiy & Brox, 2016), with the key difference being our combination of the discriminator and the encoder to improve computational and parametric efficiency (by reusing discriminator features) as well as reconstruction accuracy (as demonstrated in our CelebA ablation studies). The methods of ALI (Dumoulin et al., 2016) and BiGAN (Donahue et al., 2016) provide an orthogonal approach to GAN inference, in which an inference network is trained by an adversarial (as opposed to a variational) process.\nThe method of iGAN (Zhu et al., 2016) bears the most relation to our interface. The iGAN interface allows a user to impose shape or color constraints on an image of an object through use of a brush\ntool, then optimizes to solve for the output of a DCGAN (Radford et al., 2015) which best satisfies those constraints. Photorealistic edits are transferred to existing images via motion and color flow estimation.\nBoth iGAN and the Neural Photo Editor turn coarse user input into refined outputs through use of a generative model, but the methods differ in several key ways. First, we focus on editing portraits, rather than objects such as shoes or handbags, and are thus more concerned with modifying features, as opposed to overall color or shape, for which our method is less well-suited. Our edit transfer technique follows this difference as well: we directly transfer the local image changes produced by the model back onto the original image, rather than estimating and mimicking motion and color flow.\nSecond, our interface applies user edits one step at a time, rather than iteratively optimizing the output. This highlights the difference in design approaches: iGAN seeks to produce outputs that best match a given set of user constraints, while we seek to allow a user to guide the latent space traversal.\nFinally, we explicitly tailor our model design to the task at hand and jointly train an inference network which we use at test time to produce reconstructions in a single shot. In contrast, iGAN trains an inference network to minimize the L2 loss after training the generator network, and use the inference network to get an initial estimate of the inferred latents, which are then iteratively optimized.\nAnother related interface (Champanard, 2016) refines simple user input into complex textures through use of artistic style transfer (Gatys et al., 2015). Other related work (White, 2016) also circumvents the need for labeled attributes by constructing latent vectors by analogy and bias-correcting them."}, {"heading": "5 EXPERIMENTS", "text": "We qualitatively evaluate the IAN on 64x64 CelebA (Liu et al., 2015), 32x32 SVHN (Netzer et al., 2011), 32x32 CIFAR-10 (Krizhevsky & Hinton, 2009), and 64x64 Imagenet (Russakovsky et al., 2015). Our models are implemented in Theano (Team, 2016) with Lasagne (Dieleman et al., 2015). Samples from the IAN, randomly selected and shown in Figure 5, display the visual fidelity typical of adversarially trained networks. The IAN demonstrates high quality reconstructions on previously unseen data, shown in Figure 6, and smooth, plausible interpolations, even between drastically different samples. CIFAR and Imagenet samples, along with additional comparisons to samples from other models, are available in the appendix."}, {"heading": "5.1 DISCRIMINATIVE EXPERIMENTS", "text": "We quantitatively demonstrate the effectiveness of our MDC blocks and Orthogonal Regularization on the CIFAR-100 (Krizhevsky & Hinton, 2009) benchmark. Using standard data augmentation, we train a set of 40-layer, k=12 DenseNets (Huang et al., 2016) for 50 epochs, annealing the learning rate at 25 and 37 epochs. We add varying amounts of Orthogonal Regularization and modify the\nstandard DenseNet architecture by replacing every 3x3 filterbank with 3d3 MDC blocks, and report the test error after training in Table 1. In addition, we compare to performance using full 7x7 filters.\nThere is a noticeable increase in performance with the progressive addition of our modifications, despite a negligible increase in the number of parameters. Adding Orthogonal Regularization improves the network\u2019s generalization ability; we suspect this is because it encourages the filter weights to remain close to a desirable, non-zero manifold, increasing the likelihood that all of the available model capacity is used by preventing the magnitude of the weights from overly diminishing. Replacing 3x3 filters with MDC blocks yields additional performance gains; we suspect this is due to an increase in the expressive power and receptive field of the network, allowing it to learn longer-range dependencies with ease. We also note that substituting Full-Rank MDC blocks into a 40-Layer DenseNet improves performance by a relative 5%, with the only increased computational cost coming from using the larger filters.\nFor use in evaluating the IAN, we additionally train 40-layer, k=12 DenseNets on the CelebA attribute classification task with varying amounts of Orthogonal Regularization. A plot of the train and validation error during training is available in Figure 7. The addition of of Orthogonal Regularization improves the validation error from 6.55% to 4.22%, further demonstrating its utility."}, {"heading": "5.2 EVALUATING MODIFICATIONS", "text": "For use in editing photos, a model must produce reconstructions which are photorealistic and featurealigned, and have smooth, plausible interpolations between outputs. We perform an ablation study to investigate the effects of our proposals, and employ several metrics to evaluate model quality given these goals. In this study, we progressively add modifications to a VAE/GAN (Larsen et al., 2015) baseline, and train each network for 50 epochs.\nFor reconstruction accuracy, pixel-wise distance does not tend to correlate well with perceptual similarity. In addition to pixel-wise L2 distance, we therefore compare model reconstruction accuracy in terms of:\n\u2022 Feature-wise L2 distance in the final layer of a 40-Layer k=12 DenseNet trained for the CelebA attribute classification task.\n\u2022 Trait reconstruction error. We run our classification DenseNet to predict a binary attribute vector y(X) given an image X, and y(G(E(X))) given a model\u2019s reconstruction, then measure the percent error.\n\u2022 Fiducial keypoint Error, measured as the mean L2 distance between the facial landmarks predicted by the system of (Sankaranarayanan et al., 2016).\nGauging the visual quality of the model\u2019s outputs is notoriously difficult, but the Inception score recently proposed by (Salimans et al., 2016) has been found to correlate positively with humanevaluated sample quality. Using our CelebA attribute classification network in place of the Inception (Szegedy et al., 2016) model, we compare the Inception score of each model evaluated on 50,000 random samples. We posit that this metric is also indicative of interpolation quality, as a high visual quality score on a large sample population suggests that the model\u2019s output quality remains high regardless of the state of the latent space.\nResults of this ablation study are presented in Table 2; samples and reconstructions from each configuration are available in the appendix, along with comparisons between a fully-trained IAN and related models. As with our discriminative experiments, we find that the progressive addition of modifications results in consistent performance improvements across our reconstruction metrics and the Inception score.\nWe note that the single largest gains come from the inclusion of MDC blocks, suggesting that the network\u2019s receptive field is a critical aspect of network design for both generative and discriminative tasks, with an increased receptive field correlating positively with reconstruction accuracy and sample quality.\nThe improvements from Orthogonal Regularization suggest that encouraging weights to lie close to the orthogonal manifold is beneficial for improving the sample and reconstruction quality of generative neural networks by preventing learned weights from collapsing to an undesirable manifold; this is consistent with our experience iterating through network designs, where we have found mode collapse to occur less frequently while using Orthogonal Regularization.\nFinally, the increase in sample quality and reconstruction accuracy through use of the ternary adversarial loss suggests that including the \"reconstructed\" target in the discriminator\u2019s objective does lead to the discriminator learning a richer feature space. This comes along with our observations that training with the ternary loss, where we have observed that the generator and discriminator losses tend to be more balanced than when training with the standard binary loss."}, {"heading": "5.3 SEMI-SUPERVISED LEARNING WITH SVHN", "text": "We quantitatively evaluate the inference abilities of our architecture by applying it to the semisupervised SVHN classification task using two different procedures. We first evaluate using the procedure of (Radford et al., 2015) by training an L2-SVM on the output of the FC layer of the encoder subnetwork, and report average test error and standard deviation across 100 different SVMs, each trained on 1000 random examples from the training set.\nNext, we use the procedure of (Salimans et al., 2016), where the discriminator outputs a distribution over the K object categories and an additional \"fake\" category, for a total of K+1 outputs. The discriminator is trained to predict the category when given labeled data, to assign the \"fake\" label when provided data from the generator, and to assign k \u2208 {1, ...,K} when provided unlabeled real data. We modify feature-matching based Improved-GAN to include the encoder subnetwork and reconstruction losses detailed in Section 3, but do not include the ternary adversarial loss.\nOur performance, as shown in Table 3, is competitive with other networks evaluated in these fashions, achieving 18.5% mean classification accuracy when using SVMs and 8.34% accuracy when using the method of Improved-GAN. When using SVMs, our method tends to demonstrate improvement over previous methods, particularly over standard VAEs. We believe this is due to the encoder subnetwork being based on more descriptive features (i.e. those of the discriminator), and therefore better suited to discriminating between SVHN classes.\nWe find the lack of improvement when using the method of Improved-GAN unsurprising, as the IAN architecture does not change the goal of the discriminator; any changes in behavior are thus indirectly due to changes in the generator, whose loss is only slightly modified from feature-matching Improved-GAN."}, {"heading": "6 CONCLUSION", "text": "We introduced the Neural Photo Editor, a novel interface for exploring the learned latent space of generative models and for making specific semantic changes to natural images. Our interface makes use of the Introspective Adversarial Network, a hybridization of the VAE and GAN that outputs high fidelity samples and reconstructions, and achieves competitive performance in a semi-supervised classification task. The IAN makes use of Multiscale Dilated Convolution Blocks and Orthogonal Regularization, two improvements designed to improve model expressivity and feature quality for convolutional networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was made possible by grants and support from Renishaw plc and the Edinburgh Centre For Robotics. The work presented herein is also partially funded under the European H2020 Programme BEACONING project, Grant Agreement nr. 687676."}, {"heading": "APPENDIX: ADDITIONAL VISUAL COMPARISONS", "text": ""}], "references": [{"title": "Semantic style transfer and turning two-bit doodles into fine artwork", "author": ["A.J. Champanard"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Champanard.,? \\Q2016\\E", "shortCiteRegEx": "Champanard.", "year": 2016}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["L-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv Preprint", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "arXiv Preprint", "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "Jean", "M. Mehdi", "X. Bing", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Densely connected convolutional networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"], "venue": "arXiv Preprint", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML 2015,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR 2014,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Discriminative regularization for generative models", "author": ["A. Lamb", "V. Dumoulin", "A. Courville"], "venue": "arXiv preprint arXiv:1602.03220,", "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "Maal\u00f8e et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maal\u00f8e et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A. Odena", "C. Olah", "J. Shiens"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "An all-in-one convolutional neural network for face analysis", "author": ["S. Sankaranarayanan", "R. Ranjan", "C.D. Castillo", "R. Chellappa"], "venue": "arXiv Preprint", "citeRegEx": "Sankaranarayanan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sankaranarayanan et al\\.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Development Team. Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano"], "venue": "arXiv Preprint arXiv:", "citeRegEx": "Theano,? \\Q2016\\E", "shortCiteRegEx": "Theano", "year": 2016}, {"title": "Sampling generative networks", "author": ["T. White"], "venue": "arXiv Preprint arXiv:1609.04468,", "citeRegEx": "White.,? \\Q2016\\E", "shortCiteRegEx": "White.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "In ICLR 2016,", "citeRegEx": "Yu and Koltun.,? \\Q2016\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2016}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. Lecun"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henbuhl", "E. Shechtman", "A.A. Efros"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Two recent advances, the Variational Autoencoder (VAE)(Kingma & Welling, 2014) and Generative Adversarial Network (GAN)(Goodfellow et al., 2014), have shown great promise for use in modeling the complex, high-dimensional distributions of natural images, but significant challenges remain before these models can be used as general-purpose image editors.", "startOffset": 119, "endOffset": 144}, {"referenceID": 13, "context": "Using the intermediate activations of a pre-trained discriminative neural network as features for comparing reconstructions to originals (Lamb et al., 2016) mollifies this effect, but requires labels in order to train the discriminative network in a supervised fashion.", "startOffset": 137, "endOffset": 156}, {"referenceID": 4, "context": "Basic GANs lack an inference mechanism, though techniques to train an inference network (Dumoulin et al., 2016) (Donahue et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 2, "context": ", 2016) (Donahue et al., 2016) have recently been developed, as well as a hybridization that uses the VAE\u2019s inference network (Larsen et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": ", 2016) have recently been developed, as well as a hybridization that uses the VAE\u2019s inference network (Larsen et al., 2015).", "startOffset": 103, "endOffset": 124}, {"referenceID": 15, "context": "We demonstrate the qualitative sampling, reconstructing, and interpolating ability of the IAN on CelebA (Liu et al., 2015), SVHN (Netzer et al.", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": ", 2015), SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky & Hinton, 2009), and Imagenet (Russakovsky et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 20, "context": ", 2011), CIFAR-10 (Krizhevsky & Hinton, 2009), and Imagenet (Russakovsky et al., 2015), and quantitatively demonstrate its inference capabilities with competitive performance on the semi-supervised SVHN classification task.", "startOffset": 60, "endOffset": 86}, {"referenceID": 14, "context": "Similar to VAE/GAN (Larsen et al., 2015), we use the decoder network of the autoencoder as the generator network of the GAN, but instead of training a separate discriminator network, we combine the encoder and discriminator into a single network.", "startOffset": 19, "endOffset": 40}, {"referenceID": 13, "context": "We compare reconstructions using the intermediate activations, f(G(E(X))), of all convolutional layers of the discriminator, mirroring the perceptual losses of Discriminative Regularization (Lamb et al., 2016), VAE/GAN (Larsen et al.", "startOffset": 190, "endOffset": 209}, {"referenceID": 14, "context": ", 2016), VAE/GAN (Larsen et al., 2015), and DeepSiM (Dosovitskiy & Brox, 2016).", "startOffset": 17, "endOffset": 38}, {"referenceID": 21, "context": "We note that Feature Matching (Salimans et al., 2016) is designed to operate in a similar fashion, but without the guidance of an inference mechanism to match latent values Z to particular values of f(G(Z)).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "The standard GAN discriminator network is trained using an implicit label source (real vs fake); noting the success of augmenting the discriminator\u2019s objective with supervised labels (Odena et al., 2016), we seek additional sources of implicit labels, in the hopes of achieving similar improvements.", "startOffset": 183, "endOffset": 203}, {"referenceID": 19, "context": "Our model has the same basic structure as DCGAN (Radford et al., 2015), augmented with Multiscale Dilated Convolution (MDC) blocks in the generator, and Minibatch Discrimination (Salimans et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 21, "context": ", 2015), augmented with Multiscale Dilated Convolution (MDC) blocks in the generator, and Minibatch Discrimination (Salimans et al., 2016) in the discriminator.", "startOffset": 115, "endOffset": 138}, {"referenceID": 19, "context": "As in (Radford et al., 2015), we use Batch Normalization (Ioffe & Szegedy, 2015) and Adam (Kingma & Ba, 2014) in both networks.", "startOffset": 6, "endOffset": 28}, {"referenceID": 24, "context": "We propose a novel Inception-style (Szegedy et al., 2016) convolutional block motivated by the ideas that image features naturally occur at multiple scales, that a network\u2019s expressivity is proportional to the range of functions it can represent divided by its total number of parameters, and by the desire to efficiently expand a network\u2019s receptive field.", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "Dilated convolutions have previously been successfully applied in semantic segmentation (Yu & Koltun, 2016), and a similar scheme, minus the parameter sharing, is proposed in (Chen et al., 2016).", "startOffset": 175, "endOffset": 194}, {"referenceID": 23, "context": "We note the success of initializing weights with orthogonal matrices (Saxe et al., 2014), and posit that maintaining orthogonality throughout training is also desirable.", "startOffset": 69, "endOffset": 88}, {"referenceID": 14, "context": "Our architecture builds directly off of previous VAE/GAN hybrids (Larsen et al., 2015) (Dosovitskiy & Brox, 2016), with the key difference being our combination of the discriminator and the encoder to improve computational and parametric efficiency (by reusing discriminator features) as well as reconstruction accuracy (as demonstrated in our CelebA ablation studies).", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "The methods of ALI (Dumoulin et al., 2016) and BiGAN (Donahue et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 2, "context": ", 2016) and BiGAN (Donahue et al., 2016) provide an orthogonal approach to GAN inference, in which an inference network is trained by an adversarial (as opposed to a variational) process.", "startOffset": 18, "endOffset": 40}, {"referenceID": 29, "context": "The method of iGAN (Zhu et al., 2016) bears the most relation to our interface.", "startOffset": 19, "endOffset": 37}, {"referenceID": 19, "context": "tool, then optimizes to solve for the output of a DCGAN (Radford et al., 2015) which best satisfies those constraints.", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "Another related interface (Champanard, 2016) refines simple user input into complex textures through use of artistic style transfer (Gatys et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 5, "context": "Another related interface (Champanard, 2016) refines simple user input into complex textures through use of artistic style transfer (Gatys et al., 2015).", "startOffset": 132, "endOffset": 152}, {"referenceID": 26, "context": "Other related work (White, 2016) also circumvents the need for labeled attributes by constructing latent vectors by analogy and bias-correcting them.", "startOffset": 19, "endOffset": 32}, {"referenceID": 15, "context": "We qualitatively evaluate the IAN on 64x64 CelebA (Liu et al., 2015), 32x32 SVHN (Netzer et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 17, "context": ", 2015), 32x32 SVHN (Netzer et al., 2011), 32x32 CIFAR-10 (Krizhevsky & Hinton, 2009), and 64x64 Imagenet (Russakovsky et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 20, "context": ", 2011), 32x32 CIFAR-10 (Krizhevsky & Hinton, 2009), and 64x64 Imagenet (Russakovsky et al., 2015).", "startOffset": 72, "endOffset": 98}, {"referenceID": 7, "context": "Using standard data augmentation, we train a set of 40-layer, k=12 DenseNets (Huang et al., 2016) for 50 epochs, annealing the learning rate at 25 and 37 epochs.", "startOffset": 77, "endOffset": 97}, {"referenceID": 14, "context": "In this study, we progressively add modifications to a VAE/GAN (Larsen et al., 2015) baseline, and train each network for 50 epochs.", "startOffset": 63, "endOffset": 84}, {"referenceID": 22, "context": "\u2022 Fiducial keypoint Error, measured as the mean L2 distance between the facial landmarks predicted by the system of (Sankaranarayanan et al., 2016).", "startOffset": 116, "endOffset": 147}, {"referenceID": 21, "context": "Gauging the visual quality of the model\u2019s outputs is notoriously difficult, but the Inception score recently proposed by (Salimans et al., 2016) has been found to correlate positively with humanevaluated sample quality.", "startOffset": 121, "endOffset": 144}, {"referenceID": 24, "context": "Using our CelebA attribute classification network in place of the Inception (Szegedy et al., 2016) model, we compare the Inception score of each model evaluated on 50,000 random samples.", "startOffset": 76, "endOffset": 98}, {"referenceID": 11, "context": "Method Error rate VAE (M1 + M2) (Kingma et al., 2014) 36.", "startOffset": 32, "endOffset": 53}, {"referenceID": 19, "context": "We first evaluate using the procedure of (Radford et al., 2015) by training an L2-SVM on the output of the FC layer of the encoder subnetwork, and report average test error and standard deviation across 100 different SVMs, each trained on 1000 random examples from the training set.", "startOffset": 41, "endOffset": 63}, {"referenceID": 21, "context": "Next, we use the procedure of (Salimans et al., 2016), where the discriminator outputs a distribution over the K object categories and an additional \"fake\" category, for a total of K+1 outputs.", "startOffset": 30, "endOffset": 53}], "year": 2017, "abstractText": "The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_194"}