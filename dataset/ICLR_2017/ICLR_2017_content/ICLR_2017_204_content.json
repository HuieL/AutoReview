{"name": "ICLR_2017_204.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "emails": ["bw4mw@virginia.edu", "jg6yd@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) can efficiently learn highly accurate models and have been demonstrated to perform exceptionally well (Krizhevsky et al., 2012; Hannun et al., 2014). However, recent studies show that intelligent attackers can force many machine learning models, including DNNs, to misclassify examples by adding small and hardly visible modifications on a regular test sample.\nThe maliciously generated inputs are called \u201cadversarial examples\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) and are commonly crafted by carefully searching small perturbations through an optimization procedure. Several recent studies proposed algorithms for solving such optimization to fool DNN classifiers. (Szegedy et al., 2013) firstly observe that convolution DNNs are vulnerable to small artificial perturbations. They use box-constrained Limited-memory BFGS (L-BFGS) to create adversarial examples and find that adversarial perturbations generated from one DNN network can also force other networks to produce wrong outputs. Then, (Goodfellow et al., 2014) try to clarify that the primary cause of such vulnerabilities may be the linear nature of DNNs. They then propose the fast gradient sign method for generating adversarial examples quickly. Subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Nguyen et al., 2015) have explored other ways to explore adversarial examples for DNN (details in Section 2.1). The goal of this paper is to analyze the robustness of machine learning models in the face of adversarial examples.\nIn response to progress in generating adversarial examples, researchers attempt to design strategies for making machine-learning systems robust to various noise, in the worst case as adversarial examples. For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using a noise-corrupted version of inputs as training samples. A modified distillation strategy (Papernot et al., 2015b) is proposed to improve the robustness of DNNs against adversarial examples, though it has been shown to be unsuccessful recently (Carlini & Wagner, 2016a). The most generally successful strategy to date is adversarial training (Goodfellow et al., 2014; Szegedy et al., 2013) which injects adversarial examples into training to improve the generalization of DNN models. More recent techniques incorporate a smoothness penalty (Miyato\net al., 2016; Zheng et al., 2016) or a layer-wise penalty (Carlini & Wagner, 2016b) as a regularization term in the loss function to promote the smoothness of the DNN model distributions.\nRecent studies (reviewed by (Papernot et al., 2016b)) are mostly empirical and provide little understanding of why an adversary can fool machine learning models with adversarial examples. Several important questions have not been answered yet:\n\u2022 What makes a classifier always robust to adversarial examples? \u2022 Which parts of a classifier influence its robustness against adversarial examples more, compared\nwith the rest? \u2022 What is the relationship between a classifier\u2019s generalization accuracy and its robustness against\nadversarial examples? \u2022 Why (many) DNN classifiers are not robust against adversarial examples ? How to improve? This paper tries to answer above questions and makes the following contributions:\n\u2022 Section 2 points out that previous definitions of adversarial examples for a classifier (f1) have overlooked the importance of an oracle function (f2) of the same task. \u2022 Section 3 formally defines when a classifier f1 is always robust (\"strong-robust\") against adversarial examples. It proves four theorems about sufficient and necessary conditions that make f1 always robust against adversarial examples according to f2. Our theorems lead to a number of interesting insights, like that the feature representation learning controls if a DNN is strong-robust or not. \u2022 Section 12 is dedicated to provide practical and theoretically grounded directions for understanding and hardening DNN models against adversarial examples.\nTable 1 provides a list of important notations we use in the paper."}, {"heading": "2 DEFINE ADVERSARIAL EXAMPLES", "text": "This section provides a general definition of adversarial examples , by including the notion of an oracle. For a particular classification task, a learned classifier is represented as f1 : X \u2192 Y , where X represents the input sample space and Y is the output space representing a categorical set."}, {"heading": "2.1 PREVIOUS FORMULATIONS", "text": "Various definitions of \u201cadversarial examples\u201d exist in the recent literature, with most following Eq. (2.1). See more detailed reviews in Section 8. The basic idea is to generate a misclassified sample\nx\u2032 by \u201cslightly\u201d perturbing a correctly classified sample x, with an adversarial perturbation \u2206(x, x\u2032). Formally, when given x \u2208 X\nFind x\u2032\ns.t. f1(x) 6= f1(x\u2032) \u2206(x, x\u2032) <\n(2.1)\nHere x, x\u2032 \u2208 X . \u2206(x, x\u2032) represents the difference between x and x\u2032, which depends on the specific data type that x and x\u2032 belong to 1. Table 2 summarizes different choices of f1 and \u2206(x, x\u2032) used in the recent literature, in which norm functions on the original space X are mostly used to calculate \u2206(x, x\u2032). Multiple algorithms have been implemented to solve Eq. (2.1) as a constrained optimization (summarized by the last column of Table 2). More details are included for three such studies in Section 8.2.\nWhen searching for adversarial examples, one important property has not been fully captured by Eq. (2.1). That is, an adversarial example has been modified very slightly from its seed and these modifications can be so subtle that, for example in image classification, a human observer does not even notice the modification at all. We define the role of \u201chuman observer\u201d more formally as follows:\nDefinition 2.1. An \u201cOracle\u201d represents a decision process generating ground truth labels for a task of interest. Each oracle is task-specific, with finite knowledge and noise-free2.\n1For example, in the case of strings, \u2206(x, x\u2032) represents the difference between two strings. 2We leave all detailed analysis of when an oracle contains noise as future work.\nThe goal of machine learning is to train a learning-based predictor function f1 : X \u2192 Y to approximate an oracle classifier f2 : X \u2192 Y for the same classification task. For example, in image classification tasks, the oracle f2 is often a group of human annotators. Adding the notation of oracle, we revise Eq. (2.1) into:\nFind x\u2032\ns.t. f1(x) 6= f1(x\u2032) \u22062(x, x \u2032) <\nf2(x) = f2(x \u2032)\n(2.2)\n2.2 MEASURING SAMPLE DIFFERENCE IN WHICH SPACE? MODELING & DECOMPOSING f2\n\u22062(x, x \u2032) < reflects that adversarial examples add \u201csmall modifications\u201d that are almost imperceptible to oracle of the task. Clearly calculating \u22062(x, x\u2032) needs to accord to oracle f2. For most classification tasks, an oracle does not measure the sample difference in the original input space X . We want to emphasize that sample difference is with regards to its classification purpose. For instance, when labeling images for the hand-written digital recognition, human annotators do not need to consider those background pixels to decide if an image is \u201c0\u201d or not.\nIllustrated in Figure 1, we denote the feature space an oracle uses to consider difference among samples for the purpose of classification decision as X2. The sample difference uses a distance function d2 in this space. An oracle function f2 : X \u2192 Y can be decomposed as f2 = c2 \u25e6 g2 where g2 : X \u2192 X2 represents the operations for feature extraction from X to X2 and c2 : X2 \u2192 Y denotes the simple operation of classification in X2. Essentially g2 includes the operations that (progressively) transform input representations into an informative form of representations X2. c2 applies relatively simple functions (like linear) in X2 for the purpose of classification. d2 is the metric function (details in Section 3) an oracle uses to measure the similarity among samples (by relying on representations learned in the space X2). We illustrate the modeling and decomposition in Figure 1.\nIn Section 3 our theoretical analysis uses (X2, d2) to bring forth the fundamental causes of adversarial examples and leads to a set of novel insights to understand such examples. To the best of the authors\u2019 knowledge, the theoretical analysis made by this paper has not been uncovered by the literature.\nModeling Oracle f2: One may argue that it is hard to model f2 and (X2, d2) for real applications, since if such oracles can be easily modeled machine-learning based f1 seems not necessary. In Section 8.3, we provide examples of modeling oracles for real applications. For many securitysensitive applications about machines, oracles f2 do exist 3. For artificial intelligence tasks like image classification, humans are f2. As illustrated by cognitive neuroscience papers (DiCarlo & Cox, 2007; DiCarlo et al., 2012), human brains perform visual object recognition using the ventral visual stream, and this stream is considered to be a progressive series of visual re-representations, from V1 to V2 to V4 to IT cortex (DiCarlo & Cox, 2007). Experimental results support that human visual system makes classification decision at the final IT cortex layer. This process is captured exactly by our decomposition f2 = c2 \u25e6 g2."}, {"heading": "2.3 REVISED FORMULATION", "text": "Now we use the decomposition of f2 to rewrite \u22062(x, x\u2032) as d2(g2(x), g2(x\u2032)) in Eq. (2.2) and obtain our proposed general definition of adversarial examples:\nDefinition 2.2. adversarial example: Suppose we have two functions f1 and f2. f1 : X \u2192 Y is the classification function learned from a training set and f2 : X \u2192 Y is the classification function of the oracle that generates ground-truth labels for the same task. Given a sample x \u2208 X , an adversarial example x\u2032 \u2208 X . (x, x\u2032) satisfies Eq. (2.3).\nFind x\u2032\ns.t. f1(x) 6= f1(x\u2032) d2(g2(x), g2(x \u2032)) < \u03b42 f2(x) = f2(x \u2032)\n(2.3)\nMost previous studies (Table 2) have made an important and implicit assumption about f2 (through using \u2206(x, x\u2032) < ): f2 is almost everywhere (a.e.) continuous. We explains the a.e. continuity assumption and its indication in Section 9. Basically, when f2 is assumed continuous a.e., P(f2(x) = f2(x\u2032)|d2(g2(x), g2(x\u2032)) < \u03b42) = 1 Therefore, when f2 is continuous a.e. Eq. (2.3) can be simplified into the following Eq. (2.4).\nFind x\u2032\ns.t. f1(x) 6= f1(x\u2032) d2(g2(x), g2(x \u2032)) < \u03b42\n(2.4)\n3Oracles f2 do exist in many security-sensitive applications about machines. But machine-learning classifiers f1 are used popularly due to speed or efficiency"}, {"heading": "3 DEFINE STRONG-ROBUSTNESS", "text": "With a more accurate definition of \u201cadversarial examples\u201d, now we aim to answer the first central question: \u201cWhat makes a classifier always robust against adversarial examples?\u201d. Section 3.2 defines the concept \u201cstrong-robust\u201d describing a classifier always robust against adversarial examples. Section 3.3 and Section 3.4 present sufficient and necessary conditions for \u201cstrong-robustness\u201d. Section 4 then provides a set of theoretical insights to understand \u201cstrong-robustness\u201d.\n3.1 MODELING AND DECOMPOSING f1\nAs shown in Figure 1, we decompose f1 in a similar way as the decomposition of f2. This is to answer another key question: \u201cwhich parts of a learned classifier influence its robustness against adversarial examples more, compared with the rest?\u201d. A machine-learning classifier f1 = c1 \u25e6 g1, where g1 : X \u2192 X1 represents the feature extraction operations and c1 : X1 \u2192 Y performs a simple operation (e.g., linear) of classification. Section 8.4 provides multiple examples of decomposing state-of-the-art f1 4. d1 denotes the distance function f1 uses to measure difference among samples in X1.\nAlmost all popular machine learning classifiers satisfy the a.e. continuity assumption. It means: P(f1(x) = f1(x\u2032)|d1(g1(x), g1(x\u2032)) < \u03b41) = 1 When f1 is not continuous a.e., it is not robust to any types of noise. See Section 9 for detailed discussions.\nFor the rare cases that f1 is not continuous a.e., Section 11 discusses \"boundary points\" of f1 5. Roughly speaking, when f1 is not continuous a.e., 6 P(f1(x) 6= f1(x\u2032)|d1(g1(x), g1(x\u2032)) < \u03b41) > 0 Therefore the following probability of \u201cboundary points based adversarial examples\u201d might not be 0 for such cases 7:\nP(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d1(g1(x), g1(x \u2032)) < \u03b41, d2(g2(x), g2(x \u2032)) < \u03b42)\n(3.1)\nThe value of this probability is critical for our analysis in Theorem (3.3) and in Theorem (3.5).\n3.2 {\u03b42, \u03b7}-STRONG-ROBUST AGAINST ADVERSARIAL EXAMPLES We then apply reverse-thinking on Definition (2.2) and derive the following definition of strongrobustness for a machine learning classifier against adversarial examples:\nDefinition 3.1. {\u03b42, \u03b7}-Strong-robustness of a machine-learning classifier: A machine-learning classifier f1(\u00b7) is {\u03b42, \u03b7}-strong-robust against adversarial examples if: \u2200x, x\u2032 \u2208 X a.e., (x, x\u2032) satisfies Eq. (3.2).\n\u2200x, x\u2032 \u2208 X P(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032),\nd2(g2(x), g2(x \u2032)) < \u03b42) > 1\u2212 \u03b7\n(3.2)\nWhen f2 is continuous a.e., Eq. (3.2) simplifies into Eq. (3.3): \u2200x, x\u2032 \u2208 X, P(f1(x) = f1(x\u2032)|\nd2(g2(x), g2(x \u2032)) < \u03b42) > 1\u2212 \u03b7\n(3.3)\nEq. (3.2) defines the \u201c{\u03b42, \u03b7}-strong-robustness\u201d as a claim with the high probability. To simplify notations, in the rest of this paper, we use \u201cstrong-robust\u201d representing \u201c{\u03b42, \u03b7}-strong-robust\u201d. Also in the rest of this paper we propose and prove theorems and corollaries by using its more general form by Eq. (3.2). For all cases, if f2 is continuous a.e., all proofs and equations can be simplified by using only the term d2(g2(x), g2(x\u2032)) < \u03b42 (i.e. removing the term f2(x) = f2(x\u2032)) according to Eq. (3.3)).\nThe \u201cstrong-robustness\u201d definition leads to four important theorems in next two subsections.\n4Notice that g1 may also include implicit feature selection steps like `1 regularization. 5Boundary points are those points satisfying f1(x) 6= f1(x\u2032) and d1(g1(x), g1(x\u2032)) < \u03b41) 6When f1 is continuous a.e., P(f1(x) 6= f1(x\u2032)|d1(g1(x), g1(x\u2032)) < \u03b41) = 0. 7\u201cBoundary points based adversarial examples\u201d only attack seed samples who are boundary points of f1.\n3.3 TOPOLOGICAL EQUIVALENCE OF TWO METRIC SPACES (X1, d1) AND (X2, d2) IS SUFFICIENT IN DETERMINING STRONG-ROBUSTNESS\nIn the appendix, Section 10.1 briefly introduces the concept of metric space and the definition of topological equivalence among two metric spaces. As shown in Figure 1, here f1 defines a metric space (X1, d1) on X1 with the metric function d1. Similarly f2 defines a metric space (X2, d2) on X2 with the metric function d2.\nIf the topological equivalence ( Eq. (10.1)) exists between (X1, d1) and (X2, d2), it means that for all pair of samples from X , we have the following relationship:\n\u2200x, x\u2032 \u2208 X, d1(g1(x), g1(x \u2032)) < \u03b41 \u21d4 d2(g2(x), g2(x\u2032)) < \u03b42 (3.4)\nWhen f1 is continuous a.e., this can get us the following important theorem, indicating that the topological equivalence between (X1, d1) and (X2, d2) is a sufficient condition in determining whether or not f1 is strong-robust against adversarial examples:\nTheorem 3.2. When f1 is continuous a.e., if (X1, d1) and (X2, d2) are topologically equivalent, then the learned classifier f1(\u00b7) is strong-robust to adversarial examples.\nProof. See its proofs in Section 10.3.4\nThis theorem can actually guarantee that: \u2200x, x\u2032 \u2208 X, P(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032),\nd2(g2(x), g2(x \u2032)) < \u03b42) = 1\n(3.5)\nClearly Eq. (3.5) is a special (stronger) case of the \u201cstrong-robustness\u201d defined by Eq. (3.2).\nFor more general cases including f1 might not be continuous a.e., we need to consider the probability of the boundary point attacks (Eq. (3.1)). Therefore, we get a more general theorem as follows:\nTheorem 3.3. If (X1, d1) and (X2, d2) are topologically equivalent and P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x \u2032), d1(g1(x), g1(x \u2032)) < \u03b41, d2(g2(x), g2(x\n\u2032)) < \u03b42) < \u03b7, then the learned classifier f1(\u00b7) is strong-robust to adversarial examples.\nProof. See its proofs in Section 10.3.3.\n3.4 FINER TOPOLOGY OF (X, d\u20321) THAN (X, d \u2032 2) IS SUFFICIENT AND NECESSARY IN\nDETERMINING STRONG-ROBUSTNESS\nNow we extend the discussion from two metric spaces into two pseudometric spaces. This extension finds the sufficient and necessary condition that determines the strong-robustness of f1. The related two pseudometrics are d\u20321 (for f1) and d \u2032 2 (for f2), both directly being defined on X . Appendix Section 10.2 includes detailed descriptions of pseudometric, pseudometric spaces, topology and a finer topology relationship between two pseudometric spaces.\nEssentially, the topology in pseudometric space (X, d\u20321)) is a finer topology than the topology in pseudometric space (X, d\u20322) means: \u2200x, x\u2032 \u2208 X, d\u20322(x, x\u2032) < \u03b42 \u21d2 d\u20321(x, x\u2032) < \u03b41 (3.6) Because d\u20321(x, x \u2032) = d1(g1(x), g1(x \u2032)) and d\u20322(x, x \u2032) = d2(g2(x), g2(x \u2032)), the above equation equals to: \u2200x, x\u2032 \u2208 X, d2(g2(x), g2(x \u2032)) < \u03b42 \u21d2 d1(g1(x), g1(x\u2032)) < \u03b41 (3.7)\nUsing Eq. (3.7) and the continuity a.e. assumption, we can derive the following Theorem about the sufficient and necessary condition for f1 being strong-robust:\nTheorem 3.4. When f1 is continuous a.e., f1 is strong-robust against adversarial examples if and only if the topology in (X, d\u20321) is a finer topology than the topology in (X, d \u2032 2).\nProof. See its proof in appendix Section 10.3.1.\nActually the above theorem can guarantee that when f1 is continuous a.e.: \u2200x, x\u2032 \u2208 X,P(f1(x) = f1(x\u2032)|d2(g2(x), g2(x\u2032)) < \u03b42) = 1 (3.8) Eq. (3.8) clearly is a special (stronger) case of strong-robustness defined by Eq. (3.2).\nWhen f1 is not continuous a.e., we need to consider the probability of the boundary points based adversarial examples (Eq. (3.1)). For such a case, we get a sufficient condition 8 for the strongrobustness: Theorem 3.5. When f1 is not continuous a.e., if the topology in (X, d\u20321)) is a finer topology than the topology in (X, d\u20322) and P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d1(g1(x), g1(x\u2032)) < \u03b41, d2(g2(x), g2(x \u2032)) < \u03b42) < \u03b7, then f1 is strong-robust against adversarial examples.\nWhen f1 is not continuous a.e., its strong-robustness is significantly influenced by its boundary points and therefore relates to the c1 function. Section 11.2 provides some discussion and we omit covering such cases in the rest of this paper."}, {"heading": "4 TOWARDS PRINCIPLED UNDERSTANDING", "text": "The four theorems proposed above lead to a set of key insights about why and how an adversarial can fool a machine-learning classifier using adversarial examples. One of the most valuable insights is: feature learning step decides whether a predictor is strong-robust or not in an adversarial test setting. All the discussions in the subsection assume f1 is continuous a.e.."}, {"heading": "4.1 UNNECESSARY FEATURES RUIN STRONG-ROBUSTNESS", "text": "Theorem (3.2) and Theorem (3.4) indicate that when f1 is continuous a.e., the two feature spaces (X1, d1) and (X2, d2) or the functions g1 and g2 determine the strong-robustness of f1. Based on Theorem (3.4), we can derive a corollary as follows (proof in Section 10.3.1): Corollary 4.1. When f1 is continuous a.e., if X1 = Rn1 , X2 = Rn2 , n1 > n2, X2 ( X1, d1, d2 are norm functions, then f1(\u00b7) is not strong-robust against adversarial examples.\nThis corollary shows if unnecessary features (with regards to X2) are selected in the feature selection step, then no matter how accurate the model is trained, it is not strong-robust to adversarial examples.\nFigure 2 shows a situation that the oracle for the current task only needs to use one feature to classify samples correctly. A machine learning classifier extracts two features with one used by the oracle and the other is an extra unnecessary feature 9. In X1, f1 (actually c1) successfully classifies all the test inputs. However, it\u2019s very easy to find adversary examples satisfying Eq. (2.4) by only adding a small perturbation along the unnecessary feature dimension. In Figure 2, red circles show a few such adversarial examples. The adversarial examples are very close to seed samples in the oracle space. But they are predicted into a different class by f1.\nFor many security sensitive applications, previous studies using state-of-art learning-based classifiers normally believe that adding more features is always helpful. Apparently, our corollary indicates that\n8When f1 is not continuous a.e., it is difficult to find the necessary and sufficient condition for strongrobustness of f1. We leave this to future research.\n9Two features of X1 actually positively correlate in Figure 2. However, the oracle does not need to use the second feature for making classification decision\nthis thinking is wrong and can lead to their classifiers vulnerable to adversarial examples(Xu et al., 2016).\nAs another example, multiple DNN studies about adversarial examples claim that adversarial examples are transferable among different DNN models. This can be explained by Figure 2 (when X1 is a much higher-dimensional space). Since different DNN models learn over-complete feature spaces {X1}, there is a high chance that these different X1 involve a similar set of unnecessary features (e.g., the different learned features are correlated with others). Therefore the adversarial examples are generated along similar gradient directions. That is why many such samples can evade multiple DNN models."}, {"heading": "4.2 FEATURE SPACE MORE IMPORTANT THAN NORM", "text": "Using Theorem (3.3), we obtain another corollary as follows (proof in Section 10.3.1):\nCorollary 4.2. When f1 is continuous a.e., if d1 and d2 are norms and X1 = X2 = Rn, then f1(\u00b7) is strong-robust to adversarial examples.\nThis corollary shows that if a learned classifier and its oracle share the same derived feature space (X1 = X2), the learned classifier is strong-robust when two metrics are both norm functions (even if not the same norm). We can call this corollary as \u201cnorm doesn\u2019t matter\u201d.\nMany interesting phenomena can be answered by Corollary (4.2). For instance, for a norm regularized classifier, this corollary answers an important question that whether a different norm function will influence its robustness against adversarial examples. The corollary indicates that changing to a different norm function may not improve the robustness of the model under adversarial perturbation.\nSummarizing Theorem (3.2), Theorem (3.4), Corollary (4.2) and Corollary (4.1), the robustness of a learned classifier is decided by two factors: (1) the difference between two derived feature spaces; and (2) the difference between the metric functions. Two corollaries show that the difference between the feature spaces is more important than the difference between the two metric functions."}, {"heading": "4.3 ROBUSTNESS AND GENERALIZATION", "text": "In Table 3, we provide four situations in which the proposed theorems can be used to determine whether a classifier f1 is strong-robust against adversarial examples or not.\n\u2022 Case (I): If f1 uses some unnecessary features, it will not be strong-robust to adversarial examples. It may not be an accurate predictor if f1 misses some necessary features used by f2. \u2022 Case (II): If f1 uses some unnecessary features, it will not be strong-robust to adversarial examples. It may be an accurate predictor if f1 uses all the features used by f2. \u2022 Case (III): If f1 and f2 use the same set of features and nothing else, f1 is strong-robust and may be accurate. \u2022 Case (IV): If f1 misses some necessary features and does not extract unnecessary features, f1 is strong-robust (even tough its accuracy may not be good).\nTable 3 provides a much better understanding of the relationship between robustness and accuracy. Two interesting cases from Table 3 are worth to emphasize again: (1) If f1 misses features used by f2 and does not include unnecessary features (according to X2), f1 is strong-robust (even though it may not be accurate). (2) If f1 extracts some extra unnecessary features, it will not be strong-robust (though it may be a very accurate predictor).\nWe want to emphasize that \u201cf1 is strong-robust\u201d does not mean it is a good classifier. For example, a trivial example for strong-robust models is f1(x) \u2261 1,\u2200x \u2208 X . However, it is a useless model since it doesn\u2019t have any prediction power. In an adversarial setting, we should aim to get a classifier that is both strong-robust and precise. A better feature learning function g1 is exactly the solution that may achieve both goals.\nTable 3 indicates that c1 and c2 do not influence the strong-robustness of f1 when f1 is continuous a.e. 10. Figure 4 and Figure 5 further show two concrete example cases in which f1 is strong-robust according to f2. However, in both figures, f1 is not accurate according to f2.\n10When f1 is not continuous a.e., c1 matters for the strong-robustness. See Section 11 for details."}, {"heading": "5 TOWARDS PRINCIPLED SOLUTIONS FOR DNNS", "text": "Our theoretical analysis uncovers fundamental properties to explain the adversarial examples. In this section, we apply them to analyze DNN classifiers. More specifically, (1) we find that DNNs are not strong-robust against adversarial examples; and (ii) we connect to possible hardening solutions and introduce principled understanding of these solutions."}, {"heading": "5.1 ARE STATE-OF-THE-ART DNNS STRONG-ROBUST?", "text": "For DNN, it is difficult to derive a precise analytic form of d1 (or d\u20321). But we can observe some properties of d1 through experimental results. Table 5,Table 6,Table 7 and Table 8 show properties of d1 (and d\u20321) resulting from performing testing experiments on four state-of-art DNN networks (details in Section 12.1). All four tables indicate that the accuracy of DNN models in the adversarial setting are quite bad. The performance on randomly perturbed inputs is much better than performance on maliciously perturbed adversarial examples.\nThe phenomenon we observed can be explained by Figure 3. Comparing the second column and the third column in four tables we can conclude that d1 (and d\u20321) in a random direction is larger than d1 (and d\u20321) in the adversarial direction. This indicates that a round sphere in (X1, d1) (and (X, d\u20321)) corresponds to a very thin high-dimensional ellipsoid in (X, || \u00b7 ||) (illustrated by the left half of Figure 3). Figure 3 (I) shows a sphere in (X, d\u20321) and Figure 3 (III) shows a sphere in (X1, d1). They correspond to the very thin high-dimensional ellipsoid in (X, || \u00b7 ||) in Figure 3 (V). The norm function || \u00b7 || is defined in space X and is application-dependent. All four tables uses || \u00b7 || = || \u00b7 ||\u221e. Differently, for human oracles, a sphere in (X, d\u20322) (shown in Figure 3 (II)) or in (X2, d2) (shown in Figure 3 (IV)) corresponds to an ellipsoid in (X, || \u00b7 ||) not including very-thin directions (shown in Figure 3 (VI)). When the attackers try to minimize the perturbation size using the approximated distance function d2 = || \u00b7 ||, the thin direction of ellipsoid in Figure 3 (V) is exactly the adversarial direction."}, {"heading": "5.2 TOWARDS PRINCIPLED SOLUTIONS", "text": "Our theorems suggest a list of possible solutions that may improve the robustness of DNN classifiers against adversarial samples. Options include such as:\nBy learning a better g1: Methods like DNNs directly learn the feature extraction function g1. Table 4 summarizes multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) in the DNN literature. They mostly aim to learn a better g1 by minimizing different loss functions Lf1(x, x\n\u2032) so that when d2(g2(x), g2(x\u2032)) < (approximated by (X, || \u00b7 ||)), this loss Lf1(x, x\u2032) is small. Two major variations exist among related methods: the choice of Lf1(x, x\n\u2032) and the way to generate pairs of (x, x\u2032). For instance, to reach the strong-robustness we can force to learn a g1 that helps (X, d\u20321) to be a finer topology than (X2, d \u2032 2). Section 12.4 explores this option (\u201cSiamese training\u201d in Table 4) through Siamese architecture. Experimentally Section 12.5 compares adversarial training, stability training and Siamese training on two state-of-the-art DNN image-classification\ntasks through performance against adversarial samples (details in Section 12.5). The hardening effects of these strategies vary from task to task, however, they all improve the base DNN models\u2019 performance in the adversarial setting.\nBy modifying unnecessary features: As shown by Table 3, unnecessary features ruin the strongrobustness of learning-based classifiers. A simple way to remove the unrelated features is to identify which feature is unnecessary. In (Gao et al., 2017) the authors compare the difference between g1(x\n\u2032) and g1(x) from DNN. They hypothesize that those learned DNN feature dimensions (in X1) changing rapidly are utilized by an adversary, and thus can be removed to improve the robustness of DNN model. Another efficient method is to substitute different values of features into several equivalent classes. By this way, the adversarial perturbation in the unnecessary feature dimensions can be squeezed by projecting into the same equivalent class. A recent study (Li & Vorobeychik, 2014) explored a similar strategy by using equivalent-feature-group to replace each word feature in a group, in order to improve the robustness of spam-email classifiers against evasion attacks."}, {"heading": "6 CONCLUSION", "text": "Adversarial examples are maliciously created inputs that lead a learning-based classifier to produce incorrect output labels. An adversarial example is often generated by adding small perturbations that appear unmodified to human observers. Recent studies that tried to analyze classifiers under adversarial examples are mostly empirical and provide little understanding of why. To fill the gap, we propose a theoretical framework for analyzing machine learning classifiers, especially deep neural networks (DNN) against such examples. This paper is divided into three parts. The first section provides a revised definition of adversarial examples by taking into account of the oracle of the task. The second section defines strong-robustness and provides the principled understanding of what makes a classifier strong-robust. The third section examines practical and theoretically grounded directions for understanding and hardening DNN models against adversarial examples. Future steps will include an empirical comparison to analyze recent literature using our theorems."}, {"heading": "7 RELATED WORKS IN A BROADER CONTEXT", "text": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015). Recent studies can be roughly categorized into three types: (1) Poisoning attacks in which specially crafted attack points are injected into the training data. Multiple recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models. (2) Evasion attacks are attacks in which the adversary\u2019s goal is to create inputs that are misclassified by a deployed target classifier. Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest. (3) Privacy-aware machine learning (Duchi et al., 2014) is another important category relevant to data security in machine learning systems. Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy. This paper focuses on evasion attacks that are mostly used to attacking classifiers that try to distinguish malicious behaviors from benign behaviors. Here we extend it to a broader meaning \u2013 adversarial manipulation of test samples. Evasion attacks may be encountered during system deployment of machine learning methods in adversarial settings.\nIn the broader secure machine learning field, researchers also make attempts for hardening learning systems. For instance: (1) (Barreno et al., 2010) and (Biggio et al., 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al., 2015; Zhang et al., 2015) consider the impact of using reduced feature sets on classifiers under adversarial attacks. (Xiao et al., 2015) proposes an adversary-aware feature selection model that can improve a classifier\u2019s robustness against adversarial attacks by incorporating specific assumptions about the adversary\u2019s data manipulation strategy. (3) Another line of works, named as adversarial training (Goodfellow et al., 2014), designs a new loss function for training neural networks, which is a linear interpolation of the loss function of the original sample and the loss function of the adversarial example generated by the original sample. A scalable version of adversarial training (Kurakin et al., 2016) was recently proposed. By applying several tricks, the author can apply the adversarial training to deeper network trained by the imagenet dataset. (4) Multiple studies model adversarial scenarios with formal frameworks representing the interaction between the classifier and the adversary. Related efforts include perfect information assumptions (Dalvi et al., 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing the attack process as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst perturbation) (Dekel et al., 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997). (5) A PAC model study about learning adversary behavior in a security games also investigated the solution of computing the best defender strategy against the learned adversary behavior. It has a\nsimilar conclusion as ours ( Section 3) that the extreme cases that the defender doesn\u2019t work only has zero probability (Sinha et al., 2016)."}, {"heading": "8 FORMULATION OF ADVERSARIAL EXAMPLES", "text": ""}, {"heading": "8.1 MORE ABOUT PREVIOUS DEFINITIONS OF ADVERSARIAL EXAMPLES", "text": "For the purpose of \"fooling\" a classifier, naturally, the attacker wants to control the size of the perturbation \u2206(x, x\u2032) to ensure the perturbed sample x\u2032 still stays close enough to the original sample\nx to satisfy the intended \"fooling\" purpose. For example, in the image classification case, Eq. (2.1) can use the gradient information to find a \u2206(x, x\u2032) that makes human annotators still recognize x\u2032 as almost the same as x, though the classifier will predict x\u2032 into a different class. In another example with more obvious security implications about PDF malware (Xu et al., 2016), x\u2032 in Eq. (2.1) is found by genetic programming. A modified PDF file from a malicious PDF seed will still be recognized as malicious by an oracle machine (i.e., a virtual machine decides if a PDF file is malicious or not by actually running it), but are classified as benign by state-of-art machine learning classifiers (Xu et al., 2016).\nThe following Eq. (8.1) has been popular as well. Eq. (8.1) is a special case of Eq. (2.1). argmin x\u2032\u2208X \u2206(x, x\u2032)\nSubject to: f1(x) 6= f1(x\u2032) (8.1)\nEq. (8.1) tries to find the x\u2032 by minimizing \u2206(x, x\u2032) under some constraints. Eq. (2.1) is a more general formulation than Eq. (8.1) and can summarize most relevant studies. For example, in (Xu et al., 2016) \"adversarial examples\" are those generated PDFs that can fool PDFRate (a learning-based classifier for detecting malicious PDFs) to classify them as benign. The distances of these variant PDFs to the seed PDF are not necessarily minimal. For such cases, Eq. (2.1) still fits, while Eq. (8.1) does not.\nBesides, in the field of computer security, machine learning has been popular in classifying the malicious (y = 1) behavior versus benign behavior (y = \u22121). For such a context, two different definitions of adversarial examples exist in the literature:\nFor instance, (Biggio et al., 2013) uses a formula as follows: argmin\nx\u2032 (f1(x\n\u2032))\ns.t. \u2206(x, x\u2032) < dmax f1(x) > 0\n(8.2)\nDifferently, (Lowd & Meek, 2005) uses the following formula: argmin\nx\u2032 (\u2206(x, x\u2032))\ns.t. f1(x\u2032) < 0 f1(x) > 0\n(8.3)\nHere dmax is a small positive constant. These definitions of \u201cadversarial examples\u201d are special cases of Eq. (8.1) and Eq. (2.1)."}, {"heading": "8.2 BACKGROUND: PREVIOUS ALGORITHMS GENERATING \u201cADVERSARIAL EXAMPLES\u201d", "text": "To fool classifiers at test time, several approaches have been implemented to generate \u201cadversarial perturbations\u201d by solving Eq. (2.2). According to Eq. (2.2), an adversarial example should be able to change the classification result f1(x), which is a discrete value. To solve Eq. (2.2), we need to transform the constraint f1(x) 6= f1(x\u2032) into an optimizable formulation. Then we can easily use the Lagrangian multiplier to solve Eq. (2.2). All the previous studies define a loss function Loss(\u00b7, \u00b7) to quantify the constraint f1(x) 6= f1(x\u2032). This loss function can be the same with the training loss, or it can be chosen differently, such as hinge loss or cross entropy loss.\nWe summarize four common attacking studies as follows:\nGradient ascent method (Biggio et al., 2013) Machine learning has been popular in classifying malicious (y = 1) versus benign (y = \u22121) in computer security tasks. For such contexts, a simple way to solve Eq. (2.2) is through gradient ascent. To minimize the size of the perturbation and maximize the adversarial effect, the perturbation should follow the gradient direction (i.e., the direction providing the largest increase of function value, here from y = \u22121 to 1). Therefore, the perturbation r in each iteration is calculated as: r = \u2207xLoss(f1(x+ r),\u22121) Subject to: f1(x) = 1 (8.4) By varying , this method can find a sample x\u2032 with regard to d2(x, x\u2032) such that f1(x) 6= f1(x\u2032).\nBox L-BFGS adversary (Szegedy et al., 2013) This study views the adversarial problem as a constrained optimization problem, i.e., find a minimum perturbation in the restricted sample space. The perturbation is obtained by using Box-constrained L-BFGS to solve the following equation:\nargmin r\n(c\u00d7 d2(x, x+ r) + Loss(f1(x+ r), l)), x+ r \u2208 [0, 1]p (8.5)\nHere p is the total number of features, c is a term added for the Lagrange multiplier. (for an image classification task, it is 3 times the total number of pixels of an RGB image) l is a target label, which is different from the original label. The constraint x+ r \u2208 [0, 1]p means that the adversarial example is still in the range of sample space.\nFast gradient sign method (Goodfellow et al., 2014) The fast gradient sign method proposed by (Goodfellow et al., 2014) views d2 as the `\u221e-norm. In this case, a natural choice is to make the attack strength at every feature dimension the same. The perturbation is obtained by solving the following equation:\nargmin r (c\u00d7 d2(x, x+ r)\u2212 Loss(f1(x+ r), f1(x))), x+ r \u2208 [0, 1]p (8.6) Therefore the perturbation can be calculated directly by: r = sign(\u2207zLoss(f1(z), f1(x))) (8.7) Here the loss function is the function used to train the neural network. A recent paper (Kurakin et al., 2016) shows that adversarial examples generated by fast gradient sign method are misclassified even after these images have been recaptured by cameras.\nJacobian-based saliency map approach (Papernot et al., 2015a) (Papernot et al., 2015a) proposed the Jacobian-based saliency map approach (JSMA) to search for adversarial samples while limiting the number of pixel to modify in the image. As a targeted attack, JSMA iteratively perturbs pixels in an input that have large adversarial saliency scores. The adversarial saliency map is calculated from the Jacobian (gradient) matrix \u2207xf1(x) of the DNN model at the current input x. The (i, j)th component in Jacobian matrix \u2207xf1(x) describes the derivative of output class j with respect to feature pixel i. For each pixel i, its adversarial saliency score is calculated to reflect how this pixel will increase the output score of class j versus changing the score of other possible output classes. The process is repeated until misclassification in the target class is achieved or the maximum number of perturbed pixels has been reached. Essentially, JSMA optimizes Equation 2.1 by measuring perturbation \u2206(x,x\u2032) through the `0-norm.\n8.3 MORE ABOUT MODELING ORACLE f2\nThough difficult, we want to argue that it is possible to theoretically model \"oracles\" for some state-of-the-art applications. For instance, as illustrated by the seminal cognitive neuroscience paper \"untangling invariant object recognition\" (DiCarlo & Cox, 2007) and its follow-up study (DiCarlo et al., 2012), the authors show that one can view the information processing of visual object recognition by human brains as the process of finding operations that progressively transform retinal representations into a new form of representation (X2 in this paper), followed by the application of relatively simple decision functions (e.g., linear classifiers (Duda et al., 2012)). More specifically, in human and other primates, such visual recognition takes place along the ventral visual stream, and this stream is considered to be a progressive series of visual re-representations, from V1 to V2 to V4 to IT cortex (DiCarlo & Cox, 2007). Multiple relevant studies (e.g., (DiCarlo & Cox, 2007; Johnson, 1980; Hung et al., 2005)) have argued that this viewpoint of representation learning plus simple decision function is more productive than hypothesizing that brains directly learn very complex decision functions (highly non-linear) that operate on the retinal image representation. This is because the experimental evidence suggests that this view takes the problem apart in a way that is consistent with the architecture and response properties of the ventral visual stream. Besides, simple decision functions can be easily implemented in a single step of biologically plausible neuronal processing (i.e., a thresholded sum over weighted synapses).\nAs another example, the authors of (Xu et al., 2016) used genetic programming to find \u201cadversarial examples\u201d (by solving Eq. (2.2)) for a learning-based malicious-PDF classifier. This search needs an oracle to determine if a variant x\u2032 preserves the malicious behavior of a seed PDF x (i.e., f2(x) = f2(x\n\u2032)). The authors of (Xu et al., 2016) therefore used the Cuckoo sandbox (a malware analysis system through actual execution) to run a variant PDF sample in a virtual machine installed with a PDF reader and reported the behavior of the sample including network APIs calls. By comparing the\nbehavioral signature of the original PDF malware and the manipulated variant, this oracle successfully determines if the malicious behavior is preserved from x to x\u2032. One may argue that \"since Cuckoo sandbox works well for PDF-malware identification, why a machine-learning based detection system is even necessary?\". This is because Cuckoo sandbox is computationally expensive and runs slow. For many security-sensitive applications about machines, oracles f2 do exist, but machine-learning classifiers f1 are used popularly due to speed or efficiency.\n8.4 MORE ABOUT MODELING f1: DECOMPOSITION OF g1 AND c1\nIt is difficult to decompose an arbitrary f1 into g1 \u25e6 c1. However, since in our context, f1 is a machine learning classifier, we can enumerate many possible g1 functions to cover classic machine learning classifiers.\n\u2022 Various feature selection methods are potential g1. \u2022 For DNN, g1 includes all the layers from input layer to the layer before the classification layer. \u2022 In SVM, X1, d1 is decided by the chosen reproducing Hilbert kernel space. \u2022 Regularization is another popular implicit feature extraction method. For example, `1 regularization\ncan automatically do the feature extraction by pushing some parameters to be 0."}, {"heading": "9 ASSUMPTION: ALMOST EVERYWHERE (A.E.) CONTINUITY", "text": "Most previous studies (Table 2) have made an important and implicit assumption about f1 and f2: fi is almost everywhere (a.e.) continuous. i \u2208 {1, 2}. Definition 9.1. Suppose fi is the classification function. fi is continuous a.e., i \u2208 {1, 2}, if \u2200x \u2208 X a.e., \u2203\u03b4i > 0, such that \u2200x\u2032 \u2208 X, di(gi(x), gi(x\u2032)) < \u03b4i, fi(x) = fi(x\u2032).\nIllustrated in Figure 1, di is the metric function (details in Section 3) fi uses to measure the similarity among samples in the space Xi. For notation simplicity, we use the term \u201ccontinuous a.e.\u201d for \u201ccontinuous almost everywhere\u201d11 in the rest of the paper. The above definition is a special case of almost everywhere continuity defined in (Folland, 2013) (see Definition (9.2) in Section 9.1), since we decompose fi in a certain way (see Figure 1). The a.e. continuity has a few indications, like: \u2200x, x\u2032 \u2208 X, P(fi(x) 6= fi(x\u2032)|di(gi(x), gi(x\u2032)) < \u03b4i) = 0 (9.1) See Section 9.1 for details of indications by a.e. continuity.\nf2 is assumed continuous a.e. previously: Most previous studies find \"adversarial examples\" by solving Eq. (2.1), instead of Eq. (2.2). This made an implicit assumption that if the adversarial example x\u2032 is similar to the seed sample x, they belong to the same class according to f2. This assumption essentially is: f2 is almost everywhere (a.e.) continuous.\nf1 is continuous a.e.: Almost all popular machine learning classifiers satisfy the a.e. continuity assumption. For instance, a deep neural network is certainly continuous a.e.. Similarly to the results shown by (Szegedy et al., 2013), DNNs satisfy that |f1(x) \u2212 f1(x\u2032)| \u2264 W \u2016 x \u2212 x\u2032 \u20162 where W \u2264 \u220f Wi and Wi \u2265 ||(wi, bi)||\u221e. Here i = {1, 2, . . . , L} representing i-th linear layer in NN. Therefore, \u2200 > 0, let \u03b4 = /W . Then |f1(x)\u2212 f1(x\u2032)| < when d1(x, x\u2032) =\u2016 x\u2212 x\u2032 \u20162< \u03b4. This shows that a deep neural network is almost everywhere continuous when d1(\u00b7) = || \u00b7 ||2. In Section 9.1, we show that if f1 is not continuous a.e., it is not robust to any types of noise. Considering the generalization assumption of machine learning, machine learning classifiers should satisfy the continuity a.e. assumption. Section 9.2 provides two examples of how popular machine learning classifiers satisfy this assumption.\nFor the rare cases when f1 is not continuous a.e., see next Section 11 discussing \"boundary points\" that matter for analyzing adversarial perturbations."}, {"heading": "9.1 INDICATIONS FROM A.E.CONTINUITY ASSUMPTION", "text": "The a.e. continuity has a few indications,\n\u2022 X is not a finite space; and \u2200x, x\u2032 \u2208 X , P(fi(x) = fi(x\u2032)|di(gi(x), gi(x\u2032)) < \u03b4i) = 1 \u2022 It does not mean the function f1 is continuous in every point in its feature space X;\n11The measure (e.g., Lebesgue measure) of discontinuous set is 0.\n\u2022 If a probability distribution admits a density, then the probability of every one-point set {a} is zero; the same holds for finite and countable sets and the same conclusion holds for zero measure sets 12, for instance, straight lines or circle in Rn. \u2022 The a.e. continuity follows the same property as density function: the probability of picking one-point set {x} from the whole feature space is zero; the same holds for zero measure sets. This means: the probability of picking the discontinuous points (e.g., points on the decision boundary) is zero, because they are null sets. \u2022 Most machine learning methods focus on X = Rp or space equivalent to Rp (e.g., [0, 1]p) (see Appendix: Section 11.1). Most machine learning methods assume f1 is continuous a.e. (see Appendix: Section 9.2).\nDefinition 9.2. Suppose (X,F ,P) is a probability space(for general definition, (X,\u03a3, \u00b5) is a measure space), where X is the sample space, a \u03c3-algebra F is a collection of all the events and P is a probability measure defined in X and F . A property holds \u201calmost everywhere\u201d (a.e.) in X if and only if the probability measure of the set for which the property holds equals 1.\nLemma 9.3. If the a.e. continuity assumption doesn\u2019t hold, there exists a non-zero measure set D, such that\n\u2200x \u2208 D,\u2203x\u2032\ns.t. f1(x) 6= f1(x\u2032) d1(x, x \u2032) < \u03b41\n(9.2)\nProof. Without it, for any test sample x, you can easily find a very similar sample x\u2032 (i.e. for any small \u03b41, d1(x, x\u2032) < \u03b41) such that |f1(x)\u2212 f1(x\u2032)| > . In classification problems, this means that f1(x) 6= f1(x\u2032)(i.e. there exist very similar pair of two samples x and x\u2032 that have different labels for most x \u2208 X1).\nThe Lemma (9.3) shows that f1 is not robust to a random noise if we don\u2019t assume f1 is continuous."}, {"heading": "9.2 MOST MACHINE-LEARNING CLASSIFIERS SATISFY THE A.E. CONTINUITY ASSUMPTION", "text": "Almost all popular machine learning classifiers satisfy the a.e. continuity assumption. For example,\n\u2022 Logistic regression for text categorization with a bag of word representation. A classifier on a multivariate feature representation in which each feature representing (modified) counts of a word is naturally a.e. continuous. Since {x\u2032|d1(x, x\u2032) < \u03b41, x 6= x\u2032} = \u2205 when \u03b41 is small and x, x\u2032 are mostly sparse vectors. Logistic regression with a bag of word representation is a continuous a.e. predictor. \u2022 Support Vector Machine with continuous feature representation. Suppose we define (X1, d1) by the d21(x, x\n\u2032) = k(x, x) + k(x\u2032, x\u2032) \u2212 2k(x, x\u2032). Then support vector machine is a linear classifier on (X1, d1). Thus, the SVM prediction function is continuous a.e. with d1.\nMost machine learning methods focus on the Rn space or the space equivalent to Rn (e.g., [0, 1]n). For example, the sample space of image classification task intuitively is 255p, where p is the number of features (e.g., 3\u00d7 224\u00d7 224). However, people mostly rescale the raw image data samples into X = [0, 1]p. Therefore, the sample space X for f1 for this case is [0, 1]p."}, {"heading": "10 APPENDIX: USING METRIC SPACE AND PSEUDO METRIC SPACES TO UNDERSTAND CLASSIFIERS\u2019 ROBUSTNESS AGAINST ADVERSARIAL EXAMPLES", "text": ""}, {"heading": "10.1 METRIC SPACES AND TOPOLOGICAL EQUIVALENCE OF TWO METRIC SPACES", "text": "This subsection briefly introduces the concept of metric space and topological equivalence. A metric on a set/space X is a function d : X \u00d7X \u2192 [0,\u221e] satisfying four properties: (1) non-negativity, (2) identity of indiscernibles, (3) symmetry and (4) triangle inequality. In machine learning, for example, the most widely used metric is Euclidean distance. Kernel based methods, such as SVM, kernel\n12Zero measure sets: also named as\"Null set\": https://en.wikipedia.org/wiki/Null_set\nregression and Gaussian process, consider samples in a Reproducing kernel Hilbert space (RKHS). The metric in a RKHS is naturally defined as: d2(x, y) = K(x, x) +K(y, y)\u2212 2K(x, y), in which K(\u00b7, \u00b7) is a kernel function. Now we present an important definition, namely that of \u201ctopological equivalence\u201d, that can represent a special relationship between two metric spaces.\nDefinition 10.1. Topological Equivalence (Kelley, 1975)"}, {"heading": "A function or mapping h(\u00b7) from one topological space to another is continuous if the inverse image", "text": "of any open set is open. If this continuous function is one-to-one and onto, and the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function, in our case (X1, d1), is said to be homeomorphic to the output range, e.g., here (X2, d2). In other words, metric space (X1, d1) is topologically equivalent to the metric space (X2, d2).\nWe can state this definition as the following equation: \u2203h : X1 \u2192 X2,\u2200x1, x\u20321 \u2208 X1, h(x1) = x2, h(x \u2032 1) = x \u2032 2\nd1(x1, x \u2032 1) < \u03b41 \u21d4 d2(x2, x\u20322) < \u03b42\n(10.1)\nHere h is continuous, one-to-one and onto. \u03b41 and \u03b42 are two small constants."}, {"heading": "10.2 PSEUDOMETRIC SPACES AND FINER TOPOLOGY AMONG PSEUDOMETRIC SPACES", "text": "We have briefly reviewed the concept of metric space in Section 10.1 and proposed the related Theorem (3.2) in Section 3.3. This is partly because the concept of metric space has been widely used in many machine learning models, such as metric learning (Xing et al., 2003). Theorem (3.2) and related analysis indicate that feature spaces X1 and X2 (See Figure 1) are key determining factors for deciding learning model\u2019s strong-robustness.\nHowever, it is difficult to get the analytic form of X2 in most applications (e.g., when an oracle f2 is a human annotator). In fact, most previous studies (reviewed in Section 2.2) assume (X2, d2) equals to (X, || \u00b7 ||), where || \u00b7 || is a norm function. Therefore, we want to extend our analysis and results from the implicit feature space X2 to the original feature space X .\nWhen we extend the analysis to the original space X , it is important to point out that the distance function measuring sample similarity for a learned predictor f1 in the original space X may not be a metric. The distance function in the original feature space X for oracle f2 may not be a metric as well. This is because the distance between two different samples in the original space X may equal to 0. Because two different samples may be projected into the same point in X1 or X2. For example, a change in one pixel of background in an image does not affect the prediction of f1 or f2 since the g1 and g2 have already eliminated that (irrelevant) feature. This property contradicts the identity of indiscernibles assumption for a metric function. Therefore we need a more general concept of the distance function for performing theoretical analysis in the original space X . By using the concept of Pseudometric Space13, we derive another important theorem about strong-robustness.\nPseudometric: If a distance function d\u2032 : X \u00d7X \u2192 [0,\u221e] has the following three properties: (1) non-negativity, (2) symmetry and (3) triangle inequality, we call d is a pseudometric or generalized metric. The space (X, d\u2032) is a pseudometric space or generalized metric space. It is worth to point out that the generalized metric space is a special case of topological space and metric space is a special case of pseudometric space.\nWhy Pseudometric Space: As shown in Figure 1, we can decompose a common machine learning classifier f1 = c1 \u25e6 g1, where g1 : X \u2192 X1 represents the feature extraction and c1 : X1 \u2192 Y performs the operation of classification. Assume there exists a pseudometric d\u20321(\u00b7, \u00b7) on X and a metric d1(\u00b7, \u00b7) defined on X114, so that \u2200x, x\u2032 \u2208 X ,\nd\u20321(x, x \u2032) = d1(g1(x), g1(x \u2032)). (10.2) Since d1 is a metric in X1, d\u20321 fulfills the (1) non-negativity, (2) symmetry and (3) triangle inequality properties. However, d\u20321 may not satisfy the identity of indiscernible property (i.e., making it not a\n13The crucial problem of the original sample space X is that it\u2019s difficult to strictly define a metric on the original feature space.\n14d1(\u00b7, \u00b7) on X1 satisfies four properties:(1) non-negativity, (2) identity of indiscernibles, (3) symmetry and (4) triangle inequality.\nmetric). For example, suppose g1 only selects the first three features from X . Two samples x and x\u2032 have the same value in the first three features but different values in the rest features. Clearly, x 6= x\u2032, but d\u20321(x, x \u2032) = d1(g1(x), g1(x \u2032)) = 0. This shows that d\u20321(\u00b7, \u00b7) is a pseudometric but not a metric in X . Similarly, a pseudometric d\u20322 for the oracle can be defined as follow: d\u20322(x, x \u2032) = d2(g2(x), g2(x \u2032)). (10.3)\nTo analyze the strong robustness problem in the original feature space X , we assume it to be a generalized metric (pseudometric) space (X, d\u20321) for f1 and a generalized metric (pseudometric) space (X, d\u20322) for f2. Now we can analyze f1 and f2 on the same feature space X but relate to two different pseudometrics. This makes it possible to define a sufficient and necessary condition for determining the strong robustness of f1 against adversarial perturbation.\nBefore introducing this condition, we need to briefly introduce the definition of topology and finer/coarser topology here:\nDefinition 10.2. A topology \u03c4 is a collection of open sets in a space X .\nA topology \u03c4 is generated by a collection of open balls {B(x, \u03b41)} where x \u2208 X and B(x, \u03b41) = {z|d(x, z) < \u03b41}. The collection contains {B(x, \u03b41)}, the infinite/finite number of the union of balls, and the finite number of intersection of them.\nDefinition 10.3. Suppose \u03c41 and \u03c42 are two topologies in space X . If \u03c42 \u2286 \u03c41, the topology \u03c42 is called a coarser (weaker or smaller) topology than the topology \u03c41, and \u03c41 is called a finer (stronger or larger) topology than \u03c42."}, {"heading": "10.3 PROOFS FOR THEOREMS AND COROLLARIES", "text": "In this section, we provide the proofs for Theorem (3.2), Corollary (4.2), Theorem (3.4), and Corollary (4.1). We first prove Theorem (3.4) and Corollary (4.1). Since \u201ctopological equivalence\u201d is a stronger condition than \u201cfiner topology\u201d, Theorem (3.2) and Corollary (4.2) are straightforward.\n10.3.1 PROOF OF THEOREM (3.4) WHEN f2 IS CONTINUOUS A.E.\nProof. Let S1 = {B1(x, )} and S2 = {B2(x, )}, where B1(x, ) = {y|d\u20321(x, y) < } and B2(x, ) = {y|d\u20322(x, y) < }. Then S1 \u2282 \u03c41 and S2 \u2282 \u03c42. In fact, \u03c41 and \u03c42 are generated by S1 and S2. S1 and S2 are bases of (X, \u03c41) and (X, \u03c42).\n\u2022 First, we want to prove that given \u03b42 > 0, \u2203\u03b41 > 0 such that if d\u20322(x, x\u2032) \u2264 \u03b42, then d\u20321(x, x\u2032) \u2264 \u03b41. Consider a pair of samples (x, x\u2032) and d\u20322(x, x\n\u2032) \u2264 \u03b42. x, x\u2032 \u2208 B2(x, \u03b42). Of course, B2(x, \u03b42) \u2208 \u03c42. Suppose the (X, d\u20321) is a finer topology than (X, d \u2032 2). Then B2(x, \u03b42) \u2208 \u03c41. You can find B1(x0, \u03b41/2) \u2208 \u03c41 such that B\u03042(x, \u03b42) \u2282 B\u03041(x0, \u03b41/2), where B\u03042(x, \u03b42) is the closure of B2(x, \u03b42). Therefore d\u20321(x, x\n\u2032) \u2264 \u03b41. Based on a.e. continuity assumption of f1, since d\u20321(x, x\n\u2032) \u2264 \u03b4, f1(x) = f1(x\u2032) a.e. . This means that P(f1(x) = f1(x\u2032)|d2(g2(x), g2(x\u2032)) < \u03b42) = 1, which is our definition of strong-robustness. \u2022 Next, we want to show that if f1 is strong-robust, then \u03c41 is a finer topology than \u03c42. Suppose f1 is strong-robust, we need to prove that \u2200\u03b42 > 0, \u2203\u03b41 > 0 such that if d\u20322(x, x\u2032) \u2264 \u03b42, then d\u20321(x, x\n\u2032) \u2264 \u03b41. Assume \u03c41 is not a finer topology than \u03c42. This means there exists aB2(x, \u03b42) such thatB2(x, \u03b42) /\u2208 \u03c41. Therefore \u2200\u03b41 > 0, there exists x\u2032 \u2208 B2(x, \u03b42) such that d\u20322(x, x\u2032) < \u03b42 and d\u20321(x, x\u2032) > \u03b41. Based on a.e. continuity assumption of f1, d\u20321(x, x\n\u2032) > \u03b41 indicates that f1(x) 6= f1(x\u2032). This contradicts the strong-robust assumption. Thus, \u03c41 is a finer topology than \u03c42.\nProof of Theorem (3.4) when f2 is not continuous a.e.\nProof. Let S1 = {B1(x, )} and S2 = {B2(x, )}, where B1(x, ) = {y|d\u20321(x, y) < } and B2(x, ) = {y|d\u20322(x, y) < }. Then S1 \u2282 \u03c41 and S2 \u2282 \u03c42. In fact, \u03c41 and \u03c42 are generated by S1 and S2. S1 and S2 are bases of (X, \u03c41) and (X, \u03c42).\n\u2022 First, we want to prove that given \u03b42 > 0, \u2203\u03b41 > 0 such that if d\u20322(x, x\u2032) \u2264 \u03b42, then d\u20321(x, x\u2032) \u2264 \u03b41. Consider a pair of samples (x, x\u2032) and d\u20322(x, x\n\u2032) \u2264 \u03b42. x, x\u2032 \u2208 B2(x, \u03b42). Of course, B2(x, \u03b42) \u2208 \u03c42. Suppose the (X, d\u20321) is a finer topology than (X, d \u2032 2). Then B2(x, \u03b42) \u2208 \u03c41. You can\nfind B1(x0, \u03b41/2) \u2208 \u03c41 such that B\u03042(x, \u03b42) \u2282 B\u03041(x0, \u03b41/2), where B\u03042(x, \u03b42) is the closure of B2(x, \u03b42). Therefore d\u20321(x, x\n\u2032) \u2264 \u03b41. Based on a.e. continuity assumption of f1, since d\u20321(x, x\n\u2032) \u2264 \u03b41, f1(x) = f1(x\u2032) a.e. . This means that P(f1(x) = f1(x\u2032)|f2(x) = f2(x), d2(g2(x), g2(x\u2032)) < \u03b42) = 1, which is our definition of strong-robustness. \u2022 Next, we want to show that if f1 is strong-robust, then \u03c41 is a finer topology than \u03c42. Suppose f1 is strong-robust, we need to prove that \u2200\u03b42 > 0, \u2203\u03b41 > 0 such that if d\u20322(x, x\u2032) \u2264 \u03b42, then d\u20321(x, x\n\u2032) \u2264 \u03b41. Assume \u03c41 is not a finer topology than \u03c42. This means there exists aB2(x, \u03b42) such thatB2(x, \u03b42) /\u2208 \u03c41. Therefore \u2200\u03b41 > 0, there exists x\u2032 \u2208 B2(x, \u03b42) such that d\u20322(x, x\u2032) < \u03b42 and d\u20321(x, x\u2032) > \u03b41. Based on a.e. continuity assumption of f1, d\u20321(x, x\n\u2032) > \u03b41 indicates that f1(x) 6= f1(x\u2032). This contradicts the strong-robust assumption. Thus, \u03c41 is a finer topology than \u03c42."}, {"heading": "10.3.2 PROOF OF THEOREM (3.5)", "text": "Proof. In Section 10.3.1, we have already proved that if the (X, d\u20321) is a finer topology than (X, d \u2032 2), then we can have that \u2200 pair (x, x\u2032) (x, x\u2032 \u2208 X) d\u20322(x, x\u2032) \u2264 \u03b42, then d\u20321(x, x\u2032) \u2264 \u03b41. Therefore, P(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032), d\u20322(x, x\u2032) < \u03b42)\n=1\u2212 P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d\u20322(x, x\u2032) < \u03b42) =1\u2212 P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d1(x, x\u2032) < \u03b41,\nd\u20322(x, x \u2032) < \u03b42)\n>1\u2212 \u03b7\n(10.4)"}, {"heading": "10.3.3 PROOF OF THEOREM (3.3)", "text": "Proof. Since (X1, d1) and (X2, d2) are topologically equivalent. P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x \u2032), d1(g1(x), g1(x \u2032)) < \u03b41) = P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d2(g2(x), g2(x\u2032)) < \u03b42). Therefore,\nP(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032), d2(g2(x), g2(x\u2032)) < \u03b42) =1\u2212 P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032),\nd2(g2(x), g2(x \u2032)) < \u03b42)\n=1\u2212 P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d1(g1(x), g1(x \u2032)) < \u03b41, d2(g2(x), g2(x \u2032)) < \u03b42)\n>1\u2212 \u03b7\n(10.5)"}, {"heading": "10.3.4 PROOF OF THEOREM (3.2)", "text": "Proof. Since f1 is continuous a.e., P(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032), d1(g1(x), g1(x\u2032) < \u03b41, d2(g2(x), g2(x\n\u2032)) < \u03b42) = 0. Therefore, by Section 10.3.3, P(f1(x) = f1(x\u2032)|f2(x) = f2(x \u2032), d2(g2(x), g2(x \u2032)) < \u03b42) = 1."}, {"heading": "10.3.5 PROOF OF COROLLARY (4.2)", "text": "Proof. By (Kelley, 1975), we know that if d1 and d2 are norms in Rn, (Rn, d1) and (Rn, d2) are topological equivalent. Therefore, we have the conclusion."}, {"heading": "10.3.6 PROOF OF COROLLARY (4.1)", "text": "Proof. Suppose n1 > n2 and X2 \u2282 X1. (X, d\u20322) is a finer topology than (X, d\u20321). Therefore (X, d\u20321) is not a finer topology than (X, d\u20322), which indicates that f1 is not strong-robust against adversarial examples."}, {"heading": "10.4 MORE ABOUT PRINCIPLED UNDERSTANDING FROM PROPOSED THEOREMS", "text": "10.4.1 MORE EXAMPLES ABOUT g1 MATTERS FOR STRONG-ROBUSTNESS AND c1 NOT\nFigure 4 uses an example to illustrate Table 3 Case (III) when f1 is strong-robust. We show one case of X1 = X2 = R2 and f1, f2 are continuous a.e.. In terms of classification, f1 (green boundary line) is not accurate according to f2 (red boundary line).\nFigure 5 uses an example figure to illustrate Table 3 Case (IV) when f1 is strong-robust. We show one case of 1 = n1 < n2 = 2, X1 \u2282 X2 and f1, f2 are continuous a.e.. In terms of classification, f1 (green boundary line) is not accurate according to f2 (red boundary line).\nAll pairs of test samples (x, x\u2032) can be categorized into the three cases shown in both figures.\n\u2022 Test-case (a) is when x and x\u2032 are predicted as the same class by both. f1 gets correct predictions according to f2. There exist no adversarial examples. \u2022 Test-case (b) is when x and x\u2032 are predicted as the same class by both. But f1 gets incorrect predictions according to f2. There exist no adversarial examples. \u2022 Test-case (c) shows when f1(x) 6= f1(x\u2032), d2(x, x\u2032) < \u03b42 and f2(x) = f2(x\u2032). This case is explained in Section 11. Essentially, this is about \u201cBoundary based adversarial examples\u201d and can only attack points whose distance to the boundary of f1 is smaller than \u03b42 (f1(x) 6= f1(x\u2032), d2(x, x \u2032) < \u03b42 and f2(x) = f2(x\u2032)). When f1 is continuous a.e., the probability of this set is 0.\nClearly from the two figures, c1 does not determine the strong-robustness of f1."}, {"heading": "10.4.2 MORE ABOUT EXTRA UNNECESSARY FEATURES RUIN STRONG-ROBUSTNESS", "text": "In real-world applications, such attacks can be, for example, adding words with a very tiny font size in a spam E-mail, that is invisible to a human annotator. When a learning-based classifier tries to utilize such extra words (unnecessary for human), it can lead to many easily generated adversarial emails.\nAs another example, one previous study (Xu et al., 2016) shows that a genetic-programming based adversarial example strategy can always evade two state-of-art learning-based PDF-malware classifiers (with \"100%\" evasion rates). The reason behind such good evasion rates is the Condition (4.1). Both state-of-art PDF-malware classifiers have used many superficial features (e.g., a feature representing \"is there a long comment section\") that are not relevant to \"the malicious property\" of a PDF sample at all !\n10.4.3 WHEN f1 CONTINUOUS A.E., EITHER STRONG-ROBUST OR NOT ROBUST AT ALL A.E.\nTable 3 indicates that training a strong-robust and accurate classifier in practice is extremely difficult. For instance, Figure 2 shows only one extra irrelevant feature, which does not hurt accuracy, makes the classifier not robust to adversarial perturbation at all (i.e., for samples a.e. in X , easy to find its adversarial examples.).\nWhen f1 is continuous a.e., P(f1(x) = f1(x\u2032)|f2(x) = f2(x\u2032), d2(g2(x), g2(x\u2032)) < \u03b42) equals to either 1 or 0. This means f1 is either strong-robust or not robust under AN at all a.e.. One case with this probability as 0 is illustrated by Figure 2. Case (III) and Case (IV) from Table 3 have this probability equaling to 1.\n11 BOUNDARY POINTS OF f1 MATTER FOR ADVERSARIAL EXAMPLES WHEN f1 IS NOT CONTINUOUS A.E.\nWhen f1 is not continuous a.e., the analysis of adversarial examples needs to consider \"boundary points\" of f1 with certain properties. This section tries to clarify the definition and related scope. Definition 11.1. We define the set of boundary points of fi as the following set of sample pairs:\n{(x, x\u2032)|fi(x) 6= fi(x\u2032), di(gi(x), gi(x\u2032)) < \u03b4i, x \u2208 X,x\u2032 \u2208 X}\n(11.1)\nOur definition of the boundary points describes such points as pairs of samples that are across the classification boundary. This format of definition makes the following analysis (notation-wise) easy and concise.\nLemma 11.2. fi is not continuous a.e., if and only if x \u2208 X,x\u2032 \u2208 X,P(fi(x) 6= fi(x\u2032)|di(gi(x), gi(x\u2032)) < \u03b4i) > 0 (11.2)\nThis lemma shows that a case with probability of boundary points larger than 0 is exactly the situation when fi being not continuous a.e..\n11.1 MORE ABOUT BOUNDARY POINTS OF f1 AND BOUNDARY POINTS OF f2\nIn addition, we want to point out that all boundary pairs of f2 (satisfying f2(x) 6= f2(x\u2032) and d2(g2(x), g2(x\n\u2032)) < \u03b42) are not considered in our analysis of adversarial examples. Figure 6 illustrates three types of boundary points, using the first two columns showing boundary points of f2.\nThe third column of Figure 6 describes \u201cBoundary based adversarial examples\u201d that can only attack seed samples whose distance to the boundary of f1 is smaller than \u03b42. Essentially this attack is about those boundary points of f1 that are treated as similar and belong to the same class by f2. That is\nP(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d2(g2(x), g2(x\u2032)) < \u03b42, d1(g1(x), g1(x \u2032)) < \u03b41) (11.3)\n\u2022 When f1 is continuous a.e., Eq. (3.1) equals to 0. (derived from Eq. (9.1) in Section 9) \u2022 When f1 is not continuous a.e., Eq. (3.1) might be larger than 0. (derived from Eq. (11.2)) The value of this probability is critical for our analysis in Theorem (3.3) and in Theorem (3.5). Again, we want to emphasize that most machine learning methods assume f1 is continuous a.e. and therefore \u201cboundary based adversarial attacks\u201d are not crucial.\n11.2 WHEN f1 NOT CONTINUOUS A.E., STRONG-ROBUST IS SIGNIFICANTLY INFLUENCED BY BOUNDARY POINTS OF f1\nWhen f1 is not continuous a.e., for instance when X is a finite space, the probability of \u201cadversarial examples\u201d can be calculated as:\nP(f1(x) 6= f1(x\u2032)|f2(x) = f2(x\u2032), d2(g2(x), g2(x\u2032)) < \u03b42)\n= #{(x, x\u2032)|f2(x) = f2(x\u2032)&d2(g2(x), g2(x\u2032)) < \u03b42&f1(x) 6= f1(x\u2032)} #{(x, x\u2032)|f2(x) = f2(x\u2032)&d2(g2(x), g2(x\u2032)) < \u03b42} (11.4)\nThis is exactly the proportion of those pairs of points for which f1 classifies them into different classes and f2 treats them as similar and \"same-class\" samples. For this case, both g1 and c1 matter for the strong-robustness of f1. See Appendix Section 11.2 for an example showing how c1 makes f1 not strong robust.\n11.2.1 c1 MATTERS FOR STRONG-ROBUSTNESS WHEN f1 IS NOT A.E. CONTINUOUS\nBased on Eq. (11.4), when f1 is not continuous a.e., the strong-robustness of f1 is determined by both g1 and c1. Figure 7 shows an exemplar case in which X has only ten samples (i.e. |X| = 10). We assume the learned f1 and the oracle f2 derive the same feature space, i.e., X1 = X2. And we also assume f1 performs the classification very badly because the decision boundary (by c1) on X1 is largely different from the decision boundary on X2. The probability of \"adversarial examples\" in this case can be calculated by using Eq. (11.4). We get P(f1(x) 6= f1(x\u2032)|f2(x) = f2(x \u2032), d1(g1(x), g1(x \u2032)) < \u03b41) = 2\u22173 5\u22172 = 0.6.\nClearly in this case, c1 matters for the strong-robustness (when f1 is not a.e. continuous). This figure indicates that when (1) sample space X is finite, (2) f1 learns a wrong decision boundary and (3) the probability of test samples around f1\u2019s decision boundary is large, f1 is not strong-robust against adversarial examples. However, we want to point out that this situation is very rare for a well-trained classifier f1.\nFor cases when f1 is not continuous a.e., obtaining more samples is clearly a good way to learn a better decision boundary that might improve the adversarial robustness of the classifier at the same time."}, {"heading": "12 MORE ABOUT DNNS\u2019 ROBUSTNESS AGAINST ADVERSARIAL SAMPLES", "text": "Researchers have proposed different strategies to generate adversarial examples attacking deep neural networks (e.g., (Szegedy et al., 2013; Nguyen et al., 2015; He et al., 2015; Papernot et al., 2016a; Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b)). Previous studies mostly focus on an image classification, therefore for these domains our symbols mean:\n\u2022 f1(\u00b7): f1(\u00b7) is a DNN classifier with multiple layers, including linear perceptron layers, activation layers, convolutional layers and softmax decision layer. \u2022 (X1, d1): X1 denotes the feature space discovered by the layer right before the last fully connected layer. This feature space is automatically extracted from the original image space (e.g., RGB representation) by the DNN. (X, d\u20321) is defined by d1 using Eq. (10.2). \u2022 (X2, d2): X2 denotes the feature space that oracle (e.g., human annotators) used to decide groundtruth labels of training images. For example, a human annotator needs to recognize a hand-written digit \u201c0\u201d. X2 includes what patterns he/she needs for such a decision. (X, d\u20322) is defined by d2 using Eq. (10.3)"}, {"heading": "12.1 MORE ABOUT ARE STATE-OF-THE-ART DEEP NEURAL NETS STRONG-ROBUST ?", "text": "We can observe some properties of d1 through experimental results. Table 5,Table 6,Table 7 and Table 8 show properties of d1 (and d\u20321) resulting from performing testing experiments on four state-of-art DNN networks.\nIn Table 9, the model we use is a 200-layer residual network (He et al., 2015) trained on Imagenet dataset (Deng et al., 2009) by Facebook15. We generate two types of test samples from 50000 images in the validation set of Imagenet: (1) 50000 randomly perturbed images. The random perturbations on each image are generated by first fixing the perturbation value on every dimension to be the same, and then randomly assigning the sign on every dimension as + or \u2212 (with probability 1/2). In this way, the size of the perturbation can be described by ||x\u2212 x\u2032||\u221e that we name as the level of attacking power ( later defined in Eq. (12.6)). (2) 50000 adversarially perturbed images. We use the fast-gradient sign method (introduced in Section 8.2) to generate such adversarial perturbations on each seed image. The \u201cattacking power\u201d of such adversarial perturbations uses the same formula as Eq. (12.6). The first column of Table 9 shows different attack powers (Eq. (12.6)) we use in the experiment. The second column shows the accuracy of running the DNN model on the first group of image samples and the third column shows the accuracy of running the DNN model on the second group of image samples.\nTable 6,Table 7 and Table 8 repeat similar experiments on three other DNN models: overfeat network(Sermanet et al., 2013), the residual network(He et al., 2015) and the VGG model (Simonyan & Zisserman, 2014). The conclusion is consistent across all four models.\n15https://github.com/facebook/fb.resnet.torch"}, {"heading": "12.2 CONNECTING PREVIOUS STUDIES HARDENING DNNS", "text": "Multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) exist in the DNN literature. They mostly aim to learn a better g1 by minimizing different loss functions Lf1(x, x \u2032) so that when d2(g2(x), g2(x\u2032)) < , this loss Lf1(x, x \u2032) is small. This might improve the the topological equivalence (or finer topology). Two major variations exist among related methods: the choice of Lf1(x, x \u2032) and the way to generate pairs of (x, x\u2032).\n\u2022 Choice of loss function Lf1(x, x\u2032): Siamese training (G) (Section 12.4) and (Lee et al., 2015) use Lf1(x, x \u2032) = d1(g1(x), g1(x \u2032)). Siamese training (F) chooses Lf1(x, x \u2032) = dist(f1(x), f1(x \u2032)),\nwhere dist(\u00b7, \u00b7) is a distance function measuring the difference between f1(x) and f1(x\u2032). If f1 is continuous a.e., when d1(g1(x), g1(x\u2032)) is small\u2192 we get dist(f1(x), f1(x\u2032)) is small. However, the reverse direction may not hold. Therefore, Lf1(x, x \u2032) = dist(f1(x), f1(x \u2032)) may not work for\ncases. \u2022 Generating pairs of (x, x\u2032): Another variation is the way of generating pairs of (x, x\u2032) so that d2(g2(x), g2(x\n\u2032)) is small. There exist two common ways. One is generating x\u2032 by adding a random (e.g. Gaussian) perturbation on x. The other one is generating the adversarial perturbation to get x\u2032 from x.\nBesides, (Zheng et al., 2016) uses Lf1(x, x \u2032) = KL(f1(x), f1(x \u2032)) and uses it as a regularization term adding onto the original training loss function. Its samples x\u2032 are generated from original samples x adding a small Gaussian noise. (Miyato et al., 2016) uses the similar loss function as (Zheng et al., 2016). But (Miyato et al., 2016) uses adversarial perturbed x\u2032 from x. (Lee et al., 2015) uses Lf1(x, x \u2032) = d1(g1(x), g1(x \u2032)) and x\u2032s are generated xs by adding a small Gaussian noise. Recently proposed adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses Lf1(x, x \u2032) = L(f1(x \u2032), f2(x)) and uses adversarial perturbed x\u2032 from x. These studies are summarized and compared in Table 4."}, {"heading": "12.3 A NOVEL EVALUATION METRIC \"ADVERSARIAL ROBUSTNESS OF CLASSIFIERS (ARC)\"", "text": "Our theoretical analysis indicates that strong-robustness is a strong condition of machine learning classifiers and requires thorough understanding of oracle. Since many state-of-the-art learning models, including many DNNs, are not strong-robust, it is important to understand and quantify how far they are away from strong-robustness.\nThis section proposes a new evaluation measure \u201cAdversarial Robustness of Classifiers (ARC)\u201d to quantify how far a classifier is away from the strong-robustness. This quantitative measure considers both the predictor f1 and the oracle f2. By design, a classifier (f1)\u2019s ARC achieves the maximum (1 since ARC is rescaled to [0, 1]) if and only if f1 is strong-robust (see Theorem (12.3))."}, {"heading": "12.3.1 DEFINE ARC AND ARCA", "text": "We name such situations as \"weak-robustness\" and propose a quantitative measure to describe how robust a classification model is against adversarial examples. The proposed measure \u201cAdversarial Robustness of Classifiers (ARC)\u201d considers both the predictor f1 and the oracle f2 (introduced in Section 2.2). By design, a classifier (f1)\u2019s ARC achieves the maximum (1 since ARC is rescaled to [0, 1]) if and only if f1 is strong-robust against adversarial examples and is based on the expectation of how difficult it is to generate adversarial examples.\nDefinition 12.1. Adversarial Robustness of Classifiers (ARC)\nBy adding the constraint d2(x, x\u2032) < \u03b42 into Eq. (2.2) (our general definition of adversarial examples) and taking the expactation of d2 between adversarial example and seed sample, we define a measure\nquantifying the robustness of machine learning classifiers against adversarial examples. ARC(f1, f2) =Ex\u2208X [d2(x, x\u2032)]\nx\u2032 = argmin t\u2208X d2(x, t)\nSubject to: f1(x) 6= f1(t) d2(x, t) < \u03b42\n(12.1)\nHere for the case that x\u2032 doesn\u2019t exsit, we assume d2(x, x\u2032) = \u03b42.\nTwo recent studies (Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b) propose two similar measures both assuming d2 as norm functions, but do not consider the importance of an oracle. More importantly, (Papernot et al., 2015b) does not provide any computable way to calculate the measure. In (Moosavi-Dezfooli et al., 2015), the measure is normalized by the size of the test samples, while no evidence exists to show that the size of perturbation is related to the size of test samples.\nThe fact that previous measures neglect the oracle f2 leads to a severe problem: the generated adversarial examples are not necessarily valid. This is because if the size of perturbation is too large, oracle f2 may classify the perturbed sample into a different class (different from the class of the seed sample).\nThis motivates us to design a computable criteria to estimate Definition (12.1). For instance, for image classification tasks, we can choose d2 = || \u00b7 ||\u221e as an example. Then in Eq. (12.1), to estimate of E[||x\u2212 x\u2032||\u221e], we need to make some assumptions. Assume that there exists a threshold \u03b42, that any perturbation larger than \u03b42 will change the classification of oracle f2. That is if ||x\u2212 x\u2032||\u221e \u2265 \u03b42, then f2(x) 6= f2(x\u2032). More concretely, for image classification tasks, as the input space is discrete (with every dimension ranging from 0 to 255), ARC can be estimated by the following Eq. (12.2):\nARC\u221e(f1, f2) =E[\u2016 x\u2212 x\u2032 \u2016\u221e] = \u03b42\u22121\u2211 i=1 iP(\u2016 x\u2212 x\u2032 \u2016\u221e= i)\n+ \u03b42P(f1(x) = f1(t),\u2200 \u2016 x\u2212 t \u2016\u221e< \u03b42). x\u2032 = argmin\nt\u2208X d2(x, t)\nSubject to: f1(x) 6= f1(t) f2(x) = f2(t)\n(12.2)\nDefinition 12.2. Adversarial Robustness of Classifiers with Accuracy (ARCA)\nAs we have discussed in the Section 4, both accuracy and robustness are important properties in determining whether a classification model is preferred or not. Therefore we combine accuracy and ARC into the following unified measure ARCA:\nARCA(f1) = Accuracy(f1)\u00d7 ARC(f1, f2)\n\u03b42 (12.3)"}, {"heading": "12.3.2 ARC AND STRONG-ROBUSTNESS", "text": "It is important to understand the relationship between strong-robustness and weak-robustness. We provide an important theorem as follows that clearly shows the weak-robustness is quantitatively related to the strong-robustness. Theorem 12.3. f1 is strong-robust against adversarial examples if and only if ARC(f1)/\u03b42 = 1.\nProof of Theorem (12.3):\nProof. If ARC(f1)/\u03b42 = 1, then based on Definition (12.1), we have that\nP(d2(x, x\u2032) = \u03b42) = 1.\nThis indicates that\nP(f1(x) = f1(x\u2032)|d2(x, x\u2032) < \u03b42) = 1, which is the exact definition of strong-robustness ( Eq. (3.8)).\nIf f1 is strong-robust, then P(f1(x) = f1(x\u2032)|d2(x, x\u2032) < \u03b42) = 1. Therefore ARC(f1) = E[d2(x, x\u2032)]. Since P(f1(x) 6= f1(x\u2032)|d2(x, x\u2032) < \u03b42) = 0, we have that\nARC(f1) = E[d2(x, x\u2032)] = \u03b42P(f1(x) = f1(x\u2032)|d2(x, x\u2032) < \u03b42) = \u03b42\n(12.4)\nARC(f1)/\u03b42 = 1."}, {"heading": "12.4 USING \u201cSIAMESE ARCHITECTURE\u201d TO IMPROVE DNNS\u2019 ADVERSARIAL ROBUSTNESS", "text": "One intuitive formulation that we can use to improve a DNN\u2019s adversarial robustness is by solving the following:\n\u2200x, x\u2032 \u2208 X, if d2(g2(x), g2(x\u2032)) < , argmin\nw d1(g1(x;w), g1(x\n\u2032;w)) (12.5)\nThis essentially forces the DNN to have the finer topology between (X1, d1) and (X2, d2) by learning a better g1. We name the strategy minimizing the loss defined in Eq. (12.5) as \"Siamese Training\" because this formulation uses the Siamese architecture (Bromley et al., 1993), a classical deeplearning approach proposed for learning embedding. We feed a slightly perturbed input x\u2032 together with its original seed x to the Siamese network which contains two copies (sharing the same weights) of a DNN model we want to improve. By penalizing the difference between middle-layer (g1(\u00b7)) outputs of (x, x\u2032), \"Siamese Training\" can push two spaces (X, d\u20321) versus (X2, d \u2032 2) to approach finer topology relationship, and thus increase the robustness of the model. This can be concluded from Figure 8. By assuming d2(g2(x), g2(x\u2032)) equals (approximately) to ||\u2206(x, x\u2032)||, previous studies (summarized in Table 2) normally assume d2 is a norm function || \u00b7 ||. Because for a pair of inputs (x, x\u2032) that are close to each other (i.e., ||x \u2212 x\u2032|| is small) in (X, || \u00b7 ||), Siamese training pushes them to be close also in (X1, d1) . As a result, this means that a sphere in (X1, d1) maps to a not-too-thin high-dimensional ellipsoid in (X, || \u00b7 ||). Therefore the adversarial robustness of DNN model after Siamese training may improve. In experiments, we choose Euclidean distance \u2016 \u00b7 \u20162 for d1(\u00b7) (however, many other choices are possible)."}, {"heading": "12.5 COMPARING DNN HARDENING STRATEGIES EXPERIMENTALLY", "text": "Datasets: Currently, we are using the following 2 image datasets to evaluate our model:\n\u2022 MNIST: MNIST, released in (LeCun et al., 1998) includes a task to classify handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. Each example is a 32x32 pixel black and white image of handwritten digit. \u2022 CIFAR-10: CIFAR-10 is an image classification dataset released by (Krizhevsky & Hinton, 2009). The training set contains 50,000 32x32 color images in 10 classes, and the test set contains 10,000 32x32 color images. VGG model: We choose a VGG model (Simonyan & Zisserman, 2014) as a base DNN model. The VGG model in our experiment has 16 weight layers (55 layers in total).\nBaseline: Three different hardening strategies are compared through testing on adversarial examples (details in Section 12.2): (1) original model; (2) stability training (Zheng et al., 2016) 16; (3) Siamese training (alone); (4) adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses adversarial perturbed x\u2032 and original samples x to train a DNN model.\nThe first column of Table 10 and Table 11shows different levels of attack power (defined in Eq. (12.6)). Test accuracy reported in Figure 9(a), Figure 10(a), Table 10 and Table 11 shows different hardening approches can increase the effectiveness of the adversarial attacks. Details of our experimental set-up and datasets are included in Section 12.2.\nEvaluation Metrics: 16ATT: Stability training was shown to improve the model robustness against Gaussian noise in (Zheng et al., 2016). Differently, our experiments focus on testing a learning model\u2019s robustness against \u201cadversarial perturbation\u201d. The sole purpose of including this baseline is to show where state-of-art hardening strategies are in our experimental setting.\n\u2022 Test accuracy: We use top-1 test accuracy as the performance metric. It is defined as the number of successfully classified samples divided by the number of all test samples. The base model achieves accuracy when there\u2019s no adversarial attack. \u2022 ARC (Eq. (12.2)) : We use ARC to measure the adversarial robustness of each model. \u03b7 is chosen to be 10. \u2022 ARCA: (Eq. (12.3)) : We use ARCA to measure the total performance of each model. We generate adversarial examples using the fast gradient sign method, in which the power of the adversary attack can be easily controlled. By controlling the power of fast-sign attacks, we can obtain a complete view of how the accuracy changes according to different attack powers.\nIn the following analysis, the attack power is defined as: P = ||x\u2212 x\u2032||\u221e (12.6) For image classification tasks, we control the perturbed sample to be still in the valid input space, so that every dimension of the perturbed samples is in the range of integers between 0 and 255."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["Scott Alfeld", "Xiaojin Zhu", "Paul Barford"], "venue": null, "citeRegEx": "Alfeld et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alfeld et al\\.", "year": 2016}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pp. 16\u201325. ACM,", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "The Security of Machine Learning", "author": ["Marco Barreno", "Blaine Nelson", "Anthony D Joseph", "JD Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Adversarial pattern classification using multiple classifiers and randomisation", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": null, "citeRegEx": "Biggio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2008}, {"title": "Poisoning Attacks against Support Vector Machines", "author": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "venue": "In 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Evasion attacks against machine learning at test time", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Biggio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2013}, {"title": "Poisoning complete-linkage hierarchical clustering", "author": ["Battista Biggio", "Samuel Rota Bul\u00f2", "Ignazio Pillai", "Michele Mura", "Eyasu Zemene Mequanint", "Marcello Pelillo", "Fabio Roli"], "venue": null, "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Differentially-and non-differentially-private random decision trees", "author": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Yann LeCun"], "venue": "arXiv preprint arXiv:1410.6973,", "citeRegEx": "Bojarski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2014}, {"title": "Signature verification using a \u201csiamese\u201d time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Stackelberg Games for Adversarial Prediction Problems", "author": ["Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Br\u00fcckner and Scheffer.,? \\Q2011\\E", "shortCiteRegEx": "Br\u00fcckner and Scheffer.", "year": 2011}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini and Wagner.,? \\Q2016\\E", "shortCiteRegEx": "Carlini and Wagner.", "year": 2016}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Fitting Time Series Models to Nonstationary Processes", "author": ["Rainer Dahlhaus"], "venue": "The Annals of Statistics,", "citeRegEx": "Dahlhaus.,? \\Q1997\\E", "shortCiteRegEx": "Dahlhaus.", "year": 1997}, {"title": "Adversarial classification", "author": ["Nilesh Dalvi", "Pedro Domingos", "Sumit Sanghai", "Deepak Verma"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Dalvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "Learning to Classify with Missing and Corrupted Features", "author": ["Ofer Dekel", "Ohad Shamir", "Lin Xiao"], "venue": "Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Untangling invariant object recognition", "author": ["James J DiCarlo", "David D Cox"], "venue": "Trends in cognitive sciences,", "citeRegEx": "DiCarlo and Cox.,? \\Q2007\\E", "shortCiteRegEx": "DiCarlo and Cox.", "year": 2007}, {"title": "How does the brain solve visual object recognition? Neuron", "author": ["James J DiCarlo", "Davide Zoccolan", "Nicole C Rust"], "venue": null, "citeRegEx": "DiCarlo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "DiCarlo et al\\.", "year": 2012}, {"title": "Privacy aware learning", "author": ["John C Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "Pattern classification", "author": ["Richard O Duda", "Peter E Hart", "David G Stork"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duda et al\\.", "year": 2012}, {"title": "Differential Privacy", "author": ["Cynthia Dwork"], "venue": "In Encyclopedia of Cryptography and Security,", "citeRegEx": "Dwork.,? \\Q2011\\E", "shortCiteRegEx": "Dwork.", "year": 2011}, {"title": "Fundamental limits on adversarial robustness", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "In Proceedings of ICML, Workshop on Deep Learning, number EPFL-CONF-214923,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Real analysis: modern techniques and their applications", "author": ["Gerald B Folland"], "venue": null, "citeRegEx": "Folland.,? \\Q2013\\E", "shortCiteRegEx": "Folland.", "year": 2013}, {"title": "Deepmask: Masking dnn models for robustness against adversarial samples", "author": ["Ji Gao", "Beilun Wang", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1702.06763,", "citeRegEx": "Gao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2017}, {"title": "Nightmare at Test Time: Robust Learning by Feature Deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In 23rd International Conference on Machine Learning,", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "Grosse et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2016}, {"title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "[cs],", "citeRegEx": "Gu and Rigazio.,? \\Q2014\\E", "shortCiteRegEx": "Gu and Rigazio.", "year": 2014}, {"title": "DeepSpeech: Scaling up end-toend speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "others"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Adversarial machine learning", "author": ["Ling Huang", "Anthony D Joseph", "Blaine Nelson", "Benjamin IP Rubinstein", "JD Tygar"], "venue": "In 4th ACM Workshop on Security and Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Fast readout of object identity from macaque inferior temporal cortex", "author": ["Chou P Hung", "Gabriel Kreiman", "Tomaso Poggio", "James J DiCarlo"], "venue": null, "citeRegEx": "Hung et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hung et al\\.", "year": 2005}, {"title": "Robust convolutional neural networks under adversarial noise", "author": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06306,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Sensory discrimination: decision process", "author": ["KO Johnson"], "venue": "Journal of Neurophysiology,", "citeRegEx": "Johnson.,? \\Q1980\\E", "shortCiteRegEx": "Johnson.", "year": 1980}, {"title": "Evasion and hardening of tree ensemble classifiers", "author": ["Alex Kantchelian", "JD Tygar", "Anthony D Joseph"], "venue": "arXiv preprint arXiv:1509.07892,", "citeRegEx": "Kantchelian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kantchelian et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Adversarial machine learning at scale", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1611.01236,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Manifold regularized deep neural networks using adversarial examples", "author": ["Taehoon Lee", "Minsuk Choi", "Sungroh Yoon"], "venue": "arXiv preprint arXiv:1511.06381,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Feature cross-substitution in adversarial classification", "author": ["Bo Li", "Yevgeniy Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems, pp. 2087\u20132095,", "citeRegEx": "Li and Vorobeychik.,? \\Q2014\\E", "shortCiteRegEx": "Li and Vorobeychik.", "year": 2014}, {"title": "Differentially private distributed online learning", "author": ["Chencheng Li", "Pan Zhou"], "venue": "arXiv preprint arXiv:1505.06556,", "citeRegEx": "Li and Zhou.,? \\Q2015\\E", "shortCiteRegEx": "Li and Zhou.", "year": 2015}, {"title": "Mining adversarial patterns via regularized loss minimization", "author": ["Wei Liu", "Sanjay Chawla"], "venue": "Machine learning,", "citeRegEx": "Liu and Chawla.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Chawla.", "year": 2010}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Lowd and Meek.,? \\Q2005\\E", "shortCiteRegEx": "Lowd and Meek.", "year": 2005}, {"title": "The security of latent dirichlet allocation", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Some submodular data-poisoning attacks on machine learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Distributional smoothing with virtual adversarial training. ICLR", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Koyama Masanori"], "venue": null, "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "arXiv preprint arXiv:1511.04599,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In CVPR. IEEE,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Rademacher observations, private data, and boosting", "author": ["Richard Nock", "Giorgio Patrini", "Arik Friedman"], "venue": "arXiv preprint arXiv:1502.02322,", "citeRegEx": "Nock et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2015}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1511.07528,", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1511.04508", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "November 2015b. URL http://arxiv.org/abs/1511.04508", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Towards the science of security and privacy in machine learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Arunesh Sinha", "Michael Wellman"], "venue": "arXiv preprint arXiv:1611.03814,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["Arun Rajkumar", "Shivani Agarwal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rajkumar and Agarwal.,? \\Q2012\\E", "shortCiteRegEx": "Rajkumar and Agarwal.", "year": 2012}, {"title": "Practical Evasion of a Learning-Based Classifier: A Case Study", "author": ["N. Rndic", "P. Laskov"], "venue": "IEEE Symposium on Security and Privacy (SP),", "citeRegEx": "Rndic and Laskov.,? \\Q2014\\E", "shortCiteRegEx": "Rndic and Laskov.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Learning adversary behavior in security games: A pac model perspective", "author": ["Arunesh Sinha", "Debarun Kar", "Milind Tambe"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 214\u2013222. International Foundation for Autonomous Agents and Multiagent Systems,", "citeRegEx": "Sinha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2016}, {"title": "Differentially private algorithms for empirical machine learning", "author": ["Ben Stoddard", "Yan Chen", "Ashwin Machanavajjhala"], "venue": "arXiv preprint arXiv:1411.5428,", "citeRegEx": "Stoddard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stoddard et al\\.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Adversarial reinforcement learning", "author": ["William Uther", "Manuela Veloso"], "venue": "Technical report,", "citeRegEx": "Uther and Veloso.,? \\Q1997\\E", "shortCiteRegEx": "Uther and Veloso.", "year": 1997}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Is feature selection secure against training data poisoning", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Crypto-nets: Neural networks over encrypted data", "author": ["Pengtao Xie", "Misha Bilenko", "Tom Finley", "Ran Gilad-Bachrach", "Kristin Lauter", "Michael Naehrig"], "venue": "arXiv preprint arXiv:1412.6181,", "citeRegEx": "Xie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2014}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P. Xing", "Michael I. Jordan", "Stuart J Russell", "Andrew Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Automatically evading classifiers", "author": ["Weilin Xu", "Yanjun Qi", "David Evans"], "venue": "In Proceedings of the Network and Distributed Systems Symposium,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}], "referenceMentions": [{"referenceID": 36, "context": "Deep Neural Networks (DNNs) can efficiently learn highly accurate models and have been demonstrated to perform exceptionally well (Krizhevsky et al., 2012; Hannun et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 28, "context": "Deep Neural Networks (DNNs) can efficiently learn highly accurate models and have been demonstrated to perform exceptionally well (Krizhevsky et al., 2012; Hannun et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 25, "context": "The maliciously generated inputs are called \u201cadversarial examples\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) and are commonly crafted by carefully searching small perturbations through an optimization procedure.", "startOffset": 67, "endOffset": 114}, {"referenceID": 60, "context": "The maliciously generated inputs are called \u201cadversarial examples\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) and are commonly crafted by carefully searching small perturbations through an optimization procedure.", "startOffset": 67, "endOffset": 114}, {"referenceID": 60, "context": "(Szegedy et al., 2013) firstly observe that convolution DNNs are vulnerable to small artificial perturbations.", "startOffset": 0, "endOffset": 22}, {"referenceID": 25, "context": "Then, (Goodfellow et al., 2014) try to clarify that the primary cause of such vulnerabilities may be the linear nature of DNNs.", "startOffset": 6, "endOffset": 31}, {"referenceID": 21, "context": "Subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Nguyen et al., 2015) have explored other ways to explore adversarial examples for DNN (details in Section 2.", "startOffset": 18, "endOffset": 83}, {"referenceID": 48, "context": "Subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Nguyen et al., 2015) have explored other ways to explore adversarial examples for DNN (details in Section 2.", "startOffset": 18, "endOffset": 83}, {"referenceID": 62, "context": "For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using a noise-corrupted version of inputs as training samples.", "startOffset": 41, "endOffset": 101}, {"referenceID": 32, "context": "For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using a noise-corrupted version of inputs as training samples.", "startOffset": 41, "endOffset": 101}, {"referenceID": 25, "context": "The most generally successful strategy to date is adversarial training (Goodfellow et al., 2014; Szegedy et al., 2013) which injects adversarial examples into training to improve the generalization of DNN models.", "startOffset": 71, "endOffset": 118}, {"referenceID": 60, "context": "The most generally successful strategy to date is adversarial training (Goodfellow et al., 2014; Szegedy et al., 2013) which injects adversarial examples into training to improve the generalization of DNN models.", "startOffset": 71, "endOffset": 118}, {"referenceID": 22, "context": "almost everywhere (Folland, 2013); (defined by Definition (9.", "startOffset": 18, "endOffset": 33}, {"referenceID": 25, "context": "Previous studies f1 \u2206(x, x\u2032) Formulation of f1(x) 6= f1(x) (Goodfellow et al., 2014) Convolutional neural networks `\u221e argmax x\u2032 Loss(f1(x \u2032), f1(x))", "startOffset": 59, "endOffset": 84}, {"referenceID": 60, "context": "(Szegedy et al., 2013) Convolutional neural networks `2 argmin x\u2032 Loss(f1(x \u2032), l), subject to: l 6= f1(x)", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "(Biggio et al., 2013) Support vector machine (SVM) `2 argmin x\u2032 Loss(f1(x \u2032),\u22121), subject to: f1(x) = 1 (Kantchelian et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 34, "context": ", 2013) Support vector machine (SVM) `2 argmin x\u2032 Loss(f1(x \u2032),\u22121), subject to: f1(x) = 1 (Kantchelian et al., 2015) Decision tree and Random forest `2, `1, `\u221e argmin x\u2032 Loss(f1(x \u2032),\u22121), subject to: f1(x) = 1", "startOffset": 90, "endOffset": 116}, {"referenceID": 26, "context": "(Grosse et al., 2016) Convolutional neural networks `0 argmax x\u2032 Loss(f1(x \u2032), f1(x))", "startOffset": 0, "endOffset": 21}, {"referenceID": 66, "context": "(Xu et al., 2016) Random forest and SVM `1, `\u221e argmin x\u2032 Loss(f1(x \u2032),\u22121), subject to: f1(x) = 1", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "As illustrated by cognitive neuroscience papers (DiCarlo & Cox, 2007; DiCarlo et al., 2012), human brains perform visual object recognition using the ventral visual stream, and this stream is considered to be a progressive series of visual re-representations, from V1 to V2 to V4 to IT cortex (DiCarlo & Cox, 2007).", "startOffset": 48, "endOffset": 91}, {"referenceID": 66, "context": "this thinking is wrong and can lead to their classifiers vulnerable to adversarial examples(Xu et al., 2016).", "startOffset": 91, "endOffset": 108}, {"referenceID": 46, "context": ", 2016) random perturbation KL(f1(x), f1(x)) Classification layer (Miyato et al., 2016) adversarial perturbation KL(f1(x), f1(x)) Classification layer Adversarial training(Goodfellow et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 25, "context": ", 2016) adversarial perturbation KL(f1(x), f1(x)) Classification layer Adversarial training(Goodfellow et al., 2014) adversarial perturbation L(f1(x), f2(x)) Loss function", "startOffset": 91, "endOffset": 116}, {"referenceID": 37, "context": "Large Adversarial training(Kurakin et al., 2016) adversarial perturbation L(f1(x), f2(x)) Loss function", "startOffset": 26, "endOffset": 48}, {"referenceID": 39, "context": "(Lee et al., 2015) adversarial perturbation \u2016 g1(x)\u2212 g1(x) \u20162 Layer before classification layer Siamese Training random perturbation \u2016 g1(x)\u2212 g1(x) \u20162 Layer before classification layer", "startOffset": 0, "endOffset": 18}, {"referenceID": 46, "context": "Table 4 summarizes multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) in the DNN literature.", "startOffset": 48, "endOffset": 107}, {"referenceID": 39, "context": "Table 4 summarizes multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) in the DNN literature.", "startOffset": 48, "endOffset": 107}, {"referenceID": 23, "context": "In (Gao et al., 2017) the authors compare the difference between g1(x \u2032) and g1(x) from DNN.", "startOffset": 3, "endOffset": 21}, {"referenceID": 30, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 1, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 5, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 34, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 0, "context": "Multiple recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 23, "endOffset": 107}, {"referenceID": 6, "context": "Multiple recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 23, "endOffset": 107}, {"referenceID": 60, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 25, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 66, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 34, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 5, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 58, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013; Papernot et al., 2016b; Sinha et al., 2016) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial examples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 193}, {"referenceID": 18, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 55}, {"referenceID": 64, "context": "Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy.", "startOffset": 48, "endOffset": 187}, {"referenceID": 7, "context": "Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy.", "startOffset": 48, "endOffset": 187}, {"referenceID": 59, "context": "Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy.", "startOffset": 48, "endOffset": 187}, {"referenceID": 20, "context": "Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy.", "startOffset": 48, "endOffset": 187}, {"referenceID": 49, "context": "Recent studies have proposed various strategies (Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) to preserve the privacy of data such as differential privacy.", "startOffset": 48, "endOffset": 187}, {"referenceID": 2, "context": "For instance: (1) (Barreno et al., 2010) and (Biggio et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": ", 2010) and (Biggio et al., 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 63, "context": ", 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al., 2015; Zhang et al., 2015) consider the impact of using reduced feature sets on classifiers under adversarial attacks.", "startOffset": 126, "endOffset": 165}, {"referenceID": 63, "context": "(Xiao et al., 2015) proposes an adversary-aware feature selection model that can improve a classifier\u2019s robustness against adversarial attacks by incorporating specific assumptions about the adversary\u2019s data manipulation strategy.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "(3) Another line of works, named as adversarial training (Goodfellow et al., 2014), designs a new loss function for training neural networks, which is a linear interpolation of the loss function of the original sample and the loss function of the adversarial example generated by the original sample.", "startOffset": 57, "endOffset": 82}, {"referenceID": 37, "context": "A scalable version of adversarial training (Kurakin et al., 2016) was recently proposed.", "startOffset": 43, "endOffset": 65}, {"referenceID": 13, "context": "Related efforts include perfect information assumptions (Dalvi et al., 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing the attack process as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst perturbation) (Dekel et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 14, "context": ", 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing the attack process as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst perturbation) (Dekel et al., 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 299, "endOffset": 345}, {"referenceID": 12, "context": ", 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 80, "endOffset": 125}, {"referenceID": 58, "context": "similar conclusion as ours ( Section 3) that the extreme cases that the defender doesn\u2019t work only has zero probability (Sinha et al., 2016).", "startOffset": 120, "endOffset": 140}, {"referenceID": 66, "context": "In another example with more obvious security implications about PDF malware (Xu et al., 2016), x\u2032 in Eq.", "startOffset": 77, "endOffset": 94}, {"referenceID": 66, "context": ", a virtual machine decides if a PDF file is malicious or not by actually running it), but are classified as benign by state-of-art machine learning classifiers (Xu et al., 2016).", "startOffset": 161, "endOffset": 178}, {"referenceID": 66, "context": "For example, in (Xu et al., 2016) \"adversarial examples\" are those generated PDFs that can fool PDFRate (a learning-based classifier for detecting malicious PDFs) to classify them as benign.", "startOffset": 16, "endOffset": 33}, {"referenceID": 5, "context": "For instance, (Biggio et al., 2013) uses a formula as follows: argmin x\u2032 (f1(x \u2032)) s.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "Gradient ascent method (Biggio et al., 2013) Machine learning has been popular in classifying malicious (y = 1) versus benign (y = \u22121) in computer security tasks.", "startOffset": 23, "endOffset": 44}, {"referenceID": 60, "context": "Box L-BFGS adversary (Szegedy et al., 2013) This study views the adversarial problem as a constrained optimization problem, i.", "startOffset": 21, "endOffset": 43}, {"referenceID": 25, "context": "Fast gradient sign method (Goodfellow et al., 2014) The fast gradient sign method proposed by (Goodfellow et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 25, "context": ", 2014) The fast gradient sign method proposed by (Goodfellow et al., 2014) views d2 as the `\u221e-norm.", "startOffset": 50, "endOffset": 75}, {"referenceID": 37, "context": "A recent paper (Kurakin et al., 2016) shows that adversarial examples generated by fast gradient sign method are misclassified even after these images have been recaptured by cameras.", "startOffset": 15, "endOffset": 37}, {"referenceID": 17, "context": "For instance, as illustrated by the seminal cognitive neuroscience paper \"untangling invariant object recognition\" (DiCarlo & Cox, 2007) and its follow-up study (DiCarlo et al., 2012), the authors show that one can view the information processing of visual object recognition by human brains as the process of finding operations that progressively transform retinal representations into a new form of representation (X2 in this paper), followed by the application of relatively simple decision functions (e.", "startOffset": 161, "endOffset": 183}, {"referenceID": 33, "context": ", (DiCarlo & Cox, 2007; Johnson, 1980; Hung et al., 2005)) have argued that this viewpoint of representation learning plus simple decision function is more productive than hypothesizing that brains directly learn very complex decision functions (highly non-linear) that operate on the retinal image representation.", "startOffset": 2, "endOffset": 57}, {"referenceID": 31, "context": ", (DiCarlo & Cox, 2007; Johnson, 1980; Hung et al., 2005)) have argued that this viewpoint of representation learning plus simple decision function is more productive than hypothesizing that brains directly learn very complex decision functions (highly non-linear) that operate on the retinal image representation.", "startOffset": 2, "endOffset": 57}, {"referenceID": 66, "context": "As another example, the authors of (Xu et al., 2016) used genetic programming to find \u201cadversarial examples\u201d (by solving Eq.", "startOffset": 35, "endOffset": 52}, {"referenceID": 66, "context": "The authors of (Xu et al., 2016) therefore used the Cuckoo sandbox (a malware analysis system through actual execution) to run a variant PDF sample in a virtual machine installed with a PDF reader and reported the behavior of the sample including network APIs calls.", "startOffset": 15, "endOffset": 32}, {"referenceID": 22, "context": "The above definition is a special case of almost everywhere continuity defined in (Folland, 2013) (see Definition (9.", "startOffset": 82, "endOffset": 97}, {"referenceID": 60, "context": "Similarly to the results shown by (Szegedy et al., 2013), DNNs satisfy that |f1(x) \u2212 f1(x)| \u2264 W \u2016 x \u2212 x\u2032 \u20162 where W \u2264 \u220f Wi and Wi \u2265 ||(wi, bi)||\u221e.", "startOffset": 34, "endOffset": 56}, {"referenceID": 65, "context": "This is partly because the concept of metric space has been widely used in many machine learning models, such as metric learning (Xing et al., 2003).", "startOffset": 129, "endOffset": 148}, {"referenceID": 66, "context": "As another example, one previous study (Xu et al., 2016) shows that a genetic-programming based adversarial example strategy can always evade two state-of-art learning-based PDF-malware classifiers (with \"100%\" evasion rates).", "startOffset": 39, "endOffset": 56}, {"referenceID": 29, "context": "Table 5: Accuracy of the deep residual network(He et al., 2015) obtained from two noise-perturbed testing cases.", "startOffset": 46, "endOffset": 63}, {"referenceID": 29, "context": "In Table 9, the model we use is a 200-layer residual network (He et al., 2015) trained on Imagenet dataset (Deng et al.", "startOffset": 61, "endOffset": 78}, {"referenceID": 15, "context": ", 2015) trained on Imagenet dataset (Deng et al., 2009) by Facebook15.", "startOffset": 36, "endOffset": 55}, {"referenceID": 56, "context": "Table 6,Table 7 and Table 8 repeat similar experiments on three other DNN models: overfeat network(Sermanet et al., 2013), the residual network(He et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 29, "context": ", 2013), the residual network(He et al., 2015) and the VGG model (Simonyan & Zisserman, 2014).", "startOffset": 29, "endOffset": 46}, {"referenceID": 56, "context": "Table 6: Accuracy of the overfeat network(Sermanet et al., 2013) obtained from two noise-perturbed testing cases.", "startOffset": 41, "endOffset": 64}, {"referenceID": 29, "context": "Table 7: Accuracy of the residual network(He et al., 2015) obtained from two noise-perturbed testing cases in CIFAR-10 dataset (Krizhevsky & Hinton, 2009).", "startOffset": 41, "endOffset": 58}, {"referenceID": 46, "context": "Multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) exist in the DNN literature.", "startOffset": 29, "endOffset": 88}, {"referenceID": 39, "context": "Multiple hardening solutions (Zheng et al., 2016; Miyato et al., 2016; Lee et al., 2015) exist in the DNN literature.", "startOffset": 29, "endOffset": 88}, {"referenceID": 39, "context": "4) and (Lee et al., 2015) use Lf1(x, x \u2032) = d1(g1(x), g1(x \u2032)).", "startOffset": 7, "endOffset": 25}, {"referenceID": 46, "context": "(Miyato et al., 2016) uses the similar loss function as (Zheng et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 46, "context": "But (Miyato et al., 2016) uses adversarial perturbed x\u2032 from x.", "startOffset": 4, "endOffset": 25}, {"referenceID": 39, "context": "(Lee et al., 2015) uses Lf1(x, x \u2032) = d1(g1(x), g1(x \u2032)) and x\u2032s are generated xs by adding a small Gaussian noise.", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "Recently proposed adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses Lf1(x, x \u2032) = L(f1(x \u2032), f2(x)) and uses adversarial perturbed x\u2032 from x.", "startOffset": 39, "endOffset": 86}, {"referenceID": 37, "context": "Recently proposed adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses Lf1(x, x \u2032) = L(f1(x \u2032), f2(x)) and uses adversarial perturbed x\u2032 from x.", "startOffset": 39, "endOffset": 86}, {"referenceID": 47, "context": "Two recent studies (Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b) propose two similar measures both assuming d2 as norm functions, but do not consider the importance of an oracle.", "startOffset": 19, "endOffset": 74}, {"referenceID": 47, "context": "In (Moosavi-Dezfooli et al., 2015), the measure is normalized by the size of the test samples, while no evidence exists to show that the size of perturbation is related to the size of test samples.", "startOffset": 3, "endOffset": 34}, {"referenceID": 8, "context": "5) as \"Siamese Training\" because this formulation uses the Siamese architecture (Bromley et al., 1993), a classical deeplearning approach proposed for learning embedding.", "startOffset": 80, "endOffset": 102}, {"referenceID": 38, "context": "5 COMPARING DNN HARDENING STRATEGIES EXPERIMENTALLY Datasets: Currently, we are using the following 2 image datasets to evaluate our model: \u2022 MNIST: MNIST, released in (LeCun et al., 1998) includes a task to classify handwritten digits.", "startOffset": 168, "endOffset": 188}, {"referenceID": 25, "context": ", 2016) 16; (3) Siamese training (alone); (4) adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses adversarial perturbed x\u2032 and original samples x to train a DNN model.", "startOffset": 67, "endOffset": 114}, {"referenceID": 37, "context": ", 2016) 16; (3) Siamese training (alone); (4) adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016) uses adversarial perturbed x\u2032 and original samples x to train a DNN model.", "startOffset": 67, "endOffset": 114}], "year": 2017, "abstractText": "Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier (f1) and adds its oracle (f2, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor f1 and oracle f2, we develop necessary and sufficient conditions that can determine if f1 is always robust (strongrobust) against adversarial examples according to f2. Interestingly our theorems indicate that just one unnecessary feature can make f1 not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong robust.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_204"}