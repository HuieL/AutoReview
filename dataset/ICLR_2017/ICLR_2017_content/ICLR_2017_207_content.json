{"name": "ICLR_2017_207.pdf", "metadata": {"source": "CRF", "title": "SHIFT AGGREGATE EXTRACT NETWORKS", "authors": ["Francesco Orsini", "Daniele Baracchi", "Paolo Frasconi"], "emails": ["francesco.orsini@kuleuven.be", "daniele.baracchi@unifi.it", "paolo.frasconi@unifi.it"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many different problems in various fields of science require the classification of structured data, i.e. collections of objects bond together by some kind of relation. A natural way to represent such structures is through graphs, which are able to encode both the individual objects composing the collection (as vertices) and the relationships between them (as edges). A number of approaches to the graph classification problem has been studied in graph kernel and neural network literature.\nGraph kernels decompose input graphs in substructures such as shortest paths (Borgwardt & Kriegel, 2005), graphlets (Shervashidze et al., 2009) or neighborhood subgraph pairs (Costa & De Grave, 2010). The similarity between two graphs is then computed by comparing the respective sets of parts. Methods based on recursive neural networks unfold a neural network over input graphs and learn vector representations of their nodes employing backpropagation though structure (Goller & Kuchler, 1996). Recursive neural networks have been successfully applied to domains such as natural language (Socher et al., 2011) and biology (Vullo & Frasconi, 2004; Baldi & Pollastri, 2003). An advantage of recursive neural networks over graph kernels, is that the vector representations of the input graphs are learnt rather than handcrafted.\nLearning on social network data can be considerably hard due to their peculiar structure: as opposed to chemical compounds and parse trees, the structure of social network graphs is highly irregular. Indeed in social networks it is common to have nodes in the same graph whose degree differs by orders of magnitude. This poses a significant challenge for the substructure matching approach used by some graph kernels as the variability in connectivity generates a large number of unique patterns leading to diagonally dominant kernel matrices.\nWe propose Shift Aggregate Extract Networks (SAEN), a neural network architecture for learning representations of input graphs. SAEN decomposes input graphs intoH-hierarchies made of multiple strata of objects. Objects in each stratum are connected by \u201cpart-of\u201d relations to the objects to the stratum above.\nIn case we wish to classify graphs we can use an H-hierarchical decomposition in which the top stratum contains the graphG that we want to classify, while the intermediate strata contain subgraphs of G, subgraphs of subgraphs of G and so on, until we reach the bottom stratum which contains the vertices v of G.\nUnlike R-convolution relations in kernel methods (which decompose objects into the set of their parts), H-hierarchical decompositions are deep as they can represent the parts of the parts of an object.\nRecursive neural networks associate to the vertices of the input graphs vector representations imposing that they have identical dimensions. Moreover, the propagation follows the edge connectivity and weights are shared over the whole input graph. If we consider that vector representations of nodes (whose number of parents can differ by orders of magnitude) must share the same weights, learning on social network data with recursive neural networks might be nontrivial.\nSAEN compensates the limitations of recursive neural networks by adding the following degrees of flexibility:\n1. the SAEN computation schema unfolds a neural network over H-decompositions instead of the input graph, 2. SAEN imposes weight sharing and fixed size of the learnt vector representations on a per stratum basis instead of globally.\nIndeed SAEN allows to use vector representations of different sizes for different strata of objects (e.g. graphs, subgraphs, subgraphs of subgraphs, edges, vertices etc.) The SAEN schema computes the vector representation of each object by applying shift, aggregate and extract operations on the vector representations of its parts.\nAnother contribution of this paper is the introduction of a domain compression algorithm, that we use in our experiments to reduce memory usage and runtime. Domain compression collapses objects in the same stratum of an H-hierarchical decomposition into a compressed one whenever these objects are indistinguishable for the SAEN computation schema. In particular objects made of the same sets of parts are indistinguishable. In order obtain a lossless compression an H-hierarchical decomposition we store counts on symmetries adopting some mathematical results from lifted linear programming (Mladenov et al., 2012). The domain compression algorithm is also reminiscent of the work of Sperduti & Starita (1997) in which common substructures of recursive neural networks are collapsed in order to reduce the computational cost."}, {"heading": "2 SHIFT-AGGREGATE-EXTRACT NEURAL NETWORKS", "text": "We propose a neural network architecture that takes as input an undirected attributed graph G = (V,E,X) where V is the vertex set, E \u2286 V \u00d7 V is the edge set, and X = {xv \u2208 Rp}v\u2208V is a set of p-dimensional vertex attributes. When vertices do not have associated attributes (for example this happens in some of the social network datasets of \u00a7 4.1), we can set xv to some vertex invariant such as node centrality or betweenness.\n2.1 H-HIERARCHICAL DECOMPOSITIONS\nMost graph kernels decompose graphs into parts by using an R-convolution relation (Haussler, 1999). We extend this approach by decomposing graphs into a hierarchy of \u03c0-parametrized \u201cpart of\u201d relations. Formally, anH-hierarchical decomposition is a pair ({Sl}Ll=0, {Rl,\u03c0}Ll=1) where: \u2022 {Sl}Ll=0 are disjoint sets of objects Sl called strata, or levels of the hierarchy. The bottom stratum S0 contains non-decomposable objects (e.g. individual vertices), while the other strata Sl, l = 1, . . . , L contain composite objects, oi \u2208 Sl, whose parts oj \u2208 Sl\u22121 belong to the preceding stratum, Sl\u22121.\n\u2022 {Rl,\u03c0}Ll=1 is a set of l, \u03c0-parametrized Rl,\u03c0-convolution relations. A pair (oi, oj) \u2208 Sl \u00d7 Sl\u22121 belongs toRl,\u03c0 iff \u201coj is part of oi with membership type \u03c0\u201d. For notational convenience, the parts of oi are denoted asR\u22121l,\u03c0(oi) = {oj |(oj , oi) \u2208 Rl,\u03c0}. The membership type \u03c0 is used to represent the roles of the parts of an object. For example, we could decompose a graph as a multiset of \u03c0-neighborhood subgraphs 1 in which \u03c0 is the radius of the neighborhoods (see Figure 1 on the left). Another possible use of the \u03c0 membership type is to\n1The r-neighborhood subgraph (or ego graph) of a vertex v in a graph G is the induced subgraph of G consisting of all vertices whose shortest-path distance from v is at most r.\ndistinguish the root from the other vertices in a rooted neighborhood subgraph (see Figure 1 on the right).\nAn H-hierarchical decomposition is a multilevel generalization of R-convolution relations, and it reduces to anR-convolution relation for L = 1."}, {"heading": "2.2 SHIFT AGGREGATE EXTRACT SCHEMA FOR LEARNING REPRESENTATIONS", "text": "We propose Shift Aggregate Extract Network (SAEN) to learn vector representations for all the objects of all the strata {Sl}Ll=0 in an H-hierarchical decomposition. SAEN unfolds a neural network architecture over anH-hierarchical decomposition by using the Shift Aggregate Extract (SAE) schema.\nAccording to the SAE schema the vector representation of each object in theH-hierarchical decomposition is either computed by applying a neural network on the vertex attributes (for the objects in bottom stratum) or defined in terms of the vector representations of its parts (for the other objects).\nMore formally, the SAE schema associates a dl-dimensional representation hi \u2208 Rdl to each object oi \u2208 Sl of theH-hierarchical decomposition according to the following formula:\nhi =    f0(xvi ; \u03980) if oi \u2208 S0 fl ( \u2211 \u03c0\u2208\u03a0l \u2211 oj\u2208R\u22121l,\u03c0(oi) (z\u03c0 \u2297 hj)\ufe38 \ufe37\ufe37 \ufe38 Shift\ufe38 \ufe37\ufe37 \ufe38 Aggregate ; \u0398l )\n\ufe38 \ufe37\ufe37 \ufe38 Extract\notherwise (1)\nwhere fl(\u00b7; \u0398l), l = 0, . . . , L are multilayer neural networks with parameters \u0398l. With respect to the base case (first branch of Eq. 1) we have that each object oi in the bottom stratum S0 is in one-to-one correspondence with the vertices vi \u2208 V of the graph that we are decomposing. Indeed the vector representations hi are computed by evaluating f0(\u00b7; \u03980) in correspondence of the vertex attributes xvi \u2208 X . The recursion step (second branch of Eq. 1) follows the Shift Aggregate Extract (SAE) schema:\n\u2022 Shift: each part representation hj \u2208 Rdl\u22121 is remapped into a space R|\u03a0ldl\u22121| made of |\u03a0l| slots, where each slot has dimension dl\u22121. This transformation shifts part representations hj by using the Kronecker product \u2297 between an indicator vector z\u03c0 \u2208 R|\u03a0l| and the vector representation hj of part oj \u2208 Sl\u22121. The indicator vector z\u03c0 \u2208 R|\u03a0l| defined as zi = { 1 if i=\u03c0 0 otherwise. and it is used to\nmake sure that vector representations hj of object parts will fall in the same slot if and only if they have the same membership type \u03c0. \u2022 Aggregate: the shifted representations (z\u03c0 \u2297 hj) of the parts oj are then aggregated with a sum. \u2022 Extract: the aggregated representation is compressed to a dl-dimensional space by a \u0398lparametrized nonlinear map fl(\u00b7,\u0398l) : R|\u03a0ldl\u22121| \u2192 Rdl implemented with a multilayer neural network.\nThe shift and aggregate steps, that we have seen so far, are identical to those used in kernel design when computing the explicit feature of a kernel k(x, z) derived from a sum \u2211 \u03c0\u2208\u03a0 k\u03c0(x, z) of base kernels k\u03c0(x, z), \u03c0 \u2208 \u03a0. In principle, it would be indeed possible to turn SAEN into a kernel method by removing the extraction step E from the SAE schema. However, such an approach would increase the dimensionality of the feature space by a multiplicative factor |\u03a0l| for each level l of the Hhierarchical decomposition, thus leading to an exponential number of features. When using SAEN, the feature space growth is prevented by exploiting a distributed representation (via a multilayered neural network) during the E step of the SAE schema. As a result, SAEN can easily cope with Hhierarchical decompositions consisting of multiple strata."}, {"heading": "2.3 EXPLOITING SYMMETRIES FOR DOMAIN COMPRESSION", "text": "In this section we propose a technique, called domain compression, which allows to save memory and speedup the SAEN computation. Domain compression exploits symmetries inH-hierarchical decompositions by collapsing equivalent objects in each stratum. The greater the number of collapsed objects the highest the compression ratio.\nTwo objects a, b in a stratum Sl are collapsable a \u223c b if they share the same representation (i.e. ha = hb) for all the possible values of \u0398l. A compressed stratum S comp l is the quotient set Sl/\u223c of stratum Sl w.r.t. the collapsibility relation \u223c. We assume that the attributes of the elements in the bottom stratum S0 are categorical, so that the same vector representation can be shared by multiple elements with non-zero probability. 2 While objects in the bottom stratum S0 are collapsable when their attributes are identical, for all the other strata Sl, l = 1, . . . , L, objects are collapsable if they are made by the same sets of parts for all the membership types \u03c0.\nIn Figure 2 we provide a pictorial representation of the domain compression of an H-hierarchical decomposition (EGNN, described in \u00a7 4.2). On the left we show the H-hierarchical decomposition of a graph taken from the IMDB-BINARY dataset (see \u00a7 4.1) together with its compressed version on the right."}, {"heading": "2.3.1 DOMAIN COMPRESSION ALGORITHM", "text": "In order to compress H-hierarchical decompositions we adapt the lifted linear programming technique proposed by Mladenov et al. (2012) to the SAEN architecture. If a matrix M \u2208 Rn\u00d7p has\n2 Vectors of real valued attributes could be discretized using clustering techniques. However, we leave discretization in SAEN to future works.\nm \u2264 n distinct rows it can be decomposed as the product DM comp where M comp is a compressed version of M in which the distinct rows of M appear exactly once. The Boolean decompression matrix, D, encodes the collapsibility relation among the rows of M so that Dij = 1 iff the ith row of M falls in the equivalence class j of \u223c. A pseudo-inverse C of D can be computed by dividing the rows of D> by their sum (where D> is the transpose of D).\nExample 1 If we look at matrix M in Eq. 2 we notice that row 1 and 4 share the encoding [0, 0, 0], rows 3 and 5 share the encoding [1, 1, 0] while the encoding [1, 0, 1] appears only once at row 2. Matrix M comp is the compressed version of M .\nM =  \n0 0 0 1 0 1 1 1 0 0 0 0 1 1 0\n  M comp = [ 0 0 0 1 0 1 1 1 0 ] D =  \n1 0 0 0 1 0 0 0 1 1 0 0 0 0 1\n  C = [ 1/2 0 0 1/2 0 0 1 0 0 0 0 0 1/2 0 1/2 ] (2)\nMatrix M can be expressed as the matrix product between the decompression matrix D and the compressed version of M comp (i.e. M = DM comp), while the matrix multiplication between the compression matrix C and the M leads to the compressed matrix M comp (i.e.M comp = CM ).\nTo apply domain compression we rewrite Eq. 1 in matrix form as follows:\nHl =   \nf0(X; \u03980)\ufe38 \ufe37\ufe37 \ufe38 |S0|\u00d7d0\nif l = 0\nfl   [ Rl,1, . . . , Rl,\u03c0, . . . , Rl,|\u03a0l| ] \ufe38 \ufe37\ufe37 \ufe38\n|Sl|\u00d7|\u03a0l||Sl\u22121|\n  Hl\u22121 . . . 0 ... . . .\n... 0 . . . Hl\u22121\n \n\ufe38 \ufe37\ufe37 \ufe38 |\u03a0l||Sl\u22121|\u00d7|\u03a0l|dl\u22121\n; \u0398l  \n\ufe38 \ufe37\ufe37 \ufe38 |Sl|\u00d7dl\notherwise (3)\nwhere:\n\u2022 Hl \u2208 R|Sl|\u00d7dl is the matrix that represents the dl-dimensional encodings of the objects in Sl. The rows of Hl are the vector representations hi in Eq. 1, while the rows of Hl\u22121 are the vector representations hj in Eq. 1;\n\u2022 X \u2208 R|S0|\u00d7p is the matrix that represents the p-dimensional encodings of the vertex attributes in V (i.e. the rows of X are the xvi of Eq. 1);\n\u2022 fl(\u00b7; \u0398l) is unchanged w.r.t. Eq. 1 and is applied to its input matrices row-wise; \u2022 Rl,\u03c0 \u2208 R|Sl|\u00d7|Sl\u22121| \u2200\u03c0 \u2208 \u03a0l are the matrix representations of the Rl,\u03c0-convolution relations of Eq. 1 whose elements are (Rl,\u03c0)ij = 1 if (oj , oi) \u2208 Rl,\u03c0 and 0 otherwise. Domain compression on Eq. 3 is performed by the DOMAIN-COMPRESSION procedure (see Algorithm 3) that takes as input the attribute matrix X and the part-of matrices Rl,\u03c0 and returns their compressed versions Xcomp and the Rcompl,\u03c0 respectively. The algorithm starts by invoking (line 1) the procedure COMPUTE-CD on X to obtain the compression and decompression matrices C0 and D0 respectively. The compression matrix C0 is used to compress X (line 2) then we start iterating over the levels l = 0, . . . , L of the H-hierarchical decomposition (line 4) and compress the Rl,\u03c0 matrices. The compression of the Rl,\u03c0 matrices is done by right-multiplying them by the decompression matrixDl\u22121 of the previous level l\u22121 (line 5). In this way we collapse the parts of relation Rl,\u03c0 (i.e. the columns of Rl,\u03c0) as these were identified in stratum Sl\u22121 as identical objects (i.e. those objects corresponding to the rows of X or Rl\u22121,\u03c0 collapsed during the previous step). The result is a list Rcol comp = [Rl,\u03c0Dl\u22121, \u2200\u03c0 = 1, . . . , |\u03a0l|] of column compressed Rl,\u03c0\u2212matrices. We proceed collapsing equivalent objects in stratum Sl, i.e. those made of identical sets of parts: we find symmetries in Rcol comp by invoking COMPUTE-CD (line 6) and obtain a new pair Cl, Dl of compression, and decompression matrices respectively. Finally the compression matrix Cl is applied to the column-compressed matrices inRcol comp in order to obtain the \u03a0l compressed matrices\nDOMAIN-COMPRESSION(X,R) 1 C0, D0 = COMPUTE-CD(X) 2 Xcomp = C0X // Compress the X matrix. 3 Rcomp = {} // Initialize an empty container for compressed matrices. 4 for l = 1 to L 5 Rcol comp = [Rl,\u03c0Dl\u22121, \u2200\u03c0 = 1, . . . , |\u03a0l|] // column compression 6 Cl, Dl = COMPUTE-CD(Rcol comp) 7 for \u03c0 = 1 to |\u03a0l| 8 Rcompl,\u03c0 = ClR col comp \u03c0 // row compression 9 return Xcomp, Rcomp\nFigure 3: DOMAIN-COMPRESSION\nof stratum Sl (line 8). Algorithm 3 allows us to compute the domain compressed version of Eq. 3 which can be obtained by replacing: X with Xcomp = C0X , Rl,\u03c0 with R comp l,\u03c0 = ClRl,\u03c0Dl\u22121 and Hl with H comp l . Willing to recover the original encodings Hl we just need to employ the decompression matrix Dl on the compressed encodings H comp l , indeed Hl = DlH comp l .\nAs we can see by substituting Sl with S comp l , the more are the symmetries (i.e. when |S comp l | |Sl|) the greater the domain compression will be."}, {"heading": "3 RELATED WORKS", "text": "When learning with graph inputs two fundamental design aspects that must be taken into account are: the choice of the pattern generator and the choice of the matching operator. The former decomposes the graph input in substructures while the latter allows to compare the substructures.\nAmong the patterns considered from the graph kernel literature we have paths, shortest paths, walks (Kashima et al., 2003), subtrees (Ramon & Ga\u0308rtner, 2003; Shervashidze et al., 2011) and neighborhood subgraphs (Costa & De Grave, 2010). The similarity between graphs G and G\u2032 is computed by counting the number of matches between their common the substructures (i.e. a kernel on the sets of the substructures). The match between two substructures can be defined by using graph isomorphism or some other weaker graph invariant.\nWhen the number of substructures to enumerate is infinite or exponential with the size of the graph (perhaps this is the case for random walks and shortest paths respectively) the kernel between the two graphs is computed without generating an explicit feature map. Learning with an implicit feature map is not scalable as it has a space complexity quadratic in the number of training examples (because we need to store in memory the gram matrix).\nOther graph kernels such as the Weisfeiler-Lehman Subtree Kernel (WLST) (Shervashidze et al., 2011) and the Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) (Costa & De Grave, 2010) deliberately choose a pattern generator that scales polynomially and produces an explicit feature map. However the vector representations produced by WLST and NSPDK are handcrafted and not learned.\nA recent work by Yanardag & Vishwanathan (2015) proposes to uses pattern generators such as graphlets, shortest paths and WLST subtrees to transform input graphs into documents. The generated substructures are then treated as words and embedded in the Euclidean space with a CBOW or a Skip-gram model. The deep upgrade of existing graph kernels is performed by reweighing the counts of the substructures by the square root of their word-vector self similarity.\nAnother recent work by Niepert et al. (2016) upgrades the convolutional neural networks CNNs for images to graphs. While the receptive field of a CNN is usually a square window (Niepert et al., 2016) employ neighborhood subgraphs as receptive fields. As nodes in graphs do not have a specific temporal or spatial order, (Niepert et al., 2016) employ vertex invariants to impose an order on the nodes of the subgraphs/receptive fields."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We answer to the following experimental questions: Q1 How does SAEN compare to the state of the art? Q2 Can SAEN exploit symmetries in social networks to reduce the memory usage and the runtime?"}, {"heading": "4.1 DATASETS", "text": "In order to answer the experimental questions we tested our method on six publicly available datasets first proposed by Yanardag & Vishwanathan (2015).\n\u2022 COLLAB is a dataset where each graph represent the ego-network of a researcher, and the task is to determine the field of study of the researcher between High Energy Physics, Condensed Matter Physics and Astro Physics. \u2022 IMDB-BINARY, IMDB-MULTI are datasets derived from IMDB where in each graph the vertices represent actors/actresses and the edges connect people which have performed in the same movie. Collaboration graphs are generated from movies belonging to genres Action and Romance for IMDB-BINARYand Comedy, Romance and Sci-Fi for IMDB-MULTI, and for each actor/actress in those genres an ego-graph is extracted. The task is to identify the genre from which the ego-graph has been generated. \u2022 REDDIT-BINARY, REDDIT-MULTI5K, REDDIT-MULTI12K are datasets where each graph is derived from a discussion thread from Reddit. In those datasets each vertex represent a distinct user and two users are connected by an edge if one of them has responded to a post of the other in that discussion. The task in REDDIT-BINARYis to discriminate between threads originating from a discussion-based subreddit (TrollXChromosomes, atheism) or from a question/answers-based subreddit (IAmA, AskReddit). The task in REDDIT-MULTI5Kand REDDIT-MULTI12Kis a multiclass classification problem where each graph is labeled with the subreddit where it has originated (worldnews, videos, AdviceAnimals, aww, mildlyinteresting for REDDIT-MULTI5Kand AskReddit, AdviceAnimals, atheism, aww, IAmA, mildlyinteresting, Showerthoughts, videos, todayilearned, worldnews, TrollXChromosomes for REDDIT-MULTI12K)."}, {"heading": "4.2 EXPERIMENTS", "text": "In our experiments we chose an H-hierarchical decomposition called Ego Graph Neural Network (EGNN), that mimics the graph kernel NSPDK with the distance parameter set to 0.\nBefore applying EGNN we turn unattributed graphs (V,E) into attributed graphs (V,E,X) by annotating their vertices v \u2208 V with attributes xv \u2208 X . We label vertices v of G with their degree and encode this information into the attributes xv by employing the 1-hot encoding.\nEGNN decomposes attributed graphs G = (V,E,X) into a 3 level H-hierarchical decomposition with the following strata (see Figure 1 for a pictorial representation of EGNN): \u2022 stratum S0 contains objects ov that are in one-to-one correspondence with the vertices v \u2208 V . \u2022 stratum S1 contains vroot-rooted r-neighborhood subgraphs (i.e. ego graphs) e = (vroot, Ve, Ee) of radius r = 0, 1, . . . , R and has part-of alphabet \u03a01 = {ROOT, ELEM}. Objects ov \u2208 S0 are \u201cELEM-part-of\u201d ego graph e if v \u2208 Ve \\ {vroot}, while the are \u201cROOT-part-of\u201d ego graph e if v = vroot. \u2022 stratum S2 contains the graph G that we want to classify and has part-of alphabet \u03a02 = {0, 1} which correspond to the radius of the ego graphs e \u2208 S1 of which G is made of.\nE1 We experimented with SAEN applying the EGNN H-decomposition on all the datasets. For each dataset, we manually chose the parameters of SAEN, i.e. the number of hidden layers for each stratum, the size of each layer and the maximum radius R. We used the Leaky ReLU (Maas et al.) activation function on all the units. We report the chosen parameters in Table A1 of the appendix. In all our experiments we trained the neural networks by using the Adam algorithm to minimize a cross entropy loss.\nThe classification accuracy of SAEN was measured with 10-times 10-fold cross-validation. We manually chose the number of layers and units for each level of the part-of decomposition; the number of epochs was chosen manually for each dataset and we kept the same value for all the 100 runs of the 10-times 10-fold cross-validation.\nThe mean accuracies and their standard deviations obtained by our method are reported in Table 4, where we compare these results with those obtained by Yanardag & Vishwanathan (2015) and by Niepert et al. (2016).\nAlthough our method was conceived for social network data, it can also handle other types of graphs. For the sake of completeness in Table 5 we report the mean accuracies obtained with SAEN on the molecule and protein datasets studied in previous works (e.g. Niepert et al. (2016)).\nE2 In Table 1 we show the file sizes of the preprocessed datasets before and after the compression together with the data compression ratio. 3 We also estimate the benefit of the relational compression from a computational time point of view and report the measurement of the runtime for 1 run with and without compression together with the speedup factor.\nFor the purpose of this experiment, all tests were run on a computer with two 8-cores Intel Xeon E5-2665 processors and 94 GB RAM. Uncompressed datasets which exhausted our server\u2019s memory during the test are marked as \u201cOOM\u201d (out of memory) in the table, while those who exceeded the time limit of 100 times the time needed for the uncompressed version are marked as \u201cTO\u201d (timeout)."}, {"heading": "4.3 DISCUSSION", "text": "A1 As shown in Table 4, EGNN performs consistently better than the other two methods on all the social network datasets. This confirms that the chosenH-hierarchical decomposition is effective on this kind of problems. Also the results for molecule and protein datasets (see Table 5) are in line with the current state of the art. A2 The compression algorithm has proven to be effective in improving the computational cost of our method. Most of the datasets improved their runtimes by a factor of at least 4 while maintaining the\n3The size of the uncompressed files are shown for the sole purpose of computing the data compression ratio. Indeed the last version of our code compresses the files on the fly.\nsame expressive power. Moreover, experiments on REDDIT-MULTI5K and REDDIT-MULTI12K have only been possible thanks to the size reduction operated by the algorithm as the script exhausted the memory while executing the training step on the uncompressed files."}, {"heading": "5 CONCLUSIONS", "text": "We proposed SAEN, a novel architecture for learning vector representations of H-decompositions of input graphs. We applied SAEN for graph classification on 6 real world social network datasets, outperforming the current state of the art on 4 of them and obtaining state-of-the-art classification accuracy on the others. Another important contribution of this paper is the domain compression algorithm which greatly reduces memory usage and allowed us to speedup the training time of a factor of at least 4."}, {"heading": "APPENDIX: SHIFT AGGREGATE EXTRACT NETWORKS", "text": "Francesco Orsini 12 , Daniele Baracchi 2 and Paolo Frasconi 2\n1 Department of Computer Science\n2 Department of Information Engineering\nKatholieke Universiteit Leuven Universita\u0300 degli Studi di Firenze Celestijnenlaan 200A Via di Santa Marta 3\n3001 Heverlee, Belgium I-50139 Firenze, Italy francesco.orsini@kuleuven.be daniele.baracchi@unifi.it\npaolo.frasconi@unifi.it"}, {"heading": "A PARAMETERS USED IN THE EXPERIMENTS WITH EGNN", "text": "In Table A1 we report for each dataset: the radiuses r of the neighborhood subgraphs used in the EGNN decomposition and the number of units in the hidden layers for each stratum.\nFigure A1: Parameters for the neural networks used in the experiments. DATASET RADIUSES HIDDEN UNITS\nr S0 S1 S2 COLLAB 0, 1 15\u2212 5 5\u2212 2 5\u2212 3 IMDB-BINARY 0, 1, 2 2 5\u2212 2 5\u2212 3\u2212 1 IMDB-MULTI 0, 1, 2 2 5\u2212 2 5\u2212 3 REDDIT-BINARY 0, 1 10\u2212 5 5\u2212 2 5\u2212 3\u2212 1 REDDIT-MULTI5K 0, 1 10 10 6\u2212 5 REDDIT-MULTI12K 0, 1 10 10 20\u2212 11 MUTAG 0, 1, 2, 3 10 5\u2212 5 5\u2212 5\u2212 1 PTC 0, 1 15 15 15\u2212 1 NCI1 0, 1, 2, 3 15 15 15\u2212 10\u2212 1 PROTEINS 0, 1, 2, 3 3\u2212 2 6\u2212 5\u2212 4 6\u2212 3\u2212 1 D&D 0, 1, 2, 3 10 5\u2212 2 5\u2212 3\u2212 1"}], "references": [{"title": "The principled design of large-scale recursive neural network architectures\u2013 dag-rnns and the protein structure prediction problem", "author": ["P Baldi", "G Pollastri"], "venue": "J Mach Learn Res,", "citeRegEx": "Baldi and Pollastri.,? \\Q2003\\E", "shortCiteRegEx": "Baldi and Pollastri.", "year": 2003}, {"title": "Shortest-path kernels on graphs", "author": ["K M Borgwardt", "H-P Kriegel"], "venue": "In Proc. of the ICDM-05, pp. 8\u2013pp. IEEE,", "citeRegEx": "Borgwardt and Kriegel.,? \\Q2005\\E", "shortCiteRegEx": "Borgwardt and Kriegel.", "year": 2005}, {"title": "Fast neighborhood subgraph pairwise distance kernel", "author": ["F Costa", "K De Grave"], "venue": "In Proc. of the ICML-10,", "citeRegEx": "Costa and Grave.,? \\Q2010\\E", "shortCiteRegEx": "Costa and Grave.", "year": 2010}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["C Goller", "A Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "Goller and Kuchler.,? \\Q1996\\E", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Convolution kernels on discrete structures", "author": ["D Haussler"], "venue": "Technical report, Citeseer,", "citeRegEx": "Haussler.,? \\Q1999\\E", "shortCiteRegEx": "Haussler.", "year": 1999}, {"title": "Marginalized kernels between labeled graphs", "author": ["H Kashima", "K Tsuda", "A Inokuchi"], "venue": "In ICML-03,", "citeRegEx": "Kashima et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2003}, {"title": "Lifted linear programming", "author": ["M Mladenov", "B Ahmadi", "K Kersting"], "venue": "In AISTATS-12,", "citeRegEx": "Mladenov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mladenov et al\\.", "year": 2012}, {"title": "Learning convolutional neural networks for graphs", "author": ["M Niepert", "M Ahmed", "K Kutzkov"], "venue": "arXiv preprint arXiv:1605.05273,", "citeRegEx": "Niepert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Niepert et al\\.", "year": 2016}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J Ramon", "T G\u00e4rtner"], "venue": "In First International Workshop on Mining Graphs, Trees and Sequences,", "citeRegEx": "Ramon and G\u00e4rtner.,? \\Q2003\\E", "shortCiteRegEx": "Ramon and G\u00e4rtner.", "year": 2003}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["N Shervashidze", "SVN Vishwanathan", "T Petri", "K Mehlhorn", "K M Borgwardt"], "venue": "In AISTATS-09,", "citeRegEx": "Shervashidze et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2009}, {"title": "Weisfeilerlehman graph kernels", "author": ["N Shervashidze", "P Schweitzer", "E J van Leeuwen", "K Mehlhorn", "K M Borgwardt"], "venue": "J Mach Learn Res,", "citeRegEx": "Shervashidze et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R Socher", "C C Lin", "C Manning", "A Y Ng"], "venue": "In Proc. of the ICML-11,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Supervised neural networks for the classification of structures", "author": ["A Sperduti", "A Starita"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Sperduti and Starita.,? \\Q1997\\E", "shortCiteRegEx": "Sperduti and Starita.", "year": 1997}, {"title": "Disulfide connectivity prediction using recursive neural networks and evolutionary information", "author": ["A Vullo", "P Frasconi"], "venue": null, "citeRegEx": "Vullo and Frasconi.,? \\Q2004\\E", "shortCiteRegEx": "Vullo and Frasconi.", "year": 2004}, {"title": "Deep graph kernels", "author": ["P Yanardag", "SVN Vishwanathan"], "venue": "In Proc. of KDD-15,", "citeRegEx": "Yanardag and Vishwanathan.,? \\Q2015\\E", "shortCiteRegEx": "Yanardag and Vishwanathan.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Graph kernels decompose input graphs in substructures such as shortest paths (Borgwardt & Kriegel, 2005), graphlets (Shervashidze et al., 2009) or neighborhood subgraph pairs (Costa & De Grave, 2010).", "startOffset": 116, "endOffset": 143}, {"referenceID": 11, "context": "Recursive neural networks have been successfully applied to domains such as natural language (Socher et al., 2011) and biology (Vullo & Frasconi, 2004; Baldi & Pollastri, 2003).", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": "In order obtain a lossless compression an H-hierarchical decomposition we store counts on symmetries adopting some mathematical results from lifted linear programming (Mladenov et al., 2012).", "startOffset": 167, "endOffset": 190}, {"referenceID": 4, "context": "1 H-HIERARCHICAL DECOMPOSITIONS Most graph kernels decompose graphs into parts by using an R-convolution relation (Haussler, 1999).", "startOffset": 114, "endOffset": 130}, {"referenceID": 5, "context": "Among the patterns considered from the graph kernel literature we have paths, shortest paths, walks (Kashima et al., 2003), subtrees (Ramon & G\u00e4rtner, 2003; Shervashidze et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 10, "context": ", 2003), subtrees (Ramon & G\u00e4rtner, 2003; Shervashidze et al., 2011) and neighborhood subgraphs (Costa & De Grave, 2010).", "startOffset": 18, "endOffset": 68}, {"referenceID": 10, "context": "Other graph kernels such as the Weisfeiler-Lehman Subtree Kernel (WLST) (Shervashidze et al., 2011) and the Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) (Costa & De Grave, 2010) deliberately choose a pattern generator that scales polynomially and produces an explicit feature map.", "startOffset": 72, "endOffset": 99}, {"referenceID": 7, "context": "While the receptive field of a CNN is usually a square window (Niepert et al., 2016) employ neighborhood subgraphs as receptive fields.", "startOffset": 62, "endOffset": 84}, {"referenceID": 7, "context": "As nodes in graphs do not have a specific temporal or spatial order, (Niepert et al., 2016) employ vertex invariants to impose an order on the nodes of the subgraphs/receptive fields.", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "DATASET PSCN (k = 10) SAEN (Niepert et al., 2016) (our method) MUTAG 92.", "startOffset": 27, "endOffset": 49}], "year": 2016, "abstractText": "The Shift Aggregate Extract Network (SAEN) is an architecture for learning representations on social network data. SAEN decomposes input graphs into hierarchies made of multiple strata of objects. Vector representations of each object are learnt by applying shift, aggregate and extract operations on the vector representations of its parts. We propose an algorithm for domain compression which takes advantage of symmetries in hierarchical decompositions to reduce the memory usage and obtain significant speedups. Our method is empirically evaluated on real world social network datasets, outperforming the current state of the art.", "creator": "TeX"}, "id": "ICLR_2017_207"}