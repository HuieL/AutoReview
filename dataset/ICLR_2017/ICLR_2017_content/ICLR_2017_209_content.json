{"name": "ICLR_2017_209.pdf", "metadata": {"source": "CRF", "title": "BIT-PRAGMATIC DEEP NEURAL NETWORK COMPUT- ING", "authors": ["Jorge Albericio", "Patric Judd", "Alberto Delmas Lascorz", "Sayeh Sharify", "Andreas Moshovos"], "emails": ["delmasl1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Network (DNN) hardware typically uses either 16-bit fixed-point Chen et al. (2014) or quantized 8-bit numbers Warden (2016) and bit-parallel compute units. For convolutional layers, that account for most of the execution time in Convolutional Neural Networks (CNNs) during image classification, these bit-parallel engines perform many ineffectual computations. Specifically, these layers perform several several inner products, where multiple pairs of weights and activations are multiplied and then reduced into an output activation. Any time a zero bit of an activation or a weight is multiplied it adds nothing to the final output activations. These ineffectual bits are introduced by the conventional positional number representation and if avoided it would take even less time to calculate each product improving energy and performance. As a first step, this work targets the ineffectual bits of activations only. Section 2 shows that in recent image classification networks 93% and 69% of activation bit and weight products are ineffectual when using respectively 16-bit fixed-point and 8-bit quantized representations.\nThis work presents Pragmatic (PRA) a DNN accelerator whose goal is to process only the essential (non-zero) bits of the input activations PRA employs the following four key techniques: 1) on-thefly conversion of activations from a storage representation (e.g., conventional positional number or quantized) into an explicit representation of the essential bits only, 2) bit-serial activation/bitparallel weight processing, an idea borrowed from STR Judd et al. (2016b;a) but adapted for the aforementioned representation, 3) judicious SIMD (single instruction multiple data) lane grouping to maintain wide memory accesses and to avoid fragmenting and enlarging the multi-MB on-chip weight memories (Sections 5 and 5.1), and 4) computation re-arrangement (Section 5.1) to reduce datapath area. All evaluated PRA variants maintain wide memory accesses and use highly-parallel SIMD-style (single-instruction multiple-data) computational units. PRA introduces an additional dimension upon which software can improve performance and energy efficiency by controlling ac-\nBit-Parallel Hardware Precision\ntivation values judiciously in order to reduce their essential bit content while maintaining accuracy. This work explores such an alternative, where the software explicitly communicates how many prefix and suffix bits to discard after each layer.\nExperimental measurements with recent CNNs for image classification demonstrate that most straightforward PRA variant, boosts average performance for the convolutional layers to 2.59x over the state-of-the-art DaDN accelerator. Pragmatic\u2019s average energy efficiency is 1.48x over DaDN and its area overhead is 1.35x. Another variant further boosts performance to 3.1x over DaDN at the expense of an additional 0.7% area."}, {"heading": "2 MOTIVATION", "text": "Let us assume a p-bit bit-parallel multiplier using a straightforward implementation of the \u201cShift and Add\u201d algorithm where n\u00d7 s is calculated as \u2211p i=0 ni \u00b7 (s i), where ni the i-th bit of n. The multiplier computes p terms, each a product of s and of a bit of n, and adds them to produce the final result. The terms and their sum can be calculated concurrently to reduce latency Wallace (1964).\nWith such a hardware arrangement there are two sources of ineffectual computations that result from: 1) an Excess of Precision (EoP), and 2) Lack of Explicitness (LoE). Figure 1 shows an example illustrating these sources with a bit-parallel multiplier using an 8-bit unsigned fixed-point number with 4 fractional and 4 integer bits. While 10.101(2) requires just five bits, our 8-bit bit-parallel multiplier will zero-extend it with two prefix and one suffix bits. This is an example of EoP and is due to the fixed-precision hardware. Two additional ineffectual bits appear at positions 1 and -2 as a result of LoE in the positional number representation. In total, five ineffectual bits will be processed generating five ineffectual terms.\nOur number could be represented with an explicit list of its three constituent powers of 2: (1,-1,- 3). While such a representation may require more bits and thus be undesirable for storage, coupled with the abundant parallelism that is present in DNNs layers, it provides an opportunity to revisit hardware design improving performance and energy efficiency.\nTable 5 reports the essential bit content of the activation stream of recent CNNs for two commonly used fixed length representations: 1) 16-bit fixed-point of DaDianNao Chen et al. (2014), 2) 8-bit quantized of Tensorflow Warden (2016). The essential bit content is the average number of non-zero bits that are 1. Two measurements are presented per representation: over all neuron values (\u201cAll\u201d), and over the non-zero neurons (\u201cNZ\u201d) as accelerators that can skip zero activations for fixed-point representations have been recently proposed Han et al. (2016); Albericio et al. (2016).\nWhen considering all activations, the essential bit-content is at most 12.7% and 38.4% for the fixedpoint and the quantized representations respectively. Even when considering the non-zero activations the essential bit content remains well below 50% suggesting that the potential exists to improve performance and energy efficiency over approaches that target zero valued activations only.\n3 Pragmatic: A SIMPLIFIED EXAMPLE\nThis section illustrates the idea behind Pragmatic via a simplified example.\nThe bit-parallel unit of Figure 2a multiplies two activations with their respective weights and via an adder reduces the two products. The unit reads all activation and weight, (n0 = 001(2), n1 = 010(2)) and (s0 = 001(2), s1 = 111(2)) respectively in a single cycle. As a result, the two sources of inefficiency EoP and LoE manifest here: n0 and n1 are represented using 3 bits instead of 2 respectively due to EoP. Even in 2 bits, they each contain a zero bit due to LoE. As a result, four ineffectual terms are processed when using standard multipliers such as those derived from the Shift and Add algorithm. In general, given N activation and weight pairs, this unit will take dN/2e cycles to process them regardless of their precision and the essential bit content of the activations.\nFigure 2b shows a simplified PRA engine. In this example, activations are no longer represented as vectors of bits but as vectors of offsets of the essential bits. For example, activation n0 = 001(2) is represented as on0 = (0), and a activation value of 111(2) would be represented as (2, 1, 0). An outof-band bit (wire) not shown indicates the activation\u2019s end. A shifter per activation uses the offsets to effectively multiply the corresponding weight with the respective power of 2 before passing it to the adder tree. As a result, PRA processes only the non-zero terms avoiding all ineffectual computations that were due to EoP or LoE. To match the throughput of the bit-parallel engine of Figure 2a, we take advantage of weight reuse and processes multiple activations groups in parallel. In this example, six activations (n0 = 001(2), n1 = 010(2), n\u20320 = 000(2), n \u2032 1 = 010(2), n \u2032\u2032 0 = 010(2), n \u2032\u2032 1 = 000(2)) are combined with the two weights as shown. For this example, PRA would process the six activation and weight pairs in a single cycle, a speedup of 3\u00d7 over the bit-parallel engine."}, {"heading": "4 BASELINE SYSTEM: DADIANNAO", "text": "Pragmatic is demonstrated as a modification of the DaDianNao accelerator (DaDN) proposed by Chen et al. Chen et al. (2014). Figure 3a shows a DaDN tile which processes 16 filters concurrently calculating 16 activation and weight products per filter for a total of 256 products per cycle. To do, each cycle the tile accepts 16 weights per filter for total of 256 weight, and 16 input activations. The tile multiplies each weight with only one activation whereas each activation is multiplied with 16 weight, one per filter. The tile reduces the 16 products into a single partial output activation per filter, for a total of 16 partial output activations for the tile. Each DaDN chip comprises 16 such tiles, each\nprocessing a different set of 16 filters per cycle. Accordingly, each cycle, the whole chip processes 16 activations and 256\u00d7 16 = 4K weights producing 16\u00d7 16 = 256 partial output activations. Internally, each tile has: 1) a synapse buffer (SB) that provides 256 weights per cycle one per synapse lane, 2) an input neuron buffer1 (NBin) which provides 16 activations per cycle through 16 neuron lanes, and 3) a neuron output buffer (NBout) which accepts 16 partial output activations per cycle. In the tile\u2019s datapath, or the Neural Functional Unit (NFU) each neuron lane is paired with 16 synapse lanes one from each filter. Each synapse and neuron lane pair feed a multiplier and an adder tree per filter lane reduces the 16 per filter products into a partial sum. In all, the filter lanes produce each a partial sum per cycle, for a total of 16 partial output activations per NFU. Once a full window is processed, the 16 resulting sums, are fed through a non-linear activation function, f , to produce the 16 final output activations. The multiplications and reductions needed per cycle are implemented via 256 multipliers one per synapse lane and sixteen 17-input (16 products plus the partial sum from NBout) adder trees one per filter lane.\nDaDN\u2019s main goal was minimizing off-chip bandwidth while maximizing on-chip compute utilization. To avoid fetching weights from off-chip, DaDN uses a 2MB eDRAM SB per tile for a total of 32MB eDRAM. All inter-layer activations except for the initial input and the final output are stored in a 4MB shared central eDRAM Neuron Memory (NM) which is connected via a broadcast interconnect to the 16 NBin buffers. Off-chip accesses are needed only for reading the input image, the filter weights once per layer, and for writing the final output.\nTerminology: For clarity, in what follows n(x, y, i) and o(x, y, i) refer to an input and an output activation at coordinates (x, y, i) respectively. The weight of filter f at coordinates (x, y, i) is denoted as sf (x, y, i). The term brick refers to a set of 16 elements of a 3D activation or weight array which are contiguous along the i dimension, e.g., n(x, y, i)...n(x, y, i + 15). Bricks will be denoted by their origin element with a B subscript, e.g., nB(x, y, i). The term pallet refers to a set of 16 bricks corresponding to adjacent, using a stride S, windows along the x or y dimensions, e.g., nB(x, y, i)...nB(x, y+15\u00d7S, i) and will be denoted as nP (x, y, i). The number of activations per brick, and bricks per pallet are design parameters.\nProcessing Approach: Processing starts by reading from external memory the first layer\u2019s weights synapses, and the input image. The weights are distributed over the SBs and the input is stored into NM. Each cycle an input activation brick is broadcast to all units. Each units reads 16 weight bricks from its SB and produces a partial output activation brick which it stores in its NBout. Once computed, the output activations are stored through NBout to NM and then fed back through the NBins when processing the next layer. Loading the next set of activations from external memory can be overlapped with the processing of the current layer as necessary.\n1Chen et al. (2014) used the terms neuron and synapse to refer to activations and weights respectively and named the various components accordingly. We maintain this terminology for the design\u2019s components."}, {"heading": "5 Pragmatic", "text": "PRA\u2019s goal is to process only the essential bits of the activations. To do so PRA a) converts, on-thefly, the input activation representation into one containing only the essential bits, and b) processes one essential bit per activation and a full 16-bit weight per cycle. Since PRA processes activation bits serially, it may take up to 16 cycles to produce a product of a activation and a weight. To always match or exceed the performance of the bit-parallel units of DaDN, PRA processes more activations concurrently exploiting the abundant parallelism of the convolutional layers. The remaining of this section describes in turn: 1) an appropriate activation representation, 2) the way PRA calculates terms, 3) how multiple terms are processed concurrently to maintain performance on par with DaDN in the worst case, and 4) how PRA\u2019s units are supplied with the necessary activations from NM.\nInput Activation Representation: PRA starts with an input activation representation where it is straightforward to identify the next essential bit each cycle. One such representation is an explicit list of oneffsets, that is of the constituent powers of two. For example, an activation n = 5.5(10) = 0101.1(2) would be represented as n = (2, 0,\u22121). In the implementation described herein, activations are stored in 16-bit fixed-point in NM, and converted on-the-fly in the PRA representation as they are broadcast to the tiles. A single oneffset is processed per activation per cycle. Each oneffset is represented as (pow, eon) where pow is a 4-bit value and eon a single bit which if set indicates the end of a activation. For example, n = 101(2) is represented as nPRA = ((0010, 0)(0000, 1)).\nCalculating a (weight, activation) product: PRA calculates the product of weight s and activation n as: s\u00d7 n = \u2211\n\u2200f\u2208nPRA s\u00d7 2f = \u2211 \u2200f\u2208nPRA (n f)\nThat is, each cycle, the weight s multiplied by f , the next constituent power two of n, and the result is accumulated. This multiplication can be implemented as a shift and an AND.\nBoosting Compute Bandwidth over DaDN: To match DaDN\u2019s performance PRA needs to process the same number of effectual terms per cycle. Each DaDN tile calculates 256 activation and weight products per cycle, or 256 \u00d7 16 = 4K terms. While most of these terms will be in practice ineffectual, to guarantee that PRA always performs as well as DaDN it should process 4K terms per cycle. For the time being let us assume that all activations contain the same number of essential bits, so that when processing multiple activations in parallel, all units complete at the same time and thus can proceed with the next set of activations in sync. The next section will relax this constraint.\nSince PRA processes activations bits serially, it produces one term per activation bit and weight pair and thus needs to process 4K such pairs concurrently. The choice of which 4K activation bit and weight pairs to process concurrently can adversely affect complexity and performance. For example, it could force an increase in SB capacity and width, or an increase in NM width, or be ineffective due to unit underutilization given the commonly used layer sizes.\nFortunately, it is possible to avoid increasing the capacity and the width of the SB and the NM while keeping the units utilized as in DaDN. Specifically, a PRA tile can read 16 weight bricks and the equivalent of 256 activation bits as DaDN\u2019s tiles do (DaDN processes 16 16-bit activations or 256 activation bits per cycle). Specifically, as in DaDN, each PRA tile processes 16 weight bricks concurrently, one per filter. However, differently than DaDN where the 16 weight bricks are combined with just one activation brick which is processed bit-parallel, PRA combines each weight brick with 16 activation bricks, one from each of 16 windows, which are processed bit-serially. The same 16 activation bricks are combined with all weight bricks. These activation bricks form a pallet enabling the same weight brick to be combined with all. For example, in a single cycle a PRA title processing filters 0 through 15 could combine combine s0B(x, y, 0), ..., s 1 B5(x, y, 0) with nPRAB (x, y, 0), n PRA B (x+2, y, 0), ...n PRA B (x+31, y, 0) assuming a layer with a stride of 2. In this case, s4(x, y, 2) would be paired with nPRA(x, y, 2), nPRA(x + 2, y, 2), ..., nPRA(x + 31, y, 2) to produce the output weights on(x, y, 4) through on(x+ 15, y, 4).\nAs the example illustrates, this approach allows each weight to be combined with one activation per window whereas in DaDN each weight is combined with one activation only. In total, 256 essential activation bits are processed per cycle and given that there are 256 weights and 16 windows, PRA\nprocesses 256 \u00d7 16 = 4K activation bit and weight pairs, or terms per cycle producing 256 partial output activations, 16 per filter, or 16 partial output activation bricks per cycle.\nSupplying the Inputs: Thus far it was assumed that all input activations have the same number of essential bits. Under this assumption, all neuron lanes complete processing their terms at the same time, allowing PRA to move on to the next activation pallet and the next set of weight bricks in one step. This allows PRA to reuse STR\u2019s approach for fetching the next pallet from the singleported NM Judd et al. (2016b;a). Briefly, with unit stride the 256 weights would be typically all stored in the same NM row or at most over two adjacent NM rows and thus can be fetched in at most two cycles. When the stride is more than one, the weights will be spread over multiple rows and thus multiple cycles will be needed to fetch them all. Fortunately, fetching the next pallet can be overlapped with processing the current one. Accordingly, if it takes NMC to access the next pallet from NM, while the current pallet requires PC cycles to process, the next pallet will begin processing after max(NMC , PC) cycles. When NMC > PC performance is lost waiting for NM.\nIn practice it highly unlikely that all activations will have the same number of essential bits. In general, each neuron lane if left unrestricted will advance at a different rate. In the worst case, each neuron lane may end up needing activations from a different activation brick, thus breaking PRA\u2019s ability to reuse the same weight brick. This is undesirable if not impractical as it would require partitioning and replicating the SB so that 4K unrelated weight could be read per cycle, and it would also increase NM complexity and bandwidth.\nFortunately, these complexities can be avoided with pallet-level neuron lane synchronization where all neuron lanes \u201cwait\u201d (a neuron lane that has detected the end of its activation forces zero terms while waiting) for the one with the most essential bits to finish before proceeding with the next pallet. Under this approach it does not matter which bits are essential per activation, only how many exist. Since, it is unlikely that most pallets will contain an activation with 16 essential terms, PRA will improve performance over DaDN. Section 5.1 will discuss finer-grain synchronization schemes that lead to even better performance. Before doing so, however, we detail PRA\u2019s design."}, {"heading": "5.1 STRUCTURE AND PERFORMANCE AND AREA OPTIMIZATIONS", "text": "Figure 3b shows the Pragmatic tile architecture which comprises an array of 16 \u00d7 16 = 256 pragmatic inner product units (PIPs). PIP(i,j) processes an activation oneffset from the i-th window and its corresponding weight from the j-th filter. Specifically, all the PIPs along the i-th row receive the same weight brick belonging to the i-th filter and all PIPs along the j-th column receive an oneffset from each activation from one activation brick belonging to the j-th window. The necessary activa-\ntion oneffsets are read from NBin where they have been placed by the Dispatcher and the Oneffset generators units as Section 5.1 explains. Every cycle NBin sends 256 oneffsets 16 per window lane. All the PIPs in a column receive the same 16 oneffsets, corresponding to the activations of a single window. When the tile starts to process a new activation pallet, 256 weights are read from SB through its 256 synapse lanes as in DaDN and are stored in the synapse registers (SR) of each PIP. The weights and oneffsets are then processed by the PIPs.\nPragmatic Inner-Product Unit: Figure 4 shows the PIP internals. Every cycle, 16 weights are combined with their corresponding oneffsets. Each oneffsets controls a shifter effectively multiplying the weight with a power of two. The shifted weights are reduced via the adder tree. An AND gate per weight supports the injection of a null terms when necessary. In the most straightforward design, the oneffsets use 4-bits, each shifter accepts a 16-bit weight and can shift it by up to 15 bit positions producing a 31-bit output. Finally, the adder tree accepts 31-bit inputs. Section 5.1 presents an enhanced design that requires narrower components improving area and energy.\nDispatcher and Oneffset Generators The Dispatcher reads 16 activation bricks from NM, as expected by the PRA tiles. The oneffset generator converts their activations on-the-fly to the oneffset representation, and broadcasts one oneffset per activation per cycle for a total of 256 oneffsets to all titles. Fetching and assembling the 16 activation bricks from NM is akin to fetching words with a stride of S from a cache structure. Once the 16 activation bricks have been collected, 256 oneffset generators operate in parallel to locate and communicate the next oneffset per activation. A straightforward 16-bit leading one detector is sufficient. The latency of the oneffset generators and the dispatcher can be readily hidden as they can be pipelined as desired overlapping them with processing in the PRA tiles.\nReducing Title Area with 2-Stage Shifting: Any shift can be performed in two stages as two smaller shifts: a K = a (K \u2032 + C) = ((a K \u2032) C). Thus, to shift and add T weights by different offsets K0, ...,KT , we can decompose the offsets into sums with a common term C, e.g., Ki = K \u2032i + C. Accordingly, PIP processing can be rearranged using a two stage processing where the first stage uses a per weight specific offset K \u2032i, and the second stage, the common across all weights offset C. This arrangement can be used to reduce the width of the weight shifters and of the adder tree by sharing one common shifter after the adder tree as Figure 5a shows. A design parameter, L, defines the number of bits controlling the weight shifters so that the design can process oneffsets which differ by less than 2L in a single cycle. This reduces the size of the weight shifters and reduces the size of the adder tree to support terms of 16 + 2L \u2212 1 bits only. Increasing Performance with Per-Column Neuron Lane Synchronization: The pallet neuron lane synchronization scheme of Section 5 is one of many possible synchronization schemes. Finergrain neuron lane synchronization schemes are possible leading to higher performance albeit at a cost. Among them, per column neuron lane synchronization is an appealing scheme offering a good balance of cost vs. performance. Here each PIP column operates independently but all the PIPs along the same column synchronize before moving to the next activation brick. Since the PIPs along the same column operate in sync, they all process one set of 16 weight bricks which can be read using the existing SB interface. However, given that different PIP columns operate now out-of-\nsync, the SB would be accessed more frequently and could become a bottleneck. There are two concerns: 1) different PIP columns may need to perform two independent SB reads while there are only one SB port and one common bus connecting the PIP array to the SB, and 2) there will be repeat accesses to SB that will increase SB energy, while the SB is already a major consumer of energy. These concerns are addressed as follows: 1) only one SB access can proceed per cycle thus a PIP column may need to wait when collisions occur. 2) A set of registers, or synapse set registers (SSRs) are introduced in front of the SB each holding a recently read set of 16 weight bricks. Since all PIP columns will eventually need the same set of weight bricks, temporarily buffering them avoids fetching them repeatedly from the SB. Once a weight set has been read into an SSR, it stays there until all PIP columns have copied it (a 4-bit down counter is sufficient for tracking how many PIP columns have yet to read the weight set). This policy guarantees that the SB is accessed the same number of times as in DaDN. However, stalls may incur as a PIP column has to be able to store a new set of weights into an SSR when it reads it from the SB. Figure 6 shows an example. Since each neuron lane advances independently, in the worst case, the dispatcher may need to fetch 16 independent activation bricks each from a different pallet. The Dispatcher can buffer those pallets to avoid rereading NM, which would, at worst, require a 256 pallet buffer. However, given that the number SSRs restricts how far apart the PIP columns can be, and since Section 6.2 shows that only one SSR is sufficient, a two pallet buffer in the dispatcher is all that is needed.\nFurther Increasing Performance with Improved Oneffset Encoding: Since PIPs in Pragmatic can negate any input term, it is possible to enhance the oneffset generator to generate fewer oneffsets for neuron values containing runs of ones by allowing signed oneffsets Booth (1951).\nThis improved generator reduces runs of adjacent oneffsets a...b into pairs of the form a + 1,\u2212b. Single oneffsets or gaps inside runs are represented by a positive or negative oneffset, respectively. For example a neuron value of 11011 that would normally be encoded with oneffsets (4, 3, 1, 0) can instead be represented with (5,\u22123,+2,\u22120) or even more economically with (5,\u22122,\u22120). This is equivalent to a Radix-4 Booth encoding and will never emit more than \u230a x 2 + 1 \u230b oneffsets, where x is the neuron precision.\nThis encoding will never produce more oneffsets compared to the baseline encoding. However, because of the 2-stage shifting, it is possible that this encoding will increase the number of cycles needed. This will happen when the oneffset distribution among the bit groups being processed together during 2-stage shifting changes.\nFinally, booth encoding is conventionally used to reduce the number of cycles needed to perform multiplication in single shift-and-add multipliers typically reserved for low cost low performance designs, or to reduce the depth of bit-parallel multipliers. Pragmatic with its 2-stage shifting and judicious lane synchronization enables its practical use in a massively data-parallel accelerator boosting performance beyond what is possible with bit-parallel units.\nThe Role of Software: PRA enables an additional dimension upon which hardware and software can attempt to further boost performance and energy efficiency, that of controlling the essential activation value content. This work investigates a software guided approach where the precision requirements of each layer are used to zero out a number of prefix and suffix bits at the output of each layer. Using the profiling method of Judd et al., Judd et al. (2015), software communicates the precisions needed by each layer as meta-data. The hardware trims the output activations before writing them to NM using AND gates and precision derived bit masks."}, {"heading": "6 EVALUATION", "text": "The performance, area and energy efficiency of Pragmatic is compared against DaDN Chen et al. (2014) and Stripes Judd et al. (2016b), two state-of-the-art DNN accelerators. DaDN is the fastest bit-parallel accelerator proposed to date that processes all activations regardless of theirs values, and STR improves upon DaDN by exploiting the per layer precision requirements of DNNs. Cnvlutin improves upon DaDN by skipping most zero- or near-zero-valued activations Albericio et al. (2016), however, Stripes has been shown to outperform it.\nAfter reviewing the experimental methodology the rest of this section is organized as follows: Sections 6.1 and 6.2 explore the PRA design space considering respectively single- and 2-stage shifting configurations, and column synchronization. Section 6.2 reports energy efficiency for the best\nconfiguration. Section 6.4 analyzes the contribution of the software provided precisions. Finally, Section 6.5 reports performance for designs using an 8-bit quantized representation.\nMethodology: The same methodology is used for all systems for consistency. A custom cycleaccurate simulator models execution time. For all systems, computation was scheduled to minimize energy, which led to the same schedule for all. To estimate power and area, the designs were synthesized with the Synopsis Design Compiler Synopsys for a TSMC 65nm library. The NBin and NBout SRAM buffers were modeled using CACTI Muralimanohar & Balasubramonian. The eDRAM area and energy were modeled with Destiny Poremba et al. (2015). To compare against STR, the per layer numerical representation requirements reported in Table 2 were found using the methodology of Judd et al. Judd et al. (2016b). All PRA configurations studied exploit software provided precisions as per Section 5.1. Section 6.4 analyzes the impact of this information on overall performance. All performance measurements are for the convolutional layers only which account for more than 92% of the overall execution time in DaDN Chen et al. (2014). PRA does not affect the execution time of the remaining layers."}, {"heading": "6.1 SINGLE- AND 2-STAGE SHIFTING", "text": "This section evaluates the single-stage shifting PRA configuration of Sections 5\u2013 5.1, and the 2-stage shifting variants of Section 5.1. Section 6.1 reports performance while Section 6.1 reports area and power. In this section, All PRA systems use pallet synchronization.\nPerformance: Figure 7 shows the performance of STR (leftmost bars) and of PRA variants relative to DaDN. The PRA systems are labelled with the number of bits used to operate the first-stage, weight shifters, e.g., the weight shifters of \u201c2-bit\u201d , or PRA2b, are able to shift to four bit positions (0\u20133). \u201c4-bit\u201d or PRA4b, is the single-stage Pragmatic, or PRAsingle of Sections 5\u2013 5.1 whose weight shifters can shift to 16 bit positions (0\u201315). It has no second stage shifter.\nPRAsingle improves performance by 2.59\u00d7 on average over DaDN compared to the 1.85\u00d7 average improvement with STR. Performance improvements over DaDN vary from 2.11\u00d7 for VGG19 to 2.97\u00d7 for VGGM. As expected the 2-stage PRA variants offer slightly lower performance than PRAsingle, however, performance with PRA2b and PRA3b is always within 0.2% of PRAsingle. Even PRA0b which does not include any weight shifters outperforms STR by 20% on average. Given a set of oneffsets, PRA0b will accommodate the minimum non-zero oneffset per cycle via its second level shifter.\nArea and Power: Table 3 shows the absolute and relative to DaDN area and power. Two area measurements are reported: 1) for the unit excluding the SB, NBin and NBout memory blocks, and 2) for the whole chip comprising 16 units and all memory blocks. Since SB and NM dominate chip area, the per area area overheads Given the performance advantage of PRA, the area and power overheads are justified. PRA2b is particularly appealing as its overall area cost over BASE is only 1.35\u00d7 and its power 2.03\u00d7 while its performance is 2.59\u00d7 on average. Accordingly, we restrict attention to this configuration in the rest of this evaluation."}, {"heading": "6.2 PER-COLUMN SYNCHRONIZATION", "text": "Performance: Figure 8 reports the relative performance for PRA2b with column synchronization and as a function of the number of SSRs as per Section 5.1. Configuration PRAxR2b refers to a\nconfiguration using x SSRs. Even PRA1R2b boosts performance to 3.1\u00d7 on average close to the 3.45\u00d7 that is ideally possible with PRA\u221eR2b . Area and Power: Table 4 reports the area per unit, and the area and power per chip. The best performing PRA1R2b increases chip area by only 1.35\u00d7 and power by only 2.19\u00d7 over DaDN. Energy Efficiency: Figure 10 shows the energy efficiency of various configurations of Pragmatic. Energy Efficiency, or simply efficiency for a system NEW relative to BASE is defined as the ratio EBASE/ENEW of the energy required by BASE to compute all of the convolution layers over that of NEW. For the selected networks, STR is 16% more efficient than DaDN. The power overhead of PRAsingle (PRA4b) is more than the speedup resulting in a circuit that is 5% less efficient than DaDN. PRA2b reduces that power overhead while maintaining performance yielding an efficiency of 28%. PRA1R2b yields the best efficiency at 48% over DaDN.\nAlexnet NiN Google VGGM VGGS VGG19 geo 0\n1\n2\n3\n4\n5\nStripes PRA-0b-Pallet PRA-1b-Pallet PRA-2b-Pallet PRA-2b-1R\nFigure 9: Relative performance of Pragmatic using Improved Oneffset Encoding for different configurations. Marked: performance not using IOE"}, {"heading": "6.3 IMPROVED ONEFFSET ENCODING", "text": "Figure 9 reports performance for Pragmatic when using the enhanced oneffset generator described in Section 5.1. The considered configurations include PRA0b, PRA1b and PRA2b (with pallet synchronization), and PRA1R2b . PRA0b degrades by 7%, but the other configurations show improvements of 26%, 48%, and 41% respectively. A cause of degradation for PRA0b is the increased spread of oneffset values (for example, the pair of neurons 011101, 010101 takes 4 cycles with conventional encoding and 5 with enhanced encoding even though the total count of oneffsets is reduced from 7 to 6)."}, {"heading": "6.4 THE IMPACT OF SOFTWARE", "text": "All PRA configurations studied thus far, used software provided per layer activation precisions to reduce essential bit content. PRA does not require these precisions to operate. Table 5 shows what fraction of the performance benefits is due to the software guidance for PRA1R2b , the best configuration studied. The results demonstrate that: 1) PRA would outperform the other architectures even without software guidance, and 2) on average, software guidance improves performance by 19%."}, {"heading": "6.5 QUANTIZATION", "text": "Figure 11 reports performance for DaDN and PRA configurations using the 8-bit quantized representation used in Tensorflow Warden (2016); Google (2016). This quantization uses 8 bits to specify arbitrary minimum and maximum limits per layer for the activations and the weights separately, and maps the 256 available 8-bit values linearly into the resulting interval. This representation has higher\nflexibility and better utilization than the reduced precision approach of Stripes since the range doesnt have to be symmetrical and the limits dont have to be powers of two, while still allowing straightforward multiplication of the values. The limit values are set to the maximum and the minimum activation values for each layer and the quantization uses the recommended rounding mode.\nFigure 11 reports performance relative to DaDN for PRAsingle, PRA2b, PRA1R2b , and PRA \u221eR 2b . PRA performance benefits persist and are over 4.5\u00d7 for PRA1R2b . Measuring the area and energy of these designs is left for future work, however, the absolute area and energy needed by all will be lower due to the narrower representation. Moreover, given that the tile logic will occupy relatively less area for the whole chip and given that the SB and NM account for significant area and energy, the overall overheads of the PRA designs over DaDN will be lower than that measured for the 16-bit fixed-point configurations."}, {"heading": "7 RELATED WORK", "text": "The acceleration of Deep Learning is an active area of research and has yielded numerous proposals for hardware acceleration. DaDianNao (DaDN) is the de facto standard for high-performance DNN acceleration Chen et al. (2014). In the interest of space, this section restricts attention to methods that are either directly related to DaDN, or that follow a value-based approach to DNN acceleration, as Pragmatic falls under this category of accelerators. Value-based accelerators exploit the properties of the values being processed to further improve performance or energy beyond what is possible by exploiting computation structure alone. Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work.\nPuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al. (2016)Han et al. (2015). EIE targets fully-connected (FC) layers and was shown to be 12\u00d7 more efficient than DaDN on FC layers, and 2\u00d7 less efficient for convolutional layers. All aforementioned accelerators use bit-parallel units. While this work has demonstrated Pragmatic as a modification of DaDN, its computation units and potentially, its general approach could be compatible with all aforementioned accelerator designs. This investigation is interesting future work.\nProfiling has been used to determine the precision requirements of a neural network for a hardwired implementation Kim et al. (2014). EoP has been exploited in general purpose hardware and other application domains. For example, Brooks et al. Brooks & Martonosi (1999) exploit the prefix bits due to EoP to turn off parts of the datapath improving energy. Park et al. Park et al. (2010), use a similar approach to trade off image quality for improved energy efficiency. Neither approach directly improves performance."}, {"heading": "8 CONCLUSION", "text": "To the best of our knowledge Pragmatic is the first DNN accelerator that exploits not only the per layer precision requirements of CNNs but also the essential bit information content of the activation values. While this work targeted high-performance implementations, Pragmatic\u2019s core approach should be applicable to other hardware accelerators. We have investigated Pragmatic only for inference and with image classification convolutional neural networks. While desirable, applying the same concept to other network types, layers other than the convolutional one, is left for future work. It would also be interesting to study how the Pragmatic concepts can be applied to more general purpose accelerators or even graphics processors."}, {"heading": "9.2 ESSENTIAL BIT CONTENT DISTRIBUTIONS", "text": "This section reports the distributions of the essential bit count for the activations processed per convolutional layers for the networks studied. Three distributions are shown per network for the activations for three different representations: 1) 16-bit fixed-point, 2) per layer fixed-point, and 3) 8-bit Quantized. A peak appears for values having four bits that are 1 for the quantized representation since the value zero is mapped to a non-zero index having four bits that are one (114). Note that, as in Section 9.1, the distributions are taken before Improved Oneffset Encoding."}], "references": [{"title": "Cnvlutin: Ineffectual-neuron-free deep neural network computing", "author": ["Jorge Albericio", "Patrick Judd", "Tayler Hetherington", "Tor Aamodt", "Natalie Enright Jerger", "Andreas Moshovos"], "venue": "In 2016 IEEE/ACM International Conference on Computer Architecture", "citeRegEx": "Albericio et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Albericio et al\\.", "year": 2016}, {"title": "A signed binary multiplication technique", "author": ["A. D Booth"], "venue": "The Quarterly Journal of Mechanics and Applied Mathematics,", "citeRegEx": "Booth.,? \\Q1951\\E", "shortCiteRegEx": "Booth.", "year": 1951}, {"title": "Dynamically exploiting narrow width operands to improve processor power and performance", "author": ["David Brooks", "Margaret Martonosi"], "venue": "In Proceedings of the 5th International Symposium on High Performance Computer Architecture,", "citeRegEx": "Brooks and Martonosi.,? \\Q1999\\E", "shortCiteRegEx": "Brooks and Martonosi.", "year": 1999}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun", "O. Temam"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel", "Sze", "Vivienne"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "ShiDianNao: Shifting vision processing closer to the sensor", "author": ["Zidong Du", "R. Fasthuber", "Tianshi Chen", "P. Ienne", "Ling Li", "Tao Luo", "Xiaobing Feng", "Yunji Chen", "O. Temam"], "venue": "In 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA),", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "[cs],", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets, arXiv:1511.05236v4 [cs.LG", "author": ["Patrick Judd", "Jorge Albericio", "Tayler Hetherington", "Tor Aamodt", "Natalie Enright Jerger", "Raquel Urtasun", "Andreas Moshovos"], "venue": "arXiv.org,", "citeRegEx": "Judd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2015}, {"title": "Stripes: Bit-serial Deep Neural Network Computing", "author": ["Patrick Judd", "Jorge Albericio", "Tayler Hetherington", "Tor Aamodt", "Andreas Moshovos"], "venue": "Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Judd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2016}, {"title": "Stripes: Bit-serial Deep Neural Network Computing", "author": ["Patrick Judd", "Jorge Albericio", "Andreas Moshovos"], "venue": "Computer Architecture Letters,", "citeRegEx": "Judd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2016}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["Jonghong Kim", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "PuDianNao: A Polyvalent Machine Learning Accelerator", "author": ["Daofu Liu", "Tianshi Chen", "Shaoli Liu", "Jinhong Zhou", "Shengyuan Zhou", "Olivier Teman", "Xiaobing Feng", "Xuehai Zhou", "Yunji Chen"], "venue": "In Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Cambricon: An instruction set architecture for neural networks", "author": ["Shaoli Liu", "Zidong Du", "Jinhua Tao", "Dong Han", "Tao Luo", "Yuan Xie", "Yunji Chen", "Tianshi Chen"], "venue": "In 2016 IEEE/ACM International Conference on Computer Architecture (ISCA),", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Dynamic Bit-Width Adaptation in DCT: An Approach to Trade Off Image Quality and Computation Energy", "author": ["Jongsun Park", "Jung Hwan Choi", "K. Roy"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems,", "citeRegEx": "Park et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Park et al\\.", "year": 2010}, {"title": "Destiny: A tool for modeling emerging 3d nvm and edram caches", "author": ["M. Poremba", "S. Mittal", "Dong Li", "J.S. Vetter", "Yuan Xie"], "venue": "In Design, Automation Test in Europe Conference Exhibition (DATE),", "citeRegEx": "Poremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poremba et al\\.", "year": 2015}, {"title": "Minerva: Enabling low-power, highly-accurate deep neural network accelerators", "author": ["Brandon Reagen", "Paul Whatmough", "Robert Adolf", "Saketh Rama", "Hyunkwang Lee", "Sae Kyu Lee", "Jos Miguel Hernndez-Lobato", "Gu-Yeon Wei", "David Brooks"], "venue": "In International Symposium on Computer Architecture,", "citeRegEx": "Reagen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reagen et al\\.", "year": 2016}, {"title": "A suggestion for a fast multiplier", "author": ["Christopher S. Wallace"], "venue": "IEEE Trans. Electronic Computers,", "citeRegEx": "Wallace.,? \\Q1964\\E", "shortCiteRegEx": "Wallace.", "year": 1964}, {"title": "Low-precision matrix multiplication", "author": ["Peter Warden"], "venue": "https://petewarden.com,", "citeRegEx": "Warden.,? \\Q2016\\E", "shortCiteRegEx": "Warden.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product Wallace (1964). At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator.", "startOffset": 253, "endOffset": 268}, {"referenceID": 3, "context": "3x over the DaDiaNao (DaDN) accelerator Chen et al. (2014) and by 4.", "startOffset": 40, "endOffset": 59}, {"referenceID": 3, "context": "3x over the DaDiaNao (DaDN) accelerator Chen et al. (2014) and by 4.5x when DaDN uses an 8-bit quantized representation Warden (2016). DaDN was reported to be 300x faster than commodity graphics processors.", "startOffset": 40, "endOffset": 134}, {"referenceID": 3, "context": "1 INTRODUCTION Deep Neural Network (DNN) hardware typically uses either 16-bit fixed-point Chen et al. (2014) or quantized 8-bit numbers Warden (2016) and bit-parallel compute units.", "startOffset": 91, "endOffset": 110}, {"referenceID": 3, "context": "1 INTRODUCTION Deep Neural Network (DNN) hardware typically uses either 16-bit fixed-point Chen et al. (2014) or quantized 8-bit numbers Warden (2016) and bit-parallel compute units.", "startOffset": 91, "endOffset": 151}, {"referenceID": 12, "context": "The terms and their sum can be calculated concurrently to reduce latency Wallace (1964). With such a hardware arrangement there are two sources of ineffectual computations that result from: 1) an Excess of Precision (EoP), and 2) Lack of Explicitness (LoE).", "startOffset": 73, "endOffset": 88}, {"referenceID": 2, "context": "Table 5 reports the essential bit content of the activation stream of recent CNNs for two commonly used fixed length representations: 1) 16-bit fixed-point of DaDianNao Chen et al. (2014), 2) 8-bit quantized of Tensorflow Warden (2016).", "startOffset": 169, "endOffset": 188}, {"referenceID": 2, "context": "Table 5 reports the essential bit content of the activation stream of recent CNNs for two commonly used fixed length representations: 1) 16-bit fixed-point of DaDianNao Chen et al. (2014), 2) 8-bit quantized of Tensorflow Warden (2016). The essential bit content is the average number of non-zero bits that are 1.", "startOffset": 169, "endOffset": 236}, {"referenceID": 2, "context": "Table 5 reports the essential bit content of the activation stream of recent CNNs for two commonly used fixed length representations: 1) 16-bit fixed-point of DaDianNao Chen et al. (2014), 2) 8-bit quantized of Tensorflow Warden (2016). The essential bit content is the average number of non-zero bits that are 1. Two measurements are presented per representation: over all neuron values (\u201cAll\u201d), and over the non-zero neurons (\u201cNZ\u201d) as accelerators that can skip zero activations for fixed-point representations have been recently proposed Han et al. (2016); Albericio et al.", "startOffset": 169, "endOffset": 559}, {"referenceID": 0, "context": "(2016); Albericio et al. (2016). When considering all activations, the essential bit-content is at most 12.", "startOffset": 8, "endOffset": 32}, {"referenceID": 3, "context": "4 BASELINE SYSTEM: DADIANNAO Pragmatic is demonstrated as a modification of the DaDianNao accelerator (DaDN) proposed by Chen et al. Chen et al. (2014). Figure 3a shows a DaDN tile which processes 16 filters concurrently calculating 16 activation and weight products per filter for a total of 256 products per cycle.", "startOffset": 121, "endOffset": 152}, {"referenceID": 3, "context": "Chen et al. (2014) used the terms neuron and synapse to refer to activations and weights respectively and named the various components accordingly.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Further Increasing Performance with Improved Oneffset Encoding: Since PIPs in Pragmatic can negate any input term, it is possible to enhance the oneffset generator to generate fewer oneffsets for neuron values containing runs of ones by allowing signed oneffsets Booth (1951). This improved generator reduces runs of adjacent oneffsets a.", "startOffset": 263, "endOffset": 276}, {"referenceID": 1, "context": "Further Increasing Performance with Improved Oneffset Encoding: Since PIPs in Pragmatic can negate any input term, it is possible to enhance the oneffset generator to generate fewer oneffsets for neuron values containing runs of ones by allowing signed oneffsets Booth (1951). This improved generator reduces runs of adjacent oneffsets a...b into pairs of the form a + 1,\u2212b. Single oneffsets or gaps inside runs are represented by a positive or negative oneffset, respectively. For example a neuron value of 11011 that would normally be encoded with oneffsets (4, 3, 1, 0) can instead be represented with (5,\u22123,+2,\u22120) or even more economically with (5,\u22122,\u22120). This is equivalent to a Radix-4 Booth encoding and will never emit more than \u230a x 2 + 1 \u230b oneffsets, where x is the neuron precision. This encoding will never produce more oneffsets compared to the baseline encoding. However, because of the 2-stage shifting, it is possible that this encoding will increase the number of cycles needed. This will happen when the oneffset distribution among the bit groups being processed together during 2-stage shifting changes. Finally, booth encoding is conventionally used to reduce the number of cycles needed to perform multiplication in single shift-and-add multipliers typically reserved for low cost low performance designs, or to reduce the depth of bit-parallel multipliers. Pragmatic with its 2-stage shifting and judicious lane synchronization enables its practical use in a massively data-parallel accelerator boosting performance beyond what is possible with bit-parallel units. The Role of Software: PRA enables an additional dimension upon which hardware and software can attempt to further boost performance and energy efficiency, that of controlling the essential activation value content. This work investigates a software guided approach where the precision requirements of each layer are used to zero out a number of prefix and suffix bits at the output of each layer. Using the profiling method of Judd et al., Judd et al. (2015), software communicates the precisions needed by each layer as meta-data.", "startOffset": 263, "endOffset": 2045}, {"referenceID": 2, "context": "6 EVALUATION The performance, area and energy efficiency of Pragmatic is compared against DaDN Chen et al. (2014) and Stripes Judd et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 2, "context": "6 EVALUATION The performance, area and energy efficiency of Pragmatic is compared against DaDN Chen et al. (2014) and Stripes Judd et al. (2016b), two state-of-the-art DNN accelerators.", "startOffset": 95, "endOffset": 146}, {"referenceID": 0, "context": "Cnvlutin improves upon DaDN by skipping most zero- or near-zero-valued activations Albericio et al. (2016), however, Stripes has been shown to outperform it.", "startOffset": 83, "endOffset": 107}, {"referenceID": 9, "context": "The eDRAM area and energy were modeled with Destiny Poremba et al. (2015). To compare against STR, the per layer numerical representation requirements reported in Table 2 were found using the methodology of Judd et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 5, "context": "To compare against STR, the per layer numerical representation requirements reported in Table 2 were found using the methodology of Judd et al. Judd et al. (2016b). All PRA configurations studied exploit software provided precisions as per Section 5.", "startOffset": 132, "endOffset": 164}, {"referenceID": 3, "context": "All performance measurements are for the convolutional layers only which account for more than 92% of the overall execution time in DaDN Chen et al. (2014). PRA does not affect the execution time of the remaining layers.", "startOffset": 137, "endOffset": 156}, {"referenceID": 17, "context": "5 QUANTIZATION Figure 11 reports performance for DaDN and PRA configurations using the 8-bit quantized representation used in Tensorflow Warden (2016); Google (2016).", "startOffset": 137, "endOffset": 151}, {"referenceID": 17, "context": "5 QUANTIZATION Figure 11 reports performance for DaDN and PRA configurations using the 8-bit quantized representation used in Tensorflow Warden (2016); Google (2016). This quantization uses 8 bits to specify arbitrary minimum and maximum limits per layer for the activations and the weights separately, and maps the 256 available 8-bit values linearly into the resulting interval.", "startOffset": 137, "endOffset": 166}, {"referenceID": 2, "context": "DaDianNao (DaDN) is the de facto standard for high-performance DNN acceleration Chen et al. (2014). In the interest of space, this section restricts attention to methods that are either directly related to DaDN, or that follow a value-based approach to DNN acceleration, as Pragmatic falls under this category of accelerators.", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work.", "startOffset": 9, "endOffset": 84}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al.", "startOffset": 9, "endOffset": 295}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al.", "startOffset": 9, "endOffset": 470}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al.", "startOffset": 9, "endOffset": 559}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016).", "startOffset": 9, "endOffset": 715}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al.", "startOffset": 9, "endOffset": 922}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al. (2016)Han et al. (2015). EIE targets fully-connected (FC) layers and was shown to be 12\u00d7 more efficient than DaDN on FC layers, and 2\u00d7 less efficient for convolutional layers.", "startOffset": 9, "endOffset": 1202}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al. (2016)Han et al. (2015). EIE targets fully-connected (FC) layers and was shown to be 12\u00d7 more efficient than DaDN on FC layers, and 2\u00d7 less efficient for convolutional layers. All aforementioned accelerators use bit-parallel units. While this work has demonstrated Pragmatic as a modification of DaDN, its computation units and potentially, its general approach could be compatible with all aforementioned accelerator designs. This investigation is interesting future work. Profiling has been used to determine the precision requirements of a neural network for a hardwired implementation Kim et al. (2014). EoP has been exploited in general purpose hardware and other application domains.", "startOffset": 9, "endOffset": 1785}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al. (2016)Han et al. (2015). EIE targets fully-connected (FC) layers and was shown to be 12\u00d7 more efficient than DaDN on FC layers, and 2\u00d7 less efficient for convolutional layers. All aforementioned accelerators use bit-parallel units. While this work has demonstrated Pragmatic as a modification of DaDN, its computation units and potentially, its general approach could be compatible with all aforementioned accelerator designs. This investigation is interesting future work. Profiling has been used to determine the precision requirements of a neural network for a hardwired implementation Kim et al. (2014). EoP has been exploited in general purpose hardware and other application domains. For example, Brooks et al. Brooks & Martonosi (1999) exploit the prefix bits due to EoP to turn off parts of the datapath improving energy.", "startOffset": 9, "endOffset": 1921}, {"referenceID": 0, "context": "Cnvlutin Albericio et al. (2016) and Stripes Judd et al. (2016b)Judd et al. (2016a) are such accelerators and they have been already discussed and compared against in this work. PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs Liu et al. (2015). ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency Du et al. (2015). Cambricon is the first instruction set architecture for Deep Learning Liu et al. (2016). Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators Reagen et al. (2016). Eyeriss is a low power, real-time DNN accelerator that exploits zero valued activations for memory compression and energy reduction Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016). The Efficient Inference Engine (EIE) exploits efficient activation and weight representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations Han et al. (2016)Han et al. (2015). EIE targets fully-connected (FC) layers and was shown to be 12\u00d7 more efficient than DaDN on FC layers, and 2\u00d7 less efficient for convolutional layers. All aforementioned accelerators use bit-parallel units. While this work has demonstrated Pragmatic as a modification of DaDN, its computation units and potentially, its general approach could be compatible with all aforementioned accelerator designs. This investigation is interesting future work. Profiling has been used to determine the precision requirements of a neural network for a hardwired implementation Kim et al. (2014). EoP has been exploited in general purpose hardware and other application domains. For example, Brooks et al. Brooks & Martonosi (1999) exploit the prefix bits due to EoP to turn off parts of the datapath improving energy. Park et al. Park et al. (2010), use a similar approach to trade off image quality for improved energy efficiency.", "startOffset": 9, "endOffset": 2039}], "year": 2017, "abstractText": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product Wallace (1964). At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the nonzero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator Chen et al. (2014) and by 4.5x when DaDN uses an 8-bit quantized representation Warden (2016). DaDN was reported to be 300x faster than commodity graphics processors.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_209"}