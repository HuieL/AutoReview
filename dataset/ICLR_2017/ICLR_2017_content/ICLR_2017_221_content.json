{"name": "ICLR_2017_221.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuandong Tian"], "emails": ["yuandong@fb.com"], "sections": [{"heading": null, "text": "\u2211K j=1 \u03c3(w \u1d40 j x),\nwhere \u03c3(\u00b7) is ReLU nonlinearity. We assume that the input x follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters w\u2217 using l2 loss. We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w\u2217 with at least (1 \u2212 )/2 probability, if random weight initializations of proper standard derivation (\u223c 1/ \u221a d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For networks with many ReLU nodes (K \u2265 2), we apply our close form dynamics and prove that when the teacher parameters {w\u2217j}Kj=1 forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to w\u2217 without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with l2 loss. Simulations verify our theoretical analysis."}, {"heading": "1 INTRODUCTION", "text": "Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)]. However, till now, how and why it works remains elusive due to a lack of theoretical understanding. First, how simple approaches like gradient descent can solve a very complicated non-convex optimization effectively. Second, how the deep models, especially deep convolutional models, achieve generalization power despite massive parameters.\nIn this paper, we focus on the first problem and use dynamical system to analyze the nonlinear gradient descent dynamics of certain two-layered nonlinear network in the following form:\ng(x;w) = K\u2211 j=1 \u03c3(w\u1d40j x) (1)\nwhere \u03c3(x) = max(x, 0) is the ReLU nonlinearity. We consider the following setting: a student network learns the parameters that minimize the l2 distance between its prediction and the supervision provided by the teacher network of the same size with a fixed set of parameters w\u2217. We assume all inputs x to follow Gaussian distribution and thus the network is bias-free. Eqn. 1 is highly nonconvex and could contain exponential number of symmetrically equivalent solutions.\nTo analyze this, we first derive novel and concise gradient update rules for multilayer ReLU networks (See Lemma 2.1) in the teacher-student setting under l2 loss. Then for K = 1, we prove that the nonlinear gradient dynamics of Eqn. 1 has a close form and converges to w\u2217 with at least (1 \u2212\n)/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K \u2265 2, we prove that when the teacher parameters {wj}Kj=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w\u2217, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed \u2016w\u2217\u2016, showing that such a convergence behavior is beyond the local convex structure at w\u2217. To our knowledge, this is the first proof of its kind.\nPrevious works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function \u03c3(x) when its derivatives \u03c3\u2032, \u03c3\u2032\u2032, \u03c3\u2032\u2032\u2032 are bounded and \u03c3\u2032 > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and analyzes the dynamics of student network when the teacher\u2019s parameters w\u2217 forms a orthonomal bases; however, it uses \u03c3(x) = erf(x) as the nonlinearity and only analyzes the local behaviors of the two critical points (the saddle point in symmetric initializations, and w\u2217). In contrast, we prove the global convergence behavior in certain symmetry-breaking cases.\nMany previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent. For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al. (2015)] gives guarantees on recovering the parameters of a 2-layered neural network learnt with tensor decomposition. In comparison, we analyze ReLU networks trained with gradient descent, which is a more popular setting in practice.\nThe paper is organized as follows. Sec. 2 introduces the basic formulation and some interesting novel properties of ReLU in multilayered ReLU networks. Sec. 3 and Sec. 4 then analyze the twolayered model Eqn. 1 for K = 1 and K \u2265 2, respectively. Sec. 5 shows that simulation results are consistent with theoretical analysis. Finally Sec. 7 gives detailed proofs for all theorems."}, {"heading": "2 PRELIMINARY", "text": ""}, {"heading": "2.1 NOTATION", "text": "Denote X as a N -by-d input data matrix and w\u2217 is the parameter of the teacher network with desired N -by-1 output u = g(X;w\u2217). Now suppose we have an estimator w and the estimated output v = g(X;w). We want to know with l2 loss E(w) = 12\u2016u \u2212 v\u2016 2 = 12\u2016u \u2212 g(X;w)\u2016 2, whether gradient descent will converge to the desired solution w\u2217.\nThe gradient descent update is w(t+1) = w(t) + \u03b7\u2206w(t), where \u2206w(t) \u2261 \u2212\u2207E(w(t)). If we let \u03b7 \u2192 0, then the update rule becomes a first-order differential equation dw/dt = \u2212\u2207E(w), or more concisely, w\u0307 = \u2212\u2207E(w). In this case, E\u0307 = \u2207E(w)\u1d40w\u0307 = \u2212\u2016\u2207E(w)\u20162 \u2264 0, i.e., the function value E is nonincreasing over time. The key is to check whether there exist other critical points w 6= w\u2217 so that \u2207E(w) = 0. In our analysis, we assume entries of inputX follow Gaussian distribution. In this situation, the gradient is a random variable and \u2206w = \u2212E [\u2207E(w)]. The expected E [E(w)] is also nonincreasing no matter whether we follow the expected gradient or the gradient itself, because\nE [ E\u0307 ] = \u2212E [\u2207E(w)\u1d40\u2207E(w)] \u2264 \u2212E [\u2207E(w)]\u1d40 E [\u2207E(w)] \u2264 0 (2)\nTherefore, we analyze the behavior of expected gradient E [\u2207E(w)] rather than\u2207E(w)."}, {"heading": "2.2 PROPERTIES OF RELU", "text": "In this paper, we discover a few useful properties of ReLU that make our analysis much simpler. Denote D = D(w) = diag(Xw > 0) as a N -by-N diagonal matrix. The l-th diagnonal element of D is a binary variable showing whether the neuron is on for sample l. Using this notation, we could write \u03c3(Xw) = DXw. Note that D only depends on the direction of w but not its magnitude.\nNote that for ReLU,D is also \u201ctranparent\u201d on derivatives. For example, the Jacobian Jw[\u03c3(Xw)] = \u03c3\u2032(Xw)X = DX at differentiable regions. This gives a very concise rule for gradient descent in ReLU network: suppose we have negative gradient inflow vector g (of dimension N -by-1) on the current ReLU node with weights w, then we can simply write the update \u2206w as:\n\u2206w = Jw[\u03c3(Xw)] \u1d40g = X\u1d40Dg (3)\nThis can be easily applied to multilayer ReLU network. Denote j \u2208 [c] if node j is in layer c, dc as the width of layer c, and uj and vj as the output of teacher network and student network, respectively. A simple deduction yields the following lemma:\nLemma 2.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form:\ngj = Lj \u2211 j\u2032 (L\u2217j\u2032uj\u2032 \u2212 Lj\u2032vj\u2032) (4)\nwhere Lj and L\u2217j are N -by-N diagonal matrices. For any k \u2208 [c+ 1], Lk = \u2211 j\u2208[c] wjkDjLj and similarly for L\u2217k. For the first layer, L = L \u2217 = I .\nThe intuition here is to start from g = u \u2212 v (true for l2 loss) at the top layer and use induction. With this formulation, we could write the finite dynamics for wc (all parameters in layer c). Denote the N -by-dc+1dc matrix Rc = [LjDj ]j\u2208[c]Xc and R\u2217c = [L \u2217 jD \u2217 j ]j\u2208[c]X \u2217 c . Using gradient descent rules:\n\u2206wj = X \u1d40 cDjgj = X \u1d40 cDjLj \u2211 j\u2032 L\u2217j\u2032D \u2217 j\u2032X \u2217 cw \u2217 j\u2032 \u2212 \u2211 j\u2032 Lj\u2032Dj\u2032Xcwj\u2032  (5) = X\u1d40cDjLj (R \u2217 cw \u2217 c \u2212Rcwc) (6)\nTherefore we have: \u2206wc = R \u1d40 c (R \u2217 cw \u2217 c \u2212Rcwc) (7)"}, {"heading": "3 SINGLE RELU CASE", "text": "Let\u2019s start with the simplest case where there is only one ReLU node, K = 1. At iteration t, following Eqn. 3, the gradient update rule is:\n\u2206w(t) = X\u1d40D(t)g(t) = X\u1d40D(t)(D\u2217Xw\u2217 \u2212D(t)Xw(t)) (8)\nNote here how the notation ofD(t) comes into play (andD(t)D(t) = D(t)). Indeed, when the neuron is cut off at sample l, then (D(t))ll is zero and will block the corresponding gradient component.\nLinear case. In this situationD(t) = D\u2217 = I (no gating in either forward or backward propagation) and:\nw(t+1) = w(t) + \u03b7\nN X\u1d40X(w\u2217 \u2212w(t)) (9)\nwhere \u03b7/N is the learning rate. When it is sufficiently small so that the spectral radius \u03c1(I \u2212 \u03b7 NX\n\u1d40X) < 1, w(t+1) will converge to w\u2217 when t\u2192 +\u221e. Note that this convergence is guaranteed for any initial condition w(1), if X\u1d40X is full rank with suitable \u03b7. This is consistent with its convex nature. If entries of X follow i.i.d Gaussian distribution, then E [ 1 NX \u1d40X ]\n= I and the condition satisfies.\nNonlinear (ReLU) case. In this case, \u2206w = X\u1d40D(D\u2217Xw\u2217 \u2212DXw) in which D is a function of w. Intuitively, this term goes to zero when w \u2192 w\u2217, and should be approximated to be N2 (w\n\u2217 \u2212 w) in the i.i.d Gaussian case, since roughly half of the samples are blocked. However, once we make such approximation, we lost the nonlinear behavior of the network and would draw the wrong conclusion of global convergence.\nThen how should we analyze it? Notice that in \u2206w, both of the two terms have the form F (e,w) = X\u1d40D(e)D(w)Xw. Using this form, E [\u2206w] = E [F (w/\u2016w\u2016,w\u2217)] \u2212 E [F (w/\u2016w\u2016,w)]. Here e is a unit vector called the \u201cprojected\u201d weight. In the following, we will show that E [F (e,w)] has the following close form under i.i.d Gaussian assumption on X:\nLemma 3.1 Denote F (e,w) = X\u1d40D(e)D(w)Xw where e is a unit vector, X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xN ]\u1d40 is N -by-d sample matrix and D(w) = diag(Xw > 0) is a binary diagonal matrix. If xi \u223c N(0, I) and are i.i.d (and thus bias-free), then:\nE [F (e,w)] = N\n2\u03c0 [(\u03c0 \u2212 \u03b8)w + \u2016w\u2016 sin \u03b8e] (10)\nwhere \u03b8 = \u2220(e,w) \u2208 [0, \u03c0] is the angle between e and w.\nNote that the expectation analysis smooths out the non-differentiable property of ReLU, leaving only one singularity at e = 0. The intuition is that expectation analysis involves an integration over the data distribution. With simple algebraic manipulation, E [\u2206w] takes the following closed form:\nE [\u2206w] = N 2 (w\u2217 \u2212w) + N 2\u03c0 (\u03b1 sin \u03b8w \u2212 \u03b8w\u2217) (11)\nwhere \u03b1 = \u2016w\u2217\u2016/\u2016w\u2016 and \u03b8 \u2208 [0, \u03c0] is the angle between w and w\u2217. The first term is expected while the last two terms show the nonlinear behavior. Using Lyapunov\u2019s method, we show that the dynamics (if treated continuously) converges to w\u2217 when w(1) \u2208 \u2126 = {w : \u2016w \u2212w\u2217\u2016 < \u2016w\u2217\u2016}:\nLemma 3.2 When w(1) \u2208 \u2126 = {w : \u2016w \u2212w\u2217\u2016 < \u2016w\u2217\u2016}, following the dynamics of Eqn. 11, the Lyapunov function V (w) = 12\u2016w \u2212 w\n\u2217\u20162 has V\u0307 < 0 and the system is asymptotically stable and thus w(t) \u2192 w\u2217 when t\u2192 +\u221e.\nSee Appendix for the proof. The intuition is to represent V as a 2-by-2 bilinear form of vector [\u2016w\u2016, \u2016w\u2217\u2016], and the bilinear coefficient matrix is positive definite. One question arises: will the same approach show the dynamics converges when the initial conditions lie outside the region \u2126, in particular for any region that includes the origin? The answer is probably no. Note that w = 0 is a singularity in which \u2206w is not continuous (if approaching from different directions towards w = 0, \u2206w is different). It is due to the fact that ReLU function is not differentiable at the origin. We could remove this singularity by \u201csmoothing out\u201d ReLU around the origin. This will yield \u2206w\u2192 0 when w \u2192 0. In this case, V\u0307 (0) = 0 so Lyapunov method could only tell that the dynamics is stable but not convergent. Note that for ReLU activation, \u03c3\u2032(x) = 0 for certain negative x even after a local smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply.\nRandom Initialization. Then we study how to sample w(1) so that w(1) \u2208 \u2126. We would like to sample within \u2126, but we don\u2019t know where is w\u2217. Sampling around origin with big radius r \u2265 2\u2016w\u2217\u2016 is inefficient in particular in high-dimensional space. This is because when the sample is uniform, the probability of hitting the ball is proportional to (r/\u2016w\u2217\u2016)d \u2264 2\u2212d, which is exponentially small.\nA better idea is to sample around the origin with very small radius (but not at w = 0), so that the convergent hypersphere behaves like a hyperplane near the origin, and thus almost half of the samples is useful (Fig. 2(a)), as shown in the following theorem:\nTheorem 3.3 The dynamics in Eqn. 11 converges to w\u2217 with probability at least (1 \u2212 )/2, if the initial value w(1) is sampled uniformly from Br = {w : \u2016w\u2016 \u2264 r} with r \u2264 \u221a 2\u03c0 d+1\u2016w \u2217\u2016.\nThe intution here is to lower-bound the probability of the shaded area (Fig. 2(b)). From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w\u2217 yields the convergence. This is consistent with the analysis above."}, {"heading": "4 MULTIPLE RELUS CASE", "text": "Now we are ready to analyze the network g(x) = \u2211K j=1 \u03c3(w \u1d40 j x) for K \u2265 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)]. In this case, Lj = L\u2217j = I for 1 \u2264 j \u2264 K. Then we have the following nonlinear dynamics from Eqn. 7:\n\u2206wj = K\u2211 j\u2032=1 f(wj ,wj\u2032 ,w \u2217 j\u2032) (12)\nwhere f = F (wj/\u2016wj\u2016,w\u2217j\u2032)\u2212 F (wj/\u2016wj\u2016,wj\u2032). Therefore, using Eqn. 10, its expectation is:\n2\u03c0 N E [ f(wj ,wj\u2032 ,w \u2217 j\u2032) ] = (\u03c0 \u2212 \u03b8\u2217j \u2032 j )w \u2217 j\u2032 \u2212 (\u03c0 \u2212 \u03b8 j\u2032 j )wj\u2032 + (\u2016w\u2217j\u2032\u2016 \u2016wj\u2016 sin \u03b8\u2217j \u2032 j \u2212 \u2016wj\u2032\u2016 \u2016wj\u2016 sin \u03b8j \u2032 j ) wj (13) where \u03b8\u2217j \u2032 j \u2261 \u2220(wj ,w\u2217j\u2032) and \u03b8 j\u2032\nj \u2261 \u2220(wj ,wj\u2032). Eqn. 12 (and its expected version) gives very complicated nonlinear dynamics and could be hard to solve in general. Unlike K = 1, a similar approach with Lyaponov function does not yield a decisive conclusion. However, if we consider the symmetric case: wj = Pjw and w\u2217j = Pjw \u2217 where Pj is a cyclic permutation matrix that maps index j\u2032 + 1 to (j\u2032 + j mod K) + 1 (and P1 is the identity matrix), then RHS of the expected version of Eqn. 12 can be simplified as follows:\nE [\u2206wj ] = \u2211 j\u2032 E [ f(wj ,wj\u2032 ,w \u2217 j\u2032) ] = \u2211 j\u2032 E [f(Pjw, Pj\u2032w, Pj\u2032w\u2217)]\n= \u2211 j\u2032\u2032 E [f(Pjw, PjPj\u2032\u2032w, PjPj\u2032\u2032w\u2217)] ({Pj}Kj=1 is a group)\n= Pj \u2211 j\u2032\u2032 E [f(w, Pj\u2032\u2032w, Pj\u2032\u2032w\u2217)] (\u2016Pw1\u2016 = \u2016w1\u2016, \u2220(Pw1, Pw2) = \u2220(w1,w2))\n= PjE [\u2206w1] (14)\nwhich means that if all wj and w\u2217j are symmetric under the action of cyclic group, so does their expected gradient. Therefore, the trajectory {w(t)} keeps such cyclic structure. Instead of solving a system of K equations, we only need to solve one:\nE [\u2206w] = K\u2211 j=1 E [f(w, Pjw, Pjw\u2217)] (15)\nSurprisingly, there is another layer of symmetry in Eqn. 15 when {w\u2217j} forms an orthonomal basis (w\u2217j\u2032 \u1d40w\u2217j = \u03b4jj\u2032 ). In this case, if we start with w (1) = xw\u2217 + y \u2211 j 6=1 Pjw\n\u2217 then we could show that the trajectory keeps this structure and Eqn. 15 can be further reduced into the following 2D nonlinear dynamics:\n2\u03c0 N E [ \u2206x \u2206y ] = \u2212 { [(\u03c0 \u2212 \u03c6)(x\u2212 1 + (K \u2212 1)y)] [ 1 1 ] + [ \u03b8 \u03c6\u2217 \u2212 \u03c6 ] + \u03c6 [ x\u2212 1 y ]} + [(K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) + \u03b1 sin \u03b8] [ x y ] (16)\nHere the symmetrical factor (\u03b1 \u2261 \u2016w\u2217j\u2032\u2016/\u2016wj\u2016, \u03b8 \u2261 \u03b8 \u2217j j , \u03c6 \u2261 \u03b8\nj\u2032\nj , \u03c6 \u2217 \u2261 \u03b8\u2217j\n\u2032\nj ) are defined as follows:\n\u03b1 = (x2 + (K\u22121)y2)\u22121/2, cos \u03b8 = \u03b1x, cos\u03c6\u2217 = \u03b1y, cos\u03c6 = \u03b12(2xy+ (K\u22122)y2) (17) For this 2D dynamics, we thus have the following theorem:\nTheorem 4.1 For any K \u2265 2, the 2D dynamics (Eqn. 16) shows the following behaviors:\n(1) Symmetric case. If the initial condition x(1) = y(1) \u2208 (0, 1], then the dynamics reduces to 1D and converges to a saddle point x = y = 1\u03c0K ( \u221a K \u2212 1\u2212 arccos(1/ \u221a K) + \u03c0).\n(2) Symmetry-Breaking. If (x(1), y(1)) \u2208 \u2126 = {x \u2208 (0, 1], y \u2208 [0, 1], x > y}, then dynamics always converges to (x, y) = (1, 0).\nFrom (x(t), y(t)) we could recover w(t)j = x (t)w\u2217j + y (t) \u2211 j\u2032 6=j w \u2217 j\u2032 . Obviously, a convergence of Eqn. 16 to (1, 0) means Eqn. 12 converges to {w\u2217j}, i.e, the teacher parameters are recovered:\nCorollary 4.2 For a bias-free two-layered ReLU network g(x;w) = \u2211 j \u03c3(w \u1d40 j x) that takes Gaussian i.i.d inputs (Fig. 1), if the teacher\u2019s parameters {w\u2217j} form orthogonal bases, then when the student parameters is initialized in the form of w(1)j = x (1)w\u2217j + y (1) \u2211 j\u2032 6=j w \u2217 j\u2032 where (x(1), y(1)) \u2208 \u2126 = {x \u2208 (0, 1], y \u2208 [0, 1], x > y}, then the dynamics (Eqn. 12) converges to {w\u2217j} without being trapped into local minima.\nWhen symmetry is broken, since the closure of \u2126 includes the origin, there exists a path starting at arbitrarily small neighborhood of origin to w\u2217, regardless of how large \u2016w\u2217\u2016 is. In contrast to traditional convex analysis that only gives the local parameter-dependent convergence basin around w\u2217j , here we obtain a convergence basin that is parameter-independent. In comparison, [Saad & Solla (1996)] uses a different activation function (\u03c3(x) = erf(x)) and only analyzes local behaviors near the two fixed points (the symmetric saddle point and the teacher\u2019s weights w\u2217), leaving symmetry breaking an empirical procedure. Here we show that it is possible to give global convergence analysis on certain symmetry breaking cases for two-layered ReLU network.\nBy symmetry, Corollary 4.1 immediately suggests that when w(1) = y(1) \u2211K j=1 w \u2217 j + (x\n(1) \u2212 y(1))w\u2217j\u2032 , then the dynamics will converge to Pj\u2032w\n\u2217. Since x > y but can be arbitrarily close, a slighest preturbation on the symmetric solution x = y leads to a different fixed point, which is a permutation of w\u2217. This is very similar to Spontaneously Symmetric-Breaking (SSB) procedure in physics, in which a high energy state with full symmetry goes to a low energy state and only retains part of the symmetry. In this case, the energy is the objective function E, the high energy state is the initialization that is almost symmetrical but with small fluctuation, and the low energy state is the fixed point the dynamics converges into.\nFrom the simulation shown in Fig. 4, we could see that gradient descent takes a detour to reach the desired solution w\u2217, even when the initialization is aligned with w\u2217. This is because in the first stage, all ReLU nodes receive the residue and try to explain the data in the same way (both x and y increases); when the \u201cobvious\u201d component has been explained away, then the residue changes its direction and pushes some ReLU nodes to explain other components as well (x increases but y decreases).\nEmpirically this path also converges to w\u2217 under noise. We leave it a conjecture that the system converges in the presence of reasonably large noise. If this conjecture is true, then with high probability a random initialization stays in the convergence basin and converges to a permutation of w\u2217. The reason is that a random initialization almost never gives ties. Without a tie, there exists one leading component which will dominate the convergence.\nConjecture 4.3 When the initialization w(1) = x(1)w\u2217j + y(1) \u2211 j\u2032 6=j w \u2217 j\u2032 + , where is Gaussian noise and (x(1), y(1)) \u2208 \u2126, then the dynamics Eqn. 12 also converges to w\u2217 without trapped into local minima."}, {"heading": "5 SIMULATION", "text": ""}, {"heading": "5.1 CLOSE FORM SOLUTION FOR ONE RELU NODE", "text": "We verify our close form expression of E [F (e,w)] = E [X\u1d40D(e)D(w)Xw] (Eqn. 10) with simulation. We randomly pick e and w so that their angle \u2220(e,w) is uniformly distributed in [0, \u03c0]. We prepare the input data X with standard Gaussian distribution and compare the close form solution E [F (e,w)] with F (e,w), the actual data term in gradient descent without expectation. We use relative RMS error: err = \u2016E [F (e,w)] \u2212 F (e,w)\u2016/\u2016F (e,w)\u2016. As shown in Fig. 3(a), The error distribution on angles shows the properties of the close-form solution. For small \u03b8, D(w) and\nvery large noise is present. Both teacher and student networks use g(x) =\n\u2211K\nj=1 \u03c3(w \u1d40 j x). Each\nexperiment has 8 runs. Bottom row: Convergence when we use g2(x) = \u2211K j=1 aj\u03c3(w \u1d40 j x). Here the top weights aj is fixed at different numbers (rather than 1). Large positive aj correponds to fast convergence. When aj has positive/negative components, the network does not converge to w\u2217.\nD(e) overlaps sufficiently, giving a reliable estimation for the gradient. When \u03b8 \u2192 \u03c0, D(w) and D(e) tend not to overlap, leaving very few data involved in the gradient computation. As a result, the variance grows. Note that all our analysis operate on \u03b8 \u2208 [0, \u03c0/2] and is not affected by this behavior. In the following, angles are sampled from [0, \u03c0/2].\nFig. 3(a) shows that the close form expression becomes more accurate with more samples. We also examine other zero-mean distributions of X , e.g., uniform distribution in [\u22121/2, 1/2]. As shown in Fig. 3(d), the close form expression still works for large d, showing that it could be quite general. Note that the error is computed up to a scaling constant, due to the difference in normalization constants among different distributions. We leave it to the future work to prove its usability for broader distributions."}, {"heading": "5.2 CONVERGENCE FOR MULTIPLE RELU NODES", "text": "Fig. 4(a) and (b) shows the 2D vector field given by the 2D dynamics (Eqn. 16) and Fig. 4(c) shows the 2D trajectory towards convergence to the teacher\u2019s parameters w\u2217. Interestingly, even when we initialize the weights as (10\u22123, 0), aligning with w\u2217, the gradient descent takes detours to reach the destination. One explanation is, at the beginning all nodes move similar direction trying to explain the data, once the data have been explained partly, specialization follows (y decreases).\nFig. 5 shows empirical convergence for K \u2265 2, when the initialization deviates from symmetric initialization in Thm. 4.1. Unless the deviation is large, gradient descent converges to w\u2217. We also check the convergence of a more general network g2(x) = \u2211K j=1 aj\u03c3(w \u1d40 j x). When aj > 0 convergence follows; however, when some aj is negative, the network does not converge to w\u2217, even that the student network already knows the ground truth value of {aj}Kj=1."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper, we analyze the nonlinear dynamical behavior of certain two-layered bias-free ReLU networks in the form of g(x;w) = \u2211K j=1 \u03c3(w \u1d40 j x), where \u03c3 = max(x, 0) is the ReLU node. We assume that the input x follows Gaussian distribution and the output is generated by a teacher network with parameters w\u2217. In K = 1 we show a close-form nonlinear dynamics can be obtained and its convergence to w\u2217 can be proven, if we sample the initialization properly. Such initialization is consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of the value of w\u2217. ForK \u2265 2, when the teacher parameters {w\u2217j} form a orthonormal bases, we prove that the trajectory from symmetric initialization is trapped into a saddle point, while certain symmetric breaking initialization converges to w\u2217 without trapped into any local minima. Future work includes analysis of general cases (or symmetric case plus noise) for K \u2265 2, and a generalization to multilayer ReLU (or other nonlinear) networks."}, {"heading": "7 APPENDIX", "text": "Here we list all detailed proof for all the theorems."}, {"heading": "7.1 PROPERTIES OF RELU NETWORKS", "text": "Lemma 7.1 For neural network with ReLU nonlinearity and using l2 loss to match with a teacher network of the same size, the negative gradient inflow gj for node j at layer c has the following form:\ngj = Lj \u2211 j\u2032 (L\u2217j\u2032uj\u2032 \u2212 Lj\u2032vj\u2032) (18)\nwhere Lj and L\u2217j are N -by-N diagonal matrices. For any k \u2208 [c+ 1], Lk = \u2211 j\u2208[c] wjkDjLj and similarly for L\u2217k.\nProof We prove by induction on layer. For the first layer, there is only one node with g = u \u2212 v, therefore Lj = Lj\u2032 = I . Suppose the condition holds for all node j \u2208 [c]. Then for node k \u2208 [c+1], we have:\ngk = \u2211 j wjkDjgj = \u2211 j wjkDjLj \u2211 j\u2032 L\u2217j\u2032uj\u2032 \u2212 \u2211 j\u2032 Lj\u2032vj\u2032  =\n\u2211 j wjkDjLj \u2211 j\u2032 L\u2217j\u2032 \u2211 k\u2032 D\u2217j\u2032w \u2217 jk\u2032uk\u2032 \u2212 \u2211 j\u2032 Lj\u2032 \u2211 k\u2032 Dj\u2032wjk\u2032vk\u2032  =\n\u2211 j wjkDjLj \u2211 j\u2032 L\u2217j\u2032D \u2217 j\u2032 \u2211 k\u2032 w\u2217jk\u2032uk\u2032 \u2212 \u2211 j wjkDjLj \u2211 j\u2032 Lj\u2032Dj\u2032 \u2211 k\u2032 wjk\u2032vk\u2032\n= \u2211 k\u2032 \u2211 j wjkDjLj \u2211 j\u2032 L\u2217j\u2032D \u2217 j\u2032w \u2217 jk\u2032 uk\u2032 \u2212\u2211 k\u2032 \u2211 j wjkDjLj \u2211 j\u2032 Lj\u2032Dj\u2032wjk\u2032 vk\u2032 Setting Lk = \u2211 j wjkDjLj and L \u2217 k = \u2211 j w \u2217 jkD \u2217 jL \u2217 j (both are diagonal matrices), we thus have:\ngk = \u2211 k\u2032 LkL \u2217 k\u2032uk\u2032 \u2212 LkLk\u2032vk\u2032 = Lk \u2211 k\u2032 L\u2217k\u2032uk\u2032 \u2212 Lk\u2032vk\u2032 (19)"}, {"heading": "7.2 ONE RELU CASE", "text": "Lemma 7.2 Suppose F (e,w) = X\u1d40D(e)D(w)Xw where e is a unit vector and X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xN ]\u1d40 is N -by-d sample matrix. If xi \u223c N(0, I) and are i.i.d, then:\nE [F (e,w)] = N\n2\u03c0 ((\u03c0 \u2212 \u03b8)w + \u2016w\u2016 sin \u03b8e) (20)\nwhere \u03b8 \u2208 [0, \u03c0] is the angle between e and w.\nProof Note that F can be written in the following form: F (e,w) = \u2211\ni:x\u1d40i e\u22650,x \u1d40 i w\u22650\nxix \u1d40 iw (21)\nwhere xi are samples so that X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xn]\u1d40. We set up the axes related to e and w as in Fig. 6, while the rest of the axis are prependicular to the plane. In this coordinate system, any vector x = [r sin\u03c6, r cos\u03c6, x3, . . . , xd]. We have an orthonomal set of bases: e, e\u22a5 = \u2212e\u2212w/\u2016w\u2016 cos \u03b8sin \u03b8 (and any set of bases that span the rest of the space). Under the basis, the representation for e and w is [1,0d\u22121] and [\u2016w\u2016 cos \u03b8,\u2212\u2016w\u2016 sin \u03b8,0d\u22122]. Note that here \u03b8 \u2208 (\u2212\u03c0, \u03c0]. The angle \u03b8 is positive when e \u201cchases after\u201d w, and is otherwise negative.\nNow we consider the quality R(\u03c60) = E [ 1 N \u2211 i:\u03c6i\u2208[0,\u03c60] xix \u1d40 i ] . If we take the expectation and use polar coordinate only in the first two dimensions, we have:\nR(\u03c60) = E  1 N \u2211 i:\u03c6i\u2208[0,\u03c60] xix \u1d40 i  = E [xix\u1d40i |\u03c6i \u2208 [0, \u03c60]]P [\u03c6i \u2208 [0, \u03c60]] =\n\u222b +\u221e 0 \u222b\u222b +\u221e \u2212\u221e \u222b \u03c60 0 r sin\u03c6r cos\u03c6. . . xd  [r sin\u03c6 r cos\u03c6 . . . xd] p(r)p(\u03b8) d\u220f k=3 p(xk)rdrd\u03c6dx3 . . . dxd\nwhere p(r) = e\u2212r 2/2 and p(\u03b8) = 1/2\u03c0. Note that R(\u03c60) is a d-by-d matrix. The first 2-by-2 block can be computed in close form (note that \u222b +\u221e 0\nr2p(r)rdr = 2). Any off-diagonal element except for the first 2-by-2 block is zero due to symmetric property of i.i.d Gaussian variables. Any diagonal element outside the first 2-by-2 block will be P [\u03c6i \u2208 [0, \u03c60]] = \u03c60/2\u03c0. Finally, we have:\nR(\u03c60) = E  1 N \u2211 i:\u03c6i\u2208[0,\u03c60] xix \u1d40 i  = 1 4\u03c0 [ 2\u03c60 \u2212 sin 2\u03c60 1\u2212 cos 2\u03c60 0 1\u2212 cos 2\u03c60 2\u03c60 + sin 2\u03c60 0 0 0 2\u03c60Id\u22122 ] (22)\n= \u03c60 2\u03c0 Id + 1 4\u03c0\n[ \u2212 sin 2\u03c60 1\u2212 cos 2\u03c60 0 1\u2212 cos 2\u03c60 sin 2\u03c60 0\n0 0 0\n] (23)\nWith this equation, we could then compute E [F (e,w)]. When \u03b8 \u2265 0, the condition {i : x\u1d40i e \u2265 0,x\u1d40iw \u2265 0} is equivalent to {i : \u03c6i \u2208 [\u03b8, \u03c0]} (Fig. 6(a)). Using w = [\u2016w\u2016 cos \u03b8,\u2212\u2016w\u2016 sin \u03b8,0d\u22122] and we have:\nE [F (e,w)] = N (R(\u03c0)\u2212R(\u03b8))w (24)\n= N\n4\u03c0\n( 2(\u03c0 \u2212 \u03b8)w \u2212 \u2016w\u2016 [ \u2212 sin 2\u03b8 1\u2212 cos 2\u03b8 0 1\u2212 cos 2\u03b8 sin 2\u03b8 0\n0 0 0\n][ cos \u03b8 \u2212 sin \u03b8\n0\n]) (25)\n= N\n2\u03c0\n( (\u03c0 \u2212 \u03b8)w + \u2016w\u2016 [ sin \u03b8 0 ]) (26)\n= N\n2\u03c0 ((\u03c0 \u2212 \u03b8)w + \u2016w\u2016 sin \u03b8e) (27)\nFor \u03b8 < 0, the condition {i : x\u1d40i e \u2265 0,x \u1d40 iw \u2265 0} is equivalent to {i : \u03c6i \u2208 [0, \u03c0 + \u03b8]} (Fig. 6(b)), and similarly we get\nE [F (e,w)] = N (R(\u03c0 + \u03b8)\u2212R(0))w = N 2\u03c0 ((\u03c0 + \u03b8)w \u2212 \u2016w\u2016 sin \u03b8e) (28)\nNotice that by abuse of notation, the \u03b8 appears in Eqn. 20 is the absolute value and Eqn. 20 follows.\nLemma 7.3 In the region \u2016w(1) \u2212w\u2217\u2016 < \u2016w\u2217\u2016, following the dynamics (Eqn. 11), the Lyapunov function V (w) = 12\u2016w\u2212w\n\u2217\u20162 has V\u0307 < 0 and the system is asymptotically stable and thus w(t) \u2192 w\u2217 when t\u2192 +\u221e.\nProof Denote that \u2126 = {w : \u2016w(1) \u2212w\u2217\u2016 < \u2016w\u2217\u2016}. Note that\nV\u0307 = (w \u2212w\u2217)\u1d40\u2206w = \u2212y\u1d40My (29) where y = [\u2016w\u2217\u2016, \u2016w\u2016]\u1d40 and M is the following 2-by-2 matrix:\nM = 1\n2\n[ sin 2\u03b8 + 2\u03c0 \u2212 2\u03b8 \u2212(2\u03c0 \u2212 \u03b8) cos \u03b8 \u2212 sin \u03b8\n\u2212(2\u03c0 \u2212 \u03b8) cos \u03b8 \u2212 sin \u03b8 2\u03c0\n] (30)\nIn the following we will show that M is positive definite when \u03b8 \u2208 (0, \u03c0/2]. It suffices to show that M11 > 0, M22 > 0 and det(M) > 0. The first two are trivial, while the last one is:\n4det(M) = 2\u03c0(sin 2\u03b8 + 2\u03c0 \u2212 2\u03b8)\u2212 [(2\u03c0 \u2212 \u03b8) cos \u03b8 + sin \u03b8]2 (31) = 2\u03c0(sin 2\u03b8 + 2\u03c0 \u2212 2\u03b8)\u2212 [ (2\u03c0 \u2212 \u03b8)2 cos2 \u03b8 + (2\u03c0 \u2212 \u03b8) sin 2\u03b8 + sin2 \u03b8 ] (32)\n= (4\u03c02 \u2212 1) sin2 \u03b8 \u2212 4\u03c0\u03b8 + 4\u03c0\u03b8 cos2 \u03b8 \u2212 \u03b82 cos2 \u03b8 + \u03b8 sin 2\u03b8 (33) = (4\u03c02 \u2212 4\u03c0\u03b8 \u2212 1) sin2 \u03b8 + \u03b8 cos \u03b8(2 sin \u03b8 \u2212 \u03b8 cos \u03b8) (34)\nNote that 4\u03c02 \u2212 4\u03c0\u03b8 \u2212 1 = 4\u03c0(\u03c0 \u2212 \u03b8) \u2212 1 > 0 for \u03b8 \u2208 [0, \u03c0/2], and g(\u03b8) = sin \u03b8 \u2212 \u03b8 cos \u03b8 \u2265 0 for \u03b8 \u2208 [0, \u03c0/2] since g(0) = 0 and g\u2032(\u03b8) \u2265 0 in this region. Therefore, when \u03b8 \u2208 (0, \u03c0/2], M is positive definite.\nWhen \u03b8 = 0, M(\u03b8) = \u03c0[1,\u22121;\u22121, 1] and is semi-positive definite, with the null eigenvector being\u221a 2 2 [1, 1], i.e., \u2016w\u2016 = \u2016w\n\u2217\u2016. However, along \u03b8 = 0, the only w that satisfies \u2016w\u2016 = \u2016w\u2217\u2016 is w = w\u2217. Therefore, V\u0307 = \u2212y\u1d40My < 0 in \u2126. Note that although this region could be expanded to the entire open half-spaceH = {w : w\u1d40w\u2217 > 0}, it is not straightforward to prove the convergence inH, since the trajectory might go outsideH. On the other hand, \u2126 is the level set V < 12\u2016w\n\u2217\u20162 so the trajectory starting within \u2126 remains inside.\nTheorem 7.4 The dynamics in Eqn. 11 converges to w\u2217 with probability at least (1 \u2212 )/2, if the initial value w(1) is sampled uniformly from Br = {w : \u2016w\u2016 \u2264 r} with:\nr \u2264 \u221a 2\u03c0\nd+ 1 \u2016w\u2217\u2016 (35)\nProof Given a ball of radius r, we first compute the \u201cgap\u201d \u03b4 of sphere cap (Fig. 2(b)). First cos \u03b8 = r\n2\u2016w\u2217\u2016 , so \u03b4 = r cos \u03b8 = r2\n2\u2016w\u2217\u2016 . Then a sufficient condition for the probability argument to hold, is to ensure that the volume Vshaded of the shaded area is greater than 1\u2212 2 Vd(r), where Vd(r) is the volume of d-dimensional ball of radius r. Since Vshaded \u2265 12Vd(r)\u2212 \u03b4Vd\u22121, it suffices to have:\n1 2 Vd(r)\u2212 \u03b4Vd\u22121 \u2265 1\u2212 2 Vd(r) (36)\nwhich gives\n\u03b4 \u2264 2 Vd Vd\u22121\n(37)\nUsing \u03b4 = r 2\n2\u2016w\u2217\u2016 and Vd(r) = Vd(1)r d, we thus have:\nr \u2264 Vd(1) Vd\u22121(1) \u2016w\u2217\u2016 (38)\nwhere Vd(1) is the volume of the unit ball. Since the volume of d-dimensional unit ball is\nVd(1) = \u03c0d/2\n\u0393(d/2 + 1) (39)\nwhere \u0393(x) = \u222b\u221e 0 tx\u22121e\u2212tdt. So we have\nVd(1) Vd\u22121(1) = \u221a \u03c0 \u0393(d/2 + 1/2) \u0393(d/2 + 1) (40)\nFrom Gautschi\u2019s Inequality\nx1\u2212s < \u0393(x+ 1)\n\u0393(x+ s) < (x+ s)1\u2212s x > 0, 0 < s < 1 (41)\nwith s = 1/2 and x = d/2 we have:( d+ 1\n2\n)\u22121/2 < \u0393(d/2 + 1/2)\n\u0393(d/2 + 1) <\n( d\n2\n)\u22121/2 (42)\nTherefore, it suffices to have\nr \u2264 \u221a 2\u03c0\nd+ 1 \u2016w\u2217\u2016 (43)\nNote that this upper bound is tight when \u03b4 \u2192 0 and d\u2192 +\u221e, since all inequality involved asymptotically becomes equal."}, {"heading": "7.3 TWO LAYER CASE", "text": "Lemma 7.5 For \u03c6\u2217, \u03b8 and \u03c6 defined in Eqn. 17:\n\u03b1 \u2261 (x2 + (K \u2212 1)y2)\u22121/2 (44) cos \u03b8 \u2261 \u03b1x (45) cos\u03c6\u2217 \u2261 \u03b1y (46) cos\u03c6 \u2261 \u03b12(2xy + (K \u2212 2)y2) (47)\nwe have the following relations in the triangular region \u2126 0 = {(x, y) : x \u2265 0, y \u2265 0, x \u2265 y + 0} (Fig. 6(c)):\n(1) \u03c6, \u03c6\u2217 \u2208 [0, \u03c0/2] and \u03b8 \u2208 [0, \u03b80) where \u03b80 = arccos 1\u221aK .\n(2) cos\u03c6 = 1\u2212 \u03b12(x\u2212 y)2 and sin\u03c6 = \u03b1(x\u2212 y) \u221a 2\u2212 \u03b12(x\u2212 y)2.\n(3) \u03c6\u2217 \u2265 \u03c6 (equality holds only when y = 0) and \u03c6\u2217 > \u03b8.\nProof Propositions (1) and (2) are computed by direct calculations. In particular, note that since cos \u03b8 = \u03b1x = 1/ \u221a 1 + (K \u2212 1)(y/x)2 and x > y \u2265 0, we have cos \u03b8 \u2208 (1/ \u221a K, 1] and \u03b8 \u2208 [0, \u03b80). For Preposition (3), \u03c6\u2217 = arccos\u03b1y > \u03b8 = arccos\u03b1x because x > y. Finally, for x > y > 0, we have\ncos\u03c6 cos\u03c6\u2217 = \u03b12(2xy + (K \u2212 2)y2) \u03b1y = \u03b1(2x+ (K \u2212 2)y) > \u03b1(x+ (K \u2212 1)y) > 1 (48)\nThe final inequality is because K \u2265 2, x, y > 0 and thus (x + (K \u2212 1)y)2 > x2 + (K \u2212 1)2y2 > x2 + (K \u2212 1)y2 = \u03b1\u22122. Therefore \u03c6\u2217 > \u03c6. If y = 0 then \u03c6\u2217 = \u03c6.\nTheorem 7.6 For the dynamics defined in Eqn. 16, there exists 0 > 0 so that the trianglar region \u2126 0 = {(x, y) : x \u2265 0, y \u2265 0, x \u2265 y + 0} (Fig. 6(c)) is a convergent region. That is, the flow goes inwards for all three edges and any trajectory starting in \u2126 0 stays.\nProof We discuss the three boundaries as follows:\nCase 1: y = 0, 0 \u2264 x \u2264 1, horizontal line. In this case, \u03b8 = 0, \u03c6 = \u03c0/2 and \u03c6\u2217 = \u03c0/2. The y component of the dynamics in this line is:\nf1 \u2261 2\u03c0 N \u2206y = \u2212\u03c0 2 (x\u2212 1) \u2265 0 (49)\nSo \u2206y points to the interior of \u2126.\nCase 2: x = 1, 0 \u2264 y \u2264 1, vertical line. In this case, \u03b1 \u2264 1 and the x component of the dynamics is:\nf2 \u2261 2\u03c0\nN \u2206x = \u2212(\u03c0 \u2212 \u03c6)(K \u2212 1)y \u2212 \u03b8 + (K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) + \u03b1 sin \u03b8 (50)\n= \u2212(K \u2212 1) [(\u03c0 \u2212 \u03c6)y \u2212 (\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6)] + (\u03b1 sin \u03b8 \u2212 \u03b8) (51) Note that since \u03b1 \u2264 1, \u03b1 sin \u03b8 \u2264 sin \u03b8 \u2264 \u03b8, so the second term is non-positive. For the first term, we only need to check whether (\u03c0 \u2212 \u03c6)y \u2212 (\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) is nonnegative. Note that\n(\u03c0 \u2212 \u03c6)y \u2212 (\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) (52) = (\u03c0 \u2212 \u03c6)y + \u03b1(x\u2212 y) \u221a 2\u2212 \u03b12(x\u2212 y)2 \u2212 \u03b1 \u221a 1\u2212 \u03b12y2 (53)\n= y [ \u03c0 \u2212 \u03c6\u2212 \u03b1 \u221a 2\u2212 \u03b12(x\u2212 y)2 ] + \u03b1 [ x \u221a 2\u2212 \u03b12(x\u2212 y)2 \u2212 \u221a 1\u2212 \u03b12y2 ]\n(54)\nIn \u2126 we have (x \u2212 y)2 \u2264 1, combined with \u03b1 \u2264 1, we have 1 \u2264 \u221a 2\u2212 \u03b12(x\u2212 y)2 \u2264 \u221a\n2 and\u221a 1\u2212 \u03b12y2 \u2264 1. Since x = 1, the second term is nonnegative. For the first term, since \u03b1 \u2264 1,\n\u03c0 \u2212 \u03c6\u2212 \u03b1 \u221a\n2\u2212 \u03b12(x\u2212 y)2 \u2265 \u03c0 \u2212 \u03c0 2 \u2212 \u221a 2 > 0 (55)"}, {"heading": "So (\u03c0 \u2212 \u03c6)y \u2212 (\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) \u2265 0 and \u2206x \u2264 0, pointing inwards.", "text": "Case 3: x = y + , 0 \u2264 y \u2264 1, diagonal line. We compute the inner product between (\u2206x,\u2206y) and (1,\u22121), the inward normal of \u2126 at the line. Using \u03c6 \u2264 \u03c02 sin\u03c6 for \u03c6 \u2208 [0, \u03c0/2] and \u03c6\n\u2217 \u2212 \u03b8 = arccos\u03b1y \u2212 arccos\u03b1x \u2265 0 when x \u2265 y, we have:\nf3(y, ) \u2261 2\u03c0\nN\n[ \u2206x \u2206y ]\u1d40 [ 1 \u22121 ] = \u03c6\u2217 \u2212 \u03b8 \u2212 \u03c6+ [(K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) + \u03b1 sin \u03b8] (56)\n\u2265 (K \u2212 1) [ \u03b1 sin\u03c6\u2217 \u2212 ( 1 +\n\u03c0\n2(K \u2212 1)\n) sin\u03c6 ] = \u03b1(K \u2212 1) [\u221a 1\u2212 \u03b12y2 \u2212 ( 1 + \u03c0\n2(K \u2212 1)\n)\u221a 2\u2212 \u03b12 2 ] Note that for y > 0:\n\u03b1y = 1\u221a\n(x/y)2 + (K \u2212 1) = 1\u221a (1 + /y)2 + (K \u2212 1) \u2264 1\u221a K\n(57)\nFor y = 0, \u03b1y = 0 < \u221a 1/K. So we have \u221a 1\u2212 \u03b12y2 \u2265 \u221a 1\u2212 1/K. And \u221a 2\u2212 \u03b12 2 \u2264 \u221a\n2. Therefore f3 \u2265 \u03b1(K\u22121)(C1\u2212 C2) withC1 \u2261 \u221a 1\u2212 1/K > 0 andC2 \u2261 \u221a 2(1+\u03c0/2(K\u22121)) > 0. With = 0 > 0 sufficiently small, f3 > 0.\nLemma 7.7 (Reparametrization) Denote = x \u2212 y > 0. The terms \u03b1x, \u03b1y and \u03b1 involved in the trigometric functions in Eqn. 16 has the following parameterization:\n\u03b1 [ y x ] = 1\nK\n[ \u03b2 \u2212 \u03b22\n\u03b2 + (K \u2212 1)\u03b22 K\u03b22\n] (58)\nwhere \u03b22 = \u221a\n(K \u2212 \u03b22)/(K \u2212 1). The reverse transformation is given by \u03b2 =\u221a K \u2212 (K \u2212 1)\u03b12 2. Here \u03b2 \u2208 [1, \u221a K) and \u03b22 \u2208 (0, 1]. In particular, the critical point (x, y) = (1, 0) corresponds to (\u03b2, ) = (1, 1). As a result, all trigometric functions in Eqn. 16 only depend on the single variable \u03b2. In particular, the following relationship is useful:\n\u03b2 = cos \u03b8 + \u221a K \u2212 1 sin \u03b8 (59)\nProof This transformation can be checked by simple algebraic manipulation. For example:\n1\n\u03b1K (\u03b2 \u2212 \u03b22) =\n1\nK\n(\u221a K\n\u03b12 \u2212 (K \u2212 1) 2 \u2212\n) = 1\nK\n(\u221a (Ky + )2 \u2212 ) = y (60)\nTo prove Eqn. 59, first we notice that K cos \u03b8 = K\u03b1x = \u03b2 + (K \u2212 1)\u03b22. Therefore, we have (K cos \u03b8 \u2212 \u03b2)2 \u2212 (K \u2212 1)2\u03b222 = 0, which gives \u03b22 \u2212 2\u03b2 cos \u03b8 + 1 \u2212K sin2 \u03b8 = 0. Solving this quadratic equation and notice that \u03b2 \u2265 1, \u03b8 \u2208 [0, \u03c0/2] and we get:\n\u03b2 = cos \u03b8 + \u221a cos2 \u03b8 +K sin2 \u03b8 \u2212 1 = cos \u03b8 + \u221a K \u2212 1 sin \u03b8 (61)\nLemma 7.8 After reparametrization (Eqn. 58), f3(\u03b2, ) \u2265 0 for \u2208 (0, \u03b22/\u03b2]. Furthermore, the equality is true only if (\u03b2, ) = (1, 1) or (y, ) = (0, 1).\nProof Applying the parametrization (Eqn. 58) to Eqn. 56 and notice that \u03b1 = \u03b22 = \u03b22(\u03b2), we could write f3 = h1(\u03b2)\u2212 (\u03c6+ (K \u2212 1) sin\u03c6) (62) When \u03b2 is fixed, f3 now is a monotonously decreasing function with respect to > 0. Therefore, f3(\u03b2, ) \u2265 f3(\u03b2, \u2032) for 0 < \u2264 \u2032 \u2261 \u03b22/\u03b2. If we could prove f3(\u03b2, \u2032) \u2265 0 and only attain zero at known critical point (\u03b2, ) = (1, 1), the proof is complete.\nDenote f3(\u03b2, \u2032) = f31 + f32 where\nf31(\u03b2, \u2032) = \u03c6\u2217 \u2212 \u03b8 \u2212 \u2032\u03c6+ \u2032\u03b1 sin \u03b8 (63) f32(\u03b2, \u2032) = (K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) \u2032 (64)\nFor f32 it suffices to prove that \u2032(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) = \u03b22 sin\u03c6\u2217 \u2212 \u03b22\u03b2 sin\u03c6 \u2265 0, which is equivalent to sin\u03c6\u2217 \u2212 sin\u03c6/\u03b2 \u2265 0. But this is trivially true since \u03c6\u2217 \u2265 \u03c6 and \u03b2 \u2265 1. Therefore, f32 \u2265 0. Note that the equality only holds when \u03c6\u2217 = \u03c6 and \u03b2 = 1, which corresponds to the horizontal line x \u2208 (0, 1], y = 0. For f31, since \u03c6\u2217 \u2265 \u03c6, \u03c6\u2217 > \u03b8 and \u2032 \u2208 (0, 1], we have the following:\nf31 = \u2032(\u03c6\u2217 \u2212 \u03c6) + (1\u2212 \u2032)(\u03c6\u2217 \u2212 \u03b8)\u2212 \u2032\u03b8 + \u03b22 sin \u03b8 \u2265 \u2212 \u2032\u03b8 + \u03b22 sin \u03b8 \u2265 \u03b22 ( sin \u03b8 \u2212 \u03b8\n\u03b2\n) (65)\nAnd it reduces to showing whether \u03b2 sin \u03b8 \u2212 \u03b8 is nonnegative. Using Eqn. 59, we have:\nf33(\u03b8) = \u03b2 sin \u03b8 \u2212 \u03b8 = 1\n2 sin 2\u03b8 +\n\u221a K \u2212 1 sin2 \u03b8 \u2212 \u03b8 (66)\nNote that f \u203233 = cos 2\u03b8 + \u221a K \u2212 1 sin 2\u03b8 \u2212 1 = \u221a K cos(2\u03b8 \u2212 \u03b80) \u2212 1, where \u03b80 = arccos 1\u221aK . By Prepositions 1 in Lemma 7.5, \u03b8 \u2208 [0, \u03b80). Therefore, f \u203233 \u2265 0 and since f33(0) = 0, f33 \u2265 0. Again the equity holds when \u03b8 = 0, \u03c6\u2217 = \u03c6 and \u2032 = 1, which is the critical point (\u03b2, ) = (1, 1) or (y, ) = (0, 1).\nTheorem 7.9 For the dynamics defined in Eqn. 16, the only critical point (\u2206x = 0 and \u2206y = 0) within \u2126 is (y, ) = (0, 1).\nProof We prove by contradiction. Suppose (\u03b2, ) is a critical point other than w\u2217. A necessary condition for this to hold is f3 = 0 (Eqn. 56). By Lemma 7.8, > \u2032 = \u03b22/\u03b2 > 0 and\n\u2212 1 +Ky = 1 \u03b1 (\u03b22 \u2212 \u03b1+ \u03b2 \u2212 \u03b22) = \u03b2 \u2212 \u03b1 \u03b1 = \u03b2 \u2212 \u03b22/ \u03b1 > \u03b2 \u2212 \u03b22/ \u2032 \u03b1 = 0 (67)\nSo \u2212 1 +Ky is strictly greater than zero. On the other hand, the condition f3 = 0 implies that\n((K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) + \u03b1 sin \u03b8) = \u22121 (\u03c6\u2217 \u2212 \u03b8) + \u03c6 (68)\nUsing \u03c6 \u2208 [0, \u03c0/2], \u03c6\u2217 \u2265 \u03c6 and \u03c6\u2217 > \u03b8, we have: 2\u03c0\nN \u2206y = \u2212(\u03c0 \u2212 \u03c6)( \u2212 1 +Ky)\u2212 (\u03c6\u2217 \u2212 \u03c6)\u2212 \u03c6y + ((K \u2212 1)(\u03b1 sin\u03c6\u2217 \u2212 sin\u03c6) + \u03b1 sin \u03b8) y\n= \u2212(\u03c0 \u2212 \u03c6)( \u2212 1 +Ky)\u2212 (\u03c6\u2217 \u2212 \u03c6)\u2212 1 (\u03c6\u2217 \u2212 \u03b8)y < 0 (69)\nSo the current point (\u03b2, ) cannot be a critical point.\nTheorem 7.10 Any trajectory in \u2126 0 converges to (y, ) = (1, 0), following the dynamics defined in Eqn. 16.\nProof We have Lyaponov function V = E [E] so that V\u0307 = \u2212E [\u2206w\u1d40\u2206w] \u2264 \u2212E [\u2206w]\u1d40 E [\u2206w] \u2264 0. By Thm. 7.9, other than the optimal solution w\u2217, there is no other symmetric critical point, \u2206w 6= 0 and thus V\u0307 < 0. On the other hand, by Thm. 7.6, the triangular region \u2126 0 is convergent, in which the 2D dynamics isC\u221e differentiable. Therefore, any 2D solution curve \u03be(t) will stay within. By PoincareBendixson theorem, when there is a unique critical point, the curve either converges to a limit circle or the critical point. However, limit cycle is not possible since V is strictly monotonous decreasing along the curve. Therefore, \u03be(t) will converge to the unique critical point, which is (y, ) = (1, 0) and so does the symmetric system (Eqn. 12).\nTheorem 7.11 When x = y \u2208 (0, 1], the 2D dynamics (Eqn. 16) reduces to the following 1D case: 2\u03c0\nN \u2206x = \u2212\u03c0K(x\u2212 x\u2217) (70)\nwhere x\u2217 = 1\u03c0K ( \u221a K \u2212 1\u2212 arccos(1/ \u221a K) + \u03c0). Furthermore, x\u2217 is a convergent critical point.\nProof The 1D system can be computed with simple algebraic manipulations (note that when x = y, \u03c6 = 0 and \u03b8 = \u03c6\u2217 = arccos(1/ \u221a K)). Note that the 1D system is linear and its close form solution is x(t) = x0 + Ce\u2212K/2Nt and thus convergent."}], "references": [{"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "LeCun", "Yann", "Arous", "G\u00e9rard Ben"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Local minima and plateaus in hierarchical structures of multilayer perceptrons", "author": ["Fukumizu", "Kenji", "Amari", "Shun-ichi"], "venue": "Neural Networks,", "citeRegEx": "Fukumizu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "Computer Vision anad Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kawaguchi", "Kenji"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kawaguchi and Kenji.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi and Kenji.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Mei", "Song", "Bai", "Yu", "Montanari", "Andrea"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "Mei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "author": ["Saad", "David", "Solla", "Sara A"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Saad et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saad et al\\.", "year": 1996}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Soudry", "Daniel", "Carmon", "Yair"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "Soudry et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w\u2217 with at least (1 \u2212 )/2 probability, if random weight initializations of proper standard derivation (\u223c 1/ \u221a d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al.", "startOffset": 277, "endOffset": 294}, {"referenceID": 4, "context": "We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w\u2217 with at least (1 \u2212 )/2 probability, if random weight initializations of proper standard derivation (\u223c 1/ \u221a d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)].", "startOffset": 277, "endOffset": 315}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 108, "endOffset": 125}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 108, "endOffset": 154}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al.", "startOffset": 108, "endOffset": 177}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al.", "startOffset": 108, "endOffset": 203}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al.", "startOffset": 108, "endOffset": 258}, {"referenceID": 4, "context": "1 INTRODUCTION Deep learning has made substantial progress in many applications, including Computer Vision [He et al. (2016); Simonyan & Zisserman (2015); Szegedy et al. (2015); Krizhevsky et al. (2012)], Natural Language Processing [Sutskever et al. (2014)] and Speech Recognition [Hinton et al. (2012)].", "startOffset": 108, "endOffset": 304}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al.", "startOffset": 165, "endOffset": 182}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],.", "startOffset": 165, "endOffset": 203}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K \u2265 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w\u2217, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed \u2016w\u2217\u2016, showing that such a convergence behavior is beyond the local convex structure at w\u2217. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network.", "startOffset": 165, "endOffset": 849}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K \u2265 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w\u2217, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed \u2016w\u2217\u2016, showing that such a convergence behavior is beyond the local convex structure at w\u2217. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network.", "startOffset": 165, "endOffset": 924}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K \u2265 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w\u2217, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed \u2016w\u2217\u2016, showing that such a convergence behavior is beyond the local convex structure at w\u2217. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function \u03c3(x) when its derivatives \u03c3\u2032, \u03c3\u2032\u2032, \u03c3\u2032\u2032\u2032 are bounded and \u03c3\u2032 > 0.", "startOffset": 165, "endOffset": 1116}, {"referenceID": 2, "context": ")/2 probability, if initialized randomly with standard derivation on the order of 1/ \u221a d, verifying commonly used initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)],. When K \u2265 2, we prove that when the teacher parameters {wj}j=1 form orthonormal bases, (1) a symmetric initialization of a student network gets stuck at a saddle point and (2) under a certain symmetric breaking weight initialization, the dynamics converges to w\u2217, without getting stuck into any local minima. Note that in both cases, the initialization can be arbitrarily close to the origin for a fixed \u2016w\u2217\u2016, showing that such a convergence behavior is beyond the local convex structure at w\u2217. To our knowledge, this is the first proof of its kind. Previous works also use dynamical system to analyze deep neural networks. [Saxe et al. (2013)] analyzes the dynamics of multilayer linear network, and [Kawaguchi (2016)] shows every local minima is global for multilinear network. Very little theoretical work has been done to analyze the dynamics of nonlinear networks, especially deep ones. [Mei et al. (2016)] shows the global convergence whenK = 1 with activation function \u03c3(x) when its derivatives \u03c3\u2032, \u03c3\u2032\u2032, \u03c3\u2032\u2032\u2032 are bounded and \u03c3\u2032 > 0. Similar to our approach, [Saad & Solla (1996)] also uses the student-teacher setting and analyzes the dynamics of student network when the teacher\u2019s parameters w\u2217 forms a orthonomal bases; however, it uses \u03c3(x) = erf(x) as the nonlinearity and only analyzes the local behaviors of the two critical points (the saddle point in symmetric initializations, and w\u2217).", "startOffset": 165, "endOffset": 1291}, {"referenceID": 0, "context": "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions.", "startOffset": 14, "endOffset": 217}, {"referenceID": 0, "context": "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations.", "startOffset": 14, "endOffset": 335}, {"referenceID": 0, "context": "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network.", "startOffset": 14, "endOffset": 810}, {"referenceID": 0, "context": "For example, [Choromanska et al. (2015a;b)] relate the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). [Kawaguchi (2016)] proves that every local minimum in nonlinear network is global based on similar assumptions. [Soudry & Carmon (2016)] shows the global optimality of the local minimum in a two-layered ReLU network, by assuming small sample size and applying independent multiplicative Bernoulli noise on the activations. In practice, the activations are highly dependent due to their common input. Ignoring such dependency also misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activation is made. For sigmoid activation, [Fukumizu & Amari (2000)] gives quite complicated conditions for a local minimum to be global when adding a new node to a two-layered network. [Janzamin et al. (2015)] gives guarantees on recovering the parameters of a 2-layered neural network learnt with tensor decomposition.", "startOffset": 14, "endOffset": 952}, {"referenceID": 11, "context": "Note that for ReLU activation, \u03c3\u2032(x) = 0 for certain negative x even after a local smoothing, so the global convergence claim in [Mei et al. (2016)] for l2 loss does not apply.", "startOffset": 130, "endOffset": 148}, {"referenceID": 4, "context": "From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al.", "startOffset": 148, "endOffset": 165}, {"referenceID": 4, "context": "From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)].", "startOffset": 148, "endOffset": 186}, {"referenceID": 4, "context": "From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w\u2217 yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = \u2211K j=1 \u03c3(w T j x) for K \u2265 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].", "startOffset": 148, "endOffset": 585}, {"referenceID": 4, "context": "From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w\u2217 yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = \u2211K j=1 \u03c3(w T j x) for K \u2265 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].", "startOffset": 148, "endOffset": 609}, {"referenceID": 4, "context": "From the proof, the conclusion could be made stronger to show r \u223c 1/ \u221a d, consistent with common initialization techniques [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. Fig. 2(c) shows an example in the 2D case, in which there is a singularity at the origin, and sampling towards w\u2217 yields the convergence. This is consistent with the analysis above. 4 MULTIPLE RELUS CASE Now we are ready to analyze the network g(x) = \u2211K j=1 \u03c3(w T j x) for K \u2265 2 (Fig. 1(c)). Theoretical analysis of such networks is also the main topic in many previous works [Saad & Solla (1996); Soudry & Carmon (2016); Fukumizu & Amari (2000)].", "startOffset": 148, "endOffset": 634}, {"referenceID": 4, "context": "Such initialization is consistent with common practice [Glorot & Bengio (2010); He et al. (2015)] and is independent of the value of w\u2217.", "startOffset": 80, "endOffset": 97}], "year": 2016, "abstractText": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of g(x;w) = \u2211K j=1 \u03c3(w T j x), where \u03c3(\u00b7) is ReLU nonlinearity. We assume that the input x follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters w\u2217 using l2 loss. We first show that when K = 1, the nonlinear dynamics can be written in close form, and converges to w\u2217 with at least (1 \u2212 )/2 probability, if random weight initializations of proper standard derivation (\u223c 1/ \u221a d) is used, verifying empirical practice [Glorot & Bengio (2010); He et al. (2015); LeCun et al. (2012)]. For networks with many ReLU nodes (K \u2265 2), we apply our close form dynamics and prove that when the teacher parameters {w\u2217 j}j=1 forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to w\u2217 without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with l2 loss. Simulations verify our theoretical analysis.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_221"}