{"name": "ICLR_2017_22.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Roger Grosse", "James Martens"], "emails": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Current state-of-the-art deep neural networks (Szegedy et al., 2014; Krizhevsky et al., 2012; He et al., 2015) often require days of training time with millions of training cases. The typical strategy to speed-up neural network training is to allocate more parallel resources over many machines and cluster nodes (Dean et al., 2012). Parallel training also enables researchers to build larger models where different machines compute different splits of the mini-batches. Although we have improved our distributed training setups over the years, neural networks are still trained with various simple first-order stochastic gradient descent (SGD) algorithms. Despite how well SGD scales with the size of the model and the size of the datasets, it does not scale well with the parallel computation resources. Larger mini-batches and more parallel computations exhibit diminishing returns for SGD and related algorithms.\nSecond-order optimization methods, which use second-order information to construct updates that account for the curvature of objective function, represent a promising alternative. The canonical second-order methods work by inverting a large curvature matrix (traditionally the Hessian), but this doesn\u2019t scale well to deep neural networks with millions of parameters. Various approximations to the curvature matrix have been proposed to help alleviate this problem, such as diagonal (LeCun et al., 1998; Duchi et al., 2011; Kingma and Ba, 2014), block diagonal Le Roux et al. (2008), and low-rank ones (Schraudolph et al., 2007; Bordes et al., 2009; Wang et al., 2014; Keskar and Berahas, 2015; Moritz et al., 2016; Byrd et al., 2016; Curtis, 2016; Ramamurthy and Duffy). Another\nstrategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).\nThe usual problem with curvature approximations, especially low-rank and diagonal ones, is that they are very crude and only model superficial aspects of the true curvature in the objective function. Krylov-subspace methods on the other hand suffer because they still rely on 1st-order methods to compute their updates.\nMore recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015). In the K-FAC approach (Martens and Grosse, 2015; Grosse and Martens, 2016), these approximations result in a block-diagonal approximation to the Fisher information matrix (with blocks corresponding to entire layers) where each block is approximated as a Kronecker product of two much smaller matrices, both of which can be estimated and inverted fairly efficiently. Because the inverse of a Kronecker product of two matrices is the Kronecker product of their inverses, this allows the entire matrix to be inverted efficiently.\nMartens and Grosse (2015) found that K-FAC scales very favorably to larger mini-batches compared to SGD, enjoying a nearly linear relationship between mini-batch size and per-iteration progress for medium-to-large sized mini-batches. One possible explanation for this phenomenon is that secondorder methods make more rapid progress exploring the error surface and reaching a neighborhood of a local minimum where gradient noise (which is inversely proportional to mini-batch size) becomes the chief limiting factor in convergence1. This observation implies that K-FAC would benefit in particular from a highly parallel distributed implementation.\nIn this paper, we propose an asynchronous distributed version of K-FAC that can effectively exploit large amounts of parallel computing resources, and which scales to industrial-scale neural net models with hundreds of millions of parameters. Our method augments the traditional distributed synchronous SGD setup with additional computation nodes that update the approximate Fisher and compute its inverse. The proposed method achieves a comparable per-iteration runtime as a normal SGD using the same mini-batch size on a typical 4 GPU cluster. We also propose a \u201cdoubly factored\u201d Kronecker approximation for layers whose inputs are feature maps that are normally too large to handled by the standard Kronecker-factored approximation. Finally, we empirically demonstrate that the proposed method speeds up learning of various state-of-the-art ImageNet models by a factor of two over Batch Normalization (Ioffe and Szegedy, 2015)."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 KRONECKER FACTORED APPROXIMATE FISHER", "text": "Let DW be the gradient of the log likelihood L of a neural network w.r.t. some weight matrix W \u2208 RCout\u00d7Cin in a layer, where Cin, Cout are the number of input/output units of the layer. The block of the Fisher information matrix of that layer is given by:\nF = E x,y\u223cP\n[ vec{DW} vec{DW}> ] , (1)\nwhere P is the distribution over the input x and the network\u2019s distribution over targets y (implied by the log-likelihood objective). Throughout this paper we assume, unless otherwise stated, that expectations are taken with respect to P (and not the training distribution over y).\nK-FAC (Martens and Grosse, 2015; Grosse and Martens, 2016) uses a Kronecker-factored approximation to each block which we now describe. Denote the input activation vector to the layer as A \u2208 RCin , the pre-activation inputs as s = WA and the back-propagated loss derivatives as Ds = dLds \u2208 R\nCout . Note that the gradient of the weights is the outer product of the input activation and back-propagated derivatives DW = DsA>. K-FAC approximates the Fisher block as a\n1Mathematical evidence for this idea can be found in Martens (2014), where it is shown that (convex quadratic) objective functions decompose into noise-dependent and independent terms, and that second-order methods make much more rapid progress optimizing the noise-independent term compared to SGD, while have no effect on the noise-dependent term (which shrinks with the size of the mini-batch)\nKronecker product of the second-order statistics of the input and the backpropagated derivatives: F =E [ vec{DW} vec{DW}> ] = E [ AA> \u2297DsDs> ] \u2248 E [ AA> ] \u2297 E [ DsDs> ] , F\u0302 . (2)\nThis approximation can be interpreted as making the assumption that the second-order statistics of the activations and the backpropagated derivatives are uncorrelated."}, {"heading": "2.2 APPROXIMATE NATURAL GRADIENT USING K-FAC", "text": "The natural gradient (Amari, 1998) is defined as the inverse of the Fisher times the gradient. It is traditionally interpreted as the direction in parameter space that achieves the largest (instantaneous) improvement in the objective per unit of change in the output distribution of the network (as measured using the KL-divergence). Under certain conditions, which almost always hold in practice, it can also be interpreted as a second-order update computed by minimizing a local quadratic approximation of the log-likelihood objective, where the Hessian is approximated using the Fisher (Martens, 2014).\nTo compute the approximate natural gradient in K-FAC, one multiplies the gradient for the weights of each layer by the inverse of the corresponding approximate Fisher block F\u0302 for that layer. Denote the gradient of the loss function with respect to the weights W by GW \u2208 RCin\u00d7Cout . We will assume the use of the factorized Tikhonov damping approach described by Martens and Grosse (2015), where the addition of the damping term \u03bbI to F\u0302 is approximated by adding \u03c0A\u03bb 1 2 I to\nE [ AA> ] and \u03c0Ds\u03bb 1 2 I to E [ DsDs> ] , where \u03c0A and \u03c0Ds are adjustment factors that are described\nin detail and generalized in Sec. 4.1. (Note that one can also include the contribution to the curvature from any L2 regularization terms with \u03bb.)\nBy exploiting the basic identities (A\u2297B)\u22121 = (A\u22121\u2297B\u22121) and (A\u2297B) vec(C) = vec(BCA>), the approximate natural gradient update v can then be computed as:\nv = ( F\u0302 + \u03bbI )\u22121 vec{GW } \u2248 vec {( E [ AA> ] + \u03c0A\u03bb 1 2 I )\u22121 GW ( E [ DsDs> ] + \u03c0Ds\u03bb 1 2 I )\u22121} ,\n(3)\nwhich amounts to several matrix inversion of multiplication operations involving matrices roughly the same size as the weight matrix W ."}, {"heading": "3 DISTRIBUTED OPTIMIZATION USING K-FAC", "text": "Stochastic optimization algorithms benefit from low-variance gradient estimates (as might be obtained from larger mini-batches). Prior work suggests that approximate natural gradient algorithms might benefit more than standard SGD from reducing the variance (Martens and Grosse, 2015; Grosse and Martens, 2016). One way to efficiently obtain low-variance gradient estimates is to parallelize the gradient computation across many machines in a distributed system (thus allowing large mini-batches to be processed efficiently). Because the gradient computation in K-FAC is identical to that of SGD, we parallelize the gradient computation using the standard synchronous SGD model.\nHowever, K-FAC also introduces other forms of overhead not found in SGD \u2014 in particular, estimation of second-order statistics and computation of inverses or eigenvalues of the Kronecker factors. In this section, we describe how these additional computations can be performed asynchronously. While this asynchronous computation introduces an additional source of error into the algorithm, we find that it does not significantly affect the per-iteration progress in practice. All in all, the periteration wall clock time of our distributed K-FAC implementation is only 5-10% higher compared to synchronous SGD with the same mini-batch size."}, {"heading": "3.1 ASYNCHRONOUS FISHER BLOCK INVERSION", "text": "Computing the parameter updates as per Eq.3 requires the estimated gradients to be multiplied by the inverse of the smaller Kronecker factors. This requires periodically computing (typically) either inverses or eigendecompositions of each of these factors. While these factors typically have sizes\nonly in the hundreds or low thousands, very deep networks may have hundreds of such matrices (2 or more for each layer). Furthermore, matrix inversion and eigendecomposition see little benefit from GPU computation, so they can be more expensive than standard neural network operations. For these reasons, inverting the approximate Fisher blocks represents a significant computational cost.\nIt has been observed that refreshing the inverse of the Fisher blocks only occasionally and using stale values otherwise has only a small detrimental effect on average per-iteration progress, perhaps because the curvature changes relatively slowly (Martens and Grosse, 2015). We push this a step further by computing the inverses asynchronously while the network is still training. Because the required linear algebra operations are CPU-bound while the rest of our computations are GPU-bound, we perform them on the CPU with little effective overhead. Our curvature statistics are somewhat more stale as a result, but this does not appear to significantly affect per-iteration optimization performance. In our experiments, we found that computing the inverses asynchronously usually offered a 40-50% speed-up to the overall wall-clock time of the K-FAC algorithm."}, {"heading": "3.2 ASYNCHRONOUS STATISTICS COMPUTATION", "text": "The other major source of computational overhead in K-FAC is the estimation of the second-order statistics of the activations and derivatives, which are needed for the Kronecker factors. In the standard K-FAC algorithm, these statistics are computed on the same mini-batches as the gradients, allowing the forward pass computations to be shared between the gradient and statistics computations. By computing the gradients and statistics on separate mini-batches, we can enable a higher degree of parallelism, at the expense of slightly more total computational operations. Under this scheme, the statistics estimation is independent of the gradient computation, so it can be done on one or more separate worker nodes with their own independent data shards. These worker nodes receive parameters from the parameter server (just as in synchronous SGD) and communicate statistics back to the parameter server. In our experiments, we assigned at most one worker to computing statistics.\nIn cases where it is undesirable to devote separate worker nodes to computing statistics, we also introduce a fast approximation to the statistics for convolution layers (see Appendix A)."}, {"heading": "4 DOUBLY-FACTORED KRONECKER APPROXIMATION FOR LARGE CONVOLUTION LAYERS", "text": "Computing the standard Kronecker factored Fisher approximation for a given layer involves operations on matrices whose dimension is the number of input units or output units. The cost of these operations is reasonable for most fully-connected networks because the number of units in each layer rarely exceeds a couple thousand. Large convolutional neural networks, however, often include a fully-connected layer that \u201cpools\u201d over a large feature map before the final softmax classification. For instance, the output of the last pooling layer of AlexNet is of size 6 \u00d7 6 \u00d7 256 = 9216, which then provides inputs to the subsequent fully connected layer of 4096 ReLUs. VGG models also share a similar architecture. For the standard Kronecker-factored approximation one of the factors will be a matrix of size 9216 \u00d7 9216, which is too expensive to be explicitly inverted as often as is needed during training.\nIn this section we propose a \u201cdoubly-factored\u201d Kronecker approximation for layers whose input is a large feature map. Specifically, we approximate the second-order statistics matrix of the inputs as itself factoring as a Kronecker product. This gives an approximation which is a Kronecker product of three matrices.\nUsing the AlexNet example, the 9216 \u00d7 4096 weight matrix in the first fully connected layer is equivalent to a filterbank of 4096 filters with kernel size 6 \u00d7 6 on 256 input channels. Let A be a matrix of dimension T -by-Cin representing the input activations (for a single training case), where T = Kw \u00d7Kh is the feature map height and width, and Cin is the number of input channels. The Fisher block for such a layer can be written as:\nE[vec{DW} vec{DW}>] = E[vec{A} vec{A}> \u2297DsDs>], A \u2208 RT \u00d7Cin . (4)\nWe begin be making the following rank-1 approximation:\nA \u2248 K\u03a8>, (5)\nwhereK \u2208 RT , \u03a8 \u2208 RCin are the factors along the spatial location dimension and the input channel dimension. The optimal solution of a low-rank approximation under the Frobenius norm is given by the singular value decomposition. The activation matrix A is small enough that its SVD can be computed efficiently. Let \u03c31, u1, v1 be the first singular value and its left and right singular vectors of the activation matrix A, respectively. The factors of the rank-1 approximation are then chosen to be K = \u221a\u03c31u1 and \u03a8 = \u221a \u03c31v1. K captures the activation patterns across spatial locations in a feature map and \u03a8 captures the pattern across the filter responses. Under the rank-1 approximation of A we have:\nE[vec{A} vec{A}> \u2297DsDs>] \u2248 E[vec{K\u03a8>} vec{K\u03a8>}> \u2297DsDs>] (6) = E[KK> \u2297\u03a8\u03a8> \u2297DsDs>]. (7)\nWe further assume the second order statistics are three-way independent between the loss derivatives Ds, the activations along the input channels \u03a8, and the activations along spatial locations K:\nE[vec{DW} vec{DW}>] \u2248 E[KK>]\u2297 E[\u03a8\u03a8>]\u2297 E[DsDs>]. (8)\nThe final approximated Fisher block is a Kronecker product of three small matrices. And note that although we assumed the feature map activations have low-rank structure, the resulting approximated Fisher is not low-rank.\nThe approximate natural gradient for this layer can then be computed by multiplying the inverses of each of the smaller matrices against the respective dimensions of the gradient tensor. We define a function Ri : Rd1\u00d7d2\u00d7d3 \u2192 Rdjdk\u00d7di that constructs a matrix from a 3D tensor by \u201creshaping\u201d it so that the desired target dimension i \u2208 {1, 2, 3} maps to columns, while the remaining dimensions (j and k) are \u201cfolded together\u201d and map to the rows. Given the gradient of the weights, GW \u2208 RT \u00d7Cin\u00d7Cout we can compute the matrix-vector product with the inverse double-factored Kronecker approximated Fisher block as:\nR\u221213 ( E[DsDs>]\u22121R3 ( R\u221212 ( E[\u03a8\u03a8>]\u22121R2(R\u221211 (E[KK>]\u22121R1(GW ))) ))) . (9)\nwhich is a nested application of the reshape function R(\u00b7) at each of the dimension of the gradient tensor.\nThe doubly factored Kronecker approximation provides a computationally feasible alternative to the standard Kronecker-factored approximation for layers that have a number of parameters in the order of hundreds of millions. For example, inverting it for the first fully connected layer of AlexNet takes about 15 seconds on an 8 core Intel Xeon CPU, and such time is amortized in our asynchronous algorithm.\nUnfortunately, the homogeneous coordinate formulation is no longer applicable under this new approximation. Instead, we lump the bias parameters together and associate a full Fisher block with them, which can be explicitly computed and inverted since the number of bias parameters per layer is small."}, {"heading": "4.1 FACTORED TIKHONOV DAMPING FOR THE DOUBLE-FACTORED KRONECKER APPROXIMATION", "text": "In second-order optimization methods, \u201cdamping\u201d performs the crucial task of correcting for the inaccuracies of the local quadratic approximation of the objective that is (perhaps implicitly) optimized when computing the update (Martens and Sutskever, 2012; Martens, 2014, e.g.). In the well-known Tikhonov damping/regularization approach, one adds a multiple of the identity \u03bbI to the Fisher before inverting it (as one also does for L2-regularization / weight-decay), which roughly corresponds to imposing a spherical trust-region on the update.\nThe inverse of a Kronecker product can be computed efficiently as the Kronecker product of the inverse of its factors. Adding a multiple of the identity complicates this computation (although it can still be performed tractably using eigendecompositions). The \u201cfactored Tikhonov damping\u201d technique proposed in (Martens and Grosse, 2015) is appealing because it preserves the Kronecker structure of the factorization and thus the inverse can still be computed by inverting each of the smaller matrices (and avoiding the more expensive eigendecomposition operation). And in our experiments with large ImageNet models, we also observe the factored damping seems to perform better in practice. In this subsection we derive a generalized version of factored Tikhonov damping for the double-factored Kronecker approximation.\nSuppose we wish to add \u03bbI to our approximate Fisher block A\u2297B \u2297 C. In the factored Tikhonov scheme this is approximated by adding \u03c0a\u03bb 1 3 I , \u03c0b\u03bb 1 3 I , and \u03c0c\u03bb 1 3 I to A, B and C respectively, for non-negative scalars \u03c0a, \u03c0b and \u03c0c satisfying \u03c0a\u03c0b\u03c0c = 1. The error associated with this approximation is:\n(A+ \u03c0a\u03bb 1 3 I)\u2297 (B + \u03c0b\u03bb 1 3 I)\u2297 (C + \u03c0c\u03bb 1 3 I)\u2212 (A\u2297B \u2297 C + \u03bbI) (10)\n=\u03c0c\u03bb 1 3 I \u2297A\u2297B + \u03c0b\u03bb 1 3 I \u2297A\u2297 C + \u03c0a\u03bb 1 3 I \u2297B \u2297 C\n+ \u03c0c\u03bb i 3 I \u2297 \u03c0b\u03bb 1 3 I \u2297A+ \u03c0c\u03bb 1 3 I \u2297 \u03c0a\u03bb 1 3 I \u2297B + \u03c0a\u03bb 1 3 I \u2297 \u03c0b\u03bb 1 3 I \u2297 C (11)\nFollowing Martens and Grosse (2015), we choose \u03c0a, \u03c0b and \u03c0c by taking the nuclear norm in Eq. 11 and minimizing its triangle inequality-derived upper-bound. Note that the nuclear norm of Kronecker products is the product of the nuclear norms of each individual matrices: \u2016A \u2297 B\u2016\u2217 = \u2016A\u2016\u2217\u2016B\u2016\u2217. This gives the following formula for the value of \u03c0a\n\u03c0a = 3 \u221a( \u2016A\u2016\u2217 dA )2(\u2016B\u2016\u2217 dB \u2016C\u2016\u2217 dC )\u22121 . (12)\nwhere the d\u2019s are the number of rows (equiv. columns) of the corresponding Kronecker factor matrices. The corresponding formulae for \u03c0b and \u03c0c are analogous. Intuitively, the Eq. 12 rescales the contribution to each factor matrix according to the geometric mean of the ratio of its norm vs the norms of the other factor matrices. This results in the contribution being upscaled if the factor\u2019s norm is larger than averaged norm, for example. Note that this formula generalizes to Kronecker products of arbitrary numbers of matrices as the geometric mean of the norm ratios."}, {"heading": "5 STEP SIZE SELECTION", "text": "Although Grosse and Martens (2016) found that Polyak averaging (Polyak and Juditsky, 1992) obviated the need for tuning learning rate schedules on some problems, we observed the choice of learning rate schedules to be an important factor in our ImageNet experiments (perhaps due to higher stochasticity in the updates). On ImageNet, it is common to use a fixed exponential decay schedule (Szegedy et al., 2014; 2015). As an alternative to learning rate schedules, we instead use curvature information to control the amount by which the predictive distribution is allowed to change after each update. In particular, given a parameter update vector v, the second-order Taylor approximation to the KL divergence between the predictive distributions before and after the update is given by the (squared) Fisher norm:\nDKL[q||p] \u2248 1\n2 v>Fv (13)\nThis quantity can be computed with a curvature-vector product (Schraudolph, 2002). Observe that choosing a step size of \u03b7 will produce an update with squared Fisher norm \u03b72 v>Fv. Instead of using a learning rate schedule, we choose \u03b7 in each iteration such that the squared Fisher norm is at most some value c:\n\u03b7 = min ( \u03b7max, \u221a c\nv>Fv\n) (14)\nGrosse and Martens (2016) used this method to clip updates at the start of training, but we found it useful to use it throughout training. We use an exponential decay schedule ck = c0\u03b6k, where c0 and \u03b6 are tunable parameters, and k is incremented periodically (every half an epoch in our ImageNet experiments). Shrinking the maximum changes in the model prediction after each update is analogous to shrinking the trust region of the second-order optimization. In practice, computing curvature-vector products after every update introduces significant computational overhead, so we instead used the approximate Fisher F\u0302 in place of F , which allows the approximate Fisher norm to be computed efficiently as v>F\u0302v = v>F\u0302 (F\u0302\u22121GW ) = v>GW . The maximum step size \u03b7max was set to a large value, and in practice this maximum was reached only at the beginning of training, when F was small in magnitude. We found this outperformed simple exponential learning rate decay on ImageNet experiments (see Appendix B)."}, {"heading": "6 EXPERIMENTS", "text": "We experimentally evaluated distributed K-FAC on several large convolutional neural network training tasks involving the CIFAR-10 and ImageNet classification datasets.\nDue to computational resource constraints, we used a single GPU server with 8 Nvidia K80 GPUs to simulate a large distributed system. The GPUs were used as gradient workers that computed the gradient over a large mini-batch, with the CPUs acting as a parameter server. The Fisher block inversions were performed on the CPUs in parallel, using as many threads as possible. The secondorder statistics required for the various Fisher block approximations were computed either syncronously by the gradient workers after each gradient computation (CIFAR-10 experiments), or asynchronously using a separate dedicated \u201cstats worker\u201d (ImageNet experiments).\nMeta-parameters such as learning rates, damping parameters, and the decay-rate for the secondorder statistics, were optimized carefully by hand for each method. The momentum was fixed to 0.9.\nSimilarly to Martens and Grosse (2015), we applied an exponentially decayed Polyak averaging scheme to the sequence of output iterates produced by each method. We found this improved their convergence rate in the later stages of optimization, and reduced or eliminated the need to decay the learning rates.\nWe chose to base our implementation of distributed K-FAC on the TensorFlow framework (Abadi et al., 2016) because it provides well-engineered and scalable primitives for distributed computation. We implement distributed K-FAC in TensorFlow by scanning the gradient-computing graph for groups of parameters whose gradient computations have particular structures. Having identified such groups we compute/approximate their Fisher blocks using a method tailored to the type of structure\nobserved. See Appendix C for details. This type of implementation can be applied to existing modelspecification code without significant modification of said code. And because TensorFlow\u2019s parallel primitives were designed with scalability in mind, it should be possible to scale our implementation to a larger distributed system with hundreds of workers."}, {"heading": "6.1 CIFAR-10 CLASSIFICATION AND ASYNCHRONOUS FISHER BLOCK INVERSION", "text": "In our first experiment we evaluated the effectiveness of asynchronously computing the approximate Fisher inverses (as described in Section 3.1). We considered the effect that this has both on the quality of the updates, as measured by per-iteration progress on the objective, and on the average per-iteration wall-clock time.\nThe task is to train a basic convolutional network model on the CIFAR-10 image classification dataset (Krizhevsky and Hinton, 2009). The model has 3 convolutional layers of 32-32-64 filters, each with a receptive field size of 5x5, followed by a softmax layer that predicts 10 classes. This is a similar but not identical CIFAR-10 model that was used by Grosse and Martens (2016). All the CIFAR-10 experiments use a mini-batch size of 512.\nThe baseline method is a simple synchronous version of distributed K-FAC with a fixed learning rate, and up to 4 GPUs acting as gradient and stats workers, which recomputes the inverses of the approximate Fisher blocks once every 20 iterations. This baseline method behaves similarly to the implementation of K-FAC in Grosse and Martens (2016), while being potentially faster due to its greater use of parallelism. We compare this baseline to a version of distributed K-FAC where the approximate Fisher blocks are inverted asynchronously and in parallel with the rest of the optimization process. Note that under this scheme, inverses are updated about once every 16 iterations for the single GPU condition, and every 30 iterations for the four GPU condition. For networks larger than this relatively small CIFAR-10 net they may get updated (far) less often (e.g. the AlexNet experiments in Section 6.2.2).\nThe results of this first experiment are plotted in Fig. 2. We found that the asynchronous version iterated about 1.5 times faster than the synchronous version, while its per-iteration progress remained comparable. The plots show that the asynchronous version is better at taking advantage of parallel computation and displayed an almost linear speed-up as the number of gradient workers increases to 4. In terms of the wall-clock time, using only 4 GPUs the asynchronous version of distributed K-FAC is able to complete 700 iterations in under a minute, where it achieves the minimum test error (19%)."}, {"heading": "6.2 IMAGENET CLASSIFICATION", "text": "In our second set of experiments we benchmarked distributed K-FAC against several other popular approaches, and considered the effect of mini-batch size on per-iteration progress. To do this we trained various off-the-shelf convnet architectures for image classification on the ImageNet dataset\n(Russakovsky et al., 2015): AlexNet (Krizhevsky et al., 2012), GoogLeNet InceptionV1 (Szegedy et al., 2014) and the 50-layer Residual network (He et al., 2015).\nDespite having 1.2 million images in the ImageNet training set, a data pre-processing pipeline is almost always used for training ImageNet that includes image jittering and aspect distortion. We used a less extensive dataset augmentation/pre-processing pipeline than is typically used for ImageNet, as the purpose of this paper is not to achieve state-of-the-art ImageNet results, but rather to evaluate the optimization performance of distributed K-FAC. In particular, the dataset consists of 224x224 images and during training the original images are first resized to 256x256 and then randomly cropped back down to 224x224 before being fed to the network. Note that while it is typically the case that validation error is higher than training error, this data pre-processing pipeline for ImageNet creates an augmented training set that is more difficult than the undistorted validation set and therefore the validation error is often lower than the training error during the first 90% of training. This observation is consistent with previously published results (He et al., 2015).\nIn all our ImageNet experiments, we used the cheaper Kronecker factorization from Appendix A, and the KL-based step sized selection method described in Section 5 with parameters c0 = 0.01 and \u03b6 = 0.96. The SGD baselines use an exponential learning rate decay schedule with a decay rate of 0.96. Decaying is applied after each half-epoch for distributed K-FAC and SGD+Batch Normalization, and after every two epochs for plain SGD, which is consistent with the experimental setup of Ioffe and Szegedy (2015)."}, {"heading": "6.2.1 GOOGLELENET AND BATCH NORMALIZATION", "text": "Batch Normalization (Ioffe and Szegedy, 2015) is a reparameterization of neural networks that can make them easier to train with first-order methods, and has been successfully applied to large ImageNet models. It can be thought of as a modification of the units of a neural network so that each one centers and normalizes its own raw input over the current mini-batch (or subset thereof), after which it applies a separate shift and scaling operation via its own local \u201cbias\u201d and \u201cgain\u201d parameters (which are optimized). These shift and scaling operations can learn to effectively undo the centering and normalization, thus preserving the class of functions that the network can compute. Batch Normalization (BN) is closely related to centering techniques (Schraudolph, 1998), and likely helps for the same reason that they do, which is that the alternative parameterization gives rise to loss surfaces with more favorable curvature properties. The main difference between BN and traditional centering is that BN makes the centering and normalization operations part of the model instead of the optimization algorithm (and thus \u201cbackprops\u201d through them when computing the gradient), which helps stabilize the optimization.\nWithout any changes to the algorithm, distributed K-FAC can be used to train neural networks that have BN layers. The weight-matrix gradient for such layers has the same structure as it does for standard layers, and so Fisher blocks can be approximated using the same set of techniques. The\nper-unit gain and bias parameters cause a minor complication, but because they are relatively few in number, one can compute an exact Fisher block for each of them.\nComputing updates for BN networks over large mini-batches is usually done by splitting the minibatch into chunks of size 32, computing the gradients separately for these chunks (using only the data in the chunk to compute the mean and variance statistics), and then summing them together. Using small sample sets to compute the statistics like this introduces additional stochasticity into the BN update that acts as a regularizer, but can also hurt optimization performance. To help decouple the effect of regularization and optimization, we also compared to a BN baseline that uses larger chunks. We found using larger chunks can give a factor of 2 speed-up in optimization performance over the standard BN baseline. In our figures rbz will indicate the chunk size, which defaults 32 if left unspecified.\nIn Fig. 3, we compare distributed K-FAC to SGD on GoogLeNet with and without BN. All methods used 4 GPUs, with distributed K-FAC using the 4-th GPU as a dedicated asynchronous stats worker.\nWe observe that the per-iteration progress made by distributed K-FAC on the training objective is not significantly affected by the use of BN. Moreover, distributed K-FAC is 3.5 times faster than SGD with standard BN baseline (orange line) and 1.5-2 times faster than the enhanced BN baseline (blue line). BN, however, does help distributed K-FAC generalize better, likely due to its aforementioned regularizing effect.\nFor the simplicity of our discussion, distributed K-FAC is not combined with BN in the the rest of the experiments, as we are chiefly interested in evaluating optimization performance, not regularization, and BN doesn\u2019t seem to provide any additional benefit to distributed K-FAC in regards to the former. Note that this is not too surprising, given that K-FAC is provably invariant to the kind of centering and normalization transformations that BN does (Martens and Grosse, 2015)."}, {"heading": "6.2.2 ALEXNET AND THE DOUBLY-FACTORED KRONECKER APPROXIMATION", "text": "To demonstrate that distributed K-FAC can efficiently optimize models with very wide layers we train AlexNet using distributed K-FAC and compare to SGD+BN. The doubly-factored Kronecker approximation proposed in Section 4 is applied to the first fully-connected layer of AlexNet, which has 9216 input units and is thus too wide for the standard Kronecker approximation to be feasible. Note that even with this addtional approximation, computing all of the Fisher block inverses for AlexNet is very expensive, and in our experiments they only get updated once every few hundred iterations by our 16 core Xeon 2.2Ghz CPU.\nThe results from this experiment are plotted in Fig. 4. They show that Distributed K-FAC still works well despite potentially extreme staleness of the Fisher block inverses, speeding up training by a factor of 1.5 over the improved SGD-BN baseline."}, {"heading": "6.2.3 VERY DEEP ARCHITECTURES (RESNETS)", "text": "In recent years very deep convolutional architectures have been successfully applied to ImageNet classification. These networks are particularly challenging to train because the usual difficulties associated with deep learning are especially severe. Fortunately second-order optimization is perhaps ideally suited to addressing these difficulties in a robust and principled way (Martens, 2010).\nTo investigate whether distributed K-FAC can scale to such architectures and provide useful acceleration, we compared it to SGD+BN using the 50 layer ResNet architecture (He et al., 2015). The results from this experiment are plotted in Fig. 5. They show that distributed K-FAC provides significant speed-up during the early stages of training compared to SGD+BN."}, {"heading": "6.2.4 MINI-BATCH SIZE SCALING PROPERTIES", "text": "In our final experiment we explored how well distributed K-FAC scales as additional parallel computing resources become available. To do this we trained GoogLeNet with varying mini-batch sizes of {256, 1024, 2048}, and measured per-training-case progress. Ideally, if extra gradient data is being used efficiently, one should expect the per-training-case progress to remain relatively constant with respect to mini-batch size. The results from this experiment are plotted in Fig. 6, and show that distributed K-FAC exhibits something close to this ideal behavior, while SGD+BN rapidly loses data efficiency when moving beyond a mini-batch size of 256. These results suggest that distributed K-FAC, more so than the SGD+BN baseline, is capable of speeding up training in proportion to the amount of parallel computational resources used."}, {"heading": "7 DISCUSSION", "text": "We have introduced distributed K-FAC, an asynchronous distributed second-order optimization algorithm which computes Kronecker-factored Fisher approximations and stochastic gradients over larger mini-batches asynchronously and in parallel.\nOur experiments show that the extra overhead introduced by distributed K-FAC is mostly mitigated by the use of parallel asynchronous computation, resulting in updates that can be computed in a similar amount of time to those of distributed SGD, while making much more progress on the objective function per iteration. We showed that in practice this can lead to speedups of roughly 3.5x compared to standard SGD + Batch Normalization (BN), and 2x compared to SGD + an improved version of BN on large-scale convolutional network training tasks.\nWe also proposed a doubly-factored Kronecker approximation that allows distributed K-FAC to scale up to large models with hundreds of millions of parameters, and demonstrated the effectiveness of this approach in experiments.\nFinally, we showed that distributed K-FAC enjoys a favorable scaling property with mini-batch size that is seemingly not shared by SGD+BN. In particular, we showed that per-iteration progress tends to be proportional to the mini-batch size up to a much larger threshold than for SGD+BN. This suggests that it will yield even further reductions in total wall-clock training time when implemented in a larger distributed system than the one we considered."}, {"heading": "A A CHEAPER KRONECKER FACTOR APPROXIMATION FOR CONVOLUTION LAYERS", "text": "In a convolution layer, the gradient is the sum of the outer product between the receptive field input activation At and the back-propagated derivatives Dst at each spatial location t \u2208 T . One cannot simply apply the standard Kronecker factored approximation from Martens and Grosse (2015) to each location, sum the results, and then take the inverse, as there is no known efficient algorithm for computing the inverse of such a sum.\nIn Grosse and Martens (2016), a Kronecker-factored approximation for convolutional layers called Kronecker Factors for Convolution (KFC) was developed. It works by introducing additional statistical assumptions about how the weight gradients are related across locations. In particular, KFC assumes spatial homogeneity, i.e. that all locations have the same statistics, and spatially uncorrelated derivatives, which (essentially) means that gradients from any two different locations are statistically independent. This yields the following approximation:\nE[vec{DW} vec{DW}>] \u2248 |T |E [ AtA>t ] \u2297 E [ DstDs>t ] . (15)\nIn this section we introduce an arguably simpler Kronecker factored approximation for convolutional layers that is cheaper to compute. In practice, it appears to be competitive with the original KFC approximation in terms of per-iteration progress on the objective, working worse in some experiments and better in others, while (often) improving wall-clock time due to its cheaper cost.\nIt works by approximating the sum of the gradients over spatial locations as the outer product of the averaged receptive field activations over locations Et[At], and the averaged back-propagated derivatives Et[Dst], multipled by the number of spatial locations |T |. In other words:\nE[vec{DW} vec{DW}>] = E [ vec{\n\u2211 t\u2208T DstA>t } vec{ \u2211 t\u2208T DstA>t }>\n] (16)\n=E (\u2211 t\u2208T At \u2297Dst )(\u2211 t\u2208T At \u2297Dst )> (17) \u2248E [( |T |E\nt [At]\u2297 E t [Dst]\n)( |T |E\nt [At]\u2297 E t [Dst]\n)>] (18)\nUnder the approximation assumption that the second-order statistics of the average activations, Et[At], and the second-order statistics of the average derivatives, Et[Dst], are uncorrelated, this becomes:\n|T |2 E [ E t [At]E t [At]> ] \u2297 E [ E t [Dst]E t [Dst]> ] (19)\nThis approximation is cheaper than the original KFC approximation because it is easier to compute a single outer product (after averaging over locations) than it is to compute an outer product at each location and then average. In the synchronous setting, for the large convolutional networks we experimented with, this trick resulted in a 20-30% decrease in overall wall clock time per iteration, with little effect on per-iteration progress."}, {"heading": "B EXPERIMENTAL EVALUATION OF THE STEP-SIZE SELECTION METHOD OF", "text": "SECTION 5\nTo compare our proposed step size selection from Sec. 5 with the commonly-used exponential learning rate decay, we performed a simple experiment training GoogLeNet. Both the learning rate and threshold c on the square Fisher norm, is decayed by a factor of 0.96 after every 3200 iterations. The results of this experiment are plotted in Fig. 8, and indicate that our method outperforms the standard baseline."}, {"heading": "C AUTOMATIC CONSTRUCTION OF THE K-FAC COMPUTATION GRAPH", "text": "In recent years, deep learning libraries have moved towards the computational graph abstraction (Bergstra et al., 2010; Abadi et al., 2016) to represent neural network computations. In this section we give a high level description of an algorithm that scans a computational graph for parameters for which one of the various Kronecker-factored approximations can be applied, locates nodes containing the required information to compute the second-order statistics required by the approximations, and then constructs a new graph that computes the approximations and uses them to update the parameters.\nFor the sake of discussion, we will assume the computation graph is a directed bipartite graph that has a set of operator nodes doing some computation, and some variable nodes that holds intermediate computational results. The trainable parameters are stored in the memory that is loaded or mutated through read/write operator nodes. We also assume that the trainable parameters are grouped layer-wise as a set of weights and biases. Finally, we assume the gradient computation for the trainable parameters is performed by a computation graph (which is usually is generated via automatic differentiation).\nIn analogy to generating the gradient computation graph through automatic differentiation, given an arbitrary computation graph with a set of the trainable parameters, we would like to use the existing nodes in the given graph to automatically generate a new computation graph, a \u201cK-FAC computation graph\u201d, that computes the Kronecker-factored approximate Fisher blocks associated with each group of parameters (typically layers in a neural net), and then uses them to update the parameters.\nTo compute the Fisher block for a given layer, we want to find all the nodes holding the gradients of the trainable parameters in a computation graph. One simple strategy is to traverse the computation graph from the gradient nodes to their immediate parent nodes.\nA set of parameters has a Kronecker-factored approximation to its Fisher block if its corresponding gradient node has a matrix product or convolution operator node as its immediate parent node. For these parameters, the Kronecker factor matrices are the second-order statistics of the inputs to the parent operator node of their gradient nodes (typically the activities A and back-propagated derivatives Ds). For other sets of parameters an exact Fisher block can be computed instead (assuming they have low enough dimension).\nIn a typical neural network, most of the parameters are concentrated in weight matrices, that are used for matrix product or convolution operations, for which one of the existing Kronecker-factored approximations applies. Homogeneous coordinates can be used if the weights and biases of the same layer are annotated in the computation graph. The rest of the parameters are often gain and bias vectors for each hidden unit, and it is feasible to compute and invert exact Fisher blocks for these.\nKronecker factors can sometimes be shared by approximate Fisher blocks for two or more parameters. This is the case, for example, when a vector of units serves as inputs to two different weightmatrix multiplication operations. In such cases, the computation of the second-order statistics can be reused, which is what we do in our implementation.\nA neural network can be also instantiated multiple times in a computational graph (with shared parameters) to process different inputs. The gradient of the parameters shared across the instantiations are the sum of the individual gradients from each instantiation. Given such computation graph, the immediate parent operator node from the gradient is a summation whose inputs are computed by the same type of operators. Without additional knowledge about the computation graph, one approximation is to treat the individual gradient contributions in the summation as statistically independent of each other (similarly to how gradient contributions from multiple spatial locations are treated as independent in the KFC approximation (Grosse and Martens, 2016)). Under this approximation, the Kronecker factors associated with the gradient can be computed by lumping the statistics associated with each of the gradient contributions together.\nOur implementation of Distributed K-FAC in TensorFlow applies the above the strategy to automatically generate K-FAC computation graphs without requiring the user to modify their existing model-definition code."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Theano: A cpu and gpu math compiler in python", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proc. 9th Python in Science Conf,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "A stochastic quasi-newton method for largescale optimization", "author": ["Richard H Byrd", "SL Hansen", "Jorge Nocedal", "Yoram Singer"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Byrd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2016}, {"title": "Hessian-free optimization for learning deep multidimensional recurrent neural networks", "author": ["Minhyung Cho", "Chandra Dhir", "Jaehyung Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A self-correcting variable-metric algorithm for stochastic optimization", "author": ["Frank Curtis"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Curtis.,? \\Q2016\\E", "shortCiteRegEx": "Curtis.", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Natural neural networks", "author": ["Guillaume Desjardins", "Karen Simonyan", "Razvan Pascanu", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Desjardins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A kronecker-factored approximate fisher matrix for convolution layers", "author": ["Roger Grosse", "James Martens"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "Grosse and Martens.,? \\Q2016\\E", "shortCiteRegEx": "Grosse and Martens.", "year": 2016}, {"title": "Scaling up natural gradient by factorizing fisher information", "author": ["Roger Grosse", "Ruslan Salakhutdinov"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Grosse and Salakhutdinov.,? \\Q2015\\E", "shortCiteRegEx": "Grosse and Salakhutdinov.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Large scale distributed hessian-free optimization for deep neural network", "author": ["Xi He", "Dheevatsa Mudigere", "Mikhail Smelyanskiy", "Martin Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1606.00511,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On \u201cnatural\u201d learning and pruning in multilayered perceptrons", "author": ["Tom Heskes"], "venue": "Neural Computation,", "citeRegEx": "Heskes.,? \\Q2000\\E", "shortCiteRegEx": "Heskes.", "year": 2000}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "adaqn: An adaptive quasi-newton algorithm for training rnns", "author": ["Nitish Shirish Keskar", "Albert S Berahas"], "venue": "arXiv preprint arXiv:1511.01169,", "citeRegEx": "Keskar and Berahas.,? \\Q2015\\E", "shortCiteRegEx": "Keskar and Berahas.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Training neural networks with stochastic hessian-free optimization", "author": ["Ryan Kiros"], "venue": "arXiv preprint arXiv:1301.3641,", "citeRegEx": "Kiros.,? \\Q2013\\E", "shortCiteRegEx": "Kiros.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "University of Toronto,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas Le Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Deep learning via Hessian-free optimization", "author": ["James Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "New insights and perspectives on the natural gradient method", "author": ["James Martens"], "venue": "arXiv preprint arXiv:1412.1193,", "citeRegEx": "Martens.,? \\Q2014\\E", "shortCiteRegEx": "Martens.", "year": 2014}, {"title": "Optimizing neural networks with kronecker-factored approximate curvature", "author": ["James Martens", "Roger Grosse"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Training deep and recurrent networks with Hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "A linearly-convergent stochastic L-BFGS algorithm", "author": ["Philipp Moritz", "Robert Nishihara", "Michael Jordan"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Moritz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moritz et al\\.", "year": 2016}, {"title": "Riemannian metrics for neural networks i: feedforward networks", "author": ["Yann Ollivier"], "venue": "arXiv preprint arXiv:1303.0818,", "citeRegEx": "Ollivier.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier.", "year": 2013}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak and Juditsky.,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky.", "year": 1992}, {"title": "Parallel training of DNNs with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "In International Conference on Learning Representations: Workshop track,", "citeRegEx": "Povey et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Centering neural network gradient factors", "author": ["Nicol N. Schraudolph"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Schraudolph.,? \\Q1998\\E", "shortCiteRegEx": "Schraudolph.", "year": 1998}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N. Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph.", "year": 2002}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Nicol N Schraudolph", "Jin Yu", "Simon G\u00fcnter"], "venue": "In AISTATS,", "citeRegEx": "Schraudolph et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schraudolph et al\\.", "year": 2007}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Krylov subspace descent for deep learning", "author": ["Oriol Vinyals", "Daniel Povey"], "venue": "In AISTATS,", "citeRegEx": "Vinyals and Povey.,? \\Q2012\\E", "shortCiteRegEx": "Vinyals and Povey.", "year": 2012}, {"title": "Stochastic quasi-newton methods for nonconvex stochastic optimization", "author": ["Xiao Wang", "Shiqian Ma", "Wei Liu"], "venue": "arXiv preprint arXiv:1412.1196,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "2016), a Kronecker-factored approximation for convolutional layers called Kronecker Factors for Convolution (KFC) was developed. It works by introducing additional statistical assumptions about how the weight gradients are related across locations", "author": ["Grosse", "Martens"], "venue": "KFC assumes spatial homogeneity,", "citeRegEx": "Grosse and Martens,? \\Q2016\\E", "shortCiteRegEx": "Grosse and Martens", "year": 2016}, {"title": "CONSTRUCTION OF THE K-FAC COMPUTATION GRAPH In recent years, deep learning libraries have moved towards the computational graph abstraction (Bergstra et al., 2010; Abadi et al., 2016) to represent neural network computations. In this section we give a high level description of an algorithm that scans a computational graph for parameters for which one of the various Kronecker-factored approximations can be applied, locates nodes", "author": ["C AUTOMATIC"], "venue": null, "citeRegEx": "AUTOMATIC,? \\Q2016\\E", "shortCiteRegEx": "AUTOMATIC", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "The recently proposed K-FAC method (Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.", "startOffset": 35, "endOffset": 61}, {"referenceID": 15, "context": "Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to an improved form of Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 200, "endOffset": 225}, {"referenceID": 35, "context": "Current state-of-the-art deep neural networks (Szegedy et al., 2014; Krizhevsky et al., 2012; He et al., 2015) often require days of training time with millions of training cases.", "startOffset": 46, "endOffset": 110}, {"referenceID": 20, "context": "Current state-of-the-art deep neural networks (Szegedy et al., 2014; Krizhevsky et al., 2012; He et al., 2015) often require days of training time with millions of training cases.", "startOffset": 46, "endOffset": 110}, {"referenceID": 12, "context": "Current state-of-the-art deep neural networks (Szegedy et al., 2014; Krizhevsky et al., 2012; He et al., 2015) often require days of training time with millions of training cases.", "startOffset": 46, "endOffset": 110}, {"referenceID": 7, "context": "The typical strategy to speed-up neural network training is to allocate more parallel resources over many machines and cluster nodes (Dean et al., 2012).", "startOffset": 133, "endOffset": 152}, {"referenceID": 22, "context": "Various approximations to the curvature matrix have been proposed to help alleviate this problem, such as diagonal (LeCun et al., 1998; Duchi et al., 2011; Kingma and Ba, 2014), block diagonal Le Roux et al.", "startOffset": 115, "endOffset": 176}, {"referenceID": 9, "context": "Various approximations to the curvature matrix have been proposed to help alleviate this problem, such as diagonal (LeCun et al., 1998; Duchi et al., 2011; Kingma and Ba, 2014), block diagonal Le Roux et al.", "startOffset": 115, "endOffset": 176}, {"referenceID": 17, "context": "Various approximations to the curvature matrix have been proposed to help alleviate this problem, such as diagonal (LeCun et al., 1998; Duchi et al., 2011; Kingma and Ba, 2014), block diagonal Le Roux et al.", "startOffset": 115, "endOffset": 176}, {"referenceID": 23, "context": "strategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).", "startOffset": 129, "endOffset": 217}, {"referenceID": 37, "context": "strategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).", "startOffset": 129, "endOffset": 217}, {"referenceID": 18, "context": "strategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).", "startOffset": 129, "endOffset": 217}, {"referenceID": 5, "context": "strategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).", "startOffset": 129, "endOffset": 217}, {"referenceID": 13, "context": "strategy is to use Krylov-subspace methods and efficient matrix-vector product algorthms to avoid the inversion problem entirely (Martens, 2010; Vinyals and Povey, 2012; Kiros, 2013; Cho et al., 2015; He et al., 2016).", "startOffset": 129, "endOffset": 217}, {"referenceID": 14, "context": "More recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015).", "startOffset": 126, "endOffset": 233}, {"referenceID": 28, "context": "More recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015).", "startOffset": 126, "endOffset": 233}, {"referenceID": 11, "context": "More recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015).", "startOffset": 126, "endOffset": 233}, {"referenceID": 30, "context": "More recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015).", "startOffset": 126, "endOffset": 233}, {"referenceID": 8, "context": "More recently, several approximations have been proposed based on statistical approximations of the Fisher information matrix (Heskes, 2000; Ollivier, 2013; Grosse and Salakhutdinov, 2015; Povey et al., 2015; Desjardins et al., 2015).", "startOffset": 126, "endOffset": 233}, {"referenceID": 25, "context": "In the K-FAC approach (Martens and Grosse, 2015; Grosse and Martens, 2016), these approximations result in a block-diagonal approximation to the Fisher information matrix (with blocks corresponding to entire layers) where each block is approximated as a Kronecker product of two much smaller matrices, both of which can be estimated and inverted fairly efficiently.", "startOffset": 22, "endOffset": 74}, {"referenceID": 10, "context": "In the K-FAC approach (Martens and Grosse, 2015; Grosse and Martens, 2016), these approximations result in a block-diagonal approximation to the Fisher information matrix (with blocks corresponding to entire layers) where each block is approximated as a Kronecker product of two much smaller matrices, both of which can be estimated and inverted fairly efficiently.", "startOffset": 22, "endOffset": 74}, {"referenceID": 15, "context": "Finally, we empirically demonstrate that the proposed method speeds up learning of various state-of-the-art ImageNet models by a factor of two over Batch Normalization (Ioffe and Szegedy, 2015).", "startOffset": 168, "endOffset": 193}, {"referenceID": 25, "context": "K-FAC (Martens and Grosse, 2015; Grosse and Martens, 2016) uses a Kronecker-factored approximation to each block which we now describe.", "startOffset": 6, "endOffset": 58}, {"referenceID": 10, "context": "K-FAC (Martens and Grosse, 2015; Grosse and Martens, 2016) uses a Kronecker-factored approximation to each block which we now describe.", "startOffset": 6, "endOffset": 58}, {"referenceID": 1, "context": "The natural gradient (Amari, 1998) is defined as the inverse of the Fisher times the gradient.", "startOffset": 21, "endOffset": 34}, {"referenceID": 24, "context": "Under certain conditions, which almost always hold in practice, it can also be interpreted as a second-order update computed by minimizing a local quadratic approximation of the log-likelihood objective, where the Hessian is approximated using the Fisher (Martens, 2014).", "startOffset": 255, "endOffset": 270}, {"referenceID": 25, "context": "Prior work suggests that approximate natural gradient algorithms might benefit more than standard SGD from reducing the variance (Martens and Grosse, 2015; Grosse and Martens, 2016).", "startOffset": 129, "endOffset": 181}, {"referenceID": 10, "context": "Prior work suggests that approximate natural gradient algorithms might benefit more than standard SGD from reducing the variance (Martens and Grosse, 2015; Grosse and Martens, 2016).", "startOffset": 129, "endOffset": 181}, {"referenceID": 25, "context": "It has been observed that refreshing the inverse of the Fisher blocks only occasionally and using stale values otherwise has only a small detrimental effect on average per-iteration progress, perhaps because the curvature changes relatively slowly (Martens and Grosse, 2015).", "startOffset": 248, "endOffset": 274}, {"referenceID": 25, "context": "The \u201cfactored Tikhonov damping\u201d technique proposed in (Martens and Grosse, 2015) is appealing because it preserves the Kronecker structure of the factorization and thus the inverse can still be computed by inverting each of the smaller matrices (and avoiding the more expensive eigendecomposition operation).", "startOffset": 54, "endOffset": 80}, {"referenceID": 29, "context": "Although Grosse and Martens (2016) found that Polyak averaging (Polyak and Juditsky, 1992) obviated the need for tuning learning rate schedules on some problems, we observed the choice of learning rate schedules to be an important factor in our ImageNet experiments (perhaps due to higher stochasticity in the updates).", "startOffset": 63, "endOffset": 90}, {"referenceID": 35, "context": "On ImageNet, it is common to use a fixed exponential decay schedule (Szegedy et al., 2014; 2015).", "startOffset": 68, "endOffset": 96}, {"referenceID": 33, "context": "This quantity can be computed with a curvature-vector product (Schraudolph, 2002).", "startOffset": 62, "endOffset": 81}, {"referenceID": 0, "context": "We chose to base our implementation of distributed K-FAC on the TensorFlow framework (Abadi et al., 2016) because it provides well-engineered and scalable primitives for distributed computation.", "startOffset": 85, "endOffset": 105}, {"referenceID": 19, "context": "The task is to train a basic convolutional network model on the CIFAR-10 image classification dataset (Krizhevsky and Hinton, 2009).", "startOffset": 102, "endOffset": 131}, {"referenceID": 20, "context": ", 2015): AlexNet (Krizhevsky et al., 2012), GoogLeNet InceptionV1 (Szegedy et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 35, "context": ", 2012), GoogLeNet InceptionV1 (Szegedy et al., 2014) and the 50-layer Residual network (He et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 12, "context": ", 2014) and the 50-layer Residual network (He et al., 2015).", "startOffset": 42, "endOffset": 59}, {"referenceID": 12, "context": "This observation is consistent with previously published results (He et al., 2015).", "startOffset": 65, "endOffset": 82}, {"referenceID": 15, "context": "Batch Normalization (Ioffe and Szegedy, 2015) is a reparameterization of neural networks that can make them easier to train with first-order methods, and has been successfully applied to large ImageNet models.", "startOffset": 20, "endOffset": 45}, {"referenceID": 32, "context": "Batch Normalization (BN) is closely related to centering techniques (Schraudolph, 1998), and likely helps for the same reason that they do, which is that the alternative parameterization gives rise to loss surfaces with more favorable curvature properties.", "startOffset": 68, "endOffset": 87}, {"referenceID": 25, "context": "Note that this is not too surprising, given that K-FAC is provably invariant to the kind of centering and normalization transformations that BN does (Martens and Grosse, 2015).", "startOffset": 149, "endOffset": 175}, {"referenceID": 23, "context": "Fortunately second-order optimization is perhaps ideally suited to addressing these difficulties in a robust and principled way (Martens, 2010).", "startOffset": 128, "endOffset": 143}, {"referenceID": 12, "context": "To investigate whether distributed K-FAC can scale to such architectures and provide useful acceleration, we compared it to SGD+BN using the 50 layer ResNet architecture (He et al., 2015).", "startOffset": 170, "endOffset": 187}], "year": 2017, "abstractText": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase. Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle. Unfortunately, they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum. The recently proposed K-FAC method (Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead. In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of the method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification. Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to an improved form of Batch Normalization (Ioffe and Szegedy, 2015).", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_22"}