{"name": "ICLR_2017_238.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In many supervised learning problems, input data are high-dimensional and sparse. The high dimensionality may be inherent in the domain, such as a large vocabulary in a language model, or the result of creating hybrid conjunction features. This setting poses known statistical and computational challenges for standard supervised learning techniques, as high-dimensional inputs lead to models with a very large number of parameters.\nAn increasingly popular approach to reducing model size is to map inputs to a lower-dimensional space in a data-independent manner, using methods such as random projections, sketches, and hashing. These mappings typically attempt to preserve the hypothesis class, leading to inherent theoretical limitations on size. For example, for linearly separable unit vectors with margin \u03b3, it can be shown that at least \u2126(1/\u03b32) dimensions are needed to preserve linear separability, even if one can use arbitrary input embeddings (see Section D). It would therefore appear that data dimensionality cannot be reduced beyond this bound.\nIn this work, we show that using a slightly larger hypothesis class when decoding projections (improper learning) allows us to further reduce dimensionality while maintaining theoretical guarantees. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a very compact sketch by a single-layer neural network. The hidden layer allows us to \u201cdecode\u201d inputs from representations that are smaller than in existing work. In the simplest case, we show that for linearly separable k-sparse d-dimensional inputs, one can create a O(k log d\u03b4 )-dimensional sketch of the inputs and guarantee that a single-layer neural network can correctly classify 1\u2212\u03b4 fraction of the sketched data. In the case of polynomial functions, the required sketch size has a logarithmic dependence on the polynomial degree.\nFor binary k-sparse input vectors, we show that it suffices to have a simple feed-forward network with nonlinearity implemented via the commonly used rectified linear unit (Relu). We extend our results to real-valued data that is close to being k-sparse, using less conventional min and median nonlinearities. Furthermore, we show that data can be mapped using sparse sketching matrices. \u2217Email: kunal@google.com. Author for correspondences.\nThus, our sketches are efficient to compute and do not increase the number of non-zero input values by much, in contrast to standard dense Gaussian projections.\nWe empirically evaluate our sketches on real and synthetic datasets. Our approach leads to more compact neural networks than existing methods such as feature hashing and Gaussian random projections, at competitive or better performance. This makes our sketches appealing for deployment in settings with strict memory and power constraints, such as mobile and embedded devices."}, {"heading": "2 PREVIOUS WORK", "text": "To put our work in context, we next summarize some lines of research related to this work.\nRandom projections and sketching. Random Gaussian projections are by now a standard tool for dimensionality reduction. For general vectors, the Johnson-Lindenstrauss (1984) Lemma implies that a random Gaussian projection into O(log(1/\u03b4)/\u03b52) dimensions preserves the inner product between a pair of unit vectors up to an additive factor \u03b5, with probability 1\u2212\u03b4. A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b).\nProjections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels.\nCompressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cande\u0301s & Tao, 2006), imply that a k-sparse vector x \u2208 Rd can be reconstructed w.h.p. from a projection of dimension O(k ln dk ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs.\nParameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models. Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while others such as LeCun et al. (1989); Hassibi et al. (1993); Han et al. (2015) use techniques for pruning weights. HashedNets (Chen et al., 2015) enforce parameter sharing between random groups\nof network parameters. In contrast to these methods, sketching only involves applying a sparse, linear projection to the inputs, and does not require a specialized learning procedure or network architecture."}, {"heading": "3 SKETCHING", "text": "For simplicity, we first present our results for sparse binary vectors, and extend the discussion to real-valued vectors to Appendix A. Let Bd,k = {x \u2208 {0, 1}d : \u2016x\u20160 \u2264 k} be the set of ksparse d-dimensional binary vectors. We sketch such vectors using a family of randomized sketching algorithms based on the count-min sketch, as described next.\nGiven a parameter m and a hash function h : [d]\u2192 [m], the sketch Sh(x) of a vector x \u2208 Bd,k is a binary vector y where each bit of y is the OR of bits of x that hash to it:\nyl = \u2228\ni : h(i) = l\nxi .\nWe map data using a concatenation of several such sketches. Given an ordered set of hash functions\nh1, . . . , ht def = h1:t , the sketch Sh1:t(x) is defined as a m \u00d7 t matrix Y , where the jth column corresponds to the sketch Shj (x). We define the following procedure for decoding the i th bit of the input x from its sketch Y :\nDANDh1:t (Y, i) def = \u2227 j\u2208[t] Yhj(i)j . (1)\nThus, the decoded bit i is simply the AND of the t bits of Y that index i hashes to in the sketch.\nThe following theorem summarizes an important property of these sketches. As a reminder, a set of hash functions h1:t from [d] to [m] is pairwise independent if for all i 6= j \u2208 [d] and a, b \u2208 [m], Pr[h(i) = a \u2227 h(j) = b] = m\u22122.1\nTheorem 3.1. Let x \u2208 Bd,k and for j \u2208 [t] let hj : [d]\u2192 [m] be drawn uniformly and independently from a pairwise independent distribution with m = ek. Then for any i,\nPr[DANDh1:t (Sh1:t(x), i) 6= xi] \u2264 e \u2212t .\nProof. Fix a vector x \u2208 Bd,k . Let Eh(i) def = {i\u2032 6= i : h(i\u2032) = h(i)} denote the collision set of i for a particular hash function h. Decoding will fail if xi = 0 and for each of the t hash functions, the collision set of i for contains an index of a non-zero bit of x. For a particular h, the probability of this event is:\nPr  \u2228 i\u2032\u2208Eh(i) xi\u2032 = 1  \u2264 \u2211 i\u2032:xi\u2032=1 Pr[h(i\u2032) = h(i)] \u2264 k m = 1 e ,\nwhere the second inequality follows since the sum is over at most k terms, and each term is m\u22121 by pairwise independence. Thus Pr[Yhj(i) 6= xi] \u2264 e\u22121 for any j \u2208 [t]. Since hash functions hj are drawn independently, and decoding can fail only if all t hash functions fail, it follows that Pr[DANDh1:t (Sh1:t(x), i) 6= xi] \u2264 e \u2212t.\nLetHd,s denote the setHd,s = {w \u2208 Rd : \u2016w\u20160 \u2264 s}. We have the following corollary: Corollary 3.2. Let w \u2208 Hd,s and x \u2208 Bd,k. For t = log(s/\u03b4), and m = ek, if h1, . . . , ht are drawn uniformly and independently from a pairwise independent distribution, then\nPr [\u2211 i wiD AND h1:t (Sh1:t(x), i) 6= w >x ] \u2264 \u03b4 .\n1Such hash families can be easily constructed (see e.g. Mitzenmacher & Upfal (2005)), using a O(logm)bit seed. Moreover, each hash can be evaluated using O(1) arithmetic operations over O(log d)-sized words.\n4 SPARSE LINEAR FUNCTIONS\nx\nY=Sk(x)\nx24 x29\nDecoding layer\nSketching Step\n\u201c24\u201d \u201c29\u201d\nthen set the output weights of the network to the corresponding non-zero weights wi to get w>x.\nIt remains to show that a hidden unit can implement DANDh1:t (Y, i). Indeed, the AND of t bits can be implemented using nearly any non-linearity. With Relu(a) = max{0, a}, we can construct the activation for bit xi, ai = \u2211 (l,j) VljYlj +Bi, by setting the appropriate t weights in Vlj to 1, setting remaining weights to 0, and setting the bias Bi to 1\u2212 t. Using Corollary 3.2, we have the following theorem. Theorem 4.1. For every w \u2208 Hd,s there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k,\nPrh1:t [N(Sh1:t(x)) = w >x] \u2265 1\u2212 \u03b4 ,\nas long as m = ek and t = log(s/\u03b4). Moreover, the weights coming into each node in the hidden layer are in {0, 1} with at most t non-zeros.\nThe final property implies that when using w as a linear classifier, we get small generalization error as long as the number of examples is at least \u2126(s(1 + t logmt)). This can be proved, e.g., using standard compression arguments: each such model can be represented using only st log(mt) bits in addition to the representation size of w. Similar bounds hold when we use `1 bounds on the weight coming into each unit. Note that even for s = d (i.e. w is unrestricted), we get non-trivial input compression.\nFor comparison, we prove the following result for Gaussian projections in the appendix B. In this case, the model weights in our construction are not sparse. Theorem 4.2. For every w \u2208 Hd,s there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k,\nPrh1:t [N(Gx)) = w >x] \u2265 1\u2212 \u03b4 ,\nas long as G is a random m\u00d7 d Gaussian matrix, with m \u2265 4k log(s/\u03b4)."}, {"heading": "5 SPARSE POLYNOMIAL FUNCTIONS", "text": "For boolean inputs, Theorem 4.1 extends immediately to sparse polynomial functions. Note that we can implement the AND of two bits xi\u2227xj as the AND of the corresponding decodingsDANDh1:t (Y, i) and DANDh1:t (Y, j). Since each decoding is an AND of t bits, the overall decoding is an AND of at most 2t locations in the sketch. More generally, we have the following theorem: Theorem 5.1. Given w \u2208 Rs, and setsA1, . . . , As \u2286 [d], let g : {0, 1}d \u2192 R denote the polynomial\ng(x) = s\u2211 j=1 wj \u220f i\u2208Aj xi = s\u2211 j=1 wj \u2227 i\u2208Aj xi .\nThen there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k,\nPrh1:t [N(Sh1:t(x)) = g(x)] \u2265 1\u2212 \u03b4 ,\nas long as m = ek and t = log(| \u222aj\u2208[s] Aj |/\u03b4). Moreover, the weights coming into each node in the hidden layer are in {0, 1} with at most t \u00b7 (\u2211 j\u2208[s] |Aj | ) non-zeros overall. In particular, when\ng is a degree-p polynomial, we can set t = log(ps/\u03b4), and each hidden unit has at most pt non-zero weights.\nThis is a setting where we get a significant advantage over proper learning. To our knowldege, there is no analog of this result for Gaussian projections. Classical sketching approaches would use a sketch of x\u2297p, which is a kp-sparse vector over binary vectors of dimension dp. Known sketching techniques such as Pham & Pagh (2013) would construct a sketch of size \u2126(kp). Practical techniques such as Vowpal Wabbit also construct cross features by explicitly building them and have this exponential dependence. In stark contrast, neural networks allow us to get away with a logarithmic dependence on p.\nUsing polynomial kernels. Theorems 4.1 has a corresponding variants where the neural net is replaced by a polynomial of degree t. Similarly, the neural net in Theorem 5.1 can be replaced by a degree-pt polynomial when the polynomial g has degree p. This implies that one can use a polynomial kernel to get efficient learning.\nDeterministic sketching. A natural question that arises is whether the parameters above can improved. We show in App. C that if we allow large scalars in the sketches, one can construct a deterministic (2k+ 1)-dimensional sketch from which a shallow network can reconstruct any monomial. We also show a lower bound of k on the required dimensionality.\nLower bound for proper learning. We can also show, see App. D, that if one does not expand the hypothesis class, then even in the simplest of settings of linear classifiers over 1-sparse vectors, the required dimensionality of the projection is much larger than the dimension needed for improper learning. The result is likely folklore and thus we present a short proof in the appendix for completeness using concrete constants in the theorem and its proof below.\nNeural nets on Boolean inputs. We remark that for Boolean inputs (irrespective of sparsity), any polynomial with s monomials can be represented by a neural network in Ns(Relu) using the construction in Theorem 5.1."}, {"heading": "6 EXPERIMENTS WITH SYNTHETIC DATA", "text": "In this section, we evaluate sketches on synthetically generated datasets for the task of polynomial regression. In all the experiments here, we assume input dimension d = 104, input sparsity k = 50, hypothesis support s = 300, and n = 2 \u00d7 105 examples. We assume that only a subset of features I \u2286 [d] are relevant for the regression task, with |I| = 50. To generate an hypothesis, we select s subsets of relevant features A1, . . . , As \u2282 I each of cardinality at most 3, and generate the corresponding weight vector w by drawing corresponding s non-zero entries from the standard Gaussian distribution. We generate binary feature vectors x \u2208 Bd,k as a mixture of relevant and other features. Concretely, for each example we draw 12 feature indices uniformly at random from I, and the remaining indices from [d]. We generate target outputs as g(x) + z, where g(x) is in the form of the polynomial given in Theorem 5.1, and z is additive Gaussian noise with standard deviation 0.05. In all experiments, we train on 90% of the examples and evaluate mean squared error on the rest.\nWe first examined the effect of the sketching parameters m (hash size) and t (number of hash functions) on sparse linear regression error. We generated synthetic datasets as described above (with all feature subsets in A having cardinality 1) and trained networks in Ns(Relu). The results are shown in Figure 2 (left). As expected, increasing t leads to better performance. Using hash size m less than the input sparsity k leads to poor results, while increasing hash size beyond ek (in this case, ek u 136) for reasonable t yields only modest improvements.\nWe next examined the advantages of improper learning. We generated 10 sparse linear regression datasets and trained linear models and networks inNs(Relu) on original and sketched features with\nm = 200 and several values of t. The results are shown in Figure 2 (center). The neural network yields notably better performance than a linear model. This suggests that linear classifiers are not well-preserved after projections, as the \u2126(1/\u03b32) projection size required for linear separability can be large. Applying a neural network to sketched data allows us to use smaller projections.\n1K 2K 3K Gaussian 0.089 0.057 0.029 Sketch t = 1 0.087 0.049 0.031 Sketch t = 2 0.072 0.041 0.023 Sketch t = 6 0.041 0.033 0.022 Gaussian 0.043 0.037 0.034 Sketch t = 1 0.041 0.036 0.033 Sketch t = 2 0.036 0.027 0.024 Sketch t = 6 0.032 0.022 0.018\nWe also compared our sketches to Gaussian random projections. We generated sparse linear and polynomial regression datasets with the same settings as before, and reduce the dimensionality of the inputs to 1000, 2000 and 3000 using Gaussian random projections and sketches with t \u2208 {1, 2, 6}. We remark that in this comparison, the column headings correspond to the total sketch size mt. Thus, e.g., when we take t = 6,m is correspondingly reduced. We report the squared error averaged across examples and five datasets of one-layer neural networks in Table 1. The results demonstrate that sketches with t > 1 yield lower error than Gaussian projections. Note also that Gaussian projections are dense and hence much slower to train."}, {"heading": "7 EXPERIMENTS WITH LANGUAGE PROCESSING TASKS", "text": "Linear and low degree sparse polynomials are often used for classification. Our results imply that if we have linear or a sparse polynomial with classification accuracy 1\u2212\u03b5 over some set of examples in Bd,k \u00d7 {0, 1}, then neural networks constructed to compute the linear or polynomial function attain accuracy of at least 1\u2212\u03b5\u2212\u03b4 over the same examples. Moreover, the number of parameters in the new network is relatively small by enforcing sparsity or `1 bounds for the weights into the hidden layers. We thus get generalization bounds with negligible degradation with respect to nonsketched predictor. In this section, we evaluate sketches on the language processing classification tasks described below.\nEntity Type Tagging. Entity type tagging is the task of assigning one or more labels (such as person, location, organization, event) to mentions of entities in text. We perform type tagging on a corpus of new documents containing 110K mentions annotated with 88 labels (on average, 1.7 labels per mention). Features for each mention include surrounding words, syntactic and lexical patterns, leading to a very large dictionary. Similarly to previous work, we map each string feature to a 32 bit integer, and then further reduce dimensionality using hashing or sketches. See Gillick et al. (2014) for more details on features and labels for this task.\nReuters-news Topic Classification. The Reuters RCV1 data set consists of a collection of approximately 800,000 text articles, each of which is assigned multiple labels. There are 4 high-level categories: Economics, Commerce, Medical, and Government (ECAT, CCAT, MCAT, GCAT), and multiple more specific categories. We focus on training binary classifiers for each of the four major categories. The input features we use are binary unigram features. Post word-stemming, we get data of approximately 113,000 dimensions. The feature vectors are very sparse, however, and most examples have fewer than 120 non-zero features.\nAG-news Topic Classification. We perform topic classification on 680K articles from AG news corpus, labeled with one of 8 news categories: Business, Entertainment, Health, Sci/Tech, Sports, Europe, U.S., World. For each document, we extract binary word indicator features from the title and description; in total, there are 210K unique features, and on average, 23 non-zero features per document.\nExperimental Setup. In all experiments, we use two-layer feed-forward networks with ReLU activations and 100 hidden units in each layer. We use a softmax output for multiclass classification and multiple binary logistic outputs for multilabel tasks. We experimented with input sizes of 1000, 2000, 5000, and 10,000 and reduced the dimensionality of the original features using sketches with t \u2208 {1, 2, 4, 6, 8, 10, 12, 14} blocks. In addition, we experimented with networks trained on the original features. We encouraged parameter sparsity in the first layer using `1-norm regularization and learn parameters using the proximal stochastic gradient method. As before, we trained on 90% of the examples and evaluated on the remaining 10%. We report accuracy values for multiclass classification, and F1 score for multilabel tasks, with true positive, false positive, and false negative counts accumulated across all labels.\nResults. Since one motivation for our work is reducing the number of parameters in neural network models, we plot the performance metrics versus the number of non-zero parameters in the first layer of the network. The results are shown in Figure 3 for different sketching configurations and settings of the `1-norm regularization parameters (\u03bb1). On the entity type tagging task, we compared sketches to a single hash function of size 500,000 as the number of the original features is too large. In this case, sketching allows us to both improve performance and reduce the number of parameters. On the Reuters task, sketches achieve similar performance to the original features with fewer parameters. On AG news, sketching results in more compact models at a modest drop in accuracy. In almost all cases, multiple hash functions yield higher accuracy than a single hash function for similar model size."}, {"heading": "8 CONCLUSIONS", "text": "We have presented a simple sketching algorithm for sparse boolean inputs, which succeeds in significantly reducing the dimensionality of inputs. A single-layer neural network on the sketch can provably model any sparse linear or polynomial function of the original input. For k-sparse vectors in {0, 1}d, our sketch of size O(k log s/\u03b4) allows computing any s-sparse linear or polynomial function on a 1\u2212\u03b4 fraction of the inputs. The hidden constants are small, and our sketch is sparsity preserving. Previous work required sketches of size at least \u2126(s) in the linear case and size at least kp for preserving degree-p polynomials. Our results can be viewed as showing a compressed sensing scheme for 0-1 vectors, where the decoding algorithm is a depth-1 neural network. Our scheme requires O(k log d) measurements, and we leave open the question of whether this can be improved to O(k log dk ) in a stable way. We demonstrated empirically that our sketches work well for both linear and polynomial regression, and that using a neural network does improve over a direct linear regression. We show that on real datasets, our methods lead to smaller models with similar or better accuracy for multiclass and multilabel classification problems. In addition, the compact sketches lead to fewer trainable parameters and faster training."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Amir Globerson for numerous fruitful discussion and help with an early version of the manuscript."}, {"heading": "A GENERAL SKETCHES", "text": "The results of Section 3 and Section 4 extend naturally to positive and to general real-valued vectors. We denote by\nheadk(x) = arg min y : \u2016y\u20160 \u2264 k\n\u2016x\u2212 y\u20161\nthe closest vector to x whose support size is k and the residue tailk(x) = x\u2212headk(x). Let R+d,k,c represent the set\nR+d,k,c = {x \u2208 R d + : \u2016tailk(x)\u20161 \u2264 c} ,\nand let Rd,k,c represent the set\nRd,k,c = {x \u2208 Rd : \u2016tailk(x)\u20161 \u2264 c} .\nFor vectors in R+d,k,c, we use the following sketching and decoding procedures analogous to the binary case.\nSh1:t(x) = [y1, ...,yt] where yl = \u2211\ni:h(i)=l\nxi\nDMINh1:t def = min\nj\u2208[t] (yhj(i)) .\nTheorem A.1. Let x \u2208 R+d,k,c and let h1, . . . , ht be drawn uniformly and independently from a pairwise independent distribution, for m = e(k + 1\u03b5 ). Then for any i,\nPr [ DMINh1:t (Sh1:t(x), i) 6\u2208 [xi, xi + \u03b5c] ] \u2264 e\u2212t.\nProof. Fix a vector x \u2208 R+d,k,c. To remind the reader, for a specific i and h, we have defined the set of collision indices E(i) def= {i\u2032 6= i : h(i\u2032) = h(i)}. Note that DMINh (Sh(x), i) = yh(i), and we can rewrite yh(i) as yh(i) = xi + \u2211 i\u2032\u2208E(i) xi\u2032 \u2265 xi. We next show that\nPr [ DMINh (Sh(x), i) > xi + \u03b5c ] \u2264 1/e . (2)\nWe can rewrite DMINh (Sh(x), i)\u2212 xi = \u2211\ni\u2032\u2208S(headk(x))\u2229E(i)\nxi\u2032 + \u2211\ni\u2032\u2208S(tailk(x))\u2229E(i)\nxi\u2032 .\nBy definition of tailk(\u00b7), the expectation of the second term is c/m. Using Markov\u2019s inequality, we get that\nPr[ \u2211\ni\u2032\u2208S(tailk(x))\u2229E(i)\nxi\u2032 > \u03b5c] \u2264 1\n\u03b5m .\nTo bound the first term, note that\nPr  \u2211 i\u2032\u2208S(headk(x))\u2229E(i) xi\u2032 6= 0  \u2264 Pr  \u2211 i\u2032\u2208S(headk(x))\u2229E(i) 1 6= 0  \u2264 E  \u2211 i\u2032\u2208S(headk(x))\u2229E(i) 1  \u2264 k m .\nRecall that m = e(k + 1/\u03b5), then, using the union bound establishes (2). The rest of the proof is identical to Theorem 3.1.\nCorollary A.2. Let w \u2208 Hd,s and x \u2208 R+d,k,c . For t = log(s/\u03b4), and m = e(k + 1 \u03b5 ), if h1, . . . , ht are drawn uniformly and independently from a pairwise independent distribution, then\nPr [ | \u2211 i wi \u00b7DMINh1:t (Sh1:t(x), i)\u2212w >x| \u2265 \u03b5c\u2016w\u20161 ] \u2264 \u03b4 .\nThe proof in the case that x may not be non-negative is only slightly more complicated. Let us define the following additional decoding procedure,\nDMEDh1:t (Y, i) def = Median\nj\u2208[t] Yhj(i),j .\nTheorem A.3. Let x \u2208 Rd,k,c , and let h1, . . . , ht be drawn uniformly and independently from a pairwise independent distribution, for m = 4e2(k + 2/\u03b5). Then for any i,\nPr [ DMEDh1:t (Sh1:t(x), i) 6\u2208 [xi \u2212 \u03b5c, xi + \u03b5c] ] \u2264 e\u2212t .\nProof. As before, fix a vector x \u2208 Rd,k,c and a specific i and h. We once again write DMEDh (Sh(x), i)\u2212 xi = \u2211\ni\u2032\u2208S(headk(x))\u2229E(i)\nxi\u2032 + \u2211\ni\u2032\u2208S(tailk(x))\u2229E(i)\nxi\u2032 .\nBy the same argument as the proof of Theorem A.1, the first term is nonzero with probability k/m. The second term has expectation in [\u2212c/s, c/s]. Once again by Markov\u2019s inequality,\nPr  \u2211 i\u2032\u2208S(tailk(x))\u2229E(i) xi\u2032 > \u03b5c  \u2264 1 \u03b5m and Pr  \u2211 i\u2032\u2208S(tailk(x))\u2229E(i) xi\u2032 < \u2212\u03b5c  \u2264 1 \u03b5m .\nRecalling that m = 4e2(k + 2/\u03b5), a union bound establishes that for any j,\nPr[|DMEDhj (Shj (x), i)\u2212 xi| > \u03b5c] \u2264 1\n4e2 .\nLet Xj be indicator for the event that |DMEDhj (Shj (x), i)\u2212 xi| > \u03b5c. Thus X1, . . . , Xt are binomial random variables with Pr[Xj = 1] \u2264 14e2 . Then by Chernoff\u2019s bounds\nPr [ DMEDh1:t (Sh1:t(x), i) 6\u2208 [xi \u2212 \u03b5c, xi + \u03b5c] ] \u2264 Pr\n\u2211 j Xj > t/2  \u2264 exp ( \u2212 ( 1\n2 ln\n1/2\n1/4e2 +\n1 2 ln\n1/2\n1\u2212 1/4e2\n) t ) \u2264 exp ( \u2212 ( 1\n2 ln 2e2 +\n1 2 ln 1 2\n) t ) \u2264 exp(\u2212t) .\nCorollary A.4. Let w \u2208 Hd,s and x \u2208 Rd,k,c,. For t = log(s/\u03b4), and m = 4e2(k + 1\u03b5 ), if h1, . . . , ht are drawn uniformly and independently from a pairwise independent distribution, then\nPr [\u2223\u2223\u2223\u2223\u2223\u2211 i wiD MED h1:t (Sh1:t(x), i)\u2212w >x \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b5c\u2016w\u20161 ] \u2264 \u03b4 .\nTo implement DMIN(\u00b7) using hidden units in a neural network, we need to use a slightly nonconventional non-linearity. For a weight vector z and input vector x, a min gate implements mini:zi 6=0 zixi . Then, using Corollary A.2, we get the following theorem. Theorem A.5. For every w \u2208 Hd,s there exists a set of weights for a network N \u2208 Ns(min) such that for each x \u2208 R+d,k,c the following holds,\nPrh1:t [ |N(Sh1:t(x))\u2212w>x| \u2265 \u03b5c\u2016w\u20161 ] \u2264 \u03b4 .\nas long as m = e(k + 1\u03b5 ) and t = log(s/\u03b4). Moreover, the weights coming into each node in the hidden layer are binary with t non-zeros.\nFor real vectors x, the non-linearity needs to implement a median. Nonetheless, an analogous result still holds."}, {"heading": "B GAUSSIAN PROJECTIONS", "text": "In this section we describe and analyze a simple decoding algorithm for Gaussian projections.\nTheorem B.1. Let x \u2208 Rd, and let G be a random Gaussian matrix in Rd\u2032\u00d7d. Then for any i, there exists a linear function fi such that\nEG [ (fi(Gx)\u2212 xi)2 ] \u2264 \u2016x\u2212 xiei\u2016 2 2\nd\u2032\nProof. Recall that G \u2208 Rd\u2032\u00d7d is a random Gaussian matrix where each entry is chosen i.i.d. from N(0, 1/d\u2032). For any i, conditioned on Gji = gji, we have that the random variable Yj |Gji = gji is distributed according to the following distribution,\n(Yj |Gji = gji) \u223c gjixi + \u2211 i\u2032 6=i Gji\u2032 xi\u2032\n\u223c gjixi +N(0, \u2016x\u2212 xiei\u201622/d\u2032) . Consider a linear estimator for xi:\nx\u0302i def = \u2211 j \u03b1ji(yj/gji)\u2211\nj \u03b1ji ,\nfor some non-negative \u03b1ji\u2019s. It is easy to verify that for any vector of \u03b1\u2019s, the expectation of x\u0302i, when taken over the random choices of Gji\u2032 for i\u2032 6= i, is xi. Moreover, the variance of x\u0302i is\n\u2016x\u2212 xiei\u201622 d\n\u2211 j(\u03b1ji/gji) 2\n( \u2211 j \u03b1ji) 2 .\nMinimizing the variance of x\u0302i w.r.t \u03b1ji\u2019s gives us \u03b1ji \u221d g2ji. Indeed, the partial derivatives are, \u2202 (\u2211\nj(\u03b1ji/gji) 2 \u2212 \u03bb \u2211 j \u03b1ji ) \u2202\u03b1ji = 2\u03b1ji g2ji \u2212 \u03bb ,\nwhich is zero at \u03b1ji = \u03bbg2ji/2. This choice of \u03b1ji\u2019s translates to\nE[(x\u0302i \u2212 xi)2] = \u2016x\u2212 xiei\u201622 d\u2032 1\u2211 j g 2 ji ,\nwhich in expectation, now taken over the choices of gji\u2019s, is at most \u2016x\u2212xiei\u201622/d\u2032. Thus, the claim follows. For comparison, if x \u2208 Bd,k, then the expected error in estimating xi is \u221a k\u22121 d\u2032 , so that taking d\u2032 = (k\u2212 1) log 1\u03b4 /\u03b5 2 suffices to get a error \u03b5 estimate of any fixed bit with probablity 1\u2212\u03b4. Setting \u03b5 = 12 , we can recover xi with probability 1\u2212 \u03b4 for x \u2208 Bd,k with d = 4(k \u2212 1) log 1 \u03b4 . This implies Theorem B.2. However, note that the decoding layer is now densely connected to the input layer. Moreover, for a k-sparse vectors x that are is necessarily binary, the error grows with the 2-norm of the vector x, and can be arbitrarily larger than that for the sparse sketch. Note that Gx still contains sufficient information to recover x with error depending only on \u2016x\u2212headk(x)\u20162. To our knowledge, all the known decoders are adaptive algorithms, and we leave open the question of whether bounds depending on the 2-norm of the residual (x \u2212 headk(x)) are achievable by neural networks of small depth and complexity. Theorem B.2. For every w \u2208 Hd,s there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k,\nPrh1:t [N(Gx)) = w >x] \u2265 1\u2212 \u03b4 ,\nas long as G is a random m\u00d7 d Gaussian matrix, with m \u2265 4k log(s/\u03b4)."}, {"heading": "C DETERMINISTIC SKETCHING", "text": "First we show that if we allow large scalars in the sketches, we can construct a deterministic (2k+1)dimensional sketch from which a shallow network can reconstruct any monomial.\nWe will also show a lower bound of k on the required dimensionality.\nFor every x \u2208 Bd,k define a degree 2k univariate real polynomial by, px(z) = 1\u2212 (k + 1) \u220f\n{i|xi=1}\n(z \u2212 i)2.\nIt is easy to verify that this construction satifies the following.\nClaim C.1. Suppose that x \u2208 Bd,k, and let px(\u00b7) be defined as above. If xj = 1, then px(j) = 1. If xj = 0, then px(j) \u2264 \u2212k.\nLet the coeffients of px(z) be ai(x) so that\npx(z) def = 2k\u2211 i=0 ai(x)z i .\nDefine the deterministic sketch DSkd,k : Bd,k \u2192 R2k+1 as follows, DSkd,k(x) = (ax,0, . . . , ax,2k) (3)\nFor a non-empty subset A \u2282 [d] and y \u2208 R2k+1 define\nDecPolyd,k(y, A) =\n\u2211 j\u2208A 2k\u2211 i=0 yij i\n|A| . (4)\nTheorem C.2. For every x \u2208 Bd,k and a non-empty set A \u2282 [d] we have\u220f j\u2208A xj = Relu(DecPolyd,k(DSkd,k(x), A)) .\nProof. We have that\nDecPolyd,k(DSkd,k(x), A) =\n\u2211 j\u2208A 2k\u2211 i=0 ax,ij i\n|A|\n=\n\u2211 j\u2208A px(j)\n|A| .\nIn words, the decoding is the average value of px(j) over the indices j \u2208 A. Now first suppose that\u220f j\u2208A xj = 1. Then for each j \u2208 A we have xj = 1 so that by Claim C.1, px(j) = 1. Thus the average DecPolyd,k(DSkd,k(x), A) = 1.\nOn the other hand, if \u220f j\u2208A xj = 0 then for some j \u2208 A, say j\u2217, xj\u2217 = 0. In this case, Claim C.1 implies that px(j\u2217) \u2264 \u2212k. For every other j, px(j) \u2264 1, and each px(j) is non-negative only when xj = 1, which happens for at most k indices j. Thus the sum over non-negative px(j) can be no larger than k. Adding px(j\u2217) gives us zero, and any additional j\u2019s can only further reduce the sum. Thus the average in non-positive and hence the Relu is zero, as claimed.\nThe last theorem shows that Bd,k can be sketched in Rq , where q = 2k + 1, such that arbitrary products of variables be decoded by applying a linear function followed by a ReLU. It is natural to ask what is the smallest dimension q for which such a sketch exists. The following theorem shows that q must be at least k. In fact, this is true even if we only require to decode single variables. Theorem C.3. Let Sk : Bd,k \u2192 Rq be a mapping such that for every i \u2208 [d] there is wi \u2208 Rq satisfying xi = Relu(\u3008wi, Sk(x)\u3009) for each x \u2208 Bd,k, then q is at least k.\nProof. Denote X = {w1, . . . ,wd} and let H \u2282 {0, 1}X be the function class consisting of all functions of the form hx(wi) = sign(\u3008wi, Sk(x)\u3009) for x \u2208 Bd,k. On one hand, H is a sub-class of the class of linear separators over X \u2282 Rq , hence VC(H) \u2264 q. On the other hand, we claim that VC(H) \u2265 k which establishes the proof. In order to prove the claim it suffices to show that the set A = {w1, . . . ,wk} is shattered. Let B \u2282 A let x be the indicator vector of B. We claim that the restriction of hx to A is the indicator function of B. Indeed, we have that,\nhx(wi) = sign(\u3008wi, Sk(x)\u3009) = sign(Relu(\u3008wi, Sk(x)\u3009)) = xi\n= 1[i \u2208 B]\nWe would like to note that both the endcoding and the decoding of determinitic sketched can be computed efficiently, and the dimension of the sketch is smaller than the dimension of a random sketch. We get the following corollaries. Corollary C.4. For every w \u2208 Hd,s there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k, N(DSkd,k(x)) = w>x. Corollary C.5. Given w \u2208 Rs, and sets A1, . . . , As \u2286 [d], let g : {0, 1}d \u2192 R denote the polynomial function\ng(x) = s\u2211 j=1 wj \u220f i\u2208Aj xi = s\u2211 j=1 wj \u2227 i\u2208Aj xi .\nFor any such g, there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 Bd,k, N(DSkh1:t(x)) = g(x).\nKnown lower bounds for compressed sensing Ba et al. (2010) imply that any linear sketch has size at least \u2126(k log dk ) to allow stable recovery. We leave open the question of whether one can get the compactness and decoding properties of our (non-linear) sketch while ensuring stability."}, {"heading": "D LOWER BOUND FOR PROPER LEARNING", "text": "We now show that if one does not expand the hypothesis class, then even in the simplest of settings of linear classifiers over 1-sparse vectors, the required dimensionality of the projection is much larger than the dimension needed for improper learning. As stated earlier, the result is likely folklore and we present a proof for completeness. Theorem D.1. Suppose that there exists a distribution over maps \u03c6 : Bd,1 \u2192 Rq and \u03c8 : Bd,s \u2192 Rq such that for any x \u2208 Bd,1,w \u2208 Bd,s,\nPr [ sgn ( w>x\u2212 1\n2\n) = sgn ( \u03c8(w)>\u03c6(x) )] \u2265 9\n10 ,\nwhere the probability is taken over sampling \u03c6 and \u03c8 from the distribuion. Then q is \u2126(s).\nProof. If the error is zero, a lower bound on q would follow from standard VC dimension arguments. Concretely, the hypothesis class consisting of hw(x) = {e>i x : wi = 1} for all w \u2208 Bd,s shatters the set {e1, . . . , es}. If sgn(\u03c8(w)>\u03c6(ei)) = sgn(w>x \u2212 12 ) for each w and ei, then the points \u03c6(ei), i \u2208 [s] are shattered by h\u03c8(w)(\u00b7) where w \u2208 Bd,s, which is a subclass of linear separators in Rq . Since linear separators in Rq have VC dimension q, the largest shattered set is no larger, and thus q \u2265 s. To handle errors, we will use the Sauer-Shelah lemma and show that the set {\u03c6(ei) : i \u2208 [s]} has many partitions. To do so, sample \u03c6, \u03c8 from the distribution promised above and consider the set of points A = {\u03c6(e1), \u03c6(e2), . . . , \u03c6(es)}. Let W = {w1,w2, . . . ,w\u03ba} be a set of \u03ba vectors in Bd,s such that,\n(a) S(wi) \u2286 [s]\n(b) Distance property holds: |S(wi)4S(wj)| \u2265 s4 for i 6= j.\nSuch a collection of vectors, with \u03ba = 2cs for a positive constant c, can be shown to exist by a probabilistic argument or by standard constructions in coding theory. Let H = {h\u03c8(w) : w \u2208 W} be the linear separators defined by \u03c8(wi) for i \u2208 W . For brevity, we denote h\u03c8(wj) by hj . We will argue that H induces many different subsets of A.\nLet Aj = {y \u2208 A : hj(x) = 1} = {x \u2208 A : sgn(\u03c8(wj)>x) = 1}. Let Ej \u2286 A be the positions where the embeddings \u03c6, \u03c8 fail, that is,\nEj = {\u03c6(ei) : i \u2208 [s], sgn(w>j ei \u2212 1\n2 ) 6= sgn(\u03c8(wj)>\u03c6(ei)}.\nThus Aj = S(wj)4 Ej . By assumption, E[|Ej |] \u2264 s10 for each j, where the expectation is taken over the choice of \u03c6, \u03c8. Thus \u2211 j E[|Ej |] \u2264 s\u03ba/10. Renumber the hj\u2019s in increasing order of |Ej | so that |E1| \u2264 |E2| \u2264 . . . \u2264 |E\u03ba|. Due to the Ej\u2019s being non-empty, not all Aj\u2019s are necessarily distinct. Call a j \u2208 [\u03ba] lost if Aj = Aj\u2032 for some j\u2032 \u2264 j. By definition,Aj = S(wj)4Ej . IfAj = Aj\u2032 , then the distance property implies thatEj4Ej\u2032 \u2265 s4 . Since the Ej\u2019s are increasing in size, it follows that for any lost j, |Ej | \u2265 s8 . Thus in expectation at most 4\u03ba/5 of the j\u2019s are lost. It follows that there is a choice of \u03c6, \u03c8 in the distribution for which H induces \u03ba/5 distinct subsets of A. Since the VC dimension of H is at most q, the Sauer-Shelah lemma says that \u2211\nt\u2264q\n( s\nt\n) \u2265 \u03ba\n5 =\n2cs\n5 .\nThis implies that q \u2265 c\u2032s for some absolute constant c\u2032.\nNote that for the setting of the above example, once we scale the wj\u2019s to be unit vectors, the margin is \u0398( 1\u221a\ns ). Standard results then imply that projecting to 1\u03b32 = \u0398(s) dimension suffices, so the above\nbound is tight.\nFor this setting, Theorem 4.1 implies that a sketch of size O(log(s/\u03b4)) suffices to correctly classify 1\u2212\u03b4 fraction of the examples if one allows improper learning as we do."}, {"heading": "E NEURAL NETS ON BOOLEAN INPUTS", "text": "In this short section show that for boolean inputs (irrespective of sparsity), any polynomial with s monomials can be represented by a neural network with one hidden layer of s hidden units. Our result is a simple improvement of Barron\u2019s theorem (1993; 1994), for the special case of sparse polynomial functions on 0-1 vectors. In contrast, Barron\u2019s theorem, which works for arbitrary inputs, would require a neural network of size d \u00b7 s \u00b7 pO(p) to learn an s-sparse degree-p polynomial. The proof of the improvement is elementary and provided for completeness. Theorem E.1. Let x \u2208 {0, 1}d, and let g : {0, 1}d \u2192 R denote the polynomial function\ng(x) = s\u2211 j=1 wj \u220f i\u2208Aj xi = s\u2211 j=1 wj \u2227 i\u2208Aj xi .\nThen there exists a set of weights for a network N \u2208 Ns(Relu) such that for each x \u2208 {0, 1}d, N(x) = g(x). Moreover, the weights coming into each node in the hidden layer are in {0, 1}. Proof. The jth hidden unit implements hj = \u220f i\u2208Aj xi. As before, for boolean inputs, one can\ncompute hj as Relu( \u2211 i\u2208Aj xi \u2212 |Aj | + 1). The output node computes \u2211 j wjhj where hj is the output of jth hidden unit."}], "references": [{"title": "Database-friendly random projections: Johnson\u2013Lindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "The fast JL transform and approximate nearest neighbors. SICOMP", "author": ["N. Ailon", "B. Chazelle"], "venue": null, "citeRegEx": "Ailon and Chazelle.,? \\Q2009\\E", "shortCiteRegEx": "Ailon and Chazelle.", "year": 2009}, {"title": "The space complexity of approximating the frequency moments", "author": ["Noga Alon", "Yossi Matias", "Mario Szegedy"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Alon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1999}, {"title": "An algorithmic theory of learning: Robust concepts and random projection", "author": ["Rosa I. Arriaga", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Arriaga and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Arriaga and Vempala.", "year": 2006}, {"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS, pp", "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Lower bounds for sparse recovery", "author": ["K. Do Ba", "P. Indyk", "E. Price", "D. Woodruff"], "venue": "In SODA,", "citeRegEx": "Ba et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2010}, {"title": "Approximation and estimation bounds for artificial neural networks", "author": ["A. Barron"], "venue": "Mach. Learning,", "citeRegEx": "Barron.,? \\Q1994\\E", "shortCiteRegEx": "Barron.", "year": 1994}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R Barron"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "Rademacher chaos, random Eulerian graphs and the sparse Johnson-Lindenstrauss transform", "author": ["Vladimir Braverman", "Rafail Ostrovsky", "Yuval Rabani"], "venue": "CoRR, abs/1011.2590,", "citeRegEx": "Braverman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2010}, {"title": "Near optimal signal recovery from random projections: Universal encoding strategies", "author": ["E.J. Cand\u00e9s", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e9s and Tao.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e9s and Tao.", "year": 2006}, {"title": "Finding frequent items in data streams", "author": ["M. Charikar", "K. Chen", "M. Farach"], "venue": "Theor. Comp. Sci.,", "citeRegEx": "Charikar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2004}, {"title": "Compressing convolutional neural networks", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "CoRR, abs/1506.04449,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Y. Cheng", "F. Yu", "r. Feris", "S. Kumar", "A. Choudhary", "S-F. Chang"], "venue": "In CVPR,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["K. Clarkson", "D. Woodruff"], "venue": "In STOC,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Memory bounded deep convolutional networks", "author": ["M.D. Collins", "P. Kohli"], "venue": "CoRR, abs/1412.1442,", "citeRegEx": "Collins and Kohli.,? \\Q2014\\E", "shortCiteRegEx": "Collins and Kohli.", "year": 2014}, {"title": "An improved data stream summary: the count-min sketch and its applications", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "J. Algorithms,", "citeRegEx": "Cormode and Muthukrishnan.,? \\Q2005\\E", "shortCiteRegEx": "Cormode and Muthukrishnan.", "year": 2005}, {"title": "Summarizing and mining skewed data streams", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "In SDM, pp", "citeRegEx": "Cormode and Muthukrishnan.,? \\Q2005\\E", "shortCiteRegEx": "Cormode and Muthukrishnan.", "year": 2005}, {"title": "A sparse Johnson\u2013Lindenstrauss transform", "author": ["Anirban Dasgupta", "Ravi Kumar", "Tam\u00e1s Sarl\u00f3s"], "venue": "In STOC,", "citeRegEx": "Dasgupta et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V Le", "MarcAurelio Ranzato", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "MarcAurelio Ranzato", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Small statistical models by random feature mixing", "author": ["Kuzman Ganchev", "Mark Dredze"], "venue": "In Workshop on Mobile NLP at ACL,", "citeRegEx": "Ganchev and Dredze.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev and Dredze.", "year": 2008}, {"title": "Context-dependent fine-grained entity type tagging", "author": ["D. Gillick", "N. Lazic", "K. Ganchev", "J. Kirchner", "D. Huynh"], "venue": "CoRR, abs/1412.1820,", "citeRegEx": "Gillick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork", "Gregory J Wolff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemp. Math,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Sparser Johnson\u2013Lindenstrauss transforms", "author": ["Daniel M. Kane", "Jelani Nelson"], "venue": "J. ACM,", "citeRegEx": "Kane and Nelson.,? \\Q2014\\E", "shortCiteRegEx": "Kane and Nelson.", "year": 2014}, {"title": "A universal kernel for learning regular languages", "author": ["Leonid Kontorovich"], "venue": "In MLG,", "citeRegEx": "Kontorovich.,? \\Q2007\\E", "shortCiteRegEx": "Kontorovich.", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems, pp. 1097\u20131105,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "On variants of the Johnson\u2013Lindenstrauss", "author": ["J. Matousek"], "venue": "lemma. Rand. Struct. Algs.,", "citeRegEx": "Matousek.,? \\Q2008\\E", "shortCiteRegEx": "Matousek.", "year": 2008}, {"title": "Probability and Computing: Randomized Algorithms and Probabilistic Analysis", "author": ["Michael Mitzenmacher", "Eli Upfal"], "venue": null, "citeRegEx": "Mitzenmacher and Upfal.,? \\Q2005\\E", "shortCiteRegEx": "Mitzenmacher and Upfal.", "year": 2005}, {"title": "A deep learning approach to structured signal recovery", "author": ["Ali Mousavi", "Ankit B. Patel", "Richard G. Baraniuk"], "venue": null, "citeRegEx": "Mousavi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mousavi et al\\.", "year": 2015}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In KDD,", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS, pp", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["P.P. Talukdar", "W.W. Cohen"], "venue": "In AISTATS,", "citeRegEx": "Talukdar and Cohen.,? \\Q2014\\E", "shortCiteRegEx": "Talukdar and Cohen.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A.J. Smola"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013).", "startOffset": 88, "endOffset": 241}, {"referenceID": 32, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013).", "startOffset": 88, "endOffset": 241}, {"referenceID": 17, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013).", "startOffset": 88, "endOffset": 241}, {"referenceID": 8, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013).", "startOffset": 88, "endOffset": 241}, {"referenceID": 29, "context": "For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007).", "startOffset": 130, "endOffset": 149}, {"referenceID": 20, "context": "(Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.", "startOffset": 0, "endOffset": 34}, {"referenceID": 25, "context": "Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples).", "startOffset": 159, "endOffset": 250}, {"referenceID": 30, "context": "Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples).", "startOffset": 159, "endOffset": 250}, {"referenceID": 37, "context": "Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples).", "startOffset": 159, "endOffset": 250}, {"referenceID": 39, "context": "Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples).", "startOffset": 159, "endOffset": 250}, {"referenceID": 18, "context": "These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012).", "startOffset": 136, "endOffset": 180}, {"referenceID": 30, "context": "These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012).", "startOffset": 136, "endOffset": 180}, {"referenceID": 11, "context": "HashedNets (Chen et al., 2015) enforce parameter sharing between random groups", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting.", "startOffset": 89, "endOffset": 342}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch.", "startOffset": 89, "endOffset": 478}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007).", "startOffset": 89, "endOffset": 784}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors.", "startOffset": 89, "endOffset": 981}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds.", "startOffset": 89, "endOffset": 1130}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance.", "startOffset": 89, "endOffset": 1285}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning.", "startOffset": 89, "endOffset": 1491}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels.", "startOffset": 89, "endOffset": 1578}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs.", "startOffset": 89, "endOffset": 2173}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices.", "startOffset": 89, "endOffset": 3119}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers.", "startOffset": 89, "endOffset": 3310}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al.", "startOffset": 89, "endOffset": 3393}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models.", "startOffset": 89, "endOffset": 3493}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models. Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while others such as LeCun et al.", "startOffset": 89, "endOffset": 3627}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models. Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while others such as LeCun et al. (1989); Hassibi et al.", "startOffset": 89, "endOffset": 3734}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models. Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while others such as LeCun et al. (1989); Hassibi et al. (1993); Han et al.", "startOffset": 89, "endOffset": 3757}, {"referenceID": 0, "context": "A long line of work has sought sparser projection matrices with similar guarantees; see (Achlioptas, 2003; Ailon & Chazelle, 2009; Matousek, 2008; Dasgupta et al., 2010; Braverman et al., 2010; Kane & Nelson, 2014; Clarkson & Woodruff, 2013). Research in streaming and sketching algorithms has addressed related questions. Alon et al. (1999) showed a simple hashing-based algorithm for unbiased estimators for the Euclidean norm in the streaming setting. Charikar et al. (2004) showed an algorithm for the heavy-hitters problem based on the count sketch. Most relevant to our works is the count-min sketch of Cormode and Muthukrishnan (2005a; 2005b). Projections in learning. Random projections have been used in machine learning at least since the work of Arriaga and Vempala (2006). For fast estimation of a certain class of kernel functions, sampling has been proposed as a dimensionality reduction technique in (Kontorovich, 2007) and (Rahimi & Recht, 2007). Shi et al. (2009) propose using a count-min sketch to reduce dimensionality while approximately preserving inner products for sparse vectors. Weinberger et al. (2009) use the count-sketch to get an unbiased estimator for the inner product of sparse vectors and prove strong concentration bounds. Ganchev and Dredze (2008) empirically show that hashing is effective in reducing model size without significantly impacting performance. Hashing has also been used in Vowpal Wabbit (Langford et al., 2007). Talukdar and Cohen (2014) use the count-min sketch in graph-based semi-supervised learning. Pham and Pagh (2013) showed that a count sketch of a tensor power of a vector could be quickly computed without explicitly computing the tensor power, and applied it to fast sketching for polynomial kernels. Compressive sensing. Our work is also related to compressive sensing. For k-sparse vectors, results in this area, e.g. (Donoho, 2006; Cand\u00e9s & Tao, 2006), imply that a k-sparse vector x \u2208 R can be reconstructed w.h.p. from a projection of dimension O(k ln d k ). However, to our knowledge, no provable decoding algorithms are implementable by a low-depth neural network. Recent work by Mousavi et al. (2015) empirically explores using a deep network for decoding in compressive sensing and also considers learnt non-linear encodings to adapt to the distribution of inputs. Parameter reduction in deep learning. Our work can be viewed as a method for reducing the number of parameters in neural networks. Neural networks have become ubiquitous in many machine learning applications, including speech recognition, computer vision, and language processing tasks(see (Hinton et al., 2012; Krizhevsky et al., 2012; Sermanet et al., 2013; Vinyals et al., 2014) for a few notable examples). These successes have in part been enabled by recent advances in scaling up deep networks, leading to models with millions of parameters (Dean et al., 2012; Krizhevsky et al., 2012). However, a drawback of such large models is that they are very slow to train, and difficult to deploy on mobile and embedded devices with memory and power constraints. Denil et al. (2013) demonstrate significant redundancies in the parameterization of several deep learning architectures, and they propose training low-rank decompositions of weight matrices. Cheng et al. (2015) impose circulant matrix structure on fully connected layers. Ba and Caruana (2014) train shallow networks to predict the log-outputs of a large deep network, and Hinton et al. (2015) train a small network to match smoothed predictions of a complex deep network or an ensemble of such models. Collins and Kohli (2014) encourage zero-weight connections using sparsity-inducing priors, while others such as LeCun et al. (1989); Hassibi et al. (1993); Han et al. (2015) use techniques for pruning weights.", "startOffset": 89, "endOffset": 3776}, {"referenceID": 22, "context": "See Gillick et al. (2014) for more details on features and labels for this task.", "startOffset": 4, "endOffset": 26}], "year": 2016, "abstractText": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years. These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data. For example, preserving linear separability requires \u03a9(1/\u03b3) dimensions, where \u03b3 is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree. Despite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree. A practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_238"}