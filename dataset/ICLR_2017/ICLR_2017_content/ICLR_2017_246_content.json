{"name": "ICLR_2017_246.pdf", "metadata": {"source": "CRF", "title": "ENERGY-BASED SPHERICAL SPARSE CODING", "authors": [], "emails": ["bhkong@ics.uci.edu", "fowlkes@ics.uci.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Sparse coding has been widely studied as a representation for images, audio and other vectorial data. This has been a highly successful method that has found its way into many applications, from signal compression and denoising (Donoho, 2006; Elad & Aharon, 2006) to image classification (Wright et al., 2009), to modeling neuronal receptive fields in visual cortex (Olshausen & Field, 1997). Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).\nWhile supervised sparse coding methods have been shown to find more discriminative features leading to improved classification performance over their unsupervised counterparts, they have received much less attention in recent years and have been eclipsed by simpler feed-forward architectures.\nThis is in part because sparse coding is computationally expensive. Convex formulations of sparse coding typically consist of a minimization problem over an objective that includes a least-squares (LSQ) reconstruction error term plus a sparsity inducing regularizer.\nBecause there is no closed-form solution to this formulation, various iterative optimization techniques are generally used to find a solution (Zeiler et al., 2010; Bristow et al., 2013; Yang et al., 2013; Heide et al., 2015). In applications where an approximate solution suffices, there is work that learns non-linear predictors to estimate sparse codes rather than solve the objective more directly (Gregor & LeCun, 2010). The computational overhead for iterative schemes becomes quite significant when training discriminative models due to the demand of processing many training examples necessary for good performance, and so sparse coding has fallen out of favor by not being able to keep up with simpler non-iterative coding methods.\nIn this paper we introduce an alternate formulation of sparse coding using unit length codes and a reconstruction loss based on the cosine similarity. Optimal sparse codes in this model can be computed in a non-iterative fashion and the coding objective lends itself naturally to embedding in a discriminative, energy-based classifier which we term energy-based spherical sparse coding (EBSSC). This bi-directional coding method incorporates both top-down and bottom-up information where the features representation depends on both a hypothesized class label and the input signal. Like Cao et al. (2015), our motivation for bi-directional coding comes from the \u201cBiased Competition Theory\u201d, which suggests that visual processing can be biased by other mental processes (e.g., topdown influence) to prioritize certain features that are most relevant to current task. Fig. 1 illustrates the flow of computation used by our SSC and EB-SSC building blocks compared to a standard feed-forward layer.\nOur energy based approach for combining top-down and bottom-up information is closely tied to the ideas of Larochelle & Bengio (2008); Ji et al. (2011); Zhang et al. (2013); Li & Guo (2014)\u2014 although the model details are substantially different (e.g., Ji et al. (2011) and Zhang et al. (2013) use sigmoid non-linearities while Li & Guo (2014) use separate representations for top-down and bottom-up information). The energy function of Larochelle & Bengio (2008) is also similar but includes an extra classification term and is trained as a restricted Boltzmann machine."}, {"heading": "1.1 NOTATION", "text": "Matrices are denoted as uppercase bold (e.g., A), vectors are lowercase bold (e.g., a), and scalars are lowercase (e.g., a). We denote the transpose operator with \u1d40, the element-wise multiplication operator with , the convolution operator with \u2217, and the cross-correlation operator with ?. For vectors where we dropped the subscript k (e.g., d and z), we refer to a super vector with K components stacked together (e.g., z = [z\u1d401 , . . . , z \u1d40 K ] \u1d40)."}, {"heading": "2 ENERGY-BASED SPHERICAL SPARSE CODING", "text": "Energy-based models capture dependencies between variables using an energy function that measure the compatibility of the configuration of variables (LeCun et al., 2006). To measure the compatibility between the top-down and bottom-up information, we define the energy function of EB-SSC to be the sum of bottom-up coding term and a top-down classification term:\nE(x, y, z) = Ecode(x, z) + Eclass(y, z). (1) The bottom-up information (input signal x) and the top-down information (class label y) are tied together by a latent feature map z."}, {"heading": "2.1 BOTTOM-UP RECONSTRUCTION", "text": "To measure the compatibility between the input signal x and the latent feature maps z, we introduce a novel variant of sparse coding that is amenable to efficient feed-forward optimization. While the idea behind this variant can be applied to either patch-based or convolutional sparse coding, we specifically use the convolutional variant that shares the burden of coding an image among nearby overlapping dictionary elements. Using such a shift-invariant approach avoids the need to learn dictionary elements which are simply translated copies of each other, freeing up resources to discover more diverse and specific filters (see Kavukcuoglu et al. (2010)).\nConvolutional sparse coding (CSC) attempts to find a set of dictionary elements {d1, . . . ,dK} and corresponding sparse codes {z1, . . . , zK} so that the resulting reconstruction, r = \u2211K k=1 dk \u2217 zk accurately represents the input signal x. This is traditionally framed as a least-squares minimization with a sparsity inducing prior on z:\narg min z \u2016x\u2212 K\u2211 k=1 dk \u2217 zk\u201622 + \u03b2\u2016z\u20161. (2)\nUnlike standard feed-forward CNN models that convolve the input signal x with the filters, this energy function corresponds to a generative model where the latent feature maps {z1, . . . , zK} are convolved with the filters and compared to the input signal (Bristow et al., 2013; Heide et al., 2015; Zeiler et al., 2010).\nTo motivate our novel variant of CSC, consider expanding the squared reconstruction error \u2016x \u2212 r\u201622 = \u2016x\u201622 \u2212 2x\u1d40r + \u2016r\u201622. If we constrain the reconstruction r to have unit norm, the reconstruction error depends entirely on the inner product between x and r and is equivalent to the cosine similarity (up to additive and multiplicative constants). This suggests the closely related unit-length reconstruction problem:\narg max z x\u1d40 ( K\u2211 k=1 dk \u2217 zk ) \u2212 \u03b2\u2016z\u20161 (3)\ns.t. \u2225\u2225 K\u2211 k=1 dk \u2217 zk \u2225\u2225 2 \u2264 1\nIn Appendix A we show that, given an optimal unit length reconstruction r\u0304\u2217 with corresponding codes z\u0304\u2217, the solution to the least squares reconstruction problem (Eq. 2) can be computed by a simple scaling r\u2217 = (x\u1d40r\u0304\u2217 \u2212 \u03b22 \u2016z\u0304\n\u2217\u20161)r\u0304\u2217. The unit-length reconstruction problem is no easier than the original least-squares optimization due to the constraint on the reconstruction which couples the codes for different filters. Instead consider a simplified constraint on z which we refer to as spherical sparse coding (SSC):\narg max \u2016z\u20162\u22641 Ecode(x, z) = arg max \u2016z\u20162\u22641 x\u1d40 ( K\u2211 k=1 dk \u2217 zk ) \u2212 \u03b2\u2016z\u20161. (4)\nIn 2.3 below, we show that the solution to this problem can be found very efficiently without requiring iterative optimization.\nThis problem is a relaxation of convolutional sparse coding since it ignores non-orthogonal interactions between the dictionary elements1. Alternately, assuming unit norm dictionary elements, the code norm constraint can be used to upper-bound the reconstruction length. We have by the triangle and Young\u2019s inequality that:\u2225\u2225\u2211\nk\ndk \u2217 zk \u2225\u2225 2 \u2264 \u2211 k \u2016dk \u2217 zk\u20162 \u2264 \u2211 k \u2016dk\u20161\u2016zk\u20161 \u2264 D \u2211 k \u2016zk\u20162 (5)\nwhere the factor D is the dimension of zk and arises from switching from the 1-norm to the 2-norm. Since D \u2211 k \u2016zk\u20162 \u2264 1 is a tighter constraint we have\nmax \u2016 \u2211 k dk\u2217zk\u20162\u22641 Ecode(x, z) \u2265 max\u2211 k \u2016zk\u20162\u2264 1 D Ecode(x, z) (6)\nHowever, this relaxation is very loose, primarily due to the triangle inequality. Except in special cases (e.g., if the dictionary elements have disjoint spectra) the SSC codes will be quite different from the standard least-squares reconstruction.\n1We note that our formulation is also closely related to the dynamical model suggested by Rozell et al. (2008), but without the dictionary-dependent lateral inhibition between feature maps. Lateral inhibition can solve the unit-length reconstruction formulation of standard sparse coding but requires iterative optimization."}, {"heading": "2.2 TOP-DOWN CLASSIFICATION", "text": "To measure the compatibility between the class label y and the latent feature maps z, we use a set of one-vs-all linear classifiers. To provide more flexibility, we generalize this by splitting the code vector into positive and negative components:\nzk = z + k + z \u2212 k z + k \u2265 0 z \u2212 k \u2264 0\nand allow the linear classifier to operate on each component separately. We express the classifier score for a hypothesized class label y by:\nEclass(y, z) = K\u2211 k=1 w+\u1d40y z + k + K\u2211 k=1 w\u2212\u1d40y z \u2212 k . (7)\nThe classifier thus is parameterized by a pair of weight vectors (w+yk and w \u2212 yk) for each class label y and k-th channel of the latent feature map.\nThis splitting, sometimes referred to as full-wave rectification, is useful since a dictionary element and its negative do not necessarily have opposite visual semantics. This splitting also allows the classifier the flexibility to assign distinct meanings or alternately be completely invariant to contrast reversal depending on the problem domain. For example, Shang et al. (2016) found CNN models with ReLU non-linearities which discard the negative activations tend to learn pairs of filters which are related by negation. Keeping both positive and negative responses allowed them to halve the number of dictionary elements.\nWe note that it is also straightforward to introduce spatial average pooling prior to classification by introducing a fixed linear operator P used to pool the codes (e.g., w+\u1d40y Pz + k ). This is motivated by a variety of hand-engineered feature extractors and sparse coding models, such as Ren & Ramanan (2013), which use spatially pooled histograms of sparse codes for classification. This fixed pooling can be viewed as a form of regularization on the linear classifier which enforces shared weights over spatial blocks of the latent feature map. Splitting is also quite important to prevent information loss when performing additive pooling since positive and negative components of zk can cancel each other out."}, {"heading": "2.3 CODING", "text": "Bottom-up reconstruction and top-down classification each provide half of the story, coupled by the latent feature maps. For a given input x and hypothesized class y, we would like to find the optimal activations z that maximize the joint energy function E(x, y, z). This requires solving the following optimization:\narg max \u2016z\u20162\u22641 x\u1d40 ( K\u2211 k=1 dk \u2217 zk ) \u2212 \u03b2\u2016z\u20161 + K\u2211 k=1 w+\u1d40yk z + k + K\u2211 k=1 w\u2212\u1d40yk z \u2212 k , (8)\nwhere x \u2208 RD is an image and y \u2208 Y is a class hypothesis. zk \u2208 RF is the k-th component latent variable being inferred; z+k and z \u2212 k are the positive and negative coefficients of zk, such that zk = z + k + z \u2212 k . The parameters dk \u2208 RM , w + yk \u2208 RF , and w \u2212 yk \u2208 RF are the dictionary filter, positive coefficient classifier, and negative coefficient classifier for the k-th component respectively. A key aspect of our formulation is that the optimal codes can be found very efficiently in closedform\u2014in a feed-forward manner (see Appendix B for a detailed argument)."}, {"heading": "2.3.1 ASYMMETRIC SHRINKAGE", "text": "To describe the coding processes, let us first define a generalized version of the shrinkage function commonly used in sparse coding. Our asymmetric shrinkage is parameterized by upper and lower thresholds \u2212\u03b2\u2212 \u2264 \u03b2+\nshrink(\u03b2+,\u03b2\u2212)(v) =  v \u2212 \u03b2 + if v \u2212 \u03b2+ > 0\n0 otherwise v + \u03b2\u2212 if v + \u03b2\u2212 < 0\n(9)\nFig. 2 shows a visualization of this function which generalizes the standard shrinkage proximal operator by allowing for the positive and negative thresholds. In particular, it corresponds to the proximal operator for a version of the `1-norm that penalizes the positive and negative components with different weights |v|asym = \u03b2+\u2016v+\u20161 +\u03b2\u2212\u2016v\u2212\u20161. The standard shrink operator corresponds to shrink(\u03b2,\u2212\u03b2)(v) while the rectified linear unit common in CNNs is given by a limiting case shrink(0,\u2212\u221e)(v). We note that \u2212\u03b2\u2212 \u2264 \u03b2+ is required for shrink(\u03b2+,\u03b2\u2212) to be a proper function (see Fig. 2)."}, {"heading": "2.3.2 FEED-FORWARD CODING", "text": "We now describe how codes can be computed in a simple feed-forward pass. Let\n\u03b2+yk = \u03b2 \u2212w + yk, \u03b2 \u2212 yk = \u03b2 \u2212w \u2212 yk (10)\nbe vectors of positive and negative biases whose entries are associated with a spatial location in the feature map k for class y. The optimal code z can be computed in three sequential steps:\n1. Cross-correlate the data with the filterbank dk ? x\n2. Apply an asymmetric version of the standard shrinkage operator\nz\u0303k = shrink(\u03b2+yk,\u03b2 \u2212 yk) (dk ? x) (11)\nwhere, with abuse of notation, we allow the shrinkage function (Eq. 9) to apply entries in the vectors of threshold parameter pairs \u03b2+yk,\u03b2 \u2212 yk to the corresponding elements of the argument.\n3. Project onto the feasible set of unit length codes\nz\u2217 = z\u0303\n\u2016z\u0303\u20162 . (12)"}, {"heading": "2.3.3 RELATIONSHIP TO CNNS:", "text": "We note that this formulation of coding has a close connection to single layer convolutional neural network (CNN). A typical CNN layer consists of convolution with a filterbank followed by a nonlinear activation such as a rectified linear unit (ReLU). ReLUs can be viewed as another way of inducing sparsity, but rather than coring the values around zero like the shrink function, ReLU truncates negative values. On the other hand, the asymmetric shrink function can be viewed as the sum of two ReLUs applied to appropriately biased inputs:\nshrink(\u03b2+,\u03b2\u2212)(x) = ReLU(x\u2212 \u03b2+)\u2212 ReLU(\u2212(x+ \u03b2\u2212)),\nSSC coding can thus be seen as a CNN in which the ReLU activation has been replaced with shrinkage followed by a global normalization."}, {"heading": "3 LEARNING", "text": "We formulate supervised learning using the softmax log-loss that maximizes the energy for the true class label yi while minimizing energy of incorrect labels y\u0304.\narg min d,w+,w\u2212,\u03b2\u22650\n\u03b1 2 (\u2016w+\u201622 + \u2016w\u2212\u201622 + \u2016d\u201622)\n+ 1\nN N\u2211 i=1 [\u2212 max \u2016z\u20162\u22641 E(xi, yi, z) + log \u2211 y\u0304\u2208Y max \u2016z\u0304\u20162\u22641 eE(xi,y\u0304,z\u0304)]\ns.t. \u2212 (\u03b2 \u2212w\u2212yk) \u2264 (\u03b2 \u2212w + yk) \u2200y, k\n, (13)\nwhere \u03b1 is the hyperparameter regularizing w+y , w \u2212 y , and d. We constrain the relationship between \u03b2 and the entries of w+y and w \u2212 y in order for the asymmetric shrinkage to be a proper function (see Sec. 2.3.1 and Appendix B for details).\nIn classical sparse coding, it is typical to constrain the `2-norm of each dictionary filter to unit length. Our spherical coding objective behaves similarly. For any optimal code z\u2217, there is a 1-dimensional subspace of parameters for which z\u2217 is optimal given by scaling d inversely to w, \u03b2. For simplicity of the implementation, we opt to regularize d to assure a unique solution. However, as Tygert et al. (2015) point out, it may be advantageous from the perspective of optimization to explicitly constrain the norm of the filter bank.\nNote that unlike classical sparse coding, where \u03b2 is a hyperparameter that is usually set using crossvalidation, we treat it as a parameter of the model that is learned to maximize performance."}, {"heading": "3.1 OPTIMIZATION", "text": "In order to solve Eq. 13, we explicitly formulate our model as a directed-acyclic-graph (DAG) neural network with shared weights, where the forward-pass computes the sparse code vectors and the backward-pass updates the parameter weights. We optimize the objective using stochastic gradient descent (SGD).\nAs mentioned in Sec. 2.3 shrinkage function is assymetric with parameters \u03b2+yk or \u03b2 \u2212 yk as defined in Eq. 10. However, the inequality constraint on their relationship to keep the shrinkage function a proper function is difficult to enforce when optimizing with SGD. Instead, we introduce a central offset parameter and reduce the ordering constraint to pair of positivity constraints. Let\nw\u0302+yk = \u03b2 + yk \u2212 bk w\u0302 \u2212 yk = \u03b2 \u2212 yk + bk (14)\nbe the modified linear \u201cclassifiers\u201d relative to the central offset bk. It is straightforward to see that if \u03b2+yk and \u03b2 \u2212 yk that satisfy the constrain in Eq. 13, then adding the same value to both sides of the inequality will not change that. However, taking bk to be a midpoint between them, then both \u03b2+yk \u2212 bk and \u03b2 \u2212 yk + bk will be strictly non-negative.\nUsing this variable substitution, we rewrite the energy function (Eq. 1) as\nE\u2032(x, y, z) = x\u1d40 ( K\u2211 k=1 dk \u2217 zk ) + K\u2211 k=1 bk1 \u1d40zk \u2212 K\u2211 k=1 w\u0302+\u1d40yk z + k + K\u2211 k=1 w\u0302\u2212\u1d40yk z \u2212 k . (15)\nwhere b is constant offset for each code channel. The modified linear \u201cclassification\u201d terms now take on a dual role of inducing sparsity and measuring the compatibility between z and y.\nThis yields a modified learning objective that can easily be solved with existing implementations for learning convolutional neural nets:\narg min d,w\u0302+,w\u0302\u2212,b\n\u03b1 2 (\u2016w\u0302+\u201622 + \u2016w\u0302\u2212\u201622 + \u2016d\u201622)\n+ 1\nN N\u2211 i=1 [\u2212 max \u2016z\u20162\u22641 E\u2032(xi, yi, z) + log \u2211 y\u0304\u2208Y max \u2016z\u0304\u20162\u22641 eE \u2032(xi,y\u0304,z\u0304)]\ns.t. w\u0302+yk, w\u0302 \u2212 yk 0 \u2200y, k\n, (16)\nwhere w\u0302+ and w\u0302\u2212 are the new sparsity inducing classifiers, and b are the arbitrary origin points. In particular, adding the K origin points allows us to enforce the constraint by simply projecting w\u0302+ and w\u0302\u2212 onto the positive orthant during SGD."}, {"heading": "3.1.1 STACKING BLOCKS", "text": "We also examine stacking multiple blocks of our energy function in order to build a hierarchical representation. As mentioned in Sec. 3.1.1, the optimal codes can be computed in a simple feedforward pass\u2014this applies to shallow versions of our model. When stacking multiple blocks of our energy-based model, solving for the optimal codes cannot be done in a feed-forward pass since the codes for different blocks are coupled (bilinearly) in the joint objective. Instead, we can proceed in an iterative manner, performing block-coordinate descent by repeatedly passing up and down the hierarchy updating the codes. In this section we investigate the trade-off between the number of passes used to find the optimal codes for the stacked model and classification performance.\nFor this purpose, we train multiple instances of a 2-block version of our energy-based model that differ in the number of iterations used when solving for the codes. For recurrent networks such as this, inference is commonly implemented by \u201cunrolling\u201d the network, where the parts of the network structure are repeated with parameters shared across these repeated parts to mimic an iterative algorithm that stops at a fixed number of iterations rather than at some convergence criteria.\nIn Fig. 3, we compare the performance between models that were unrolled zero to four times. We see that there is a difference in performance based on how many sweeps of the variables are made. In terms of the training objective, more unrolling produces models that have lower objective values with convergence after only a few passes. In terms of testing error, however, we see that full code inference is not necessarily better, as unrolling once or twice has lower errors than unrolling three or four times. The biggest difference was between not unrolling and unrolling once, where both the training objective and testing error goes down. The testing error decreases from 0.0131 to 0.0074. While there is a clear benefit in terms of performance for unrolling at least once, there is also a trade-off between performance and computational resource, especially for deeper models."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate the benefits of combining top-down and bottom-up information to produce classspecific features on the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset using a deep version of our EB-SSC. All experiments were performed using MatConvNet (Vedaldi & Lenc, 2015) framework with the ADAM optimizer (Kingma & Ba, 2014). The data was preprocessed and augmented following the procedure in Goodfellow et al. (2013). Specifically, the data was made zero mean and whitened, augmented with horizontal flips (with a 0.5 probability) and random cropping. No weight decay was used, but we used a dropout rate of 0.3 before every convolution layer except for the first. For these experiments we consider a single forward pass (no unrolling)."}, {"heading": "4.1 CLASSIFICATION", "text": "We compare our proposed EB-SSC model to that of Springenberg et al. (2015), which uses rectified linear units (ReLU) as its non-linearity. This model can be viewed as a basic feed-forward version of our proposed model which we take as a baseline. We also consider variants of the baseline model that utilize a subset of architectural features of our proposed model (e.g., concatenated rectified linear units (CReLU) and spherical normalization (SN)) to understand how subtle design changes of the network architecture affects performance.\nWe describe the model architecture in terms of the feature extractor and classifier. Table 1 shows the overall network architecture of feature extractors, which consist of seven convolution blocks and two pooling layers. We test two possible classifiers: a simple linear classifier (LC) and our energy-based classifier (EBC), and use softmax-loss for all models. For linear classifiers, a numerical subscript indicates which of the seven conv blocks of the feature extractor is used for classification (e.g., LC7 indicates the activations out of the last conv block is fed into the linear classifier). For energy-based classifiers, a numerical subscript indicates which conv blocks of the feature extractor are replace with a energy-based classifier (e.g., EBC6\u22127 indicates the activations out of conv5 is fed into the energy-based classifier and the energy-based classifier has a similar architecture to the conv blocks it replaces). The notation differ because for energy-based classifiers, the optimal activations are a function of the hypothesized class label, whereas for linear classifiers, they are not.\nThe results shown in Table 2 compare our proposed model to the baselines ReLU+LC7 (Springenberg et al., 2015) and CReLU+LC7 (Shang et al., 2016), and to intermediate variants. The baseline models all perform very similarly with some small reductions in error rates over the baseline CReLU+LC7. However, CReLU+LC7 reduces the error rate over ReLU+LC7 by more than one percent (from 11.40% to 10.17%), which confirms the claims by Shang et al. (2016) and demonstrates the benefits of splitting positive and negative activations. Likewise, we see further decrease in the error rate (to 9.74%) from using spherical normalization. Though normalizing the activations doesn\u2019t add any capacity to the model, this improved performance is likely because scale-invariant activations makes training easier. On the other hand, further sparsifying the activations yielded no\nbenefit. We tested values \u03b2 = {0.001, 0.01} and found 0.001 to perform better. Replacing the linear classifier with our energy-based classifier further decreases the error rate by another half percent (to 9.23%)."}, {"heading": "4.2 DECODING CLASS-SPECIFIC CODES", "text": "A unique aspect of our model is that it is generative in the sense that each layer is explicitly trying to encode the activation pattern in the prior layer. Similar to the work on deconvolutional networks built on least-squares sparse coding (Zeiler et al., 2010), we can synthesize input images from activations in our spherical coding network by performing repeated deconvolutions (transposed convolutions) back through the network. Since our model is energy based, we can further examine how the topdown information of a hypothesized class effects the intermediate activations.\nThe first column in Fig. 4 visualizes reconstructions of a given input image based on activations from different layers of the model by convolution transpose. In this case we put in zeros for class biases (i.e., no top-down) and are able to recover high fidelity reconstructions of the input. In the remaining columns, we use the same deconvolution pass to construct input space representations of the learned classifier biases. At low levels of the feature hierarchy, these biases are spatially smooth since the receptive fields are small and there is little spatial invariance capture in the activations. At higher levels these class-conditional bias fields become more tightly localized.\nFinally, in Fig. 5 we shows decodings from the conv2 and conv5 layer of the EB-SSC model for a given input under different class hypotheses. Here we subtract out the contribution of the top-down bias term in order to isolate the effect of the class conditioning on the encoding of input features. As visible in the figure, the modulation of the activations focused around particular regions of the image and the differences across class hypotheses becomes more pronounced at higher layers of the network."}, {"heading": "5 CONCLUSION", "text": "We presented an energy-based sparse coding method that efficiently combines cosine similarity, convolutional sparse coding, and linear classification. Our model shows a clear mathematical connection between the activation functions used in CNNs to introduce sparsity and our cosine similarity convolutional sparse coding formulation. Our proposed model outperforms the baseline model and we show which attributes of our model contributes most to the increase in performance. We also demonstrate that our proposed model provides an interesting framework to probe the effects of class-specific coding."}, {"heading": "APPENDIX A", "text": "Here we show that spherical sparse coding (SSC) with a norm constraint on the reconstruction is equivalent to standard convolutional sparse coding (CSC). Expanding the least squares reconstruction error and dropping the constant term \u2016x\u20162 gives the CSC problem:\nmax z 2x\u1d40 ( K\u2211 k=1 dk \u2217 zk ) \u2212 \u2016 K\u2211 k=1 dk \u2217 zk\u201622 \u2212 \u03b2 K\u2211 k=1 \u2016zk\u20161.\nLet = \u2016 \u2211K k=1 dk \u2217 zk\u20162 be the norm of the reconstruction for some code z and let u be the reconstruction scaled to have unit norm so that:\nu =\n\u2211K k=1 dk \u2217 zk\n\u2016 \u2211K k=1 dk \u2217 zk\u20162 = K\u2211 k=1 dk \u2217 z\u0304k with z\u0304 = 1 z\nWe rewrite the least-squares objective in terms of these new variables:\nmax z\u0304, >0 g(z\u0304, ) = max z\u0304, >0\n2x\u1d40 ( u ) \u2212 \u2016 u\u201622 \u2212 \u03b2\u2016 z\u0304\u20161\n= max z\u0304, >0\n2 ( x\u1d40u\u2212 \u03b2\n2 \u2016z\u0304\u20161\n) \u2212 2\nTaking the derivative of g w.r.t. yields the optimal scaling \u2217 as a function of z\u0304:\n(z\u0304)\u2217 = x\u1d40u\u2212 \u03b2 2 \u2016z\u0304\u20161.\nPlugging (z\u0304)\u2217 back into g yields:\nmax z\u0304, >0 g(z\u0304, ) = max z\u0304,\u2016u\u20162=1\n( x\u1d40u\u2212 \u03b2\n2 \u2016z\u0304\u20161\n)2 .\nDiscarding solutions with < 0 can be achieved by simply dropping the square which results in the final constrained problem:\narg max z\u0304 x\u1d40 ( K\u2211 k=1 dk \u2217 z\u0304k ) \u2212 \u03b2 2 K\u2211 k=1 \u2016z\u0304k\u20161\ns.t. \u2016 K\u2211 k=1 dk \u2217 z\u0304k\u20162 \u2264 1."}, {"heading": "APPENDIX B", "text": "We show in this section that coding in the EB-SSC model can be solved efficiently by a combination of convolution, shrinkage and projection, steps which can be implemented with standard libraries on a GPU. For convenience, we first rewrite the objective in terms of cross-correlation rather than convolution (i.e., , x\u1d40(dk \u2217 zk) = (dk ? x)\u1d40zk). For ease of understanding, we first consider the coding problem when there is no classification term.\nz\u2217 = arg max \u2016z\u201622\u22641 v\u1d40z\u2212 \u03b2\u2016z\u20161,\nwhere v = [(d1 ? x)\u1d40, . . . , (dK ? x)\u1d40]\u1d40. Pulling the constraint into the objective, we get its Lagrangian function: L(z, \u03bb) = v\u1d40z\u2212 \u03b2\u2016z\u20161 + \u03bb ( 1\u2212 \u2016z\u201622 ) .\nFrom the partial subderivative of the Lagrangian w.r.t. zi we derive the optimal solution as a function of \u03bb; and from that find the conditions in which the solutions hold, giving us:\nzi(\u03bb) \u2217 =\n1\n2\u03bb \u00b7\n{ vi \u2212 \u03b2 vi > \u03b2\n0 otherwise vi + \u03b2 vi < \u03b2 . (17)\nThis can also be compactly written as:\nz(\u03bb)\u2217 = 1\n2\u03bb z\u0303, (18)\nz\u0303 = s2 v \u2212 \u03b2s\nwhere s = sign(z\u2217) \u2208 {\u22121, 0, 1}|z| and s2 = s s \u2208 {0, 1}|z|. The sign vector of z\u2217 can be determined without knowing \u03bb, as \u03bb is a Lagrangian multiplier for an inequality it must be nonnegative and therefore does not change the sign of the optimal solution. Lastly, we define the squared `2-norm of z\u0303, a result that will be used later:\n\u2016z\u0303\u201622 = z\u0303\u1d40(s2 v)\u2212 \u03b2z\u0303\u1d40s = z\u0303\u1d40v \u2212 \u03b2\u2016z\u0303\u20161. (19)\nSubstituting z(\u03bb)\u2217 back into the Lagrangian we get:\nL(z(\u03bb)\u2217, \u03bb) = 1 2\u03bb v\u1d40z\u0303\u2212 \u03b2 2\u03bb \u2016z\u0303\u20161 + \u03bb\n( 1\u2212 1\n4\u03bb2 \u2016z\u0303\u201622\n) ,\nand the derivative w.r.t. \u03bb is:\n\u2202L(z(\u03bb)\u2217\n\u2202\u03bb = \u2212 1 2\u03bb2 v\u1d40z\u0303 + \u03b2 2\u03bb2 \u2016z\u0303\u20161 + 1 + 1 4\u03bb2 \u2016z\u0303\u201622.\nSetting the derivative equal to zero and using the result from Eq. 19, we can find the optimal solution to \u03bb:\n\u03bb2 = 1 2 z\u0303\u1d40v \u2212 \u03b2 2 \u2016z\u0303\u20161 \u2212 1 4 \u2016z\u0303\u201622 = 1 2 \u2016z\u0303\u201622 \u2212 1 4 \u2016z\u0303\u201622\n=\u21d2 \u03bb\u2217 = 1 2 \u2016z\u0303\u20162.\nFinally, plugging \u03bb\u2217 into Eq. 18 we find the optimal solution\nz\u2217 = z\u0303\n\u2016z\u0303\u20162 . (20)"}], "references": [{"title": "Fast convolutional sparse coding", "author": ["Hilton Bristow", "Anders Eriksson", "Simon Lucey"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bristow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bristow et al\\.", "year": 2013}, {"title": "Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks", "author": ["Chunshui Cao", "Xianming Liu", "Yi Yang", "Yinan Yu", "Jiang Wang", "Zilei Wang", "Yongzhen Huang", "Liang Wang", "Chang Huang", "Wei Xu"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Compressed sensing", "author": ["David L Donoho"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["Michael Elad", "Michal Aharon"], "venue": "IEEE Transactions on Image processing,", "citeRegEx": "Elad and Aharon.,? \\Q2006\\E", "shortCiteRegEx": "Elad and Aharon.", "year": 2006}, {"title": "Learning fast approximations of sparse coding", "author": ["Karol Gregor", "Yann LeCun"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gregor and LeCun.,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Fast and flexible convolutional sparse coding", "author": ["Felix Heide", "Wolfgang Heidrich", "Gordon Wetzstein"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Heide et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heide et al\\.", "year": 2015}, {"title": "Hierarchical discriminative sparse coding via bidirectional connections", "author": ["Zhengping Ji", "Wentao Huang", "G. Kenyon", "L.M.A. Bettencourt"], "venue": "In International Joint Converence on Neural Networks (IJCNN),", "citeRegEx": "Ji et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2011}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent K-SVD", "author": ["Zhuolin Jiang", "Zhe Lin", "Larry S Davis"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Jiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["Koray Kavukcuoglu", "Pierre Sermanet", "Y-Lan Boureau", "Karol Gregor", "Micha\u00ebl Mathieu", "Yann L Cun"], "venue": "In Advances in neural information processing systems (NIPS),", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Hugo Larochelle", "Yoshua Bengio"], "venue": "In International conference on Machine learning (ICML),", "citeRegEx": "Larochelle and Bengio.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio.", "year": 2008}, {"title": "A tutorial on energy-based learning", "author": ["Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Bi-directional representation learning for multi-label classification", "author": ["Xin Li", "Yuhong Guo"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML KDD)", "citeRegEx": "Li and Guo.,? \\Q2014\\E", "shortCiteRegEx": "Li and Guo.", "year": 2014}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A Olshausen", "David J Field"], "venue": "Vision research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Histograms of sparse codes for object detection", "author": ["Xiaofeng Ren", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ren and Ramanan.,? \\Q2013\\E", "shortCiteRegEx": "Ren and Ramanan.", "year": 2013}, {"title": "Sparse coding via thresholding and local competition in neural circuits", "author": ["Christopher J Rozell", "Don H Johnson", "Richard G Baraniuk", "Bruno A Olshausen"], "venue": "Neural computation,", "citeRegEx": "Rozell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rozell et al\\.", "year": 2008}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["Wenling Shang", "Kihyuk Sohn", "Diogo Almeida", "Honglak Lee"], "venue": "In International conference on Machine learning (ICML),", "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "M Riedmiller"], "venue": "In International conference on Learning Representations (ICLR) (workshop track),", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Convolutional networks and learning invariant to homogeneous multiplicative scalings", "author": ["Mark Tygert", "Arthur Szlam", "Soumith Chintala", "Marc\u2019Aurelio Ranzato", "Yuandong Tian", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1506.08230,", "citeRegEx": "Tygert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tygert et al\\.", "year": 2015}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In ACM International Conference on Multimedia,", "citeRegEx": "Vedaldi and Lenc.,? \\Q2015\\E", "shortCiteRegEx": "Vedaldi and Lenc.", "year": 2015}, {"title": "Robust face recognition via sparse representation", "author": ["John Wright", "Allen Y Yang", "Arvind Ganesh", "S Shankar Sastry", "Yi Ma"], "venue": "IEEE transactions on pattern analysis and machine intelligence (TPAMI),", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Fastminimization algorithms for robust face recognition", "author": ["Allen Y Yang", "Zihan Zhou", "Arvind Ganesh Balasubramanian", "S Shankar Sastry", "Yi Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Supervised translation-invariant sparse coding", "author": ["Jianchao Yang", "Kai Yu", "Thomas Huang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Discriminative tensor sparse coding for image classification", "author": ["Yangmuzi Zhang", "Zhuolin Jiang", "Larry S Davis"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Learning inter-related visual dictionary for object recognition", "author": ["Ning Zhou", "Yi Shen", "Jinye Peng", "Jianping Fan"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "This has been a highly successful method that has found its way into many applications, from signal compression and denoising (Donoho, 2006; Elad & Aharon, 2006) to image classification (Wright et al.", "startOffset": 126, "endOffset": 161}, {"referenceID": 21, "context": "This has been a highly successful method that has found its way into many applications, from signal compression and denoising (Donoho, 2006; Elad & Aharon, 2006) to image classification (Wright et al., 2009), to modeling neuronal receptive fields in visual cortex (Olshausen & Field, 1997).", "startOffset": 186, "endOffset": 207}, {"referenceID": 7, "context": "Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).", "startOffset": 282, "endOffset": 398}, {"referenceID": 23, "context": "Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).", "startOffset": 282, "endOffset": 398}, {"referenceID": 6, "context": "Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).", "startOffset": 282, "endOffset": 398}, {"referenceID": 25, "context": "Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).", "startOffset": 282, "endOffset": 398}, {"referenceID": 24, "context": "Since its introduction, subsequent works have brought sparse coding into the supervised learning setting by introducing classification loss terms to the original formulation to encourage features that are not only able to reconstruct the original signal but are also discriminative (Jiang et al., 2011; Yang et al., 2010; Zeiler et al., 2010; Ji et al., 2011; Zhou et al., 2012; Zhang et al., 2013).", "startOffset": 282, "endOffset": 398}, {"referenceID": 0, "context": "Because there is no closed-form solution to this formulation, various iterative optimization techniques are generally used to find a solution (Zeiler et al., 2010; Bristow et al., 2013; Yang et al., 2013; Heide et al., 2015).", "startOffset": 142, "endOffset": 224}, {"referenceID": 22, "context": "Because there is no closed-form solution to this formulation, various iterative optimization techniques are generally used to find a solution (Zeiler et al., 2010; Bristow et al., 2013; Yang et al., 2013; Heide et al., 2015).", "startOffset": 142, "endOffset": 224}, {"referenceID": 5, "context": "Because there is no closed-form solution to this formulation, various iterative optimization techniques are generally used to find a solution (Zeiler et al., 2010; Bristow et al., 2013; Yang et al., 2013; Heide et al., 2015).", "startOffset": 142, "endOffset": 224}, {"referenceID": 12, "context": "Energy-based models capture dependencies between variables using an energy function that measure the compatibility of the configuration of variables (LeCun et al., 2006).", "startOffset": 149, "endOffset": 169}, {"referenceID": 0, "context": ", zK} are convolved with the filters and compared to the input signal (Bristow et al., 2013; Heide et al., 2015; Zeiler et al., 2010).", "startOffset": 70, "endOffset": 133}, {"referenceID": 5, "context": ", zK} are convolved with the filters and compared to the input signal (Bristow et al., 2013; Heide et al., 2015; Zeiler et al., 2010).", "startOffset": 70, "endOffset": 133}, {"referenceID": 18, "context": "The results shown in Table 2 compare our proposed model to the baselines ReLU+LC7 (Springenberg et al., 2015) and CReLU+LC7 (Shang et al.", "startOffset": 82, "endOffset": 109}, {"referenceID": 17, "context": ", 2015) and CReLU+LC7 (Shang et al., 2016), and to intermediate variants.", "startOffset": 22, "endOffset": 42}], "year": 2016, "abstractText": "In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors where reconstruction quality is evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_246"}