{"name": "ICLR_2017_25.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xingyi Li", "Fuxin Li", "Xiaoli Fern"], "emails": ["lixin@eecs.oregonstate.edu", "lif@eecs.oregonstate.edu", "xfern@eecs.oregonstate.edu", "raich@eecs.oregonstate.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, deep learning approaches have seen tremendous successes. They have redefined the state-of-the-art for a variety of challenging domains including computer vision and natural language processing. A particularly appealing property of deep learning is that it eliminates the need for feature engineering on complex inputs such as natural images and raw texts. For the task of image classification, deep convolutional neural networks (CNN) have rapidly become the standard approach. CNN builds upon many sequential convolutional and pooling layers. They are capable of automatically extracting highly discriminative features from raw pixel inputs without prior domain knowledge, leading to impressive performances even surpassing that of humans on the challenging ImageNet dataset (He et al., 2016). One of the key reasons for the success of CNNs is that the network is only connected locally. Traditional CNN designs use small square filters, which have been quite successful in analyzing and classifying natural images. Also, translation invariance in natural images allow the local filter weights to be shared among image locations. These two constructs greatly reduce the number of parameters and make the learning process easier.\nThe success of CNN inspires us to consider the application of CNN to other domains that do not necessarily resemble natural images, and do not have millions of labeled training examples to learn from. Such domains are abundant in natural and biomedical sciences, where the cost of collecting and annotating data can be high and small-sample performance is important. In order to extend CNN to such domains, we start by examining the definition of locality and the use of square filters in CNN. We introduce a novel correlation analysis methodology that analyzes the cross-correlation between neighboring pixels across the whole dataset. A theoretical justification based on Gaussian complexities is provided to demonstrate why such statistics can lead to a natural design methodology of filter shapes for CNN. When applied to natural images, it justifies the use of square filters in image classification tasks.\nWe believe that multiple layers in a deep network do not have to use the same filter shapes since they may capture different types of information. Spectrograms present one such example, where local filters tend to capture variations within one frequency region, while global filters could capture the\nrelationships between different harmonics and syllables (each syllable indicates a single utterance of a bird). We propose a data-driven filter design algorithm that automatically derives multiple levels of different convolutional filter shapes based on the correlation analysis of the data. We believe that the generalization capability of CNNs, especially on small training samples, would improve if those customized data-dependent filter designs are adopted rather than standard square filters, since they capture more of the correlation structure in the data.\nWe evaluate our proposed method in two diverse domains, bioacoustic spectrograms and gene sequence data. Empirical results suggest that by customizing the filter designs, the proposed approach not only significantly outperforms traditional CNN approaches in classification accuracy, but also demonstrates superior robustness to hyperparameter tuning, especially with a small training set size.\nWhile we have only considered two application domains, we expect the contribution to go beyond these specific application domains. In particular, the proposed method is generally applicable to a variety of other data analysis applications where data is scarce and demonstrate spatial autocorrelation patterns that differ from natural images."}, {"heading": "2 CORRELATION ANALYSIS", "text": ""}, {"heading": "2.1 THE CORRELATION ANALYSIS METHODOLOGY AND NATURAL IMAGES", "text": "We focus on examining the use of locality in CNN. It is well-known that locality makes strong intuitive sense in natural images \u2013 the computer vision domain is dependent on building from low-level to high-level features, where the low-level features, such as edges and corners, can be determined within a local neighborhood. Intuitively, local patterns such as edges and corners are discriminative even in a 3\u00d7 3 box in the image, because normally, the values between each pixel and its neighboring pixels are highly correlated. If a pixel is white, it is very likely that all the pixels in its 8-neighborhood are also white. This makes boundaries significant patterns as some of the pixels behave significantly differently from such a strong prior.\nIn other words, we could think of such strong correlation as a natural description of a neighborhood: a CNN neighborhood can be defined as the pixels that have the highest correlations with each pixel. To capture this intuition, we introduce the following correlation analysis procedure:\nLetX = [X1, . . . , XN ] be a dataset withN examples where the dimensions of eachXi, i = 1, . . . , N has an integer lattice structure, i.e., Xi = {Xi(k),k \u2208 Zp} where k is a p-dimensional index vector. Let Xi[k] denote Xi shifted k indices, i.e. Xi[k1] = {Xi(k + k1),k \u2208 Zp} then\nC(k) = \u2211N i=1 cov(Xi[k], Xi)\u2211N i=1 cov(Xi, Xi)\n(1)\nwhere cov(A,B) = \u2211\ni\u2208Zp(A(i) \u2212 \u00b5A)(B(i) \u2212 \u00b5B) is the covariance function, and \u00b5A, \u00b5B is the average value across the corresponding dataset. In practice, we collect the tensor only with k \u2208 Rp \u2282 Zp with R being a short range (e.g. {\u221250, 50}). We also exclude data that represent obvious noise in the computation, such as background white noise in the spectrogram data. These white noise do not contain signals yet show strong correlation within local regions, hence can bias C towards a regular ball shape.\nFig. 1 shows the results of such correlation analysis on 1, 000 images from the PASCAL VOC dataset (Everingham et al.). It can be seen that each pixel is correlated the most with the pixels in its 4-neighborhood, followed by ones in its 8-neighborhood and bigger. This corresponds well with the common filter designs in natural images such as 3\u00d7 3 (Simonyan & Zisserman, 2014), 5\u00d7 5 (Szegedy et al., 2014) and 7\u00d7 7 (Krizhevsky et al., 2012)."}, {"heading": "2.2 THEORETICAL JUSTIFICATION OF THE CORRELATION ANALYSIS APPROACH", "text": "The insight we developed in the previous section has justifications from machine learning theory. The capability of generalization from neural networks can be characterized using Rademacher complexities (Bartlett & Mendelson, 2002):\nTheorem 1. (Theorem 18, (Bartlett & Mendelson, 2002)) Suppose that \u03c3 : R 7\u2192 [\u22121, 1] has Lipschitz constant L and satisfies \u03c3(0) = 0. Define the class computed by a two-layer neural network with 1-norm weight constraints as:\nF = { x 7\u2192\n\u2211 i vi\u03c3(wi \u00b7 x) : \u2016v\u20161 \u2264 1, \u2016wi\u20161 \u2264 B\n} . (2)\nFor any x1, . . . ,xN \u2208 Rd, we have\nG\u0302N (F ) \u2264 cLB(ln d)1/2\nN max j,j\u2032 \u221a\u221a\u221a\u221a N\u2211 i=1 (xij \u2212 xij\u2032)2, (3)\nwhere G\u0302N (F ) = E [ supf\u2208F 2 N \u2211N i=1 gif(xi) ] is the empirical Gaussian complexity of the function class F , with gi being independent standard normal random variables. The Gaussian complexity (and the closely-related Rademacher complexity) measures the capacity of the function class and is linked to the generalization capability of the learner. The learner from a function class F with a smaller Gaussian complexity is potentially able to generalize better (Bartlett & Mendelson, 2002), but has less capacity to fit the training data well. Thus a trade-off is required for practical learners.\nConvolutional neural networks are neural networks with locallydefined connections hence similar results as above hold for CNNs, with the change that maxj,j\u2032 is now defined within the range of each single filter, as well as a change from the L1 norm to the L2 norm: Theorem 2. Suppose that \u03c3 : R 7\u2192 R is a contraction mapping.\nDefine the class computed by a two-layer convolutional neural network with one convolutional layer and one fully-connected layer with 2-norm weight constraints as:\nF = { x 7\u2192\n\u2211 i vi\u03c3(wi \u2217 x) : \u2016v\u201622 \u2264 1, \u2016w\u20161 \u2264 B\n} . (4)\nFor any x1, . . . ,xN \u2208 Rd, we have\nG\u0302N (F ) \u2264 cB(ln d)1/2\nN max j\u2212j\u2032\u2208N \u221a\u221a\u221a\u221a N\u2211 i=1 \u2016xi(j)\u2212 xi(j\u2032)\u20162, (5)\nwhere N \u2282 Zp defines the shape of the convolution filter, i.e. (wi \u2217 x) (k) = \u2211 j\u2208N wi,jx[k](j) (6)\nThe proof can be found in the appendix.\nThis result suggests that in order to minimize G\u0302N (F ) of the learner, one needs to select a convolutional filter that minimizes maxj\u2208N ,j\u2032\u2208N \u221a\u2211N\ni=1 \u2016xi(j)\u2212 xi(j\u2032)\u20162. This requires to select only dimensions that are highly correlated to each other (ones that maximize cov(xi(j),xi(j\u2032))) and avoid having dimensions that are less correlated in the same convolutional filter. In natural images, this justifies selecting 3\u00d7 3, 5\u00d7 5 or 7\u00d7 7 squares in natural images since those are neighborhoods that maximize the correlation within them."}, {"heading": "3 CUSTOMIZE THE SHAPE OF CNN FILTERS", "text": ""}, {"heading": "3.1 FILTER SHAPING METHODOLOGY", "text": "Our intuition for filter design is that the filters for a CNN framework should be capable of representing the correlation image obtained from the covariance analysis, hence trading-off between learner\ncapacity and its Gaussian complexity. If the CNN filter allows data dimensions with little correlation in the same filter, it will make the bound in (5) worse, hence increasing the Gaussian complexity and negatively impact generalization, especially in small-sample scenarios.\nIn one layer, the solution can be obtained via the following LASSO optimization problem:\nmin f1 \u2016C\u2212 f1 \u2217C0\u20162F + \u03bb||f1||1 (7)\nwhere C is the correlation tensor computed from the analysis in (1), C0 is an initial correlation tensor with a 1 at the center location and 0s at all other locations, and \u03bb is a nonnegative regularization parameter trading off the representation error with the sparsity of the filters.\nThe LASSO optimization will attempt to convolve the C0 tensor so that it is most similar to C. With this setup, the solution would be selecting the locations with the highest correlations to the center location, i.e. finding the \u2206js maximizing \u2211N i=1 xi(j + \u2206j)xi(j), directly corresponding to the theorem. By controlling \u03bb, one can control how many nonzeros will be in f1. Since CNN is more appropriate than the unsupervised LASSO for learning the exact filter weights, we will not keep the weights learned by the LASSO. Instead, only the shape of f1 will be used, in the sense that only the locations with values in f1 that are above a threshold will be accepted in the shape of the filter, and the rest will have their weights fixed to 0.\nWith more than one layer, the theory is less clear about what would be the shape of the convolutional filter. We propose a simple approach that repeatedly uses the previous convolution result to reconstruct C. We assume that the next layers CNN filter fi convolve with fi\u22121 \u2217 (fi\u22122 . . . f1(C0)) would be more similar to the correlation tensor. Hence, we can solve again the same LASSO problem to approximate the full correlation tensor. Repeatedly applying this would amount to solve:\nmin fi \u2016C\u2212 fi \u2217Ci\u22121\u20162F + \u03bb \u2016fi\u20161 (8)\nwhere Ci\u22121 = fi\u22121 \u2217 (fi\u22122 . . . f1(C0)) is the solution from all the previous filters. The filter shape is then determined by the same approach as before. With appropriate parameters, this methodology can fully recover multiple levels of 3\u00d7 3 squared filters in natural images."}, {"heading": "3.2 FILTER SHAPING FOR DISCRETE DATA", "text": "The above theory and computation works for continuous data. For discrete data where the minus operator is not defined, we could use an analogous approach to (8) with group LASSO instead of LASSO. Suppose again the dimensions of each Xi has an integer lattice structure and each element comes from a categorical distribution, i.e. Xi(k) \u2208 {1, . . . ,K},k \u2208 Zp. In this case, the discrete correlation tensor is defined as\nC(A,B,k) = Ei\u2208{1,...,N},j\u2208Zp [I(Xi[k](j) = A)I(Xi(j) = B)]\u2212 P (A)P (B)\u221a\nP (A)(1\u2212 P (A))P (B)(1\u2212 P (B)) (9)\nwhere P (A) is the probability of any entry in the dataset attaining a value of A.\nAfter collecting a similar covariance tensor as in the continuous case, one can solve a group LASSO problem instead of a LASSO problem in order to handle the discrete entries in the covariance:\nmin fi \u2016C\u2212 fi \u2217Ci\u22121\u20162F + \u03bb\u2016f\u20162,1 (10)\nwhere fi is a K \u00d7K \u00d7Zp filter, \u2016f\u20162,1 = \u2211\nj\u2208Zp \u221a\u2211K A=1 \u2211K B=1 f(A,B, j)\n2 regularizes on all the entries at position j. The optimization allows the optimization to set all entries f(A,B, j) to 0 for a certain j, which is then equivalent with filter shaping in the continuous case. In the discrete case, the optimization starts with C0(A,B,0) = 1,\u2200A = B \u2208 {1, . . . ,K} and other entries equal to 0."}, {"heading": "3.3 ACCOUNTING FOR MAX-POOLING BY AVERAGE POOLING", "text": "Max-pooling layers are essential to the performance of deep neural networks. Therefore, we need an approach to account for max-pooling layers in our filter shaping algorithm. However, directly\nmax-pooling on the solved correlation will destroy the symmetry of the Ci matrix. Therefore, as a compromise we utilize average pooling in C instead of max-pooling. Suppose a max-pooling layer is after fi. We will resize both Ci and C according to the max-pooling size reduction, and populate the values with bilinear interpolation (average over all the items that max-pooling is supposed to consider). One potential justification of using average pooling instead of max is that, each small box considered by max pooling in each example may attain the maximal value at a separate location. One can assume that the chance of each location attaining the max is equal, hence the expectation of max-pooling will be average pooling to resize Ci and C."}, {"heading": "4 PROBLEM DOMAINS", "text": ""}, {"heading": "4.1 BIO-ACOUSTICS", "text": "One special problem domain we consider is bird bio-acoustics, where the images we analyze are spectrograms. A spectrogram is a 2-dimensional image where the x-axis indicates time and the y-axis indicates frequency. The brighter a \u201cpixel\u201d is, the higher the energy at this time and frequency. Spectrograms are commonly used for sound classification (Stowell & Plumbley, 2011) (Ranjard & Ross, 2008) (Lasseck, 2013). Fig. 2 shows an example spectrogram of a recording of hummingbird wingbeats. Our goal is to predict the species of the bird based on in-situ recording of wingbeats (for hummingbirds) or bird-songs (for song birds). Consider Fig. 2(a), we notice that the wingbeats are reflected as a collection of bright wavy lines at the low frequency range. The lines are regularly spaced, revealing a clear harmonic structure. From the figure, it is reasonable to conclude that the pixels that are horizontally closer to each other tend to be more similar when viewed on the local scale. On a global scale, there are strong correlations between harmonics even they are separated about 25Hz apart. Such correlation patterns appear to be highly different from natural images.\nWe performed the correlation analysis on wingbeat spectrograms and bird-song spectrograms separately based on formula 1. Fig. 2(c) and 2(d) are resultant correlation images for wingbeats and bird-song respectively. One can see significant differences w.r.t. the pattern in natural images."}, {"heading": "4.2 GENE SEQUENCE ANALYSIS: RSNP PREDICTION", "text": "In this domain, our data are gene sequences, each describing a genetic variation in human genome called Single-Nucleotide Polymorphism (SNP). Human genome contains millions of SNPs and most of them reside in non-coding region of the genome and there is limited biological knowledge about their functionality. There is great interest in the biomedical community to use machine learning to help identify what non-coding SNPs are regulatory. Given a database of SNPs that have been annotated by experts based on prior biological knowledge as regulatory or non-regulatory, the goal is to learn a model to predict whether a SNP serves any regulatory function based on the gene sequence around it. Each SNP is represented as a 2001 base-pair gene sequence centered at the SNP location.\nBecause gene sequences are discrete data, we use formula 9 to calculate the correlations. The computed Cgene is shown in Fig. 3. It indicates that there are strong correlations in every other column near the center of the gene covariance. Those patters will disappear after the first pooling layer. In other words, the customized filter CNN only differ with the baseline model in the first convolutional layer.\n5 RELATED WORK\nThere has been significant work on CNN network design.For example, AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2014) and ResNet (He et al., 2016) all involved significant redesign process of the network structure. However, they all work on natural images hence their filter designs are mainly selecting the size for the square filters. Xie et al. (2016) proposed a customized shape for CNN filters by making a 3 by 3 hole in a 7 by 7 convolutional filter to tackle the structured labeling problems that focus on modeling object and context separately.\nThis specific filter shape is designed manually and not learned from the data. (Bruna et al., 2014; Henaff et al., 2015) explored defining CNN on a graph via a hierarchical clustering approach and a harmonic analysis approach. Their approach can be used to recover a CNN on a lattice, however its sparsity is mostly in the frequency domain, and usually cannot achieve sparsity in the original domain as our approach. (Wen et al., 2016) adopted group lasso methodology to make the convolutional layer sparse. Our experiments showed that L1 regularization could improve the performance due to the sparsity but not as significant as our proposed model.\nConvolutional neural networks are suitable for a wide range of application problem domains beyond natural images. Kalchbrenner et al. (2014) proposed a Dynamic Convolutional Neural Network (DCNN) on the problems of semantic modelling of sentences, which reduced at least 25% error reduction comparing with the strongest baseline framework. Also in the domain of Natural Language Processing (NLP), Ma et al. (2015) built a dependency tree between words within a sentence in order to capture the words that highly correlated with each other but located further apart within the sentence. Instead of concatenating successive words within a sequence, they concatenated words based on the dependency tree which redefined the state-of-the-art performance in both sentiment analysis and question classification. Shen et al. (2014) used convolutional neural network to learn semantic representations for search queries and Web documents, and the proposed frame work outperforms the previous state-of-the-art by a significant margin. Zbontar & LeCun (2015) successfully applied convolutional neural network to predict the degree that two image patches match for stereo matching. CNNs have also seen tremendous success in analyzing human speech (Abdel-Hamid et al. (2014)). Recently, there are increasing interest in applying CNNs for analyzing bird songs for the species prediction problem (Goe\u0308au et al., 2016). But to the best of our knowledge, there has been no prior study that focuses on classifying hummingbird species based on the sound of wingbeats.\nIn R-SNP prediction for gene sequences, a related work that we know of utilizes CNN is by Zhou & Troyanskaya (2015). They use CNN to extract high level features from gene sequences based on relevant domain knowledge in order to perform functional-variant predictions. We aim to predict the R-SNP directly from sequence-based data without integrating any domain knowledge.\nIn other related work, (Zhang & LeCun, 2015) uses the similar Rademacher complexity as in this work to characterize the sample complexity of CNN, but focus on a different research problem that creates an abstain option for the classifiers."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 FILTER SHAPES AND NETWORK DESIGN", "text": "Experiments are conducted on three different tasks, two in the bird-bioacoustics domain and one for sequence-based rSNP prediction.\nThe first task is to identify the species of hummingbirds based on recorded wingbeats. This dataset contains 434 labeled recordings from a total of 16 species, where each recording is 20\u2212 30 seconds long. Since the wingbeat signals almost exclusively reside in the low frequency range of the spectrogram, we only consider the frequency range 0Hz \u2212 300Hz. Note that although every\nrecording contains some wingbeats, the signal may only occupy part of the recording. We apply a pre-processing step to extract wingbeat regions with high energy in the low frequency range. We then apply a sliding window to the resulting spectrograms and each window is considered as a separate instance. This process results in a total of 5922 labeled instances.\nFor the second task, our goal is to recognize the species of song-bird based on recorded bird songs. In this case, our data contains a total of 122 segments that have been manually extracted from spectrograms and are labeled with species. Each segment is a spectrogram in a short interval which contains some vocalization from a single bird and is labeled with its species. Similarly, a sliding window is applied to the segment and each window is treated as a separate instance for classification. In total we have 14 species and 1177 labeled instances.\nThe last task is to predict whether a non-coding Single-Nucleotide Polymorphism (SNP) serves any regulatory function. Our data contains 4300 SNPs that have been annotated as either positive (serving regulatory function) or negative (no known regulatory function). The data contains roughtly the same number of positive and negative instances.\nThe covariance analysis procedure is applied to all datasets. We do not directly work with the regularization parameter \u03bb in (7). Instead, we specify the maximum number of nonzero elements (namely DFMax) in the Lasso solution. For all datasets, we selected the DFmax values from 9, 11 and 13, and picked the smallest value that allowed for non-degenerate filters across all layers. The selected value is 13 for the wingbeat data, 9 for the bird-song data and the gene sequencing data. A stability study for a wider range of DFMax values (3-15) was conducted and the results were shown in the appendix. Fig. 4 shows the filter shapes learned from the wingbeat data where eight convolutional layers are used. The first 4 filters primarily capture local horizontal patterns, whereas the last 4 filters capture more of the global harmonic structure in the data. More details and filter shapes in the other two datasets can be found in the appendix."}, {"heading": "6.2 WINGBEATS CLASSIFICATION RESULTS", "text": "We consider three baselines: 3\u00d7 3, 3\u00d7 5, 5\u00d7 5 convolutional filters. These are selected since 3\u00d7 3 filter have roughly equal number of parameters compared to our customized filters, and 3\u00d7 5 covers the domain of the customized filter in the first 3 levels. In the baseline of 5\u00d7 5 filters, we added L1 regularization on the weights, in order to test whether the same effect of filter shaping can be learned directly from sparsity priors on the weights. The swipe range for the L1 regularization parameter is from 10\u22123 to 10\u221210, and we selected the one that generated best performance. 5-fold cross-validation is conducted at the recording level. Thus, no validation example comes from the same recording as any of the training examples. For training, we set the batch size to 20 for all designs and the learning\nrate to 10\u22124 for the customized filters and 10\u22125 and 10\u22126 for the 3\u00d7 3 and 3\u00d7 5 filters respectively. Fig. 5(a) shows the cross-validated performance as a function of training epochs. As can be seen from the figures that the cross-validation accuracy of our customized filter significantly outperforms that of the traditional filters, and L1 regularization cannot achieve the same effect as filter shaping.\nIn order to further illustrate the robustness of our model in small-sample situations, we conducted an experiment with half of the instances from each recording, resulting 2961 instances in total. As shown in Fig. 5(b), the performance gap between the customized filter and in 3\u00d7 3 widened significantly. More results on parameter sensitivity can be found in the appendix."}, {"heading": "6.3 BIRD SONG CLASSIFICATION RESULTS", "text": "For the bird song dataset, we compare to a baseline of 3\u00d7 3 filter as well as a 5\u00d7 5 filter with L1 regularization, and the swipe range for the L1 regularization parameter is from 10\u22123 to 10\u221210 as well. 5-fold recording-level cross validation is performed and the averaged cross-validation accuracies are plotted in Fig. 6(a) as a function of the number of training epochs. From the figures, we observe that the customized filter achieves ( 5%) higher validation accuracy compared to the 3\u00d7 3 filter. L1 regularization on the weights negatively impacted performance for the 5\u00d7 5 filter, perhaps because that there are not enough data to learn the sparsity structure of the weights."}, {"heading": "6.4 RSNP PREDICTION RESULTS", "text": "For the gene dataset, we compare with a baseline that is similar with the framework used by Zhou & Troyanskaya (2015), which contains several layers of CNN with 1 \u00d7 9 shape filters. 5-fold crossvalidation was performed and the results are shown in Fig. 6(b). The customized model was 0.8% better than the baseline. The performance gap is not as significant as in the bird-bioacoustics data, mainly because that the solved filters are not different with traditional ones after max-pooling (see\nappendix for details). Due to the fact that we apply no domain knowledge, our performance may not be comparable with some recent papers which have used additional domain knowledge. However, it demonstrates the strength of the customized filter design over traditional ones."}, {"heading": "7 CONCLUSION", "text": "In this paper, we introduced a theoretically justified correlation analysis approach to reveal the neighborhood structure in image-like domains. We proposed an algorithm that automatically designs multiple levels of filters in a CNN by solving repeated lasso problems. Experiments show that our approach significantly outperforms traditional fixed-size filters and exhibit higher robustness to parameters, especially on smaller datasets. In future work we would like to extend this methodology to other domains such as computational astronomy and biomedical imaging, as well as work on the theory in the discrete and multi-layer cases."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is partially supported by the National Science Foundation grants IIS-1464371, CCF1254218, and DBI-1356792 and IIS-1055113."}, {"heading": "A PROOF OF THEOREM", "text": "The proof of the theorem follows from various Slepian-type variance comparison theorems. We state a couple of them here without proof. For proofs see Bartlett & Mendelson (2002); Ledoux & Talagrand (1991).\nTheorem 3. Ledoux & Talagrand (1991) Let X and Y be Gaussian random vectors in Rd such that for every i, j, E|Yi \u2212 Yj |2 \u2264 E|Xi \u2212Xj |2 (11) , then for every non-negative convex increasing function F on R+,\nEF (max i,j |Yi \u2212 Yj |) \u2264 EF (max i,j |Xi \u2212Xj |). (12)\nLemma 1. Bartlett & Mendelson (2002) For x \u2208 Rd, define\nF1 = {x 7\u2192 w>x : w \u2208 Rd, \u2016w\u20161 \u2264 1}.\nwhere \u00b7 represents an inner product. For any x1, . . . ,xn \u2208 Rd, we have\nG\u0302n(F1) \u2264 c\nn (ln d)1/2 max j,j\u2032\n( n\u2211\ni=1\n(xij \u2212 xij\u2032)2 )1/2 . (13)\nwhere G\u0302n(F ) is the Gaussian complexity of the function class F , c is a constant.\nThis lemma characterizes the Gaussian complexity of simple inner product functions. Convolutional operators are also inner product functions, albeit they share weights over the entire domain (image) they are defined. Hence, this lemma can be used to bound the Gaussian complexity of each individual convolutional filter. In the proof, we convert the result of the convolutional operation into a number of inner product operations, and utilize Lemma 1 to bound the Gaussian complexity of each inner product operation.\nProof. (of Theorem ) Without loss of generality, the result of the convolution operator with parameters W = [w1, . . . ,wD] \u2217xi can be written as\nfW(xi) = W \u2217 xi = [ w1 \u00b7 xi1,w1 \u00b7 xi2, . . . ,w1 \u00b7 xim, . . . ,wD \u00b7 xi1,wD \u00b7 xi2 . . .wD \u00b7 xim ] (14) where xij represents a part in xi equivalent to the size of the convolutional filter and m represents the number of such parts (size of the image). Let\nXvW = N\u2211 i=1 gi (v \u00b7 \u03c3(fW(xi))) (15)\nwhere gi \u223c N (0, 1) are Gaussian random variables, so that\n\u2016XvW \u2212Xv,W\u2032\u20162 \u2264 \u2016v\u20162\u2016 N\u2211 i=1 gi\u03c3(fW)\u2212 N\u2211 i=1 gi\u03c3(fW\u2032)\u20162 (16)\n\u2264 \u2016v\u20162 D\u2211\nk=1 \u2016wk \u00b7 N\u2211 i=1 gi m\u2211 j=1 xij \u2212w\u2032k \u00b7 N\u2211 i=1 gi m\u2211 j=1 xij\u20162 (17)\nThe first inequality is Cauchy-Schwarz, and the second inequality is because \u03c3 is a contraction mapping Ledoux & Talagrand (1991).\nNow, if we define Ywk = wk \u00b7 \u2211N i=1 gi \u2211m j=1 x ij then according to Slepian\u2019s lemma Bartlett & Mendelson (2002); Ledoux & Talagrand (1991) there exist constants c and B so that\nE[supv,W(Xvw)] \u2264 cB \u2211k i=1 E[supf (Ywk)], and according to Lemma 1,\nE[sup f\n(Ywk)] \u2264 c\nN (ln d)1/2 max t,t\u2032  N\u2211 i=1  m\u2211 j=1 xijt \u2212 m\u2211 j=1 xijt\u2032 2  1/2\n(18)\nPutting those together and noting G\u0302N (F ) = E[supv,WXvW] (F as defined in Theorem 2) finishes the proof of the theorem."}, {"heading": "B FILTER SHAPES AND NETWORK STRUCTURE FOR BIRDSONG AND GENE EXPRESSION", "text": "Fig. 4 and Fig. 7 show the filter shapes that are learned from the covariance images of the wingbeat data and the birdsong data respectively.\nSimilarly, for the birdsong data, filters in Fig.7(1) to 7(4) are long in the vertical direction and capture local vertical patterns, whereas filters in Fig.7(5) to 7(8) are stretched along the horizontal direction, which accounts for longer temporal patterns that occur at a more global scale.\nTables 1, 2 and 3 describes the network structure for the wingbeats, birdsong and gene data, respectively. Every convolutional layer is followed by a ReLU layer."}, {"heading": "C SENSITIVITY TEST ON PARAMETERS", "text": ""}, {"heading": "D DFMAX STABILITY", "text": ""}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2014}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bruna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2014}, {"title": "The pascal visual object classes challenge", "author": ["M. Everingham", "L. Van Gool", "Chris Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2012}, {"title": "Lifeclef bird identification task 2016: The arrival of deep learning", "author": ["Herv\u00e9 Go\u00ebau", "Herv\u00e9 Glotin", "Willem-Pier Vellinga", "Robert Planqu\u00e9", "Alexis Joly"], "venue": "In Working Notes of CLEF 2016-Conference and Labs of the Evaluation forum,", "citeRegEx": "Go\u00ebau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Go\u00ebau et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep convolutional networks on graph-structured data", "author": ["Mikael Henaff", "Joan Bruna", "Yann LeCun"], "venue": "arXiv preprint arXiv:1506.05163,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Bird song classification in field recordings: winning solution for nips4b 2013 competition", "author": ["Mario Lasseck"], "venue": "In Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod. org/nips4b, joint to NIPS,", "citeRegEx": "Lasseck.,? \\Q2013\\E", "shortCiteRegEx": "Lasseck.", "year": 2013}, {"title": "Isoperimetry and Processes in Probability in Banach Spaces", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1507.01839,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Unsupervised bird song syllable classification using evolving neural networks", "author": ["Louis Ranjard", "Howard A Ross"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Ranjard and Ross.,? \\Q2008\\E", "shortCiteRegEx": "Ranjard and Ross.", "year": 2008}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Birdsong and c4dm: A survey of uk birdsong and machine recognition for music researchers", "author": ["Dan Stowell", "Mark D Plumbley"], "venue": null, "citeRegEx": "Stowell and Plumbley.,? \\Q2011\\E", "shortCiteRegEx": "Stowell and Plumbley.", "year": 2011}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Convolutional pseudo-prior for structured labeling", "author": ["Saining Xie", "Xun Huang", "Zhuowen Tu"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Computing the stereo matching cost with a convolutional neural network", "author": ["Jure Zbontar", "Yann LeCun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zbontar and LeCun.,? \\Q2015\\E", "shortCiteRegEx": "Zbontar and LeCun.", "year": 2015}, {"title": "Universum prescription: Regularization using unlabeled data", "author": ["Xiang Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.03719,", "citeRegEx": "Zhang and LeCun.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}, {"title": "Predicting effects of noncoding variants with deep learning-based sequence model", "author": ["Jian Zhou", "Olga G Troyanskaya"], "venue": "Nature methods,", "citeRegEx": "Zhou and Troyanskaya.,? \\Q2015\\E", "shortCiteRegEx": "Zhou and Troyanskaya.", "year": 2015}, {"title": "Theorem 3", "author": ["Talagrand"], "venue": "Ledoux & Talagrand (1991) Let X and Y be Gaussian random vectors in R such that for every i, j,", "citeRegEx": "Talagrand,? 1991", "shortCiteRegEx": "Talagrand", "year": 1991}], "referenceMentions": [{"referenceID": 5, "context": "They are capable of automatically extracting highly discriminative features from raw pixel inputs without prior domain knowledge, leading to impressive performances even surpassing that of humans on the challenging ImageNet dataset (He et al., 2016).", "startOffset": 232, "endOffset": 249}, {"referenceID": 9, "context": "Spectrograms are commonly used for sound classification (Stowell & Plumbley, 2011) (Ranjard & Ross, 2008) (Lasseck, 2013).", "startOffset": 106, "endOffset": 121}, {"referenceID": 8, "context": "For example, AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 5, "context": ", 2014) and ResNet (He et al., 2016) all involved significant redesign process of the network structure.", "startOffset": 19, "endOffset": 36}, {"referenceID": 2, "context": "(Bruna et al., 2014; Henaff et al., 2015) explored defining CNN on a graph via a hierarchical clustering approach and a harmonic analysis approach.", "startOffset": 0, "endOffset": 41}, {"referenceID": 6, "context": "(Bruna et al., 2014; Henaff et al., 2015) explored defining CNN on a graph via a hierarchical clustering approach and a harmonic analysis approach.", "startOffset": 0, "endOffset": 41}, {"referenceID": 16, "context": "(Wen et al., 2016) adopted group lasso methodology to make the convolutional layer sparse.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Recently, there are increasing interest in applying CNNs for analyzing bird songs for the species prediction problem (Go\u00ebau et al., 2016).", "startOffset": 117, "endOffset": 137}], "year": 2017, "abstractText": "Convolutional neural networks (CNNs) are powerful tools for classification of visual inputs. An important property of CNN is its restriction to local connections and sharing of local weights among different locations. In this paper, we consider the definition of appropriate local neighborhoods in CNN.We provide a theoretical analysis that justifies the traditional square filter used in CNN for analyzing natural images. The analysis also provides a principle for designing customized filter shapes for application domains that do not resemble natural images. We propose an approach that automatically designs multiple layers of different customized filter shapes by repeatedly solving lasso problems. It is applied to customize the filter shape for both bioacoustic applications and gene sequence analysis applications. In those domains with small sample sizes we demonstrate that the customized filters achieve superior classification accuracy, improved convergence behavior in training and reduced sensitivity to hyperparameters.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_25"}