{"name": "ICLR_2017_26.pdf", "metadata": {"source": "CRF", "title": "EXTENDING NETWORK NORMALIZATION SCHEMES", "authors": ["Mengye Ren", "Renjie Liao", "Raquel Urtasun", "Fabian H. Sinz", "Richard S. Zemel"], "emails": ["urtasun}@cs.toronto.edu", "fabian.sinz@epagoge.de,", "zemel@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Standard deep neural networks are difficult to train. Even with non-saturating activation functions such as ReLUs (Krizhevsky et al., 2012), gradient vanishing or explosion can still occur, since the Jacobian gets multiplied by the input activation of every layer. In AlexNet (Krizhevsky et al., 2012), for instance, the intermediate activations can differ by several orders of magnitude. Tuning hyperparameters governing weight initialization, learning rates, and various forms of regularization thus become crucial in optimizing performance.\nIn current neural networks, normalization abounds. One technique that has rapidly become a standard is batch normalization (BN) in which the activations are normalized by the mean and standard deviation of the training mini-batch (Ioffe & Szegedy, 2015). At inference time, the activations are normalized by the mean and standard deviation of the full dataset. A more recent variant, layer normalization (LN), utilizes the combined activities of all units within a layer as the normalizer (Ba et al., 2016). Both of these methods have been shown to ameliorate training difficulties caused by poor initialization, and help gradient flow in deeper models.\nA less-explored form of normalization is divisive normalization (DN) (Heeger, 1992), in which a neuron\u2019s activity is normalized by its neighbors within a layer. This type of normalization is a well established canonical computation of the brain (Carandini & Heeger, 2012) and has been extensively studied in computational neuroscience and natural image modelling (see Section 2). However, with few exceptions (Jarrett et al., 2009; Krizhevsky et al., 2012) it has received little attention in conventional supervised deep learning.\nHere, we provide a unifying view of the different normalization approaches by characterizing them as the same transformation but along different dimensions of a tensor, including normalization across\n\u2217indicates equal contribution\nexamples, layers in the network, filters in a layer, or instances of a filter response. We explore the effect of these varieties of normalizations in conjunction with regularization, on the prediction performance compared to baseline models. The paper thus provides the first study of divisive normalization in a range of neural network architectures, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and tasks such as image classification, language modeling and image super-resolution. We find that DN can achieve results on par with BN in CNN networks and out-performs it in RNNs and super-resolution, without having to store batch statistics. We show that casting LN as a form of DN by incorporating a smoothing parameter leads to significant gains, in both CNNs and RNNs. We also find advantages in performance and stability by being able to drive learning with higher learning rate in RNNs using DN. Finally, we demonstrate that adding an L1 regularizer on the activations before normalization is beneficial for all forms of normalization."}, {"heading": "2 RELATED WORK", "text": "In this section we first review related work on normalization, followed by a brief description of regularization in neural networks."}, {"heading": "2.1 NORMALIZATION", "text": "Normalization of data prior to training has a long history in machine learning. For instance, local contrast normalization used to be a standard effective tool in vision problems (Pinto et al., 2008; Jarrett et al., 2009; Sermanet et al., 2012; Le, 2013). However, until recently, normalization was usually not part of the machine learning algorithm itself. Two notable exceptions are the original AlexNet by Krizhevsky et al. (2012) which includes a divisive normalization step over a subset of features after ReLU at each pixel location, and the work by Jarrett et al. (2009) who demonstrated that a combination of nonlinearities, normalization and pooling improves object recognition in two-stage networks.\nRecently Ioffe & Szegedy (2015) demonstrated that standardizing the activations of the summed inputs of neurons over training batches can substantially decrease training time in deep neural networks. To avoid covariate shift, where the weight gradients in one layer are highly dependent on previous layer outputs, Batch Normalization (BN) rescales the summed inputs according to their variances under the distribution of the mini-batch data. Specifically, if zj,n denotes the activation of a neuron j on example n, and B(n) denotes the mini-batch of examples that contains n, then BN computes an affine function of the activations standardized over each mini-batch:\nz\u0303n,j = \u03b3 zn,j \u2212 E[zj ]\u221a 1\n|B(n)| (zn,j \u2212 E[zj ])2 + \u03b2 E[zj ] =\n1 |B(n)| \u2211\nm\u2208B(n)\nzm,j\nHowever, training performance in Batch Normalization strongly depends on the quality of the aquired statistics and, therefore, the size of the mini-batch. Hence, Batch Normalization is harder to apply in cases for which the batch sizes are small, such as online learning or data parallelism. While classification networks can usually employ relatively larger mini-batches, other applications such as image segmentation with convolutional nets use smaller batches and suffer from degraded performance. Moreover, application to recurrent neural networks (RNNs) is not straightforward and leads to poor performance (Laurent et al., 2015).\nSeveral approaches have been proposed to make Batch Normalization applicable to RNNs. Cooijmans et al. (2016) and Liao & Poggio (2016) collect separate batch statistics for each time step. However, neither of this techniques address the problem of small batch sizes and it is unclear how to generalize them to unseen time steps.\nMore recently, Ba et al. (2016) proposed Layer Normalization (LN), where the activations are normalized across all summed inputs within a layer instead of within a batch:\nz\u0303n,j = \u03b3 zn,j \u2212 E[zn]\u221a 1\n|L(j)| (zn,j \u2212 E[zn])2 + \u03b2 E[zn] =\n1 |L(j)| \u2211\nk\u2208L(j)\nzn,k\nwhere L(j) contains all of the units in the same layer as j. While promising results have been shown on RNN benchmarks, direct application of layer normalization to convolutional layers often leads to\na degradation of performance. The authors hypothesize that since the statistics in convolutional layers can vary quite a bit spatially, normalization with statistics from an entire layer might be suboptimal.\nUlyanov et al. (2016) proposed to normalize each example on spatial dimensions but not on channel dimension, and was shown to be effective on image style transfer applications (Gatys et al., 2016).\nLiao et al. (2016a) proposed to accumulate the normalization statistics over the entire training phase, and showed that this can speed up training in recurrent and online learning without a deteriorating effect on the performance. Since gradients cannot be backpropagated through this normalization operation, the authors use running statistics of the gradients instead.\nExploring the normalization of weights instead of activations, Salimans & Kingma (2016) proposed a reparametrization of the weights into a scale independent representation and demonstrated that this can speed up training time.\nDivisive Normalization (DN) on the other hand modulates the neural activity by the activity of a pool of neighboring neurons (Heeger, 1992; Bonds, 1989). DN is one of the most well studied and widely found transformations in real neural systems, and thus has been called a canonical computation of the brain (Carandini & Heeger, 2012). While the exact form of the transformation can differ, all formulations model the response of a neuron z\u0303j as a ratio between the acitivity in a summation field Aj , and a norm-like function of the suppression field Bj\nz\u0303j = \u03b3 \u2211 zi\u2208Aj uizi(\n\u03c32 + \u2211\nzk\u2208Bj wkz p k\n) 1 p , (1)\nwhere {ui} are the summation weights and {wk} the suppression weights. Previous theoretical studies have outlined several potential computational roles for divisive normalization such as sensitivity maximization (Carandini & Heeger, 2012), invariant coding (Olsen et al., 2010), density modelling (Balle\u0301 et al., 2016), image compression (Malo et al., 2006), distributed neural representations (Simoncelli & Heeger, 1998), stimulus decoding (Ringach, 2009; Froudarakis et al., 2014), winner-take-all mechanisms (Busse et al., 2009), attention (Reynolds & Heeger, 2009), redundancy reduction (Schwartz & Simoncelli, 2001; Sinz & Bethge, 2008; Lyu & Simoncelli, 2008; Sinz & Bethge, 2013), marginalization in neural probabilistic population codes (Beck et al., 2011), and contextual modulations in neural populations and perception (Coen-Cagli et al., 2015; Schwartz et al., 2009)."}, {"heading": "2.2 REGULARIZATION", "text": "Various regularization techniques have been applied to neural networks for the purpose of improving generalization and reduce overfitting. They can be roughly divided into two categories, depending on whether they regularize the weights or the activations.\nRegularization on Weights: The most common regularizer on weights is weight decay which just amounts to using the L2 norm squared of the weight vector. An L1 regularizer (Goodfellow et al., 2016) on the weights can also be adopted to push the learned weights to become sparse. Scardapane et al. (2016) investigated mixed norms in order to promote group sparsity.\nRegularization on Activations: Sparsity or group sparsity regularizers on the activations have shown to be effective in the past (Roz, 2008; Kavukcuoglu et al., 2009) and several regularizers have been proposed that act directly on the neural activations. Glorot et al. (2011) add a sparse regularizer on the activations after ReLU to encourage sparse representations. Dropout developed by Srivastava et al. (2014) applies random masks to the activations in order to discourage them to co-adapt. DeCov proposed by Cogswell et al. (2015) tries to minimize the off-diagonal terms of the sample covariance matrix of activations, thus encouraging the activations to be as decorrelated as possible. Liao et al. (2016b) utilize a clustering-based regularizer to encourage the representations to be compact."}, {"heading": "3 A UNIFIED FRAMEWORK FOR NORMALIZING NEURAL NETS", "text": "We first compare the three existing forms of normalization, and show that we can modify batch normalization (BN) and layer normalization (LN) in small ways to make them have a form that matches divisive normalization (DN). We present a general formulation of normalization, where existing normalizations involve alternative schemes of accumulating information. Finally, we propose a regularization term that can be optimized jointly with these normalization schemes to encourage decorrelation and/or improve generalization performance."}, {"heading": "3.1 GENERAL FORM OF NORMALIZATION", "text": "Without loss of generality, we denote the hidden input activation of one arbitrary layer in a deep neural network as z \u2208 RN\u00d7L. HereN is the mini-batch size. In the case of a CNN, L = H\u00d7W \u00d7C, where H,W are the height and width of the convolutional feature map and C is the number of filters. For an RNN or fully-connected layers of a neural net, L is the number of hidden units.\nDifferent normalization methods gather statistics from different ranges of the tensor and then perform normalization. Consider the following general form:\nzn,j = \u2211 i wi,jxn,i + bj (2)\nvn,j = zn,j \u2212 EAn,j [z] (3) z\u0303n,j = vn,j\u221a\n\u03c32 + EBn,j [v2] (4)\nwhere Aj and Bj are subsets of z and v respectively. A and B in standard divisive normalization are referred to as summation and suppression fields (Carandini & Heeger, 2012). One can cast each normalization scheme into this general formulation, where the schemes vary based on how they define these two fields. These definitions are specified in Table 1. Optional parameters \u03b3 and \u03b2 can be added in the form of \u03b3j z\u0303n,j + \u03b2j to increase the degree of freedom.\nFig. 1 shows a visualization of the normalization field in a 4-D ConvNet tensor setting. Divisive normalization happens within a local spatial window of neurons across filter channels. Here we set d(\u00b7, \u00b7) to be the spatial L\u221e distance."}, {"heading": "3.2 NEW MODEL COMPONENTS", "text": "Smoothing the Normalizers: One obvious way in which the normalization schemes differ is in terms of the information that they combine for normalizing the activations. A second more subtle but important difference between standard BN and LN as opposed to DN is the smoothing term \u03c3, in the denominator of Eq. (1). This term allows some control of the bias of the variance estimation, effectively smoothing the estimate. This is beneficial because divisive normalization does not utilize information from the mini-batch as in BN, and combines information from a smaller field than LN. A\nsimilar but different denominator bias term max(\u03c3, c) appears in (Jarrett et al., 2009), which is active when the activation variance is small. However, the clipping function makes the transformation not invertible, losing scale information.\nMoreover, if we take the nonlinear activation function after normalization into consideration, we find that \u03c3 will change the overall properties of the non-linearity. To illustrate this effect, we use a simple 1-layer network which consists of: two input units, one divisive normalization operator, followed by a ReLU activation function. If we fix one input unit to be 0.5, varying the other one with different values of \u03c3 produces different output curves (Fig. 2, left). These curves exhibit different non-linear properties compared to the standard ReLU. Allowing the other input unit to vary as well results in different activation functions of the first unit depending on the activity of the second (Fig. 2, right). This illustrates potential benefits of including this smoothing term \u03c3, as it effectively modulates the rectified response to vary from a linear to a highly saturated response.\nIn this paper we propose modifications of the standard BN and LN which borrow this additive term \u03c3 in the denominator from DN. We study the effect of incorporating this smoother in the respective normalization schemes below.\nL1 regularizer: Filter responses on lower layers in deep neural networks can be quite correlated which might impair the estimate of the variance in the normalizer. More independent representations help disentangle latent factors and boost the networks performance (Higgins et al., 2016). Empirically, we found that putting a sparse (L1) regularizer\nLL1 = \u03b1 1\nNL \u2211 n,j |vn,j | (5)\non the centered activations vn,j helps decorrelate the filter responses (Fig. 5). Here, N is the batch size and L is the number of hidden units, and LL1 is the regularization loss which is added to the training loss.\nA possible explanation for this effect is that the L1 regularizer might have a similar effect as maximum likelihood estimation of an independent Laplace distribution. To see that, let pv (v) \u221d exp (\u2212\u2016v\u20161) and x = W\u22121v, with W a full rank invertible matrix. Under this model px (x) = pv (Wx) |detW |.\nThen, minimization of the L1 norm of the activations under the volume-conserving constraint detA = const. corresponds to maximum likelihood on that model, which would encourage decorrelated responses. We do not enforce such a constraint, and the filter matrix might even not be invertible. However, the supervised loss function of the network benefits from having diverse non-zero filters. This encourages the network to not collapse filters along the same direction or put them to zero, and might act as a relaxation of the volume-conserving constraint."}, {"heading": "3.3 SUMMARY OF NEW MODELS", "text": "DN and DN*: We propose DN as a new local normalization scheme in neural networks. In convolutional layers, it operates on a local spatial window across filter channels, and in fully connected layers it operates on a slice of a hidden state vector. Additionally, DN* has a L1 regularizer on the pre-normalization centered activation (vn,j).\nBN-s and BN*: To compare with DN and DN*, we also propose modifications to original BN: we denote BN-s with \u03c32 in the denominator\u2019s square root, and BN* with the L1 regularizer on top of BN-s.\nLN-s and LN*: We apply the same changes as from BN to BN-s and BN*. In order to narrow the differences in the normalization schemes down to a few parameter choices, we additionally remove the affine transformation parameters \u03b3 and \u03b2 from LN such that the difference between LN* and DN* is only the size of the normalization field. \u03b3 and \u03b2 can really be seen as a separate layer and in practice we find that they do not improve the performance in the presence of \u03c32."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate the normalization schemes on three different tasks:\n\u2022 CNN image classification: We apply different normalizations on CNNs trained on the CIFAR-10/100 datasets for image recognition, each of which contains 50,000 training images and 10,000 test images. Each image is of size 32 \u00d7 32 \u00d7 3 and has been labeled an object class out of 10 or 100 total number of classes.\n\u2022 RNN language modeling: We apply different normalizations on RNNs trained on the Penn Treebank dataset for language modeling, containing 42,068 training sentences, 3,370 validation sentences, and 3,761 test sentences.\n\u2022 CNN image super-resolution: We train a CNN on low resolution images and learn cascades of non-linear filters to smooth the upsampled images. We report performance of trained CNN on the standard Set 14 and Berkeley 200 dataset.\nFor each model, we perform a grid search of three or four choices of each hyperparameter including the smoothing constant \u03c3, and L1 regularization constant \u03b1, and learning rate on the validation set."}, {"heading": "4.1 CIFAR EXPERIMENTS", "text": "We used the standard CNN model provided in the Caffe library. The architecture is summarized in Table 2. We apply normalization before each ReLU function. We implement DN as a convolutional operator, fixing the local window size to 5\u00d7 5, 3\u00d7 3, 3\u00d7 3 for the three convolutional layers in all the CIFAR experiments.\nWe set the learning rate to 1e-3 and momentum 0.9 for all experiments. The learning rate schedule is set to {5K, 30K, 50K} for the baseline model and to {30K, 50K, 80K} for all other models. At every stage we multiply the learning rate by 0.1. Weights are randomly initialized from a zero-mean normal distribution with standard deviation {1e-4, 1e-2, 1e-2} for the convolutional layers, and {1e-1, 1e-1} for fully connected layers. Input images are centered on the dataset image mean.\nTable 3 summarizes the test performances of BN*, LN* and DN*, compared to the performance of a few baseline models and the standard batch and layer normalizations. We also add standard regularizers to the baseline model: L2 weight decay (WD) and dropout. Adding the smoothing constant and L1 regularization consistently improves the classification performance, especially for\nthe original LN. The modification of LN makes it now better than the original BN, and only slightly worse than BN*. DN* achieves comparable performance to BN* on both datasets, but only relying on a local neighborhood of hidden units.\nResNet Experiments. Residual networks (ResNet) (He et al., 2016), a type of CNN with residual connections between layers, achieve impressive performance on many image classification benchmarks. The original architecture uses BN by default. If we remove BN, the architecture is very difficult to train or converges to a poor solution. We first reproduced the original BN ResNet-32, obtaining 92.6% accuracy on CIFAR10, and 69.8% on CIFAR-100. Our best DN model achieves 91.3% and 66.6%, respectively. While this performance is lower than the original BN-ResNet, there is certainly room to improve as we have not performed any hyperparameter optimization. Importantly, the beneficial effects of sigma (2.5% gain on CIFAR-100) and the L1 regularizer (0.5%) are still found, even in the presence of other regularization techniques such as data augmentation and weight decay in the training.\nSince the number of sigma hyperparameters scales with the number of layers, we found that setting sigma as a learnable parameter for each layer helps the performance (1.3% gain on CIFAR-100). Note that training this parameter is not possible in the formulation by Jarrett et al. (2009). The learned sigma shows a clear trend: it tends to decrease with depth, and in the last convolution layer it approaches 0 (see Fig. 3)."}, {"heading": "4.2 RNN EXPERIMENTS", "text": "To apply divisive normalization in fully connected layers of RNNs, we consider a local neighborhood in the hidden state vector hj\u2212R:j+R, where R is the radius\nof the neighborhood. Although the hidden states are randomly initialized, this structure will impose local competition among the neighbors.\nvj = zj \u2212 1\n2R+ 1 R\u2211 r=\u2212R zj+r (6)\nz\u0303j = vj\u221a\n\u03c32 + 12R+1 \u2211R r=\u2212R v 2 j+r\n(7)\nWe follow Cooijmans et al. (2016)\u2019s batch normalization implementation for RNNs: normalizers are separate for input transformation and hidden transformation. Let BN(\u00b7), LN(\u00b7), DN(\u00b7) be BatchNorm, LayerNorm and DivNorm, and g be either tanh or ReLU.\nht+1 = g(Wxxt +Whht\u22121 + b) (8)\nh (BN) t+1 = g(BN(Wxxt + bx) +BN(Whh (BN) t\u22121 + bh)) (9)\nh (LN) t+1 = g(LN(Wxxt +Whh (LN) t\u22121 + b)) (10)\nh (DN) t+1 = g(DN(Wxxt +Whh (DN) t\u22121 + b)) (11)\nNote that in recurrent BN, the additional parameters \u03b3 and \u03b2 are shared across timesteps whereas the moving averages of batch statistics are not shared. For the LSTM version, we followed the released implementation from the authors of layer normalization 1, and apply LN at the same places as BN and BN*, which is after the linear transformation of Wxx and Whh individually. For LN* and DN, we modified the places of normalization to be at each non-linearity, instead of jointly with a concatenated vector for different non-linearity. We found that this modification improves the performance and makes the formulation clearer since normalization is always a combined operation with the activation function. We include details of the LSTM implementation in the Appendix.\nThe RNN model is provided by the Tensorflow library (Abadi et al., 2016) and the LSTM version was originally proposed in Zaremba et al. (2014). We used a two-layer stack-RNN of size 400 (vanilla RNN) or 200 (LSTM). R is set to 60 (vanilla RNN) and 30 (LSTM). We tried both tanh and ReLU as the activation function for the vanilla RNN. For unnormalized baselines and BN+ReLU, the initial learning rate is set to 0.1 and decays by half every epoch, starting at the 5th epoch for a maximum of 13 epochs. For the other normalized models, the initial learning rate is set to 1.0 while the schedule is kept the same. Standard stochastic gradient descent is used in all RNN experiments, with gradient clipping at 5.0.\nTable 4 shows the test set perplexity for LSTM models and vanilla models. Perplexity is defined as ppl = exp(\u2212 \u2211 x log p(x)). We find that BN and LN alone do not improve the final performance relative to the baseline, but similar to what we see in the CNN experiments, our modified versions BN* and LN* show significant improvements. BN* on RNN is outperformed by both LN* and DN. By applying our normalization, we can improve the vanilla RNN perplexity by 20%, comparable to an LSTM baseline with the same hidden dimension.\n1https://github.com/ryankiros/layer-norm"}, {"heading": "4.3 SUPER RESOLUTION EXPERIMENTS", "text": "We also evaluate DN on the low-level computer vision problem of single image super-resolution. We adopt the SRCNN model of Dong et al. (2016) as the baseline which consists of 3 convolutional layers and 2 ReLUs. From bottom to top layers, the sizes of the filters are 9, 5, and 5 2. The number of filters are 64, 32, and 1, respectively. All the filters are initialized with zero-mean Gaussian and standard deviation 1e-3. Then we respectively apply batch normalization (BN) and our divisive normalization with L1 regularization (DN*) to the convolutional feature maps before ReLUs. We construct the training set in a similar manner as Dong et al. (2016) by randomly cropping 5 million patches (size 33\u00d7 33) from a subset of the ImageNet dataset of Deng et al. (2009). We only train our model for 4 million iterations which is less than the one adopted by SRCNN, i.e., 15 million, as the gain of PSNR and SSIM by spending that long time is marginal.\nWe report the average test results, utilizing the standard metrics PSNR and SSIM (Wang et al., 2004), on two standard test datasets Set14 (Zeyde et al., 2010) and BSD200 (Martin et al., 2001). We compare with two state-of-the-art single image super-resolution methods, A+ (Timofte et al., 2013) and SRCNN (Dong et al., 2016). All measures are computed on the Y channel of YCbCr color space. We also provide a visual comparison in Fig. 4.\nAs show in Tables 5 and 6 DN* outperforms the strong competitor SRCNN, while BN does not perform well on this task. The reason may be that BN applies the same statistics to all patches of one image which causes some overall intensity shift (see Figs. 4). From the visual comparisons, we can see that our method not only enhances the resolution but also removes artifacts, e.g., the ringing effect in Fig. 4."}, {"heading": "4.4 ABLATION STUDIES AND DISCUSSION", "text": "Finally, we investigated the differential effects of the \u03c32 term and the L1 regularizer on the performance. We ran ablation studies on CIFAR-10/100 as well as PTB experiments. The results are listed in Table 7.\nWe find that adding the smoothing term \u03c32 and the L1 regularization consistently increases the performance of the models. In the convolutional networks, we find that L1 and \u03c3 both have similar effects on the performance. L1 seems to be slightly more important. In recurrent networks, \u03c32 has a much more dramatic effect on the performance than the L1 regularizer.\nFig. 5 plots randomly sampled pairwise pre-normalization responses (after the linear transform) in the first layer at the same spatial location of the feature map, along with the average pair-wise\n2We use the setting of the best model out of all three SRCNN candidates.\ncorrelation coefficient (Corr) and mutual information (MI). It is evident that both \u03c3 and L1 encourages independence of the learned linear filters.\nThere are several factors that could explain the improvement in performance. As mentioned above, adding the L1 regularizer on the activations encourages the filter responses to be less correlated. This can increase the robustness of the variance estimate in the normalizer and lead to an improved scaling of the responses to a good regime. Furthermore, adding the smoother to the denominator in the normalizer can be seen as implicitly injecting zero mean noise on the activations. While noise injection would not change the mean, it does add a term to the variance of the data, which is represented by \u03c32. This term also makes the normalization equation invertible. While dividing by the standard deviation decreases the degrees of freedom in the data, the smoothed normalization equation is fully information preserving. Finally, DN type operations have been shown to decrease the redundancy of filter responses to natural images and sound (Schwartz & Simoncelli, 2001; Sinz & Bethge, 2008; Lyu & Simoncelli, 2008). In combination with the L1 regularizer this could lead to a more independent representation of the data and thereby increase the performance of the network."}, {"heading": "5 CONCLUSIONS", "text": "We have proposed a unified view of normalization techniques which contains batch and layer normalization as special cases. We have shown that when combined with a sparse regularizer on the activations, our framework has significant benefits over standard normalization techniques. We have demonstrated this in the context of both convolutional neural nets as well as recurrent neural networks. In the future we plan to explore other regularization techniques such as group sparsity. We also plan to conduct a more in-depth analysis of the effects of normalization on the correlations of the learned representations.\nAcknowledgements RL is supported by Connaught International Scholarships. FS would like to thank Edgar Y. Walker, Shuang Li, Andreas Tolias and Alex Ecker for helpful discussions. Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government."}, {"heading": "A EFFECT OF SIGMA AND L1 ON CIFAR-10/100 VALIDATION SET", "text": "We plot the effect of \u03c3 and L1 regularization on the validation performance in Figure 6. While sigma makes the most contributions to the improvement, L1 also provides much gain for the original version of LN and BN."}, {"heading": "B LSTM IMPLEMENTATION DETAILS", "text": "In LSTM experiments, we found that have an individual normalizer for each non-linearity (sigmoid and tanh) helps the performance for both LN and DN. Eq. 12-14 are the standard LSTM equations, and let N be the normalizer function, our new normalizer is replacing the nonlinearity with Eq. 15-16. This modification can also be thought as combining normalization and activation as a single activation function.\nThis is different from the released implementation of LN and BN in LSTM, which separately normalized the concatenated vector Whht\u22121 and Wxxt. For all LN* and DN experiments we choose this new formulation, whereas LN experiments are consistent with the released version. ftitot\ngt  = Whht\u22121 +Wxxt + b (12) ct = \u03c3(ft) ct\u22121 + \u03c3(it) tanh(gt) (13) ht = \u03c3(ot) tanh(ct) (14)\n\u03c3\u0304(x) = \u03c3(N(x)) (15)\ntanh(x) = tanh(N(x)) (16)"}, {"heading": "C MORE RESULTS ON IMAGE SUPER-RESOLUTION", "text": "We include results on another standard dataset Set5 Bevilacqua et al. (2012) in Table 8 and show more visual results in Fig. 7."}], "references": [{"title": "Tensorflow: A system for large-scale machine", "author": ["Abadi", "Mart\u0131\u0301n", "Barham", "Paul", "Chen", "Jianmin", "Zhifeng", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu", "Ghemawat", "Sanjay", "Irving", "Geoffrey", "Isard", "Michael", "Kudlur", "Manjunath", "Levenberg", "Josh", "Monga", "Rajat", "Moore", "Sherry", "Murray", "Derek Gordon", "Steiner", "Benoit", "Tucker", "Paul A", "Vasudevan", "Vijay", "Warden", "Pete", "Wicke", "Martin", "Yu", "Yuan", "Zhang", "Xiaoqiang"], "venue": "learning. CoRR,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Density modeling of images using a generalized normalization transformation", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": null, "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "Marginalization in Neural Circuits with Divisive Normalization", "author": ["J.M. Beck", "P.E. Latham", "A. Pouget"], "venue": "The Journal of neuroscience : the official journal of the Society for Neuroscience,", "citeRegEx": "Beck et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2011}, {"title": "Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding", "author": ["Bevilacqua", "Marco", "Roumy", "Aline", "Guillemot", "Christine", "Morel", "Marie-Line Alberi"], "venue": "In BMVC,", "citeRegEx": "Bevilacqua et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bevilacqua et al\\.", "year": 2012}, {"title": "Role of Inhibition in the Specification of Orientation Selectivity of Cells in the Cat Striate Cortex", "author": ["A.B. Bonds"], "venue": "Visual Neuroscience,", "citeRegEx": "Bonds,? \\Q1989\\E", "shortCiteRegEx": "Bonds", "year": 1989}, {"title": "Representation of Concurrent Stimuli by Population Activity in Visual Cortex", "author": ["L. Busse", "A.R. Wade", "M. Carandini"], "venue": "doi: 10.1016/j. neuron.2009.11.004", "citeRegEx": "Busse et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Busse et al\\.", "year": 2009}, {"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature reviews. Neuroscience,", "citeRegEx": "Carandini and Heeger,? \\Q2012\\E", "shortCiteRegEx": "Carandini and Heeger", "year": 2012}, {"title": "Flexible gating of contextual influences in natural vision", "author": ["R. Coen-Cagli", "A. Kohn", "O. Schwartz"], "venue": "Nature Neuroscience,", "citeRegEx": "Coen.Cagli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coen.Cagli et al\\.", "year": 2015}, {"title": "Reducing overfitting in deep networks by decorrelating representations", "author": ["Cogswell", "Michael", "Ahmed", "Faruk", "Girshick", "Ross", "Zitnick", "Larry", "Batra", "Dhruv"], "venue": null, "citeRegEx": "Cogswell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cogswell et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Image super-resolution using deep convolutional networks", "author": ["Dong", "Chao", "Loy", "Chen Change", "He", "Kaiming", "Tang", "Xiaoou"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2016}, {"title": "Population code in mouse V1 facilitates readout of natural scenes through increased sparseness", "author": ["Froudarakis", "Emmanouil", "Berens", "Philipp", "Ecker", "Alexander S", "Cotton", "R James", "Sinz", "Fabian H", "Yatsenko", "Dimitri", "Saggau", "Peter", "Bethge", "Matthias", "Tolias", "Andreas S"], "venue": "Nature neuroscience,", "citeRegEx": "Froudarakis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Froudarakis et al\\.", "year": 2014}, {"title": "Image style transfer using convolutional neural networks", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": null, "citeRegEx": "Gatys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Normalization of cell responses in cat striate cortex", "author": ["D.J. Heeger"], "venue": "Vis Neurosci,", "citeRegEx": "Heeger,? \\Q1992\\E", "shortCiteRegEx": "Heeger", "year": 1992}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M.A. Ranzato", "Y. LeCun"], "venue": null, "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "Ranzato", "M.\u2019A", "R. Fergus", "Y. LeCun"], "venue": "In CVPR Workshops,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Batch normalized recurrent neural networks", "author": ["Laurent", "C\u00e9sar", "Pereyra", "Gabriel", "Brakel", "Phil\u00e9mon", "Zhang", "Ying", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1510.01378,", "citeRegEx": "Laurent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2015}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "author": ["Q. Liao", "T. Poggio"], "venue": null, "citeRegEx": "Liao and Poggio,? \\Q2016\\E", "shortCiteRegEx": "Liao and Poggio", "year": 2016}, {"title": "Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning", "author": ["Liao", "Qianli", "Kawaguchi", "Kenji", "Poggio", "Tomaso"], "venue": "CoRR, abs/1610.06160,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Learning deep parsimonious representations", "author": ["Liao", "Renjie", "Schwing", "Alexander", "Zemel", "Richard", "Urtasun", "Raquel"], "venue": null, "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Reducing statistical dependencies in natural signals using radial Gaussianization", "author": ["Lyu", "Siwei", "Simoncelli", "Eero P"], "venue": null, "citeRegEx": "Lyu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2008}, {"title": "Nonlinear image representation for efficient perceptual coding", "author": ["J. Malo", "I. Epifanio", "R. Navarro", "E.P. Simoncelli"], "venue": null, "citeRegEx": "Malo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malo et al\\.", "year": 2006}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["Martin", "David", "Fowlkes", "Charless", "Tal", "Doron", "Malik", "Jitendra"], "venue": "In ICCV,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Divisive Normalization in Olfactory Population", "author": ["Olsen", "S. R", "V. Bhandawat", "R.I. Wilson"], "venue": "Codes. Neuron,", "citeRegEx": "Olsen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Olsen et al\\.", "year": 2010}, {"title": "Why is Real-World Visual Object Recognition Hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLoS Comput Biol,", "citeRegEx": "Pinto et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2008}, {"title": "The normalization model of attention", "author": ["J.H. Reynolds", "D.J. Heeger"], "venue": "jan", "citeRegEx": "Reynolds and Heeger,? \\Q2009\\E", "shortCiteRegEx": "Reynolds and Heeger", "year": 2009}, {"title": "Population coding under normalization", "author": ["D.L. Ringach"], "venue": "Vision Research,", "citeRegEx": "Ringach,? \\Q2009\\E", "shortCiteRegEx": "Ringach", "year": 2009}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Salimans", "Tim", "Kingma", "Diederik P"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Group sparse regularization for deep neural networks", "author": ["S. Scardapane", "D. Comminiello", "A. Hussain", "A. Uncin"], "venue": null, "citeRegEx": "Scardapane et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Scardapane et al\\.", "year": 2016}, {"title": "Natural signal statistics and sensory gain control", "author": ["O. Schwartz", "E.P. Simoncelli"], "venue": "Nat Neurosci,", "citeRegEx": "Schwartz and Simoncelli,? \\Q2001\\E", "shortCiteRegEx": "Schwartz and Simoncelli", "year": 2001}, {"title": "Perceptual organization in the tilt illusion", "author": ["Schwartz O", "T. Sejnowski", "Dayan"], "venue": "Journal of Vision,", "citeRegEx": "O. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "O. et al\\.", "year": 2009}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "Proceedings of International Conference on Pattern Recognition ICPR12,", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "A model of neuronal responses in visual area MT", "author": ["E.P. Simoncelli", "D.J. Heeger"], "venue": "Vision Research,", "citeRegEx": "Simoncelli and Heeger,? \\Q1998\\E", "shortCiteRegEx": "Simoncelli and Heeger", "year": 1998}, {"title": "Temporal Adaptation Enhances Efficient Contrast Gain Control on Natural Images", "author": ["Sinz", "Fabian", "Bethge", "Matthias"], "venue": "PLoS Computational Biology,", "citeRegEx": "Sinz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sinz et al\\.", "year": 2013}, {"title": "The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction", "author": ["Sinz", "Fabian H", "Bethge", "Matthias"], "venue": "In NIPS,", "citeRegEx": "Sinz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sinz et al\\.", "year": 2008}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Anchored neighborhood regression for fast example-based super-resolution", "author": ["Timofte", "Radu", "De Smet", "Vincent", "Van Gool", "Luc"], "venue": "In ICCV,", "citeRegEx": "Timofte et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Timofte et al\\.", "year": 2013}, {"title": "Instance normalization: The missing ingredient for fast stylization", "author": ["Ulyanov", "Dmitry", "Vedaldi", "Andrea", "Lempitsky", "Victor S"], "venue": null, "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan C", "Sheikh", "Hamid R", "Simoncelli", "Eero P"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "On single image scale-up using sparserepresentations", "author": ["Zeyde", "Roman", "Elad", "Michael", "Protter", "Matan"], "venue": "In International conference on curves and surfaces,", "citeRegEx": "Zeyde et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeyde et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "A less-explored form of normalization is divisive normalization (DN) (Heeger, 1992), in which a neuron\u2019s activity is normalized by its neighbors within a layer.", "startOffset": 69, "endOffset": 83}, {"referenceID": 18, "context": "However, with few exceptions (Jarrett et al., 2009; Krizhevsky et al., 2012) it has received little attention in conventional supervised deep learning.", "startOffset": 29, "endOffset": 76}, {"referenceID": 29, "context": "For instance, local contrast normalization used to be a standard effective tool in vision problems (Pinto et al., 2008; Jarrett et al., 2009; Sermanet et al., 2012; Le, 2013).", "startOffset": 99, "endOffset": 174}, {"referenceID": 18, "context": "For instance, local contrast normalization used to be a standard effective tool in vision problems (Pinto et al., 2008; Jarrett et al., 2009; Sermanet et al., 2012; Le, 2013).", "startOffset": 99, "endOffset": 174}, {"referenceID": 36, "context": "For instance, local contrast normalization used to be a standard effective tool in vision problems (Pinto et al., 2008; Jarrett et al., 2009; Sermanet et al., 2012; Le, 2013).", "startOffset": 99, "endOffset": 174}, {"referenceID": 20, "context": "Moreover, application to recurrent neural networks (RNNs) is not straightforward and leads to poor performance (Laurent et al., 2015).", "startOffset": 111, "endOffset": 133}, {"referenceID": 12, "context": "(2016) proposed to normalize each example on spatial dimensions but not on channel dimension, and was shown to be effective on image style transfer applications (Gatys et al., 2016).", "startOffset": 161, "endOffset": 181}, {"referenceID": 16, "context": "Divisive Normalization (DN) on the other hand modulates the neural activity by the activity of a pool of neighboring neurons (Heeger, 1992; Bonds, 1989).", "startOffset": 125, "endOffset": 152}, {"referenceID": 4, "context": "Divisive Normalization (DN) on the other hand modulates the neural activity by the activity of a pool of neighboring neurons (Heeger, 1992; Bonds, 1989).", "startOffset": 125, "endOffset": 152}, {"referenceID": 28, "context": "Previous theoretical studies have outlined several potential computational roles for divisive normalization such as sensitivity maximization (Carandini & Heeger, 2012), invariant coding (Olsen et al., 2010), density modelling (Ball\u00e9 et al.", "startOffset": 186, "endOffset": 206}, {"referenceID": 1, "context": ", 2010), density modelling (Ball\u00e9 et al., 2016), image compression (Malo et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 26, "context": ", 2016), image compression (Malo et al., 2006), distributed neural representations (Simoncelli & Heeger, 1998), stimulus decoding (Ringach, 2009; Froudarakis et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 31, "context": ", 2006), distributed neural representations (Simoncelli & Heeger, 1998), stimulus decoding (Ringach, 2009; Froudarakis et al., 2014), winner-take-all mechanisms (Busse et al.", "startOffset": 91, "endOffset": 132}, {"referenceID": 11, "context": ", 2006), distributed neural representations (Simoncelli & Heeger, 1998), stimulus decoding (Ringach, 2009; Froudarakis et al., 2014), winner-take-all mechanisms (Busse et al.", "startOffset": 91, "endOffset": 132}, {"referenceID": 5, "context": ", 2014), winner-take-all mechanisms (Busse et al., 2009), attention (Reynolds & Heeger, 2009), redundancy reduction (Schwartz & Simoncelli, 2001; Sinz & Bethge, 2008; Lyu & Simoncelli, 2008; Sinz & Bethge, 2013), marginalization in neural probabilistic population codes (Beck et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 2, "context": ", 2009), attention (Reynolds & Heeger, 2009), redundancy reduction (Schwartz & Simoncelli, 2001; Sinz & Bethge, 2008; Lyu & Simoncelli, 2008; Sinz & Bethge, 2013), marginalization in neural probabilistic population codes (Beck et al., 2011), and contextual modulations in neural populations and perception (Coen-Cagli et al.", "startOffset": 221, "endOffset": 240}, {"referenceID": 7, "context": ", 2011), and contextual modulations in neural populations and perception (Coen-Cagli et al., 2015; Schwartz et al., 2009).", "startOffset": 73, "endOffset": 121}, {"referenceID": 14, "context": "An L1 regularizer (Goodfellow et al., 2016) on the weights can also be adopted to push the learned weights to become sparse.", "startOffset": 18, "endOffset": 43}, {"referenceID": 19, "context": "Regularization on Activations: Sparsity or group sparsity regularizers on the activations have shown to be effective in the past (Roz, 2008; Kavukcuoglu et al., 2009) and several regularizers have been proposed that act directly on the neural activations.", "startOffset": 129, "endOffset": 166}, {"referenceID": 18, "context": "similar but different denominator bias term max(\u03c3, c) appears in (Jarrett et al., 2009), which is active when the activation variance is small.", "startOffset": 65, "endOffset": 87}, {"referenceID": 15, "context": "Residual networks (ResNet) (He et al., 2016), a type of CNN with residual connections between layers, achieve impressive performance on many image classification benchmarks.", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "The RNN model is provided by the Tensorflow library (Abadi et al., 2016) and the LSTM version was originally proposed in Zaremba et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 43, "context": "We report the average test results, utilizing the standard metrics PSNR and SSIM (Wang et al., 2004), on two standard test datasets Set14 (Zeyde et al.", "startOffset": 81, "endOffset": 100}, {"referenceID": 45, "context": ", 2004), on two standard test datasets Set14 (Zeyde et al., 2010) and BSD200 (Martin et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 41, "context": "We compare with two state-of-the-art single image super-resolution methods, A+ (Timofte et al., 2013) and SRCNN (Dong et al.", "startOffset": 79, "endOffset": 101}], "year": 2017, "abstractText": "Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work well in the recurrent setting. In this paper we propose a unified view of normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework in the context of convolutional neural nets and recurrent neural networks, showing improvements over baselines in image classification, language modeling as well as super-resolution.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_26"}