{"name": "ICLR_2017_288.pdf", "metadata": {"source": "CRF", "title": "SAMPLE IMPORTANCE IN TRAINING DEEP NEURAL NETWORKS", "authors": ["Tianxiang Gao", "Vladimir Jojic"], "emails": ["tgao@cs.unc.edu", "vjojic@cs.unc.edu"], "sections": [{"heading": null, "text": "The contribution of each sample during model training varies across training iterations and the model\u2019s parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that \u201ceasy\u201d samples \u2013 samples that are correctly and confidently classified at the end of the training \u2013 shape parameters closer to the output, while the \u201chard\u201d samples impact parameters closer to the input to the network. Further, \u201ceasy\u201d samples are relevant in the early training stages, and \u201chard\u201d in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance."}, {"heading": "1 INTRODUCTION", "text": "Sample importance is the sample\u2019s contribution to the parameter change during training. In statistics, the concept \u201cleverage\u201d of a point is used (St Laurent & Cook (1992)) to measure the impact of a sample on the training of a model. In the context of SVM, the most important samples are the support vectors as they define the separating hyperplane. Understanding the importance of the samples can help us interpret trained models and structure training to speed up convergence and improve prediction accuracy. For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness. In this framework, easiness is related to the prediction error and can be estimated from the model. However, easiness of a sample may not be sufficient to decide when it should be introduced to a learner. Maintaining diversity among the training samples can have a substantial effect on the training (Jiang et al. (2014)).\nIn this work, we explore the sample importance in deep neural networks. Deep learning methods have been successfully applied in many tasks and routinely achieve better generalization error than classical shallow methods (LeCun et al. (2015)). One of the key characteristics of a deep network is its capacity to construct progressively more complex features throughout its layers (Lee et al. (2011)). An intuitive question arises: which samples contribute the most to the training of the different layer\u2019s parameters? From literature Saxe et al. (2011), we know that even randomly generated filters can compute features that lead to good performance \u2013 presumably on easy samples. However, to learn hard samples correctly, the model may need to construct complex features, which require both more training time and refined filters from bottom layers. Hence, we hypothesized that the hard samples shape the bottom layers \u2013 closer to the input \u2013 and easy samples shape the top layers \u2013 closer to the output.\nMotivated by the above hypothesis, we analyzed the sample importance in a 3 layer ReLU network on two standard datasets. The results reveal several interesting facts about the sample importance in easy and hard samples:\n1. Easy and hard samples impact the parameters in different training stages. The biggest impact of easy samples on parameters are mostly during the early training stage, while the impact of hard samples become large in the late training stage.\n2. Easy and hard samples impact the parameters in different layers. Easy samples impact have larger impact on top layer parameters, while hard samples shape the bottom layer parameters.\n3. Mixing hard samples with easy samples in each batch helps training. We conducted batches with homogeneous or mixed \u201ceasiness\u201d. We found that use of homogeneous batches hinders the training. Hence, it is preferable for network to see both easy and hard samples during all stages of training.\nNext, we are going to give the definition of sample importance in Section 2. The empirical analysis for sample importance in the deep neural network in two real datasets is discussed in Section 3. Extension about sample importance is showed in Section 4."}, {"heading": "2 SAMPLE IMPORTANCE", "text": "In this section, we are going to introduce the terminology and provide a quantitative measurement of sample importance for a training procedure."}, {"heading": "2.1 SAMPLE WEIGHT", "text": "In supervised learning, a model is trained by optimizing an objective over a set of observed training samples (xi, yi). Let f(xi,\u03b8) be the output of a model for parameter \u03b8. The training objective can be written as:\nn\u2211 i=1 L(yi, f(xi,\u03b8)) +R(\u03b8), (1)\nwhere L(yi, f(xi,\u03b8)) is the loss on sample i, and R(\u03b8) is the regularization on the parameters. In order to highlight contribution of each sample, we can introduce sample specific weights vi \u2208 [0, 1] which scale sample\u2019s contribution to the loss. Hence, the objective in (1) can be rewritten as:\nn\u2211 i=1 viL(yi, f(xi,\u03b8)) +R(\u03b8), (2)\nWe define the weight vi as the sample weight. Similar definitions on vi has been proposed in Self-paced learning (SPL) Kumar et al. (2010).\nIn Stochastic Gradient descend (SGD) methods, parameters \u03b8 are updated with a certain step size \u03b7 in each iteration with regard to a set of training samples. If we allow different sample weights in different iterations, a single update can be written as:\n\u03b8t+1 = \u03b8t \u2212 \u03b7 n\u2211\ni=1\nvtig t i \u2212 \u03b7rt,\nwhere \u03b8t is the parameter vector at epoch t, gti = \u2202 \u2202\u03b8t Li(yi, f(xi,\u03b8 t)), rt = \u2202 \u2202\u03b8t R(\u03b8t), and vti is the weight of ith sample at iteration t."}, {"heading": "2.2 SAMPLE IMPORTANCE", "text": "If we change the weight of a sample i at iteration t, how would such change impact the parameter training in that iteration? We can answer this question by calculating the first order derivative of parameter change \u2206\u03b8t = \u03b8t+1 \u2212 \u03b8t with regard to sample weight vti :\n\u03c6ti = \u2202\n\u2202vti \u2206\u03b8t = \u2212\u03b7gti .\nWe call \u03c6ti the parameter affectibility by ith sample at iteration t. \u03c6 t i is a vector consists of parameter affectibility from all parameters in the network. Specifically, \u03c6ti,j is the parameter affectibility for jth parameter in the network. \u03c6ti reflects the relationship between parameter change and different samples.\nTypical deep networks contains millions of parameters. Hence, we are going to focus on groups of parameters of interests. We define ith sample\u2019s importance for parameters of dth layer of as:\n\u03b2ti,d = \u2211 j\u2208Qd (\u03c6ti,j) 2,\nwhere Qd is a set consists of the indexes of all parameters in layer d. Hence, sample\u2019s importance for all the parameters in the model is:\n\u03b1ti = \u2211 j (\u03c6ti,j) 2,\nObviously, we have \u03b1ti = \u2211D d=1 \u03b2 t i,d.\nThe sum of sample\u2019s importance across all iterations is defined as overall importance of a sample: \u03c4i = \u2211 t \u03b1ti\nIn general, for each sample i, computing \u03b2ti,d allows us to decompose its influence in the model\u2019s training across training stages and different layers.\nWe note that the sample importance is a high-level measurement of the samples influence on parameters at each iteration of the update. This quantity is not an accurate measurement of the relationship between a sample and final trained model. Refinements of this concept are discussed in Section 4."}, {"heading": "3 EMPIRICAL ANALYSIS OF SAMPLE IMPORTANCE", "text": "We are going to explore the samples\u2019 importance for different layers at different epoch through a series of empirical experiments on two standard datasets."}, {"heading": "3.1 EXPERIMENT SETUP", "text": "Dataset All the analysis are performed on two standard datasets: MNIST 1 (LeCun et al. (1998)), a benchmark dataset that contains handwritten digit images. Each sample is a 28 \u00d7 28 image from 10 classes. We used 50000 samples for training and 10000 samples for testing. CIFAR-10 2 (Krizhevsky & Hinton (2009)), a dataset contains 32\u00d7 32 tiny color images from 10 classes. Each sample has 3072 features. We used 50000 samples for training and 10000 samples for testing.\nArchitecture We used a multilayer feed forward neural network with 3 hidden layers of 512 hidden nodes with rectified linear units (ReLU) activation function, a linear output layer, and a softmax layer on top for classification. The weights in each hidden layer are initialized according to Glorot & Bengio (2010). For hyper-parameters, we used learning rate of 0.1, batch size of 100, 50 total epochs, and weight-decay of 1e \u2212 5. No momentum or learning decay was used. All the code are based on a common deep learning package Theano from Bergstra et al. (2010); Bastien et al. (2012)."}, {"heading": "3.2 SAMPLE IMPORTANCE IS STABLE WITH RESPECT TO DIFFERENT INITIALIZATIONS", "text": "Firstly, we want to explore whether the sample importance is stable under different initializations. We used three different random seeds to initialize the network parameters and calculated the sample importance every five epochs. We computed the Spearman\u2019s rank correlation between sample importance to the model, \u03b1ti, in each pair of initializations. This correlation remains high in all epochs,\n1http://yann.lecun.com/expdb/mnist/ 2https://www.cs.toronto.edu/\u02dckriz/cifar.html\nabove 0.9, as shown in Figure 1. This indicates that the sample importance is relatively stable to initialization of the network. Hence, all the following analysis are based on the results from initialization seed 1. (Details of training and test error for the chosen model can be viewed in Appendix Figure 9)."}, {"heading": "3.3 DECOMPOSITION OF SAMPLE IMPORTANCE", "text": "To better understand and visualize the sample importance, we firstly calculate the overall sample importance at each epoch as At = \u2211n i=1 \u03b1 t i. Similarly, the overall sample importance to layer d\nis Btd = \u2211n i=1 \u03b2 t i,d. We show the overall sample importance and its decomposition in layers for two datasets in Figure 2. Firstly, we found that even with a fixed learning rate, the overall sample importance is different under different epochs. Output layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops. Each layer contributes differently to the total sample importance. In both MNIST and CIFAR-10, the 2nd layer contributes more than the 3rd layer. In CIFAR-10, the 1st layer\u2019s provides largest contribution the total sample importance, as it contains much more parameters than other layers. Interestingly, all classes do not provide the same amount of sample importance.\nWe found that most samples have small sample importance (Appendix Figure 10). To visualize the contribution of different samples, we split the samples based on their total importance into three groups: 10%, top 10% \u2013 top 20% most important samples, and other 80% samples. We show the decomposition of importance contribution in each layer in Figure 3. In MNIST, the top 10% samples contribute almost all the sample importance. In CIFAR-10, most important samples contribute more in lower layers rather than output layer. This result indicates that top 20 % most important samples contribute to the majority of the sample importance."}, {"heading": "3.4 SAMPLE IMPORTANCE AND NEGATIVE LOG-LIKELIHOOD", "text": "Negative log likelihood (NLL) is the loss metric we used for training objective. It has been used to measure the \u201ceasiness\u201d of a sample in Curriculum learning Bengio et al. (2009) and Self-paced learning Kumar et al. (2010). Intuitively, the samples with large NLL should also have large sample importance (SI). However, in our experiment, we found that this is not always the case. In Figure 4, we found that 1) NLL and SI become more correlated as training goes on. However, 2) NLL is not predictive of the SI. There are many points with high NLL but small SI, and otherwise."}, {"heading": "3.5 CLUSTERING SAMPLES BASED ON SAMPLE IMPORTANCE", "text": "To better visualize the importance of different samples, we provide three representative clusters of samples for each dataset. In MNIST, we clustered all digit \u201c5\u201d samples into 20 clusters based on their epoch-specific, layer-specific sample importance. In CIFAR-10, we clustered all \u201chorse\u201d samples into 30 clusters using the same features as MNIST. Kmeans algorithm is used for clustering.\nMNIST In Figure 5, we showed 3 example clusters on digit \u201c5\u201d. In the cluster of easy samples, where NLL converges very fast, most of the sample importance is concentrated in the first few epochs in output layer parameters. The cluster of medium samples has a slow NLL convergence compared to the easy cluster. The biggest impact is in middle training stage and more towards bottom layer parameters. Hard samples hardly converge even during the late stage of training. As training goes on, the sample importance for the bottom layer parameters become larger.\nCIFAR-10 In Figure 6, we showed 3 examples clusters on class \u201chorse\u201d. We observed very similar sample importance changing pattern as for the MNIST examples for easy, medium and hard clusters. Comparing to MNIST, all three clusters in CIFAR-10 have a very large impact on the parameters in the bottom layer. We note that the CIFAR-10 has almost 4 times larger number of parameters (3075\u00d7 512 \u2248 1574k) in the first layer than MNIST (785\u00d7 512 \u2248 401k)."}, {"heading": "3.6 BATCH ORDER AND SAMPLE IMPORTANCE", "text": "With the observations from empirical analysis on sample importance, we know that time \u2013 iteration \u2013 and place \u2013 layer \u2013 of sample\u2019s impact varies according to its \u201ceasiness\u201d . We wanted to know whether constructing batches based on the sample importance or negative log likelihood would make a difference in training. Hence, we designed an experiment to explore how the different construction of batches influence the training. We note that the information used to structure the batches (negative log-likelihood and sample importance) \u2013 was obtained from a full training run.\nWe split all 50, 000 samples into b = 500 batch subsets {B1,B2, . . . ,Bb}. Each batch has batch size |Bi| = 100. In our experiment, each training sample must be in exactly one batch. There is no intersection between batches.\nDuring training, in each epoch, we update the parameters with each batch in order 1, 2, . . . , b iteratively.\nWe used seven different batch construction methods in this experiment:\n1. Rand: Randomly constructed batch. All 50k samples are randomly split into b batches before training. The batches and orders stay fixed during training.\n2. NLO: Negative Log-likelihood Order. We sort all the samples based on their final NLL from low to high. The batches are constructed based on the sorted samples. First 100 samples with least NLL are in B1, 101 to 200 samples are in B2, and so on. Hence, during training, the batches with small NLL will be trained first.\n3. RNLO Reverse-Negative Log-likelihood Order. We construct the batches same as NLO. During training, we update the batches in reverse order Bb,Bb\u22121, . . . ,B1. Hence, the batches with large NLL will be trained first.\n4. NLM Negative Log-likelihood Mixed. We sort all the samples based on their final NLL from low to high. Next, for each sample i in the sorted sequence, we put that sample into batch j = (i mod b) + 1. This ordering constructs batches out of samples with diverse levels of NLL.\n5. SIO: Sample Importance Order. Similar to NLO, except that we sort all the samples based on their sum sample importance over all epochs from low to high. Hence, batches with small sample importance will be trained first.\n6. RSIO Reverse-Sample Importance Order. We construct the batches same as SIO. During training, we update the batches in reverse order Bb,Bb\u22121, . . . ,B1. Hence, during training, the batches with large sample importance will be trained first.\n7. SIM Sample Important Mixed. Similar to NLM, but we sort the samples based on overall sample importance. Thus, batches contain samples with divers sample importance.\nWe performed five different runs (with different random initializations) on MNIST and CIFAR-10. The result is shown in Figure 7. From the result, we found that: 1) In both MNIST and CIFAR-10, Rand, SIS, and NLS have the lowest test error compared to all other methods. This indicates that diverse batches are helpful for training. 2) NLO and SIO got the worst performance in CIFAR10. Their training error even goes up after the early stage. RNLO and RSIO have same batch constructions as NLO and SIO, but their performances are drastically different. This indicates that the order of batches during training is important. Further, training on easy samples first and hard later seems to be counter-productive.\nTo better understand the impact of different batch construction, we performed the principle component analysis on the learned parameters in each epoch (Figure 8). In MNIST, the impact of batch construction is not very significant. In CIFAR-10, batch construction and even the order of batch training do have a large impact on the training.\nOur experiment result shows a different conclusion to Curriculum Learning and Self-paced learning, where easy samples are trained on before introducing hard samples. We found that constructing and ordering the batches \u2013 hard to easy and easy to hard \u2013 seems to hinder the performance of learning. Having hard samples mixed in with the easy ones in each batch helps the training.\nAlso, the results show that we want to learn from the hard samples in early epochs and \u201csee\u201d hard samples more frequently, even if their major impact on parameters is during the late stage. As hard\nexamples are few compared to easy samples and hard examples need a longer time to train, we do want to mix the hard samples into each batch to start learning from those samples early and learn longer."}, {"heading": "4 EXTENSIONS OF SAMPLE IMPORTANCE", "text": "We calculated the sample importance in each iteration in Stochastic Gradient Descent. However, such quantity only reflects the impact on the change in parameters within each iteration. The influence of a sample at a particular iteration can be accumulated through updates and impact the final model. Here, we are going to derive the exact calculation of the sample\u2019s influence to the model. We rewrite the Objective (2) in Section 2 here:\nmin \u03b8 n\u2211 i=1 viL(yi, f(xi,\u03b8)) +R(\u03b8), (3)\nHere, we deem the sample weight vi is fixed across all iterations. The update rule for stochastic gradient descent in each iteration is:\n\u03b8t+1 = \u03b8t \u2212 \u03b7 \u2211 i vig t i \u2212 \u03b7rti\nThe derivative of \u03b8t+1 with respect to sample weight vi is:\n\u2202\n\u2202vi \u03b8t+1 =\n\u2202\n\u2202vi \u03b8t \u2212 \u03b7gti \u2212 \u03b7H(\u03b8\nt) \u2202\n\u2202vi \u03b8t\n\u2202\n\u2202vi \u03b81 = \u2212\u03b7g0i ,\nwhere H(\u03b8t) is the Hessian matrix of the objective in (2) with regard to all parameters in iteration t. If we iterate the updates until convergence, then we can assume that \u03b8T is a fix-point, \u03b8\u2217 = \u03b8T+1 = \u03b8T , and we obtain:\n\u2202 \u2202vi \u03b8T+1 \u2212 \u2202 \u2202vi \u03b8T = \u2212\u03b7gti \u2212 \u03b7H(\u03b8 \u2217) \u2202 \u2202vi \u03b8\u2217\nHence, the derivative of parameters in the final model with regard to a sample weight is:\n\u2202\n\u2202vi \u03b8\u2217 = \u2212H(\u03b8\u2217)\u22121gTi (4)\nEquation (4) indicates that we can calculate the sample specific impact on final trained model by using the parameters learned at the convergence point. In deep learning methods, due to early stopping, fix point might not be achieved, and Equation (4) might not be an accurate.\nFor any target quantity T (\u03b8\u2217) that depends on the final trained parameter \u03b8\u2217, we can calculate the impact of a particular sample on that target as:\n\u2202\n\u2202vi T (\u03b8\u2217) =\n\u2202 \u2202\u03b8\u2217 T (\u03b8\u2217) \u2202 \u2202vi \u03b8\u2217 (5)\nFor example, if we are interested in the sum of predictions on a set of samples T (\u03b8\u2217) =\u2211 i\u2208Sc f(xi,\u03b8 t), we can use Equation (5) to calculate the derivative:\n\u2202\n\u2202vi T (\u03b8\u2217) = \u2211 i\u2208Sc \u2202 \u2202\u03b8\u2217 f(xi,\u03b8 t) \u2202 \u2202vi \u03b8\u2217\nWe note evaluating the exact impact of a sample, as shown above, is computationally cumbersome for all but the simplest models."}, {"heading": "5 DISCUSSION", "text": "Samples\u2019 impact on the deep network\u2019s parameters vary across stages of training and network\u2019s layers. In our work, we found that easy samples predominantly shape parameters the top layers at the early training stages, while hard samples predominantly shape the parameters of the bottom layers at the late training stage. Our experiments show that it is important to mix hard samples into different batches rather than keep them together in the same batch and away from other examples.\nThere are many future extensions to the current work. Firstly, we want to expand our sample importance analysis to different deep learning structures, like Convolution Neural Network and Recurrent Neural Networks. Secondly, we want to use the sample importance as a guidance to extract a minimal subset of samples that are sufficient to achieve performance comparable to a network trained on the full dataset."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Self-paced learning with diversity", "author": ["Lu Jiang", "Deyu Meng", "Shoou-I Yu", "Zhenzhong Lan", "Shiguang Shan", "Alexander Hauptmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Self-paced learning for latent variable models", "author": ["M Pawan Kumar", "Benjamin Packer", "Daphne Koller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Unsupervised learning of hierarchical representations with convolutional deep belief networks", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "A.Y. Ng"], "venue": "Communications of the ACM,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "On random weights and unsupervised feature learning", "author": ["Andrew Saxe", "Pang W Koh", "Zhenghao Chen", "Maneesh Bhand", "Bipin Suresh", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Leverage and superleverage in nonlinear regression", "author": ["Roy T St Laurent", "R Dennis Cook"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Laurent and Cook.,? \\Q1992\\E", "shortCiteRegEx": "Laurent and Cook.", "year": 1992}], "referenceMentions": [{"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning.", "startOffset": 43, "endOffset": 64}, {"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness.", "startOffset": 43, "endOffset": 326}, {"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness. In this framework, easiness is related to the prediction error and can be estimated from the model. However, easiness of a sample may not be sufficient to decide when it should be introduced to a learner. Maintaining diversity among the training samples can have a substantial effect on the training (Jiang et al. (2014)).", "startOffset": 43, "endOffset": 718}, {"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness. In this framework, easiness is related to the prediction error and can be estimated from the model. However, easiness of a sample may not be sufficient to decide when it should be introduced to a learner. Maintaining diversity among the training samples can have a substantial effect on the training (Jiang et al. (2014)). In this work, we explore the sample importance in deep neural networks. Deep learning methods have been successfully applied in many tasks and routinely achieve better generalization error than classical shallow methods (LeCun et al. (2015)).", "startOffset": 43, "endOffset": 961}, {"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness. In this framework, easiness is related to the prediction error and can be estimated from the model. However, easiness of a sample may not be sufficient to decide when it should be introduced to a learner. Maintaining diversity among the training samples can have a substantial effect on the training (Jiang et al. (2014)). In this work, we explore the sample importance in deep neural networks. Deep learning methods have been successfully applied in many tasks and routinely achieve better generalization error than classical shallow methods (LeCun et al. (2015)). One of the key characteristics of a deep network is its capacity to construct progressively more complex features throughout its layers (Lee et al. (2011)).", "startOffset": 43, "endOffset": 1118}, {"referenceID": 1, "context": "For example, Curriculum learning (CL) from Bengio et al. (2009) shows that training with easy samples first, then gradually transitioning to difficult samples can improve the learning. In CL, the \u201ceasiness\u201d of a sample is predefined either manually or using an evaluation model. Self-paced learning (SPL) (Kumar et al. (2010)) shows that it is possible to learn from samples in order of easiness. In this framework, easiness is related to the prediction error and can be estimated from the model. However, easiness of a sample may not be sufficient to decide when it should be introduced to a learner. Maintaining diversity among the training samples can have a substantial effect on the training (Jiang et al. (2014)). In this work, we explore the sample importance in deep neural networks. Deep learning methods have been successfully applied in many tasks and routinely achieve better generalization error than classical shallow methods (LeCun et al. (2015)). One of the key characteristics of a deep network is its capacity to construct progressively more complex features throughout its layers (Lee et al. (2011)). An intuitive question arises: which samples contribute the most to the training of the different layer\u2019s parameters? From literature Saxe et al. (2011), we know that even randomly generated filters can compute features that lead to good performance \u2013 presumably on easy samples.", "startOffset": 43, "endOffset": 1272}, {"referenceID": 6, "context": "Similar definitions on vi has been proposed in Self-paced learning (SPL) Kumar et al. (2010). In Stochastic Gradient descend (SGD) methods, parameters \u03b8 are updated with a certain step size \u03b7 in each iteration with regard to a set of training samples.", "startOffset": 73, "endOffset": 93}, {"referenceID": 1, "context": "All the code are based on a common deep learning package Theano from Bergstra et al. (2010); Bastien et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 1, "context": "It has been used to measure the \u201ceasiness\u201d of a sample in Curriculum learning Bengio et al. (2009) and Self-paced learning Kumar et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 1, "context": "It has been used to measure the \u201ceasiness\u201d of a sample in Curriculum learning Bengio et al. (2009) and Self-paced learning Kumar et al. (2010). Intuitively, the samples with large NLL should also have large sample importance (SI).", "startOffset": 78, "endOffset": 143}], "year": 2017, "abstractText": "The contribution of each sample during model training varies across training iterations and the model\u2019s parameters. We define the concept of sample importance as the change in parameters induced by a sample. In this paper, we explored the sample importance in training deep neural networks using stochastic gradient descent. We found that \u201ceasy\u201d samples \u2013 samples that are correctly and confidently classified at the end of the training \u2013 shape parameters closer to the output, while the \u201chard\u201d samples impact parameters closer to the input to the network. Further, \u201ceasy\u201d samples are relevant in the early training stages, and \u201chard\u201d in the late training stage. Further, we show that constructing batches which contain samples of comparable difficulties tends to be a poor strategy compared to maintaining a mix of both hard and easy samples in all of the batches. Interestingly, this contradicts some of the results on curriculum learning which suggest that ordering training examples in terms of difficulty can lead to better performance.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_288"}