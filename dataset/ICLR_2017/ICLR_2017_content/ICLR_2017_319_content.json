{"name": "ICLR_2017_319.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["BATCH NORMALIZATION", "Etai Littwin"], "emails": ["etailittwin@gmail.com", "liorwolf@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Residual Networks (He et al., 2015) (ResNets) are neural networks with skip connections. These networks, which are a specific case of Highway Networks (Srivastava et al., 2015), present state of the art results in the most competitive computer vision tasks including image classification and object detection.\nThe success of residual networks was attributed to the ability to train very deep networks when employing skip connections (He et al., 2016). A complementary view is presented by Veit et al. (2016), who attribute it to the power of ensembles and present an unraveled view of ResNets that depicts ResNets as an ensemble of networks that share weights, with a binomial depth distribution around half depth. They also present experimental evidence that short paths of lengths shorter than half-depth dominate the ResNet gradient during training.\nThe analysis presented here shows that ResNets are ensembles with a dynamic depth behavior. When starting the training process, the ensemble is dominated by shallow networks, with depths lower than half-depth. As training progresses, the effective depth of the ensemble increases. This increase in depth allows the ResNet to increase its effective capacity as the network becomes more and more accurate.\nOur analysis reveals the mechanism for this dynamic behavior and explains the driving force behind it. This mechanism remarkably takes place within the parameters of Batch Normalization (Ioffe & Szegedy, 2015), which is mostly considered as a normalization and a fine-grained whitening mechanism that addresses the problem of internal covariate shift and allows for faster learning rates.\nWe show that the scaling introduced by batch normalization determines the depth distribution in the virtual ensemble of the ResNet. These scales dynamically grow as training progresses, shifting the effective ensemble distribution to bigger depths.\nThe main tool we employ in our analysis is spin glass models. Choromanska et al. (2015a) have created a link between conventional networks and such models, which leads to a comprehensive study of the critical points of neural networks based on the spin glass analysis of Auffinger et al. (2013). In our work, we generalize these results and link ResNets to generalized spin glass models. These models allow us to analyze the dynamic behavior presented above. Finally, we apply the results of Auffinger & Arous (2013) in order to study the loss surface of ResNets."}, {"heading": "2 A RECAP OF CHOROMANSKA ET AL. (2015A)", "text": "We briefly summarize Choromanska et al. (2015a), which connects the loss function of multilayer networks with the hamiltonian of the p spherical spin glass model, and state their main contributions and results. The notations of our paper are summarized in Appendix A and slightly differ from those in Choromanska et al. (2015a).\nA simple feed forward fully connected network N , with p layers and a single output unit is considered. Let ni be the number of units in layer i, such that n0 is the dimension of the input, and np = 1. It is further assumed that the ReLU activation functions denoted by R() are used. The output Y of the network given an input vector x \u2208 Rd can be expressed as\nY = d\u2211 i=1 \u03b3\u2211 j=1 xijAij p\u220f k=1 w (k) ij , (1)\nwhere the first summation is over the network inputs x1...xd, and the second is over all paths from input to output. There are \u03b3 = \u220fp i=1 ni such paths and \u2200i, xi1 = xi2 = ...xi\u03b3 . The variable Aij \u2208 {0, 1} denotes whether the path is active, i.e., whether all of the ReLU units along this path are producing positive activations, and the product \u220fp k=1 w (k) ij represents the specific weight configurationw1ij ...w k ij multiplying xi given path j. It is assumed throughout the paper that the input variables are sampled i.i.d from a normal Gaussian distribution.\nDefinition 1. The mass of the network N is defined as \u03c8 = \u220fp i=0 ni.\nThe variablesAij are modeled as independent Bernoulli random variables with a success probability \u03c1, i.e., each path is equally likely to be active. Therefore,\nEA[Y ] = d\u2211 i=1 \u03b3\u2211 j=1 xij\u03c1 p\u220f k=1 w (k) ij . (2)\nThe task of binary classification using the networkN with parameters w is considered, using either the hinge loss LhN or the absolute loss LaN :\nLhN (w) = EA[max(0, 1\u2212 YxY )], LaN (w) = EA[|Yx \u2212 Y |] (3)\nwhere Yx is a random variable corresponding to the true label of sample x. In order to equate either loss with the hamiltonian of the p-spherical spin glass model, a few key approximations are made:\nA1 Variable independence - The inputs xij are modeled as independent normal Gaussian random variables.\nA2 Redundancy in network parameterization - It is assumed the set of all the network weights [w1, w2...wN ] contains only \u039b unique weights such that \u039b < N .\nA3 Uniformity - It is assumed that all unique weights are close to being evenly distributed on the graph of connections defining the network N . Practically, this means that we assume every node is adjacent to an edge with any one of the \u039b unique weights.\nA4 Spherical constraint - The following is assumed:\n1\n\u039b \u039b\u2211 i=1 w2i = C 2 (4)\nfor some constant C > 0.\nThese assumptions are made for the sake of analysis, and do not necessarily hold. The validity of these assumption was posed as an open problem in Choromanska et al. (2015b), where a different degree of plausibility was assigned to each. Specifically, A1, as well as the independence assumption of Aij , were deemed unrealistic, and A2 - A4 as plausible. For example, A1 does not hold since each input xi is associated with many different paths and xi1 = xi2 = ...xi\u03b3 . See Choromanska et al. (2015a) for further justification of these approximations.\nUnder A1\u2013A4, the loss takes the form of a centered Gaussian process on the sphere S\u039b\u22121( \u221a\n\u039b). Specifically, it is shown to resemble the hamiltonian of the a spherical p-spin glass model given by:\nHp,\u039b(w\u0303) = 1\n\u039b p\u22121 2 \u039b\u2211 i1...ip xi1...ip r\u220f k=1 w\u0303ik (5)\nwith spherical constraint 1\n\u039b \u039b\u2211 i=1 w\u03032i = 1 (6)\nwhere xi1...ip are independent normal Gaussian variables.\nIn Auffinger et al. (2013), the asymptotic complexity of spherical p spin glass model is analyzed based on random matrix theory. In Choromanska et al. (2015a) these results are used in order to shed light on the optimization process of neural networks. For example, the asymptotic complexity of spherical spin glasses reveals a layered structure of low-index critical points near the global optimum. These findings are then given as a possible explanation to several central phenomena found in neural networks optimization, such as similar performance of large nets, and the improbability of getting stuck in a \u201cbad\u201d local minima.\nAs part of our work, we follow a similar path. First, a link is formed between residual networks and the hamiltonian of a general multi-interaction spherical spin glass model as given by:\nHp,\u039b(w\u0303) = p\u2211 r=1 r \u039b r\u22121 2 \u039b\u2211 i1,i2...ir=1 xi1,i2...ir r\u220f k=1 w\u0303ik (7)\nwhere 1... p are positive constants. Then, using Auffinger & Arous (2013), we obtain insights on residual networks. The other part of our work studies the dynamic behavior of residual networks, where we relax the assumptions made for the spin glass model."}, {"heading": "3 RESIDUAL NETS AND GENERAL SPIN GLASS MODELS", "text": "We begin by establishing a connection between the loss function of deep residual networks and the hamiltonian of the general spherical spin glass model. We consider a simple feed forward fully connected network N , with ReLU activation functions and residual connections. For simplicity of notations without the loss of generality, we assume n1 = ... = np = n. n0 = d as before. In our ResNet model, there exist p\u2212 1 identity connections skipping a single layer each, starting from the first hidden layer. The output of layer l > 1 is given by:\nNl(x) = R(W>l Nl\u22121(x)) +Nl\u22121(x) (8) where Wl denotes the weight matrix connecting layer l\u2212 1 with layer l. Notice that the first hidden layer has no parallel skip connection, and so N1(x) = R(W>1 x). Without loss of generality, the scalar output of the network is the sum of the outputs of the output layer p and is expressed as\nY = p\u2211 r=1 d\u2211 i=1 \u03b3r\u2211 j=1 x (r) ij A (r) ij r\u220f k=1 w (r)(k) ij (9)\nwhere A(r)ij \u2208 {0, 1} denotes whether path j of length r is open, and \u2200j, j\u2032, r, r\u2032 xrij = xr \u2032\nij\u2032 . The residual connections in N imply that the output Y is now the sum of products of different lengths, indexed by r. Since our ResNet model attaches a skip connection to every layer except the first, 1 \u2264 r \u2264 p. See Sec. 6 regarding models with less frequent skip connections. Each path of length r includes r \u2212 1 non-skip connections (those involving the first term in Eq. 8 and not the second, identity term) out of layers l = 2..p. Therefore, \u03b3r = ( p\u22121 r\u22121 ) nr. We define the following measure on the network: Definition 2. The mass of a depth r subnetwork in N is defined as \u03c8r = d\u03b3r.\nThe properties of redundancy in network parameters and their uniform distribution, as described in Sec. 2, allow us to re-index Eq. 9.\nLemma 1. Assuming assumptions A2 \u2212 A4 hold, and n\u039b \u2208 Z, then the output can be expressed after reindexing as:\nY = p\u2211 r=1 \u039b\u2211 i1,i2...ir=1 \u03c8r \u039br\u2211 j=1 x (j) i1,i2...ir A (j) i1,i2...ir r\u220f k=1 wik . (10)\nAll proofs can be found in Appendix B.\nMaking the modeling assumption that the ReLU gates are independent Bernoulli random variables with probability \u03c1, we obtain that for every path of length r, EA(j)i1,i2...ir = \u03c1 r and\nEA[Y ] = p\u2211 r=1 \u039b\u2211 i1,i2...ir=1 \u03c8r \u039br\u2211 j=1 x (j) i1,i2...ir \u03c1r r\u220f k=1 wik . (11)\nIn order to connect ResNets to generalized spherical spin glass models, we denote the variables:\n\u03bei1,i2...ir = \u03c8r \u039br\u2211 j=1 xji1,i2...ir , x\u0303i1,i2...ir = \u03bei1,i2...ir Ex[\u03be2i1,i2...ir ] 1 2\n(12)\nNote that since the input variables x1...xd are sampled from a centered Gaussian distribution (dependent or not), then the set of variables x\u0303i1,i2...ir are dependent normal Gaussian variables. Lemma 2. Assuming A2\u2212A3 hold, and n\u039b \u2208 N then \u2200r,i1...ir the following holds:\n1 d ( \u03c8r \u039br )2 \u2264 E[\u03be2i1,i2...ir ] \u2264 ( \u03c8r \u039br )2. (13)\nWe approximate the expected output EA(Y ) with Y\u0302 by assuming the minimal value in 13 holds such that \u2200r,i1...ir E[\u03be2i1,i2...ir ] = 1 d ( \u03c8r \u039br )\n2. This approximation holds exactly when \u039b = n, since all weight configurations of a particular length in Eq. 10 will appear the same number of times. When \u039b 6= n, the uniformity assumption dictates that each configuration of weights would appear approximately equally regardless of the inputs, and the expectation values would be very close to the lower bound. The following expression for Y\u0302 is thus obtained:\nY\u0302 = p\u2211 r=1 ( \u03c1 \u039b )r \u03c8r\u221a d \u039b\u2211 i1,i2...ir=1 x\u0303i1,i2...ir r\u220f k=1 wik . (14)\nThe independence assumption A1 was not assumed yet, and 14 holds regardless. Assuming A4 and denoting the scaled weights w\u0303i = 1Cwi, we can link the distribution of Y\u0302 to the distribution on x\u0303:\nY\u0302 = p\u2211 r=1 \u03c8r\u221a d ( \u03c1C \u039b )r \u039b\u2211 i1,i2...ir=1 x\u0303i1,i2...ir r\u220f k=1 w\u0303ik\n= z p\u2211 r=2 r \u039b r\u22121 2 \u039b\u2211 i1,i2...ir=1 x\u0303i1,i2...ir r\u220f k=1 w\u0303ik (15)\nwhere r = r = 1z ( p\u22121 r\u22121 ) (\u03c1nC\u221a \u039b )r and z is a normalization factor such that \u2211p r=1 2 r = 1.\nThe following lemma gives a generalized expression for the binary and hinge losses of the network.\nLemma 3 ( Choromanska et al. (2015a)). Assuming assumptions A2 \u2212 A4 hold, then both the losses LhN (x) and LaN (x) can be generalized to a distribution of the form:\nLN (x) = C1 + C2Y\u0302 (16)\nwhere C1, C2 are positive constants that do not affect the optimization process.\nThe model in Eq. 16 has the form of a spin glass model, except for the dependency between the variables x\u0303i1,i2...ir . We later use an assumption similar to A1 of independence between these variables in order to link the two binary classification losses and the general spherical spin glass model. However, for the results in this section, this is not necessary.\nWe denote the important quantities:\n\u03b2 = \u03c1nC\u221a\n\u039b , r =\n1\nz ( p\u2212 1 r \u2212 1 ) \u03b2r (17)\nThe series ( r) p r=1 determines the weight of interactions of a specific length in the loss surface. Notice that for constant depth p and large enough \u03b2, arg maxr( r) = p. Therefore, for wide networks, where n and, therefore, \u03b2 are large, interactions of order p dominate the loss surface, and the effect of the residual connections diminishes. Conversely, for constant \u03b2 and a large enough p (deep networks), we have that arg maxr( r) < p, and can expect interactions of order r < p to dominate the loss. The asymptotic behavior of is captured by the following lemma:\nTheorem 1. Assuming \u03b21+\u03b2 p \u2208 N, we have that:\nlim p\u2192\u221e\n1 p arg max r ( r) = \u03b2 1 + \u03b2 (18)\nAs the next theorem shows, the epsilons are concentrated in a narrow band near the maximal value.\nTheorem 2. For any \u03b11 < \u03b21+\u03b2 < \u03b12, and assuming \u03b11p, \u03b12p, \u03b2 1+\u03b2 p \u2208 N, it holds that:\nlim p\u2192\u221e \u03b12p\u2211 r=\u03b11p 2r = 1 (19)\nThm. 2 implies that for deep residual networks, the contribution of weight products of order far away from the maximum \u03b21+\u03b2 p is negligible. The loss is, therefor, similar in complexity to that of an ensemble of potentially shallow conventional nets. The next Lemma shows that we can shift the effective depth to any value by simply controlling C.\nLemma 4. For any integer 1 \u2264 k \u2264 p there exists a global scaling parameter C such that arg maxr( r(C)) = k.\nA simple global scaling of the weights is, therefore, enough to change the loss surface, from an ensemble of shallow conventional nets, to an ensemble of deep nets. This is illustrated in Fig. 1(a-c) for various values of \u03b2. In a common weight initialization scheme for neural networks,C = 1\u221a\nn (Orr\n& Mu\u0308ller, 2003; Glorot & Bengio, 2010). With this initialization and \u039b = n, \u03b2 = \u03c1 and the maximal weight is obtained at less than half the network\u2019s depth limp\u2192\u221e arg maxr( r) < p 2 . Therefore, at the initialization, the loss function is primarily influenced by interactions of considerably lower order than the depth p, which facilitates easier optimization."}, {"heading": "4 DYNAMIC BEHAVIOR OF RESIDUAL NETS", "text": "The expression for the output of a residual net in Eq. 15 provides valuable insights into the machinery at work when optimizing such models. Thm. 1 and 2 imply that the loss surface resembles that of an ensemble of shallow nets (although not a real ensemble due to obvious dependencies), with various depths concentrated in a narrow band. As noticed in Veit et al. (2016), viewing ResNets as ensembles of relatively shallow networks helps in explaining some of the apparent advantages of these models, particularly the apparent ease of optimization of extremely deep models, since deep paths barely affect the overall loss of the network. However, this alone does not explain the increase in accuracy of deep residual nets over actual ensembles of standard networks. In order to explain the improved performance of ResNets, we make the following claims:\n1. The distribution of the depths of the networks within the ensemble is controlled by the scaling parameter C.\n2. During training, C changes and causes a shift of focus from a shallow ensemble to deeper and deeper ensembles, which leads to an additional capacity.\n3. In networks that employ batch normalization, C is directly embodied as the scale parameter \u03bb. The starting condition of \u03bb = 1 offers a good starting condition that involves extremely shallow nets.\nFor the remainder of Sec.4, we relax all assumptions, and assume that at some point in time 1 \u039b \u2211\u039b i=1 w 2 i = C\n2, and \u039b = N . Using Eq. 9 for the output of the network Y\u0302 in Lemma. 3, the loss can be expressed:\nLN (x,w) = C1 + C2 p\u2211 r=1 d\u2211 i=1 \u03b3r\u2211 j=1 x (r) ij A (r) ij r\u220f k=1 w (r)(k) ij (20)\nwhere C1, C2 are some constants that do not affect the optimization process. In order to gain additional insight into this dynamic mechanism, we investigate the derivative of the loss with respect to the scale parameter C. Using Eq. 9 for the output, we obtain:\n\u2202LN (x,w) \u2202C = C2 C p\u2211 r=1 r d\u2211 i=1 \u03b3r\u2211 j=1 x (r) ij A (r) ij r\u220f k=1 w (r)(k) ij (21)\nNotice that the addition of a multiplier r indicates that the derivative is increasingly influenced by deeper networks."}, {"heading": "4.1 BATCH NORMALIZATION", "text": "Batch normalization has shown to be a crucial factor in the successful training of deep residual networks. As we will show, batch normalization layers offer an easy starting condition for the network, such that the gradients from early in the training process will originate from extremely shallow paths.\nWe consider a simple batch normalization procedure, which ignores the additive terms, has the output of each ReLU unit in layer l normalized by a factor \u03c3l and then is multiplied by some parameter \u03bbl. The output of layer l > 1 is therefore:\nNl(x) = \u03bbl \u03c3l R(W>l Nl\u22121(x)) +Nl\u22121(x) (22)\nwhere \u03c3l is the mean of the estimated standard deviations of various elements in the vector R(W>l Nl\u22121(x)). Furthermore, a typical initialization of batch normalization parameters is to set \u2200l, \u03bbl = 1. In this case, providing that units in the same layer have equal variance \u03c3l, the recursive relation E[Nl+1(x)2j ] = 1 + E[Nl(x)2j ] holds for any unit j in layer l. This, in turn, implies that the output of the ReLU units should have increasing variance \u03c32l as a function of depth. Multiplying the weight parameters in deep layers with an increasingly small scaling factor 1\u03c3l , effectively reduces the influence of deeper paths, so that extremely short paths will dominate the early stages of optimization. We next analyze how the weight scaling, as introduced by batch normalization, provides a driving force for the effective ensemble to become deeper as training progresses."}, {"heading": "4.2 THE DRIVING FORCE BEHIND THE SCALE INCREASE", "text": "The analysis below focuses on a single residual connection skipping a block of one or more layers. Since it holds for each block individually, it holds also for a residual network of multiple skipped blocks of arbitrary depth.\nWe consider a simple network of depth p, with a single residual connection skipping p \u2212m layers. We further assume that batch normalization is applied at the output of each ReLU unit as described in Eq. 22. We denote by l1...lm the indices of layers that are not skipped by the residual connection, and \u03bb\u0302m = \u220fm i=1\n\u03bbli \u03c3li\n, \u03bb\u0302p = \u220fp i=1 \u03bbi \u03c3i . Since every path of length m is multiplied by \u03bb\u0302m, and every\npath of length p is multiplied by \u03bb\u0302p, the expression for the loss can be expressed using Eq. 20 and\nignoring constant terms:\nLN (x,w) = \u03bb\u0302m d\u2211 i=1 \u03b3m\u2211 j=1 x (m) ij A (m) ij r\u220f k=1 w (m)(k) ij + \u03bb\u0302p d\u2211 i=1 \u03b3p\u2211 j=1 x (m) ij A (p) ij p\u220f k=1 w (p)(k) ij\n= Lm(x,w) + Lp(x,w) (23)\nWe denote by \u2207w the derivative operator with respect to the parameters w, and the gradient g = \u2207wLN (x,w) = gm + gp evaluated at point w.\nTheorem 3. Considering the loss in 23, and assuming \u2202LN (x,w)\u2202\u03bbl = 0, then for a small learning rate 0 < \u00b5 << 1 the following hold:\n1. For any \u03bbl\u2208l1...lm then: \u2223\u2223\u2223\u2223\u03bbl \u2212 \u00b5\u2202LN (x,w \u2212 \u00b5g)\u2202\u03bbl \u2223\u2223\u2223\u2223 > |\u03bbl| (24)\n2. For any \u03bbl 6\u2208l1...lm , if \u2016gp\u201622 + g>p gm > 0 then:\u2223\u2223\u2223\u2223\u03bbl \u2212 \u00b5\u2202LN (x,w \u2212 \u00b5g)\u2202\u03bbl \u2223\u2223\u2223\u2223 > |\u03bbl| (25)\nThm. 3 suggests that |\u03bbl| will increase for layers l that do not have skip-connections. Conversely, if layer l has a parallel skip connection, then |\u03bbl| will increase if \u2016gp\u20162 > \u2016gm\u20162, where the later condition implies that shallow paths are nearing a local minima. Notice that an increase in |\u03bbl 6\u2208l1...lm | results in an increase in |\u03bb\u0303p|, while |\u03bb\u0303m| remains unchanged, therefore shifting the balance into deeper ensembles.\nThis steady increase of |\u03bbl|, as predicted in our theoretical analysis, is also backed in experimental results, as depicted in Fig. 1(d). Note that the first layer, which cannot be skipped, behaves differently than the other layers. More experiments can be found in Appendix C.\nIt is worth noting that the mechanism for this dynamic property of residual networks can also be observed without the use of batch normalization, as a steady increase in the L2 norm of the weights, as shown in Fig. 1(e). In order to model this, consider the residual network as discussed above, without batch normalization layers. Recalling, \u2016w\u20162 = C \u221a \u039b, w\u0303 = wC , the loss of this network is expressed as:\nLN (x,w) = Cm d\u2211 i=1 \u03b3m\u2211 j=1 x (m) ij A (m) ij r\u220f k=1 w\u0303 (m)(k) ij + C p d\u2211 i=1 \u03b3p\u2211 j=1 x (m) ij A (p) ij p\u220f k=1 w\u0303 (p)(k) ij\n= Lm(x,w) + Lp(x,w) (26)\nTheorem 4. Considering the loss in 26, and assuming \u2202LN (x,w)\u2202C = 0, then for a small learning rate 0 < \u00b5 << 1 the following hold:\n\u2202LN (x,w \u2212 \u00b5g) \u2202C \u2248 \u2212\u00b5 1 C (m\u2016gm\u201622 + p\u2016gp\u201622 + (m+ p)g>p gm) (27)\nThm. 4 indicates that if either \u2016gp\u20162 or \u2016gm\u20162 is dominant (for example, near local minimas of the shallow network, or at the start of training), the scaling of the weights C will increase. This expansion will, in turn, emphasize the contribution of deeper paths over shallow paths, and increase the overall capacity of the residual network. This dynamic behavior of the effective depth of residual networks is of key importance in understanding the effectiveness of these models. While optimization starts off rather easily with gradients largely originating from shallow paths, the overall advantage of depth is still maintained by the dynamic increase of the effective depth."}, {"heading": "5 THE LOSS SURFACE OF ENSEMBLES", "text": "We now present the results of Auffinger & Arous (2013) regarding the asymptotic complexity in the case of lim\u039b\u2192\u221e of the multi-spherical spin glass model given by:\nH ,\u039b = \u2212 \u221e\u2211 r=2 r \u039b r\u22121 2 \u039b\u2211 i1,...ir=1 Jri1...ir w\u0303i2 ...w\u0303ir (28)\nwhere Jri1...ir are independent centered standard Gaussian variables, and = ( r)r>2 are positive real numbers such that \u2211\u221e r=2 r2\nr < \u221e. A configuration w of the spin spherical spin-glass model is a vector in R\u039b satisfying the spherical constraint:\n1\n\u039b \u039b\u2211 i=1 w2i = 1, \u221e\u2211 r=2 2r = 1 (29)\nNote that the variance of the process is independent of :\nE[H2 ,\u039b] = \u221e\u2211 r=2 \u039b1\u2212r 2r( \u039b\u2211 i=1 w2i ) r = \u039b \u221e\u2211 r=1 2r = \u039b (30)\nDefinition 3. We define the following:\nv\u2032 = \u221e\u2211 r=2 2rr, v \u2032\u2032 = \u221e\u2211 r=2 2rr(r \u2212 1), \u03b12 = v\u2032\u2032 + v\u2032 \u2212 v\u20322 (31)\nNote that for the single interaction spherical spin model \u03b12 = 0. The index of a critical point of H ,\u039b is defined as the number of negative eigenvalues in the hessian\u22072H ,\u039b evaluated at the critical point w. Definition 4. For any 0 \u2264 k < \u039b and u \u2208 R, we denote the random number Crt\u03bb,k(u, ) as the number of critical points of the hamiltonian in the set BX = {\u039bX|X \u2208 (\u2212\u221e, u)} with index k. That is:\nCrt\u039b,k(u, ) = \u2211\nw:\u2207H ,\u039b=0 1 {H ,\u039b \u2208 \u039bu}1\n{ i(\u22072H ,\u039b) = k } (32)\nFurthermore, define \u03b8k(u, ) = lim\u039b\u2192\u221e 1\u039b log E[Crt\u039b,k(u )]. Corollary 1.1 of Auffinger & Arous (2013) states that for any k > 0:\n\u03b8k(R, ) = 1\n2 log(\nv\u2032\u2032\nv\u2032 )\u2212 v\n\u2032\u2032 \u2212 v\u2032\nv\u2032\u2032 + v\u2032 (33)\nEq. 33 provides the asymptotic mean total number of critical points with non-diverging index k. It is presumed that the SGD algorithm will easily avoid critical points with a high index that have many descent directions, and maneuver towards low index critical points. We, therefore, investigate how the mean total number of low index critical points vary as the ensemble distribution embodied in ( r)r>2 changes its shape by a steady increase in \u03b2.\nFig. 1(f) shows that as the ensemble progresses towards deeper networks, the mean amount of low index critical points increases, which might cause the SGD optimizer to get stuck in local minima. This is, however, resolved by the the fact that by the time the ensemble becomes deep enough, the loss function has already reached a point of low energy as shallower ensembles were more dominant earlier in the training. In the following theorem, we assume a finite ensemble such that\u2211\u221e r=p+1 r2\nr \u2248 0. Theorem 5. For any k \u2208 N, p > 1, we denote the solution to the following constrained optimization problems:\n\u2217 = arg max \u03b8k(R, ) s.t p\u2211 r=2 2r = 1 (34)\nIt holds that:\n\u2217r =\n{ 1, r = p\n0, otherwise (35)\nThm. 5 implies that any heterogeneous mixture of spin glasses contains fewer critical points of a finite index, than a mixture in which only p interactions are considered. Therefore, for any distribution of that is attainable during the training of a ResNet of depth p, the number of critical points is lower than the number of critical points for a conventional network of depth p."}, {"heading": "6 DISCUSSION", "text": "In this work, we use spin glass analysis in order to understand the dynamic behavior ResNets display during training and to study their loss surface. In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska et al. (2015a). The last two assumptions, i.e., the two independence assumptions, are deemed in Choromanska et al. (2015b) as unrealistic, while the remaining are considered plausible.\nOur analysis of critical points in ensembles (Sec. 5) requires all of the above assumptions. However, Thm. 1 and 2, as well as Lemma. 4, do not assume the last assumption, i.e., the independence between the different copies of the input. Moreover, the analysis of the dynamic behavior of residual nets (Sec. 4) does not assume any of the above assumptions.\nOur results are well aligned with some of the results shown in Larsson et al. (2016), where it is noted empirically that the deepest column trains last. This is reminiscent of our claim that the deeper networks of the ensemble become more prominent as training progresses. The authors of Larsson et al. (2016) hypothesize that this is a result of the shallower columns being stabilized at a certain point of the training process. In our work, we discover the exact driving force that comes into play.\nIn addition, our work offers an insight into the mechanics of the recently proposed densely connected networks (Huang et al., 2016). Following the analysis we provide in Sec. 3, the additional shortcut paths decrease the initial capacity of the network by offering many more short paths from input to output, thereby contributing to the ease of optimization when training starts. The driving force mechanism described in Sec. 4.2 will then cause the effective capacity of the network to increase.\nNote that the analysis presented in Sec. 3 can be generalized to architectures with arbitrary skip connections, including dense nets. This is done directly by including all of the induced sub networks in Eq. 9. The reformulation of Eq. 10 would still holds, given that \u03a8r is modified accordingly."}, {"heading": "7 CONCLUSION", "text": "Ensembles are a powerful model for ResNets, which unravels some of the key questions that have surrounded ResNets since their introduction. Here, we show that ResNets display a dynamic ensemble behavior, which explains the ease of training such networks even at very large depths, while still maintaining the advantage of depth. As far as we know, the dynamic behavior of the effective capacity is unlike anything documented in the deep learning literature. Surprisingly, the dynamic mechanism typically takes place within the outer multiplicative factor of the batch normalization module."}, {"heading": "A SUMMARY OF NOTATIONS", "text": "Table 1 presents the various symbols used throughout this work and their meaning."}, {"heading": "B PROOFS", "text": "Proof of Lemma 1. There are a total of \u03c8r paths of length r from input to output, and a total of \u039br unique r length configurations of weights. The uniformity assumption then implies that each configuration of weights is repeated \u03c8r\u039br times. By summing over the unique configurations, and re indexing the input we arrive at Eq. 10.\nProof of Lemma 2. From 12, we have that \u03bei1,i2...ir is defined as a sum of \u03c8r \u039br inputs. Since there are only p distinct inputs, it holds that for each \u03bei1,i2...ir there exists a sequence \u03b1 = (\u03b1i) p i=1 \u2208 N such\nthat \u2211d i=1 \u03b1i = \u03c8r \u039br , and \u03bei1,i2...ir = \u2211d i=1 \u03b1ixi. We, therefore, have that E[\u03be2i1,i2...ir ] = \u2016\u03b1\u2016 2 2. Note that the minimum value of E[\u03be2i1,i2...ir ] is a solution to the following:\nmin(E[\u03be2i1,i2...ir ]) = min\u03b1(\u2016\u03b1\u20162) s.t \u2016\u03b1\u20161 = \u03c8r \u039br , (\u03b1i) p i=1 \u2208 N, (36)\nwhich achieves its minimal value at \u2200i, \u03b1i = 1d \u03c8r \u039br . Similarly, the maximum value is achieved at \u03b1i = \u03c8r \u039br \u03b4i for some index i.\nProof of Thm. 1. We use the stirling approximation, which states limp\u2192\u221e 1p log( ( p \u03b1p ) ) = H(\u03b1), where H(\u03b1) = \u2212\u03b1log(\u03b1)\u2212 (1\u2212\u03b1)log(1\u2212\u03b1). Ignoring the constants which do not depend on \u03b1,\nlim p\u2192\u221e\n1 p log(\n( p\n\u03b1p\n) \u03b2\u03b1p) = H(\u03b1) + \u03b1log(\u03b2) (37)\nwhich achieves its maximum value at \u03b1 = \u03b1\u2217.\nProof of Thm. 2. For brevity, we provide a sketch of the proof. It is enough to show that limp\u2192\u221e \u2211\u03b11p r=1 2 r = 0 for \u03b2 < 1. Ignoring the constants in the binomial terms, we have:\nlim p\u2192\u221e \u03b11p\u2211 r=1 2i = lim p\u2192\u221e \u2211\u03b11p i=1 ( p r )2 \u03b22r z2 \u2264 lim p\u2192\u221e \u03b11p ( p \u03b11p )2 \u03b22\u03b11p z2 (38)\nWhere z2 = \u2211p r=1 ( p r )2 \u03b22r, which can be expressed using the Legendre polynomial of order p:\nz2 = (1\u2212 \u03b22)pPp( 1 + \u03b22\n1\u2212 \u03b22 ) (39)\nIn order to compute the limit of Eq. 38, we use the asymptotic of the Legendre polynomial of order p for x > 1, Pp(x) \u223c 1\u221a2\u03c0p (x+ \u221a x2\u22121)p+ 1 2\n(x2\u22121) 1 4\n. For the term in the nominator of Eq. 38 , we use the\nStirling approximation for factorials p! \u223c \u221a\n2\u03c0p(pe ) p. Substituting both approximations in Eq. 38\nand taking the limit completes the proof.\nProof of Lemma 4. For simplicity, we ignore the constants in the binomial coefficient, and assume r = 1 z ( p r ) \u03b2r. Notice that for \u03b2\u2217 = ( p p 2 ) , we have that arg maxr( r(\u03b2 \u2217)) = p, arg maxr( r( 1 \u03b2\u2217 )) = 1 and arg maxr( r(1)) = p 2 . From the monotonicity and continuity of \u03b2\nr, any value 1 \u2265 k \u2265 p can be attained. The linear dependency \u03b2(C) = \u03c1nC\u221a\n\u039b completes the proof.\nProof of Thm. 3. 1. Notice that by definition, layer l is not skipped by the residual connection, and therefore \u03bbl multiplies every path in the network. Therefore,\n\u2202LN (x,w) \u2202\u03bbl = 1\u03bbl (Lm(x,w) + Lp(x,w)). Using taylor series expansion:\n\u2202LN (x,w \u2212 \u00b5g) \u2202\u03bbl \u2248 \u2202LN (x,w) \u2202\u03bbl \u2212 \u00b5\u2207w \u2202LN (x,w) \u2202\u03bbl g (40)\nSubstituting\u2207w \u2202LN (x,w)\u2202\u03bbl = 1 \u03bbl (gm + gp) in 40 we have:\n\u2202LN (x,w\u2212 \u00b5gw) \u2202\u03bbl \u2248 0\u2212 \u00b5 1 \u03bbl (gm + gp) >(gm + gp) = \u2212\u00b5 1 \u03bbl \u2016gm + gp\u201622 < 0 (41)\nAnd hence:\n\u03bbl \u2212 \u00b5 \u2202LN (x,w\u2212 \u00b5gw)\n\u2202\u03bbl = \u03bbl + \u00b5\n2 1\n\u03bbl \u2016gm + gp\u201622\n= \u03bbl(1 + \u00b5 2 1\n\u03bb2l \u2016gm + gp\u201622) (42)\nFinally:\n|\u03bbl(1 + \u00b52 1\n\u03bb2l \u2016gm + gp\u201622)| = |\u03bbl|(1 + \u00b52\n1\n\u03bb2l ) \u2265 |\u03bbl| (43)\n2. Since paths of length m skip layer l, we have that\u2207w \u2202LN (x,w)\u2202\u03bbl = 1 \u03bbl gp. Therefore:\n\u2202LN (x,w \u2212 \u00b5g) \u2202\u03bbl \u2248 0\u2212 \u00b5 1 \u03bbl (gm + gp) >gp = \u2212\u00b5 1 \u03bbl (g>mgp + \u2016gp\u201622) (44)\nThe condition \u2016gp\u20162 > \u2016gm\u20162 implies that g>mgp + \u2016gp\u201622 > 0, completing the proof.\nProof of Thm 4. Notice that \u2202LN (x,w)\u2202C = \u2202LN (x,w) \u2202w \u2202w \u2202\u2016w\u20162\n\u221a \u039b = g>w\u0303 = 0, and hence the gradient\nis orthogonal to the weights. We have that \u2202LN (x,w)\u2202C = 1 C (mLm(x,w) + pLp(x,w)). Using taylor series expansion we have:\n\u2202LN (x,w \u2212 \u00b5g) \u2202C \u2248 \u2202LN (x,w) \u2202C \u2212 \u00b5\u2207w \u2202LN (x,w) \u2202C g (45)\nFor the last term we have:\n\u2207w \u2202LN (x,w)\n\u2202C g = (mLm(x,w) + pLp(x,w))\u2207w\n\u221a \u039b\n\u2016w\u20162 g +\n1 C (mgm + pgp) >g\n= (mLm(x,w) + pLp(x,w)) w>g\nC 3 2\n+ 1\nC (mgm + pgp)\n>g = 1\nC (mgm + pgp)\n>g, (46)\nwhere the last step stems from the fact thatw>g = 0. Substituting\u2207w \u2202LN (x,w)\u2202C = 1 C (mgm+pgp) in 45 we have:\n\u2202LN (x,w\u2212 \u00b5gw) \u2202C \u2248 0\u2212 \u00b5 1 C (mgm + pgp) >(gm + gp)\n= \u2212\u00b5 1 C (m\u2016gp\u201622 + p\u2016gp\u201622 + (m+ p)g>p gm) (47)\nProof of Thm 5. Inserting Eq. 31 into Eq. 33 we have that:\n\u03b8k(R, ) = 1\n2 log(\n\u2211p r=2\n2 rr(r \u2212 1)\u2211p\nr=2 2 rr\n)\u2212 \u2211p r=2 2 rr(r \u2212 2)\u2211p\nr=2 2 rr\n2 (48)\nWe denote the matrices V \u2032 and V \u2032\u2032 such that V \u2032ij = r\u03b4ij and V \u2032\u2032 ij = r(r \u2212 1)\u03b4ij . We then have:\n\u03b8k(R, ) = 1\n2 log(\n>V \u2032\u2032 >V \u2032 )\u2212 >(V \u2032\u2032 \u2212 V \u2032) >(V \u2032\u2032 + V \u2032)\n(49)\nmax \u03b8k(R, ) \u2264 max ( 1\n2 log(\n>V \u2032\u2032 >V \u2032 ))\u2212min ( >(V \u2032\u2032 \u2212 V \u2032) >(V \u2032\u2032 + V \u2032) )\n= 1\n2 log\n( maxi(V \u2032\u2032 iiV \u2032\u22121 ii ) ) \u2212mini ( (V \u2032\u2032ii \u2212 V \u2032ii)(V \u2032\u2032ii + V \u2032ii)\u22121 ) = 1\n2 log(p\u2212 1)\u2212 (1\u2212 2 p ) = \u03b8k(R, \u2217) (50)"}, {"heading": "C ADDITIONAL EXPERIMENTS", "text": "Fig. 1(d) and 1(e) report the experimental results of a straightforward setting, in which the task is to classify a mixture of 10 multivariate Gaussians in 50D. The input is therefore of size 50. The loss employed is the cross entropy loss of ten classes. The network has 10 blocks, each containing 20 hidden neurons, a batch normalization layer, and a skip connection. Training was performed on 10,000 samples, using SGD with minibatches of 50 samples.\nNext, we provide additional experiments performed on the public CIFAR-10 and CIFAR-100 data sets (Krizhevsky, 2009). The public ResNet code of https://github.com/facebook/fb. resnet.torch is used for networks of depth 32.\nAs noted in Sec. 4.2, the dynamic behavior can be present in the Batch Normalization multiplicative coefficient or in the weight matrices themselves. In the following experiments, it seems that\nuntil the learning rate is reduced, the dynamic behavior is manifested in the Batch Normalization multiplicative coefficients and then it moves to the convolution layers themselves. We therefore absorb the BN coefficients into the convolutional layer using the public code of https: //github.com/e-lab/torch-toolbox/tree/master/BN-absorber. Note that the multiplicative coefficient of Batch Normalization is typically refereed to as \u03b3. However, throughout our paper, since we follow the notation of Choromanska et al. (2015a), \u03b3 refers to the number of paths. The multiplicative factor of Batch normalization appears as \u03bb in Sec. 4.\nFig. 2 depicts the results. There are two types of plots: Fig. 2(a,c) presents for CIFAR-10 and CIFAR-100 respectively the magnitude of the various convolutional layers for multiple epochs (similar in type to Fig. 1(d) in the paper). Fig. 2(b,d) depict for the two datasets the mean of these norms over all convolutional layers as a function of epoch (similar to Fig. 1(e)).\nAs can be seen, the dynamic phenomenon we describe is very prominent in the public ResNet implementation when applied to these conventional datasets: the dominance of paths with fewer skip connections increases over time. Moreover, once the learning rate is reduced in epoch 81 the phenomenon we describe speeds up.\nIn Fig. 3 we present the multiplicative coefficient of the Batch Normalization when not absorbed. As future work, we would like to better understand why these coefficients start to decrease once the learning rate is reduced. As shown above, taking the magnitude of the convolutions into account, the dynamic phenomenon we study becomes even more prominent at this point. The change of location from the multiplicative coefficient of the Batch Normalization layers to the convolutions themselves might indicate that Batch Normalization is no longer required at this point. Indeed, Batch Normalization enables larger training rates and this shift happens exactly when the training rate is reduced. A complete analysis is left for future work."}], "references": [{"title": "Complexity of random smooth functions on the highdimensional sphere", "author": ["Antonio Auffinger", "Gerard Ben Arous"], "venue": "Annals of Probability, 41(6):4214\u20134247,", "citeRegEx": "Auffinger and Arous.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger and Arous.", "year": 2013}, {"title": "Random matrices and complexity of spin glasses", "author": ["Antonio Auffinger", "Grard Ben Arous", "Ji ern"], "venue": "Communications on Pure and Applied Mathematics, 66(2):165\u2013201,", "citeRegEx": "Auffinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Micha\u00ebl Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Yann LeCun", "G\u00e9rard Ben Arous"], "venue": "In COLT,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Neural networks: tricks of the trade", "author": ["Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "Orr and M\u00fcller.,? \\Q2003\\E", "shortCiteRegEx": "Orr and M\u00fcller.", "year": 2003}, {"title": "Residual networks behave like ensembles of relatively shallow networks", "author": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "venue": "In NIPS,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "2015a), \u03b3 refers to the number of paths. The multiplicative factor of Batch normalization appears as \u03bb in Sec. 4. Fig. 2 depicts the results. There are two types of plots: Fig. 2(a,c) presents for CIFAR-10 and CIFAR-100 respectively the magnitude of the various convolutional layers for multiple epochs (sim", "author": ["Choromanska"], "venue": null, "citeRegEx": "Choromanska,? \\Q2015\\E", "shortCiteRegEx": "Choromanska", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "1 INTRODUCTION Residual Networks (He et al., 2015) (ResNets) are neural networks with skip connections.", "startOffset": 33, "endOffset": 50}, {"referenceID": 6, "context": "The success of residual networks was attributed to the ability to train very deep networks when employing skip connections (He et al., 2016).", "startOffset": 123, "endOffset": 140}, {"referenceID": 2, "context": "1 INTRODUCTION Residual Networks (He et al., 2015) (ResNets) are neural networks with skip connections. These networks, which are a specific case of Highway Networks (Srivastava et al., 2015), present state of the art results in the most competitive computer vision tasks including image classification and object detection. The success of residual networks was attributed to the ability to train very deep networks when employing skip connections (He et al., 2016). A complementary view is presented by Veit et al. (2016), who attribute it to the power of ensembles and present an unraveled view of ResNets that depicts ResNets as an ensemble of networks that share weights, with a binomial depth distribution around half depth.", "startOffset": 34, "endOffset": 523}, {"referenceID": 1, "context": "Choromanska et al. (2015a) have created a link between conventional networks and such models, which leads to a comprehensive study of the critical points of neural networks based on the spin glass analysis of Auffinger et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(2015a) have created a link between conventional networks and such models, which leads to a comprehensive study of the critical points of neural networks based on the spin glass analysis of Auffinger et al. (2013). In our work, we generalize these results and link ResNets to generalized spin glass models.", "startOffset": 190, "endOffset": 214}, {"referenceID": 1, "context": "(2015a) have created a link between conventional networks and such models, which leads to a comprehensive study of the critical points of neural networks based on the spin glass analysis of Auffinger et al. (2013). In our work, we generalize these results and link ResNets to generalized spin glass models. These models allow us to analyze the dynamic behavior presented above. Finally, we apply the results of Auffinger & Arous (2013) in order to study the loss surface of ResNets.", "startOffset": 190, "endOffset": 436}, {"referenceID": 2, "context": "(2015A) We briefly summarize Choromanska et al. (2015a), which connects the loss function of multilayer networks with the hamiltonian of the p spherical spin glass model, and state their main contributions and results.", "startOffset": 29, "endOffset": 56}, {"referenceID": 2, "context": "(2015A) We briefly summarize Choromanska et al. (2015a), which connects the loss function of multilayer networks with the hamiltonian of the p spherical spin glass model, and state their main contributions and results. The notations of our paper are summarized in Appendix A and slightly differ from those in Choromanska et al. (2015a). A simple feed forward fully connected network N , with p layers and a single output unit is considered.", "startOffset": 29, "endOffset": 336}, {"referenceID": 2, "context": "The validity of these assumption was posed as an open problem in Choromanska et al. (2015b), where a different degree of plausibility was assigned to each.", "startOffset": 65, "endOffset": 92}, {"referenceID": 2, "context": "The validity of these assumption was posed as an open problem in Choromanska et al. (2015b), where a different degree of plausibility was assigned to each. Specifically, A1, as well as the independence assumption of Aij , were deemed unrealistic, and A2 - A4 as plausible. For example, A1 does not hold since each input xi is associated with many different paths and xi1 = xi2 = ...xi\u03b3 . See Choromanska et al. (2015a) for further justification of these approximations.", "startOffset": 65, "endOffset": 419}, {"referenceID": 1, "context": "In Auffinger et al. (2013), the asymptotic complexity of spherical p spin glass model is analyzed based on random matrix theory.", "startOffset": 3, "endOffset": 27}, {"referenceID": 1, "context": "In Auffinger et al. (2013), the asymptotic complexity of spherical p spin glass model is analyzed based on random matrix theory. In Choromanska et al. (2015a) these results are used in order to shed light on the optimization process of neural networks.", "startOffset": 3, "endOffset": 159}, {"referenceID": 12, "context": "As noticed in Veit et al. (2016), viewing ResNets as ensembles of relatively shallow networks helps in explaining some of the apparent advantages of these models, particularly the apparent ease of optimization of extremely deep models, since deep paths barely affect the overall loss of the network.", "startOffset": 14, "endOffset": 33}, {"referenceID": 7, "context": "In addition, our work offers an insight into the mechanics of the recently proposed densely connected networks (Huang et al., 2016).", "startOffset": 111, "endOffset": 131}, {"referenceID": 2, "context": "In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska et al. (2015a). The last two assumptions, i.", "startOffset": 266, "endOffset": 293}, {"referenceID": 2, "context": "In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska et al. (2015a). The last two assumptions, i.e., the two independence assumptions, are deemed in Choromanska et al. (2015b) as unrealistic, while the remaining are considered plausible.", "startOffset": 266, "endOffset": 401}, {"referenceID": 2, "context": "In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska et al. (2015a). The last two assumptions, i.e., the two independence assumptions, are deemed in Choromanska et al. (2015b) as unrealistic, while the remaining are considered plausible. Our analysis of critical points in ensembles (Sec. 5) requires all of the above assumptions. However, Thm. 1 and 2, as well as Lemma. 4, do not assume the last assumption, i.e., the independence between the different copies of the input. Moreover, the analysis of the dynamic behavior of residual nets (Sec. 4) does not assume any of the above assumptions. Our results are well aligned with some of the results shown in Larsson et al. (2016), where it is noted empirically that the deepest column trains last.", "startOffset": 266, "endOffset": 905}, {"referenceID": 2, "context": "In particular, we use at one point or another the assumptions of redundancy in network parameters, near uniform distribution of network weights, independence between the inputs and the paths and independence between the different copies of the input as described in Choromanska et al. (2015a). The last two assumptions, i.e., the two independence assumptions, are deemed in Choromanska et al. (2015b) as unrealistic, while the remaining are considered plausible. Our analysis of critical points in ensembles (Sec. 5) requires all of the above assumptions. However, Thm. 1 and 2, as well as Lemma. 4, do not assume the last assumption, i.e., the independence between the different copies of the input. Moreover, the analysis of the dynamic behavior of residual nets (Sec. 4) does not assume any of the above assumptions. Our results are well aligned with some of the results shown in Larsson et al. (2016), where it is noted empirically that the deepest column trains last. This is reminiscent of our claim that the deeper networks of the ensemble become more prominent as training progresses. The authors of Larsson et al. (2016) hypothesize that this is a result of the shallower columns being stabilized at a certain point of the training process.", "startOffset": 266, "endOffset": 1130}], "year": 2016, "abstractText": "Deep Residual Networks present a premium in performance in comparison to conventional networks of the same depth and are trainable at extreme depths. It has recently been shown that Residual Networks behave like ensembles of relatively shallow networks. We show that these ensembles are dynamic: while initially the virtual ensemble is mostly at depths lower than half the network\u2019s depth, as training progresses, it becomes deeper and deeper. The main mechanism that controls the dynamic ensemble behavior is the scaling introduced, e.g., by the Batch Normalization technique. We explain this behavior and demonstrate the driving force behind it. As a main tool in our analysis, we employ generalized spin glass models, which we also use in order to study the number of critical points in the optimization of Residual Networks.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_319"}