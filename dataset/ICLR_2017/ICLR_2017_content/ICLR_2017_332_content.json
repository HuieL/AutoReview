{"name": "ICLR_2017_332.pdf", "metadata": {"source": "CRF", "title": "SPATIAL TRANSFORMATIONS", "authors": ["Jo\u00e3o F. Henriques", "Andrea Vedaldi"], "emails": ["joao@robots.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "A crucial aspect of current deep learning architectures is the encoding of invariances. This fact is epitomized in the success of convolutional neural networks (CNN), where equivariance to image translation is key: translating the input results in a translated output. When invariances are present in the data, encoding them explicitly in an architecture provides an important source of regularization, which allows to reduce the amount of training data required for learning. Invariances may also be used to improve the efficiency of implementations; for instance, a convolutional layer requires orders of magnitude less memory and also less computation compared to an equivalent fully-connected layer.\nThe success of CNNs indicates that translation invariance is an important property of images. However, this does not explain why translation equivariant operators work well for image understanding. The common interpretation is that such operators are matched to the statistics of natural images, which are well known to be translation invariant (Hyv\u00e4rinen et al., 2009). However, natural image statistics are also (largely) invariant to other transformations such as isotropic scaling and rotation, which suggests that alternative neural network designs may also work well with images. Furthermore, in specific applications, invariances other than translation may be more appropriate.\nTherefore, it is natural to consider generalizing convolutional architectures to other image transformations, and this has been the subject of extensive study (Kanazawa et al., 2014; Bruna et al., 2013; Cohen & Welling, 2016). Unfortunately these approaches do not possess the same memory and speed benefits that CNNs enjoy. The reason is that, ultimately, they have to transform (warp) an image or filter several times (Kanazawa et al., 2014; Marcos et al., 2016; Dieleman et al., 2015), incurring a high computational burden. Another approach is to consider a basis of filters (analogous to eigen-images) encoding the desired invariance (Cohen & Welling, 2014; Bruna et al., 2013; Cohen & Welling, 2016), which requires more storage than a convolutional filter.\nAlthough they are able to handle transformations with many pose parameters, in practice most recent proposals are limited to very coarsely discretized transformations, such as horizontal/vertical flips and 90\u25e6 rotations (Dieleman et al., 2015; Cohen & Welling, 2014).\nIn this work we propose a generalization of CNNs that overcomes these disadvantages. Our main result shows that a linear layer with equivariance w.r.t. a large class of 2-parameters transformations can always be implemented efficiently, using a standard convolution in a warped image space. The image warp can be implemented using bilinear resampling, a simple and fast operation that has been popularized by spatial transformer networks (Jaderberg et al., 2015), and is part of most deep learning toolboxes. Unlike previous proposals, the proposed warped convolutions can handle continuous transformations, such as fine rotation and scaling.\nThis makes generalized convolution easily implementable in neural networks, including using fast convolution algorithms on GPU hardware, such as Winograd (Lavin, 2015) or the Fast Fourier Transform (Lyons, 2010). We present these notions in the simplest possible way (sections 2 to 4), but we note that they can be derived in broader generality from well know concepts of group theory (section 4.2)."}, {"heading": "2 GENERALIZING CONVOLUTION", "text": ""}, {"heading": "2.1 CONVOLUTIONS OF CONTINUOUS IMAGES", "text": "We start by looking at the basic building block of CNNs, i.e. the convolution operator. This operator computes the inner product of an image I \u2208 Rm\u00d7n with a translated version of the filter F \u2208 Rr\u00d7s, producing a new image as output:\nHj = \u2211 k IkFk+j , (1)\nwhere k, j \u2208 Z2 are two-dimensional vectors of indexes, and the summation ranges inside the extents of both arrays.1 To handle continuous deformations of the input, it is more natural to express eq. 1 as an integral over continuous rather than discrete inputs:\nH(u; I) = \u02c6 I(x)F (x+ u) dx, (2)\nwhere I(x) and F (x) are continuous functions over a bounded 2D region \u2126 \u2282 R2, that is: I, F : \u2126 \u2192 R. The real-valued 2D vectors x \u2208 \u2126 now play the role of the indexes k \u2208 Z2. Equation 2 reduces to the discrete case of eq. 1 if we define I(x) and F (x) as the sum of delta functions on grids. Intermediate values can be obtained by interpolation, such as bilinear (which amounts to convolution of the delta functions with a triangle filter (Jaderberg et al., 2015)). Importantly, such continuous images can be deformed by very rich continuous transformations of the input coordinates, whereas strictly discrete operations would be more limiting.\nOver the next sections it will be more convenient to translate the image I instead of the filter F . This alternative form of eq. 2 is obtained by replacing x+ u\u2192 x:\nH(u; I) = \u02c6 I(x\u2212 u)F (x) dx. (3)"}, {"heading": "2.2 BEYOND IMAGE TRANSLATIONS", "text": "The standard convolution operator of eq. 3 can be interpreted as applying the filter to translated versions of the image. Translations can be replaced by other transformations as follows (Henriques et al., 2014):\nH(t; I) = \u02c6 I(t(x))F (x) dx, t \u2208 G (4)\nwhere G is a set of transformation functions t : \u2126 \u2192 \u2126 (assumed to be invertible). Intuitively, this generalized convolution performs an exhaustive search for a pattern, at many different poses (Henriques et al., 2014; Kanazawa et al., 2014). The interest in this definition lies in the fact that it makes convolution equivariant (Lenc & Vedaldi, 2015):\n1Note that eq. 1 defines cross-correlation in the signal processing literature, but here we follow the convention used in machine learning and call it convolution. We also ignore the possibility that the input image has more than one channel, and that convolution layers in CNN involve banks of filters instead of single ones. All such details are immaterial to our discussion.\nLemma 1 (Equivariance). Consider the generalized convolution operator H(t; I) of eq. 4. Generalized convolution \u201ccommutes\u201d with any transformation q \u2208 G of the image:\nH(t; I \u25e6 q) = H(q \u25e6 t; I).\nProof. One has immediately H(t; I \u25e6 q) = \u00b4 I(q(t(x)))F (x) dx = H(q \u25e6 t; I).\nA notable case is when transformations have an additive parametrization t : \u2126 \u00d7 R2 \u2192 \u2126, with (x, u) 7\u2192 tu(x) and tu \u25e6 tv = tu+v . In this case, the equivariance relation can be written as\nH(u; I \u25e6 tv) = H(v + u; I). (5)\nIn particular, standard convolution is obtained when tu(x) = x \u2212 u is the translation operator. In this case, the lemma above simply states that any translation of the input of the convolution results in a corresponding translation of the output.\nIn section 5, we will look in more detail at a few concrete examples of transformations other than translations. Although we will not do so explicitly, in this construction it is also possible to let one or more dimensions of the parameter space R2 be given modulus a period Q, in the sense of replacing R with R/Z(Q); the latter is required to parameterise transformations such as rotation."}, {"heading": "3 COMPUTATIONAL EFFICIENCY", "text": "Unfortunately, what eq. 4 gains us in generality, it loses in both performance and ease of implementation. Most works in computer vision that looked at filtering under generalized transformations (e.g. scale pyramids (Kanazawa et al., 2014) or rotated filter banks (Marcos et al., 2016; Cohen & Welling, 2014; 2016; Henriques et al., 2014)) compute eq. 4 directly by evaluating a large number of transformations t \u2208 G. This entails warping (transforming) either the image or the filter once per transformation t, which can be expensive.\nOpting to transform the filter instead of the image can be advantageous, since it is smaller in size. On the other hand, the filter and its domain then become spatially-varying, which foregoes the benefit of the regular, predictable, and local pattern of computations in standard convolution. It precludes the use of fast convolution routines such as Winograd\u2019s algorithm (Lavin, 2015), or the Fast Fourier Transform (Lyons, 2010), which has lower computational complexity than exhaustive search (eq. 3).\nIn practice, most recent works focus on very coarse transformations that do not change the filter support and can be implemented strictly via permutations, like horizontal/vertical flips and 90\u25e6 rotations (Dieleman et al., 2015; Cohen & Welling, 2014). Such difficulties explain why generalized convolutions are not as widespread as CNNs.\nIn section 4 we will show that, for an important class of transformations, including the ones considered in previous works (such as Kanazawa et al. (2014); Cohen & Welling (2014); Marcos et al. (2016)) it is possible to perform generalized convolution by composing a single warp with a standard convolution, instead of several warps. Thus, we are able to take full advantage of modern convolution implementations (Lavin, 2015; Lyons, 2010), including those with lower computational complexity."}, {"heading": "4 MAIN RESULT", "text": "Our main contribution is to show that the generalized convolution operator of eq. 4 can be implemented efficiently by a standard convolution, by pre-warping the input image and filter appropriately. The warp is the same for any image, depending solely on the nature of the relevant transformations, and can be written in closed form. This result, given in theorem 1, allows us to implement very efficient generalized convolutions using simple computational blocks, as shown in section 4.1. We name this method warped convolution.\nThe strongest assumption is that transformations must have an additive parametrization. By this, we mean that there exists a bijection tu : \u2126 \u2192 \u2126 such that, for any u, v \u2208 R2, parameters compose additively tu \u25e6 tv = tu+v . The second assumption is that there exists a pivot point x0 \u2208 \u2126 such\nthat u 7\u2192 tu(x0) defines a bijection R2 \u2192 \u2126 from the parameter space to the real plane. The latter requirements means that any point x \u2208 \u2126 can be \u201creached\u201d by transforming x0 under a suitable tu. We then have that: Theorem 1. Consider the generalized convolution of eq. 4. Assume that the transformation is additive (tu \u25e6 tv = tu+v). Assume also that, for a fixed pivot point x0, the function u 7\u2192 tu(x0) is bijective. Then we can rewrite generalized convolution (eq. 4) as the standard convolution\nH(u; I) = \u02c6 I\u0302(u+ v)F\u0302 (v) dv, (6)\nwhere I\u0302 and F\u0302 are the warped image and filter given by:\nI\u0302(u) = I(tu(x0)), F\u0302 (u) = F (tu(x0)) \u2223\u2223\u2223\u2223dtu(x0)du \u2223\u2223\u2223\u2223 . (7)\nProof. We use the variable substitution x = tv(x0) in eq. 4. Then:\nH(u; I) = \u02c6 I(tu(x))F (x) dx\n= \u02c6 I(tu(tv(x0)))F (tv(x0)) \u2223\u2223\u2223\u2223dtv(x0)dv \u2223\u2223\u2223\u2223 dv\n= \u02c6 I(tu+v(x0))\ufe38 \ufe37\ufe37 \ufe38\nI\u0302(u+v)\nF (tv(x0)) \u2223\u2223\u2223\u2223dtv(x0)dv \u2223\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38\nF\u0302 (v)\ndv\n= \u02c6 I\u0302(u+ v)F\u0302 (v) dv.\nThe warp that is applied to both inputs in eq. 7 can be interpreted as follows. We start with an arbitrary pivot point x0 in the image and them sample other points by repeatedly applying the transformation tu(x0) to the pivot (by varying u). When discretized, this sampling is performed over a 2D grid of parameters u. Finally, sampling the input at these points (for example, by bilinear interpolation) yields the warped input.\nAn illustration is given in fig. 1, for various transformations (each one is discussed in more detail in section 5). The red dot shows the pivot point x0, and the two arrows pointing away from it show the two directions of increasing u values (recall that transformation parameters are two-dimensional). The grids were generated by sampling u at regular intervals. Note that the warp grids are independent of the image contents \u2013 they can be computed once offline and then applied to any image.\nThe last factor in eq. 7 is the determinant of the Jacobian of the image transformation t. It rescales the image values to account for the stretching and shrinking of space due to non-linear warps. It can also be computed offline, and its application amounts to an element-wise product by a constant array. A generalization using group theory is discussed in section 4.2."}, {"heading": "4.1 PRACTICAL CONSIDERATIONS", "text": "There are a few interesting aspects that simplify the use of theorem 1 in practice.\nFirst, since in most applications the filter F is learned, we are free to ignore the constant warp and Jacobian in eq. 7 (which amounts to a simple reparametrization), and learn F\u0302 directly. In practice, this means that we warp only the input image I to obtain I\u0302 , and then perform a standard convolution with a filter F\u0302 . The learned warped filter F\u0302 has a one-to-one correspondence to an image-space filter F by means of eq. 7, although there is no real need to build the latter explicitly.\nSecond, we can choose either one or two spatial transformations for the generalized convolution (e.g. scale and rotation, simultaneously). The reason is that the input image is 2D, so the parameterspace after warping is also 2D. The choice is not arbitrary though: the two transformations must commute, in order to respect additivity. This will be the case of the pairs we study in section 5.\nAlgorithm 1 Warped convolution.\nGrid generation (offline)\n\u2022 Apply the spatial transformation t repeatedly to a pivot point x0, using a 2D grid of parameters u = {(u1 + i\u03b41, u2 + j\u03b42) : i = 0, . . . ,m, j = 0, . . . , n}, obtaining the 2D warp grid tu(x0).\nWarped convolution\n1. Resample input image I using the warp grid tu(x0), by bilinear interpolation.\n2. Convolve the warped image I\u0302 with filter F\u0302 . By theorem 1, these steps are equivalent to a generalized convolution, which performs an exhaustive search across the pose-space of transformation t, but at a much lower computational cost."}, {"heading": "4.2 RELATIONSHIP TO GROUP THEORY", "text": "This section relates our results, which have been presented using a simple formalism and in a restricted setting, to a more general approach based on group theory (Folland, 1995).\nTo this end, let G be a group of transformations. Under very mild conditions (the group has to be locally compact and Hausdorff), there exists a unique measure on the group, the Haar measure, which is invariant to the group action, in the sense that, given a measurable function I\u0303 : G \u2192 R, then \u00b4 I\u0303(g\u2032g) dg = \u00b4 I\u0303(g) dg. Using this measure, one can define generalized convolution as\n(I\u0303 \u2217 F\u0303 )(t) = \u00b4 G I\u0303(tg)F\u0303 (g\u22121) dg. This resembles our definition (4), although image and filter are defined on the group G instead of the spatial domain R2. Lemma 1 translates immediately to this case (Folland, 1995).\nIn order to extend Theorem 1, we need to make this general but abstract construction concrete. Here one assumes that the group acts transitively on a subsetX \u2282 R2 (which means that any point x \u2208 X can be written as x = gx0, for a fixed point x0 \u2208 X and a suitable transformation g \u2208 G). Then one can define the image as I\u0303(g) = I(g(x0)), where I is a function of the spatial domain X instead of the group G, and likewise for the filter. Next, it is necessary to explicitly calculate the integral over G. If the group is an Abelian (commutative) Lie group, then one can show that there exists a map exp : V \u2192 G, the exponential map, defined on a vector space V . Under commutativity, this map is also additive, in the sense that exp(u) exp(v) = exp(u+ v). The structure of V depends on the specific group, but under such restrictive conditions, it is a torus, which allows the calculation of\u00b4 I\u0303(g) dg as \u00b4 I\u0303(exp(u)x0) du.\nFinally, in order to swap integration over the group parameters with integration over space, one assumes that x = exp(u)x0 defines a smooth bijection V \u2192 X , so that it is possible to use the change of variable u \u2192 u(x) where exp(u(x))x0 = x. This allows writing the integral as\u00b4 I\u0303(exp(u)x0) du = \u00b4 I(x) |du/dx| dx. Note that this Jacobian is the inverse of the one found in\n(1) due to the fact that we started by defining our convolution using I instead of I\u0303 ."}, {"heading": "5 EXAMPLES OF SPATIAL TRANSFORMATIONS", "text": "We now give some concrete examples of pairs of spatial transformations that obey the conditions of theorem 1, and can be useful in practice."}, {"heading": "5.1 SCALE AND ASPECT RATIO", "text": "Detection tasks require predicting the extent of an object as a bounding box. While the location can be found accurately by a standard CNN, which is equivariant to translation, the size prediction could similarly benefit from equivariance to horizontal and vertical scale (equivalently, scale and aspect ratio).\nSuch a spatial transformation, from which a warp can be constructed, is given by:\ntu(x) =\n[ x1s u1\nx2s u2\n] (8)\nThe s constant controls the total degree of scaling applied. Notice that the output must be exponential in the scale parameters u; this ensures the additive structure required by theorem 1: tu(tv(x)) = tu+v(x). The resulting warp grid can be visualized in fig. 1-b. In this case, the domain of the image must be \u2126 \u2208 R2+, since a pivot x0 in one quadrant cannot reach another quadrant by any amount of (positive) scaling."}, {"heading": "5.2 SCALE AND ROTATION (LOG-POLAR WARP)", "text": "Planar scale and rotation are perhaps the most obvious spatial transformations in images, and are a natural test case for works on spatial transformations (Kanazawa et al., 2014; Marcos et al., 2016). Rotating a point x by u1 radians and scaling it by u2, around the origin, can be performed with\ntu(x) = [ su2 \u2016x\u2016 cos(atan2(x2, x1) + u1) su2 \u2016x\u2016 sin(atan2(x2, x1) + u1) ] , (9)\nwhere atan2 is the standard 4-quadrant inverse tangent function (atan2). The domain in this case must exclude the origin (\u2126 \u2208 R2 \\ {0}), since a pivot x0 = 0 cannot reach any other points in the image by rotation or scaling.\nThe resulting warp grid can be visualized in fig. 1-c. It is interesting to observe that it corresponds exactly to the log-polar domain, which is used in the signal processing literature to perform correlation across scale and rotation (Tzimiropoulos et al., 2010; Reddy & Chatterji, 1996). In fact, it was the source of inspiration for this work, which can be seen as a generalization of the log-polar domain to other spatial transformations."}, {"heading": "5.3 3D SPHERE ROTATION UNDER PERSPECTIVE", "text": "We will now tackle a more difficult spatial transformation, in an attempt to demonstrate the generality of theorem 1. The transformations we will consider are yaw and pitch rotations in 3D space, as seen by a perspective camera. In the experiments (section 6) we will show how to apply it to face pose estimation.\nIn order to maintain additivity, the rotated 3D points must remain on the surface of a sphere. We consider a simplified camera and world model, whose only hyperparameters are a focal length f , the radius of a sphere r, and its distance from the camera center d. The equations for the spatial transformation corresponding to yaw and pitch rotation under this model are in appendix A.\nThe corresponding warp grid can be seen in fig. 1-d. It can be observed that the grid corresponds to what we would expect of a 3D rendering of a sphere with a discrete mesh. An intuitive picture of the effect of the warp grid in such cases is that it wraps the 2D image around the surface of the 3D object, so that translation in the warped space corresponds to moving between vertexes of the 3D geometry."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 ARCHITECTURE", "text": "As mentioned in section 2.2, generalized convolution performs an exhaustive search for patterns across spatial transformations, by varying pose parameters. For tasks where invariance to that transformation is important, it is usual to pool the detection responses across all poses (Marcos et al., 2016; Kanazawa et al., 2014).\nIn the experiments, however, we will test the framework in pose prediction tasks. As such, we do not want to pool the detection responses (e.g. with a max operation) but rather find the pose with the strongest response (i.e., an argmax operation). To perform this operation in a differentiable manner, we implement a soft argmax operation, defined as follows:\ns1(a) = mn\u2211 ij i m \u03c3ij(a), s2(a) = mn\u2211 ij j n \u03c3ij(a), (10)\nwhere \u03c3(a) \u2208 Rm\u00d7n is the softmax over all spatial locations, and \u03c3ij(a) indexes the element at (i, j). The outputs are the two spatial coordinates of the maximum value, s(a) \u2208 R2. Our base architecture then consists of the following blocks, outlined in fig. 2. First, the input image is warped with a pre-generated grid, according to section 4. The warped image is then processed by a standard CNN, which is now equivariant to the spatial transformation that was used to generate the warp grid. A soft argmax (eq. 10) then finds the maximum over pose-space. To ensure the pose prediction is well registered to the reference coordinate system, a learnable scale and bias are applied\nto the outputs. Training proceeds by minimizing the L1 loss between the predicted pose and ground truth pose."}, {"heading": "6.2 GOOGLE EARTH", "text": "For the first task in our experiments, we will consider aerial photos of vehicles, which have been used in several works that deal with rotation invariance (Liu et al., 2014; Schmidt & Roth, 2012; Henriques et al., 2014).\nDataset. The Google Earth dataset (Heitz & Koller, 2008) contains bounding box annotations, supplemented with angle annotations from (Henriques et al., 2014), for 697 vehicles in 15 large images. We use the first 10 for training and the rest for validation. Going beyond these previous works, we focus on the estimation of both rotation and scale parameters. The object scale is taken to be the diagonal length of the bounding box.\nImplementation. A 48\u00d748 image around each vehicle is cropped and downscaled by 50%, and then fed to a network for pose prediction. The proposed method, Warped CNN, follows the architecture of section 6.1 (visualized in fig. 2). The CNN block contains 3 convolutional layers with 5 \u00d7 5 filters, with 20, 50 and 1 output channels respectively. Recall that the output of the CNN block is a single-channel response map over 2D pose-space, which in this case consists of rotation and scale. Between the convolutional layers there are 3 \u00d7 3 max-pooling operators, with a stride of 2, and a ReLU before the last layer. All networks are trained for 20 epochs with SGD, using hyperparameters chosen by cross-validation.\nBaselines and results. The results of the experiments are presented in table 1, which shows angular and scale error in the validation set. Qualitative results are shown in fig. 3. To verify whether the proposed warped convolution is indeed responsible for a boost in performance, rather than other architectural details, we compare it against a number of baselines with different components removed. The first baseline, CNN+softargmax, consists of the same architecture but without the warp (section 5.2). This is a standard CNN, with the soft argmax at the end. Since CNNs are equivariant to translation, rather than scale and rotation, we observe a drop in performance. For the second baseline, CNN+FC, we replace the soft argmax with a fully-connected layer, to allow a prediction that is not equivariant with translation. The FC layer improves the angular error, but not the scale error. The proposed Warped CNN has a similar (slightly lower) capacity to the CNN+FC baseline, but we see it achieve better performance, since its architectural equivariance seems to be better matched to the data distribution."}, {"heading": "6.3 FACES", "text": "We now turn to face pose estimation in unconstrained photos, which requires handling more complex 3D rotations under perspective.\nDataset. For this task we use the Annotated Facial Landmarks in the Wild (AFLW) dataset (Koestinger et al., 2011). It contains about 25K faces found in Flickr photos, and includes yaw (left-right) and pitch (up-down) annotations. We removed 933 faces with yaw larger than 90 degrees (i.e., facing away from the camera), resulting in a set of 24,384 samples. 20% of the faces were set aside for validation.\nImplementation. The region in each face\u2019s bounding box is resized to a 64 \u00d7 64 image, which is then processed by the network. Recall that our simplified 3D model of yaw and pitch rotation (section 5.3) assumes a spherical geometry. Although a person\u2019s head roughly follows a spherical shape, the sample images are centered around the face, not the head. As such, we use an affine Spatial Transformer Network (STN) (Jaderberg et al., 2015) as a first step, to center the image correctly. Similarly, because the optimal camera parameters (f , r and d) are difficult to set by hand, we let the network learn them, by computing their derivatives numerically (which has a low overhead, since they are scalars). The rest of the network follows the same diagram as before (fig. 2). The main CNN has 4 convolutional layers, the first two with 5\u00d75 filters, the others being 9\u00d79. The numbers of output channels are 20, 50, 20 and 1, respectively. A 3 \u00d7 3 max-pooling with a stride of 2 is performed after the first layer, and there are ReLU non-linearities between the others. As for the STN, it has 3 convolutional layers (5 \u00d7 5), with 20, 50 and 6 output channels respectively, and 3\u00d7 3 max-pooling (stride 2) between them.\nBaselines and results. The angular error of the proposed equivariant pose estimation, Warped CNN, is shown in table 2, along with a number of baselines. Qualitative results are shown in fig. 4. The goal of these experiments is to demonstrate that it is possible to achieve equivariance to complex 3D rotations. We also wish to disentangle the performance benefits of the warped convolution from the other architectural aspects. The first baseline, STN+softargmax, is the same as the proposed method, but without the warp. The large performance drop indicates that the spherical model incorporates important domain knowledge, which is ignored by a translation-equivariant STN. To allow nonequivariant models, we also test two other baselines where the softargmax is replaced with a fullyconnected (FC) layer. The STN+FC includes an affine Spatial Transformer, while the CNN+FC does not, corresponding to a standard CNN of equivalent capacity. We observe that neither the FC or the STN components can account up for the performance of the warped convolution, which better exploits the natural 3D rotation equivariance of the data."}, {"heading": "7 CONCLUSIONS", "text": "In this work we show that it is possible to reuse highly optimized convolutional blocks, which are equivariant to image translation, and coax them to exhibit equivariance to other operators, including 3D transformations. This is achieved by a simple warp of the input image, implemented with off-the-shelf components of deep networks, and can be used for image recognition tasks involving a large range of image transformations. Compared to other works, warped convolutions are simpler, relying on highly optimized convolution routines, and can flexibly handle many types of continuous transformations. Studying generalizations that support more than two parameters seems like a fruitful direction for future work. In addition to the practical aspects, our analysis offers some insights into the fundamental relationships between arbitrary image transformations and convolutional architectures."}, {"heading": "A SPATIAL TRANSFORMATION FOR 3D SPHERE ROTATION UNDER PERSPECTIVE", "text": "Our simplified model consists of a perspective camera with focal length f and all other camera parameters equal to identity, at a distance d from a centered sphere of radius r (see fig. 1-d).\nA 2D point x in image-space corresponds to the 3D point\np = (x1, x2, f). (11)\nRaycasting it along the z axis, it will intersect the sphere surface at the 3D point\nq = p\n\u2016p\u2016\n( k \u2212 \u221a k2 \u2212 d2 + r2 ) , k = fd\n\u2016p\u2016 . (12)\nIf the argument of the square-root is negative, the ray does not intersect the sphere and so the point transformation is undefined. This means that the domain of the image \u2126 should be restricted to the sphere region. In practice, in such cases we simply leave the point unmodified.\nThen, the yaw and pitch coordinates of the point q on the surface of the sphere are\n\u03c61 = cos \u22121 ( \u2212q2 r ) , \u03c62 = atan2 ( \u2212 q1 d\u2212 q3 ) . (13)\nThese polar coordinates are now rotated by the spatial transformation parameters, \u03c6\u2032 = \u03c6+ u.\nConverting the polar coordinates back to a 3D point q\u2032\nq\u2032 =\n[ r sin\u03c6\u20321 sin\u03c6 \u2032 2\n\u2212r cos\u03c6\u20321 r sin\u03c6\u20321 cos\u03c6 \u2032 2 \u2212 d\n] . (14)\nFinally, projection of q\u2032 into image-space yields\ntu(x) = \u2212 f\nq\u20323 [ q\u20321 q\u20322 ] . (15)"}], "references": [{"title": "Learning stable group invariant representations with convolutional networks", "author": ["Joan Bruna", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3537,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Learning the Irreducible Representations of Commutative Lie Groups", "author": ["Taco Cohen", "Max Welling"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Cohen and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2014}, {"title": "Group equivariant convolutional networks", "author": ["Taco Cohen", "Max Welling"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "Cohen and Welling.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2016}, {"title": "Rotation-invariant convolutional neural networks for galaxy morphology prediction", "author": ["Sander Dieleman", "Kyle W Willett", "Joni Dambre"], "venue": "Monthly notices of the royal astronomical society,", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "A course in abstract harmonic analysis", "author": ["Gerald B Folland"], "venue": null, "citeRegEx": "Folland.,? \\Q1995\\E", "shortCiteRegEx": "Folland.", "year": 1995}, {"title": "Learning spatial context: Using stuff to find things", "author": ["Geremy Heitz", "Daphne Koller"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Heitz and Koller.,? \\Q2008\\E", "shortCiteRegEx": "Heitz and Koller.", "year": 2008}, {"title": "Fast training of pose detectors in the fourier domain", "author": ["J.F. Henriques", "P. Martins", "R. Caseiro", "J. Batista"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Henriques et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henriques et al\\.", "year": 2014}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision., volume 39", "author": ["Aapo Hyv\u00e4rinen", "Jarmo Hurri", "Patrick O Hoyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman"], "venue": "In Advances in Neural Information Processing Systems, pp. 2017\u20132025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Locally scale-invariant convolutional neural networks", "author": ["Angjoo Kanazawa", "Abhishek Sharma", "David Jacobs"], "venue": "arXiv preprint arXiv:1412.5104,", "citeRegEx": "Kanazawa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kanazawa et al\\.", "year": 2014}, {"title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization", "author": ["Martin Koestinger", "Paul Wohlhart", "Peter M. Roth", "Horst Bischof"], "venue": "In First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies,", "citeRegEx": "Koestinger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koestinger et al\\.", "year": 2011}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Andrew Lavin"], "venue": "arXiv preprint arXiv:1509.09308,", "citeRegEx": "Lavin.,? \\Q2015\\E", "shortCiteRegEx": "Lavin.", "year": 2015}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["Karel Lenc", "Andrea Vedaldi"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "Lenc and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Lenc and Vedaldi.", "year": 2015}, {"title": "Rotation-Invariant HOG Descriptors Using Fourier Analysis in Polar and Spherical Coordinates", "author": ["Kun Liu", "Henrik Skibbe", "Thorsten Schmidt", "Thomas Blein", "Klaus Palme", "Thomas Brox", "Olaf Ronneberger"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Understanding digital signal processing", "author": ["Richard G Lyons"], "venue": "Pearson Education,", "citeRegEx": "Lyons.,? \\Q2010\\E", "shortCiteRegEx": "Lyons.", "year": 2010}, {"title": "Learning rotation invariant convolutional filters for texture classification", "author": ["Diego Marcos", "Michele Volpi", "Devis Tuia"], "venue": "arXiv preprint arXiv:1604.06720,", "citeRegEx": "Marcos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marcos et al\\.", "year": 2016}, {"title": "An FFT-based technique for translation, rotation, and scale-invariant image registration", "author": ["B. Srinivasa Reddy", "Biswanath N. Chatterji"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Reddy and Chatterji.,? \\Q1996\\E", "shortCiteRegEx": "Reddy and Chatterji.", "year": 1996}, {"title": "Learning rotation-aware features: From invariant priors to equivariant descriptors", "author": ["Uwe Schmidt", "Stefan Roth"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Schmidt and Roth.,? \\Q2012\\E", "shortCiteRegEx": "Schmidt and Roth.", "year": 2012}, {"title": "Robust FFTbased scale-invariant image registration with image gradients", "author": ["Georgios Tzimiropoulos", "Vasileios Argyriou", "Stefanos Zafeiriou", "Tania Stathaki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tzimiropoulos et al\\.,? \\Q1899\\E", "shortCiteRegEx": "Tzimiropoulos et al\\.", "year": 1899}], "referenceMentions": [{"referenceID": 7, "context": "The common interpretation is that such operators are matched to the statistics of natural images, which are well known to be translation invariant (Hyv\u00e4rinen et al., 2009).", "startOffset": 147, "endOffset": 171}, {"referenceID": 9, "context": "Therefore, it is natural to consider generalizing convolutional architectures to other image transformations, and this has been the subject of extensive study (Kanazawa et al., 2014; Bruna et al., 2013; Cohen & Welling, 2016).", "startOffset": 159, "endOffset": 225}, {"referenceID": 0, "context": "Therefore, it is natural to consider generalizing convolutional architectures to other image transformations, and this has been the subject of extensive study (Kanazawa et al., 2014; Bruna et al., 2013; Cohen & Welling, 2016).", "startOffset": 159, "endOffset": 225}, {"referenceID": 9, "context": "The reason is that, ultimately, they have to transform (warp) an image or filter several times (Kanazawa et al., 2014; Marcos et al., 2016; Dieleman et al., 2015), incurring a high computational burden.", "startOffset": 95, "endOffset": 162}, {"referenceID": 15, "context": "The reason is that, ultimately, they have to transform (warp) an image or filter several times (Kanazawa et al., 2014; Marcos et al., 2016; Dieleman et al., 2015), incurring a high computational burden.", "startOffset": 95, "endOffset": 162}, {"referenceID": 3, "context": "The reason is that, ultimately, they have to transform (warp) an image or filter several times (Kanazawa et al., 2014; Marcos et al., 2016; Dieleman et al., 2015), incurring a high computational burden.", "startOffset": 95, "endOffset": 162}, {"referenceID": 0, "context": "Another approach is to consider a basis of filters (analogous to eigen-images) encoding the desired invariance (Cohen & Welling, 2014; Bruna et al., 2013; Cohen & Welling, 2016), which requires more storage than a convolutional filter.", "startOffset": 111, "endOffset": 177}, {"referenceID": 3, "context": "Although they are able to handle transformations with many pose parameters, in practice most recent proposals are limited to very coarsely discretized transformations, such as horizontal/vertical flips and 90\u25e6 rotations (Dieleman et al., 2015; Cohen & Welling, 2014).", "startOffset": 220, "endOffset": 266}, {"referenceID": 8, "context": "The image warp can be implemented using bilinear resampling, a simple and fast operation that has been popularized by spatial transformer networks (Jaderberg et al., 2015), and is part of most deep learning toolboxes.", "startOffset": 147, "endOffset": 171}, {"referenceID": 11, "context": "This makes generalized convolution easily implementable in neural networks, including using fast convolution algorithms on GPU hardware, such as Winograd (Lavin, 2015) or the Fast Fourier Transform (Lyons, 2010).", "startOffset": 154, "endOffset": 167}, {"referenceID": 14, "context": "This makes generalized convolution easily implementable in neural networks, including using fast convolution algorithms on GPU hardware, such as Winograd (Lavin, 2015) or the Fast Fourier Transform (Lyons, 2010).", "startOffset": 198, "endOffset": 211}, {"referenceID": 8, "context": "Intermediate values can be obtained by interpolation, such as bilinear (which amounts to convolution of the delta functions with a triangle filter (Jaderberg et al., 2015)).", "startOffset": 147, "endOffset": 171}, {"referenceID": 6, "context": "Translations can be replaced by other transformations as follows (Henriques et al., 2014): H(t; I) = \u02c6 I(t(x))F (x) dx, t \u2208 G (4) where G is a set of transformation functions t : \u03a9 \u2192 \u03a9 (assumed to be invertible).", "startOffset": 65, "endOffset": 89}, {"referenceID": 6, "context": "Intuitively, this generalized convolution performs an exhaustive search for a pattern, at many different poses (Henriques et al., 2014; Kanazawa et al., 2014).", "startOffset": 111, "endOffset": 158}, {"referenceID": 9, "context": "Intuitively, this generalized convolution performs an exhaustive search for a pattern, at many different poses (Henriques et al., 2014; Kanazawa et al., 2014).", "startOffset": 111, "endOffset": 158}, {"referenceID": 9, "context": "scale pyramids (Kanazawa et al., 2014) or rotated filter banks (Marcos et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 15, "context": ", 2014) or rotated filter banks (Marcos et al., 2016; Cohen & Welling, 2014; 2016; Henriques et al., 2014)) compute eq.", "startOffset": 32, "endOffset": 106}, {"referenceID": 6, "context": ", 2014) or rotated filter banks (Marcos et al., 2016; Cohen & Welling, 2014; 2016; Henriques et al., 2014)) compute eq.", "startOffset": 32, "endOffset": 106}, {"referenceID": 11, "context": "It precludes the use of fast convolution routines such as Winograd\u2019s algorithm (Lavin, 2015), or the Fast Fourier Transform (Lyons, 2010), which has lower computational complexity than exhaustive search (eq.", "startOffset": 79, "endOffset": 92}, {"referenceID": 14, "context": "It precludes the use of fast convolution routines such as Winograd\u2019s algorithm (Lavin, 2015), or the Fast Fourier Transform (Lyons, 2010), which has lower computational complexity than exhaustive search (eq.", "startOffset": 124, "endOffset": 137}, {"referenceID": 3, "context": "In practice, most recent works focus on very coarse transformations that do not change the filter support and can be implemented strictly via permutations, like horizontal/vertical flips and 90\u25e6 rotations (Dieleman et al., 2015; Cohen & Welling, 2014).", "startOffset": 205, "endOffset": 251}, {"referenceID": 11, "context": "Thus, we are able to take full advantage of modern convolution implementations (Lavin, 2015; Lyons, 2010), including those with lower computational complexity.", "startOffset": 79, "endOffset": 105}, {"referenceID": 14, "context": "Thus, we are able to take full advantage of modern convolution implementations (Lavin, 2015; Lyons, 2010), including those with lower computational complexity.", "startOffset": 79, "endOffset": 105}, {"referenceID": 4, "context": "This section relates our results, which have been presented using a simple formalism and in a restricted setting, to a more general approach based on group theory (Folland, 1995).", "startOffset": 163, "endOffset": 178}, {"referenceID": 4, "context": "Lemma 1 translates immediately to this case (Folland, 1995).", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "Planar scale and rotation are perhaps the most obvious spatial transformations in images, and are a natural test case for works on spatial transformations (Kanazawa et al., 2014; Marcos et al., 2016).", "startOffset": 155, "endOffset": 199}, {"referenceID": 15, "context": "Planar scale and rotation are perhaps the most obvious spatial transformations in images, and are a natural test case for works on spatial transformations (Kanazawa et al., 2014; Marcos et al., 2016).", "startOffset": 155, "endOffset": 199}, {"referenceID": 15, "context": "For tasks where invariance to that transformation is important, it is usual to pool the detection responses across all poses (Marcos et al., 2016; Kanazawa et al., 2014).", "startOffset": 125, "endOffset": 169}, {"referenceID": 9, "context": "For tasks where invariance to that transformation is important, it is usual to pool the detection responses across all poses (Marcos et al., 2016; Kanazawa et al., 2014).", "startOffset": 125, "endOffset": 169}, {"referenceID": 13, "context": "For the first task in our experiments, we will consider aerial photos of vehicles, which have been used in several works that deal with rotation invariance (Liu et al., 2014; Schmidt & Roth, 2012; Henriques et al., 2014).", "startOffset": 156, "endOffset": 220}, {"referenceID": 6, "context": "For the first task in our experiments, we will consider aerial photos of vehicles, which have been used in several works that deal with rotation invariance (Liu et al., 2014; Schmidt & Roth, 2012; Henriques et al., 2014).", "startOffset": 156, "endOffset": 220}, {"referenceID": 6, "context": "The Google Earth dataset (Heitz & Koller, 2008) contains bounding box annotations, supplemented with angle annotations from (Henriques et al., 2014), for 697 vehicles in 15 large images.", "startOffset": 124, "endOffset": 148}, {"referenceID": 10, "context": "For this task we use the Annotated Facial Landmarks in the Wild (AFLW) dataset (Koestinger et al., 2011).", "startOffset": 79, "endOffset": 104}, {"referenceID": 8, "context": "As such, we use an affine Spatial Transformer Network (STN) (Jaderberg et al., 2015) as a first step, to center the image correctly.", "startOffset": 60, "endOffset": 84}], "year": 2017, "abstractText": "Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made equivariant to a wide range of 2-parameters spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective).", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_332"}