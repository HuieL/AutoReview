{"name": "ICLR_2017_335.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Soravit Changpinyo", "Mark Sandler"], "emails": ["schangpi@usc.edu", "sandler@google.com", "azhmogin@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep neural networks combined with large-scale labeled data have become a standard recipe for achieving state-of-the-art performance on supervised learning tasks in recent years. Despite of their success, the capability of deep neural networks to model highly nonlinear functions comes with high computational and memory demands both during the model training and inference. In particular, the number of parameters of neural network models is often designed to be huge to account for the scale, diversity, and complexity of data that they learn from. While advances in hardware have somewhat alleviated the issue, network size, speed, and power consumption are all limiting factors when it comes to production deployment on mobile and embedded devices. On the other hand, it is wellknown that there is significant redundancy among the weights of neural networks. For example, Denil et al. (2013) show that it is possible to learn less than 5% of the network parameters and predict the rest without losing predictive accuracy. This evidence suggests that neural networks are often over-parameterized.\nThese motivate the research on neural network compression. However, several immediate questions arise: Are these parameters easy to identify? Could we just make the network 5% of its size and retrain? Or are more advanced methods required? There is an extensive literature in the last few years that explores the question of network compression using advanced techniques, including network prunning, loss-based compression, quantization, and matrix decomposition. We overview many of these directions in the next section. However, there is surprisingly little research on whether this over-parameterization can simply be re-captured by more efficient architectures that could be obtained from original architectures via simple transformations.\nOur approach is inspired by a very simple yet successful method called depth multiplier (Howard, 2017). In this method the depth (the number of channels) of each convolutional layer in a given network is simply reduced by a fixed fraction and the network is retrained. We generalize this approach by removing the constraint that every input filter (or channel) must be fully connected to every output filter. Instead, we use a sparse connection matrix, where each output convolution chan-\n\u2217The work was done while the author was doing an internship at Google Research.\nnel is connected only to a small random fraction of the input channels. Note that, for convolutional networks, this still allows for efficient computation since the one channel spatial convolution across the entire plane remains unchanged.\nWe empirically demonstrate the effectiveness of our approach on four networks (MNIST, CIFAR Net, Inception-V3 and VGG-16) of different sizes. Our results suggest that our approach outperforms dense convolutions with depth multiplier at high compression rates.\nFor Inception V3 (Szegedy et al., 2016), we show that we can train a network with only about 300K of convolutional parameters1 and about 100M multiply-adds that achieves above 52% accuracy after it is fully trained. The corresponding depth-multiplier network has only about 41% accuracy. Another network that we consider is VGG-16n, a slightly modified version of VGG-16 (Simonyan & Zisserman, 2015), with 7x fewer parameters and similar accuracy.2 We found VGG-16n to start training much faster than the original VGG-16 which was trained incrementally in the original literature. We explore the impact of sparsification and the number of parameters on the quality of the network by building the networks up to 30x smaller than VGG-16n (200x smaller than the original VGG-16).\nIn terms of model flexibility, sparse connections allow for an incremental training approach, where connection structure between layers can be densified as training progresses. More importantly, the incremental training approach can potentially speed up the training significantly due to savings in the early stages of training.\nThe rest of the paper is organized as follows. Section 2 summarizes relevant work. We describe our approach in Section 3 and then present some intuition in Section 4. Finally, we show our experimental results in Section 5."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 COMPRESSION TECHNIQUES FOR NEURAL NETWORKS", "text": "Our work is closely related to a compression technique based on network pruning. However, the important difference is that we do not try to select the connections which are redundant. Instead, we just fix a random connectivity pattern and let the network train around it. We also give a brief overview of other two popular techniques: quantization and decomposition, though these directions are not the main focus and could be complementary to our work.\nNetwork pruning Much initial work on neural network compression focuses on removing unimportant connections using weight decay. Hanson & Pratt (1989) introduce hyperbolic and exponential biases to the objective. Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi & Stork, 1993) prune the networks based on second-order derivatives of the objectives. Recent work by Han et al. (2015; 2016a) alternates between pruning near-zero weights, which are encouraged by `1 or `2 regularization, and retraining the pruned networks.\nMore complex regularizers have also been considered. Wen et al. (2016) and Li et al. (2016) put structured sparsity regularizers on the weights, while Murray & Chiang (2015) put them on the hidden units. Feng & Darrell (2015) explore a nonparametric prior based on the Indian buffet processes (Griffiths & Ghahramani, 2011) on layers. Hu et al. (2016) prune neurons based on the analysis of their outputs on a large dataset. Anwar et al. (2015b) consider special sparsity patterns: channel-wise (removing a feature map/channel from a layer), kernel-wise (removing all connections between two feature maps in consecutive layers), and intra-kernel-strided (removing connections between two features with particular stride and offset). They also propose to use particle filter to decide the importance of connections and paths during training.\nAnother line of work explores fixed network architectures with some subsets of connections removed. For example, LeCun et al. (1998) remove connections between the first two convolutional feature maps in a completely uniform manner. This is similar to our approach but they only con-\n1Here and elsewhere, we ignore the parameters for the softmax classifier since they simply describe a linear transformation and depend on number of classes.\n2As of this writing, VGG networks have not finished training, but the training trajectory suggest similar performance.\nsider a pre-defined pattern in which the same number of input feature map are assigned to each output feature map (Random Connection Table in Torch\u2019s SpatialConvolutionMap function). Further, they do not explore how sparse connections affect performance compared to dense networks. Along a similar vein, Cires\u0327an et al. (2011) remove random connections in their MNIST experiments. However, they do not try to preserve the spatial convolutional density and it might be a challenge to harvest the savings on existing hardware. Ioannou et al. (2016a) explore three types of hierarchical arrangements of filter groups for CNNs, which depend on different assumptions about co-dependency of filters within each layer. These arrangements include columnar topologies inspired by AlexNet (Krizhevsky et al., 2012), tree-like topologies previously used by Ioannou et al. (2016b), and root-like topologies. Finally, Howard (2017) proposes the depth multiplier method to scale down the number of filters in each convolutional layer by a factor. In this case, depth multiplier can be thought of channel-wise pruning mentioned in (Anwar et al., 2015b). However, depth multiplier modifies the network architectures before training and removes each layer\u2019s feature maps in a uniform manner.\nWith the exception of (Anwar et al., 2015b; Li et al., 2016; Ioannou et al., 2016a) and depth multiplier (Howard, 2017), the above previous work performs connection pruning that leads to irregular network architectures. Thus, those techniques require additional efforts to represent network connections and might or might not allow for direct computational savings.\nQuantization Reducing the degree of redundancy of model parameters can be done in the form of quantization of network parameters. Hwang & Sung (2014); Arora et al. (2014) and Courbariaux et al. (2015; 2016); Rastegari et al. (2016) propose to train CNNs with ternary weights and binary weights, respectively. Gong et al. (2014) use vector quantization for parameters in fully connected layers. Anwar et al. (2015a) quantize a network with the squared error minimization. Chen et al. (2015) randomly group network parameters using a hash function. We note that this technique could be complementary to network pruning. For example, Han et al. (2016a) combine connection pruning in (Han et al., 2015) with quantization and Huffman coding.\nDecomposition Another approach is based on low-rank decomposition of the parameters. Decomposition methods include truncated SVD (Denton et al., 2014), decomposition to rank-1 bases (Jaderberg et al., 2014), CP decomposition (PARAFAC or CANDECOMP) (Lebedev et al., 2015), Tensor-Train decomposition of Oseledets (2011) (Novikov et al., 2015), sparse dictionary learning of Mairal et al. (2009) and PCA (Liu et al., 2015), asymmetric (3D) decomposition using reconstruction loss of non-linear responses combined with a rank selection method based on PCA accumulated energy (Zhang et al., 2015b;a), and Tucker decomposition using the kernel tensor reconstruction loss combined with a rank selection method based on global analytic variational Bayesian matrix factorization (Kim et al., 2016)."}, {"heading": "2.2 REGULARIZATION OF NEURAL NETWORKS", "text": "Hinton et al. (2012); Srivastava et al. (2014) propose Dropout for regularizing fully connected layers within neural networks layers by randomly setting a subset of activations to zero during training. Wan et al. (2013) later propose DropConnect, a generalization of Dropout that instead randomly sets a subset of weights or connections to zero. Our approach could be thought as related to DropConnect, but (1) we remove connections before training; (2) we focus on connections between convolutional layers; and (3) we kill connections in a more regular manner by restricting connection patterns to be the same along spatial dimensions.\nRecently, Han et al. (2016b) and Jin et al. (2016) propose a form of regularization where dropped connections are unfrozen and the network is retrained. This idea is similar to our incremental training approach. However, (1) we do not start with a full network; (2) we do not unfreeze connections all at once; and (3) we preserve regularity of the convolution operation."}, {"heading": "2.3 NEURAL NETWORK ARCHITECTURES", "text": "Network compression and architectures are closely related. The goal of compression is to remove redundancy in network parameters; therefore, the knowledge about traits that determine architecture\u2019s success would be desirable. Other than the discovery that depth is an important factor (Ba & Caruana, 2014), little is known about such traits.\nSome previous work performs architecture search but without the main goal of doing compression (Murray & Chiang, 2015; De Brabandere et al., 2016). Recent work proposes shortcut/skip connections to convolutional networks. See, among others, highway networks (Srivastava et al., 2015), residual networks (He et al., 2016a;b), networks with stochastic depth (Huang et al., 2016b), and densely connected convolutional networks (Huang et al., 2016a)."}, {"heading": "3 APPROACH", "text": "A CNN architecture consist of (1) convolutional layers, (2) pooling layers, (3) fully connected layers, and (4) a topology that governs how these layers are organized. Given an architecture, our general goal is to transform it into another architecture with a smaller number of parameters. In this paper, we limit ourselves to transformation functions that keep the general topology of the input architecture intact. Moreover, the main focus will be on the convolutional layers and convolution operations, as they impose highest computational and memory burden for most if not all large networks."}, {"heading": "3.1 DEPTH MULTIPLIER", "text": "We first give a description of the depth multiplier method used in Howard (2017). Given a hyperparameter \u03b1 \u2208 (0, 1], the depth multiplier approach scales down the number of filters in each convolutional layers by \u03b1. Note that depth here refers to the third dimension of the activation volume of a single layer, not the number of layers in the whole network.\nLet nl\u22121 and nl be the number of input and output filters at layer l, respectively. After the operation nl\u22121 and nl become d\u03b1nl\u22121e and d\u03b1nle and the number of parameters (and the number of multiplications) becomes \u2248 \u03b12 of the original number. The result of this operation is a network that is both 1/\u03b12 smaller and faster. Many large networks can be significantly reduced in size using this method with only a small loss of precision (Howard, 2017). It is our belief that this method establishes a strong baseline to which any other advanced techniques should compare themselves. To the best of our knowledge, we are not aware of such comparisons in the literature."}, {"heading": "3.2 SPARSE RANDOM", "text": "Instead of looking at depth multiplier as deactivating channels in the convolutional layers, we can look at it from the perspective of deactivating connections. From this point of view, depth multiplier kills the connections between two convolutional layers such that (a) the connection patterns are still the same across spatial dimensions and (b) all \u201calive\u201d input channels are fully connected to all \u201calive\u201d output channels.\nWe generalize this approach by relaxing (b) while maintaining (a). That is, for every output channel, we connect it to a small subset of input channels. In other words, dense connections between a small number of channels become sparse connections between larger number of channels. This can be summarized in Fig. 1. The advantage of this is that the actual convolution can still be computed efficiently because sparsity is introduced only at the outer loop of the convolution operation and we can still take the advantage of the continuous memory layout. For more details regarding implementations of the two approaches, please refer to the Appendix.\nMore concretely, let nl\u22121 and nl be the number of channels of layer l \u2212 1 and layer l, respectively. For a sparsity coefficient \u03b1, each output filter j only connects to an \u03b1 fraction of filters of the previous layer. Thus, instead of having a connectivity matrix Wsij of dimension k2 \u00d7 nl\u22121 \u00d7 nl, we have a sparse matrix with non-zero entries at Wsaijj , where aij is an index matrix of dimension k2 \u00d7 \u03b1nl\u22121 \u00d7 nl and k is the kernel size."}, {"heading": "3.2.1 INCREMENTAL TRAINING", "text": "In contrast to depth multiplier, a sparse convolutional network defines a connection pattern on a much bigger network. Therefore, an interesting extension is to consider incremental training: we start with a network that only contains a small fraction of connections (in our experiments we use 1% and 0.1%) and add connections over time. This is motivated by an intuition that the network can use learned channels in new contexts by introducing additional connections. The potential practical\nadvantage of this approach is that since we start training with very small networks and grow them over time, this approach has a potential to speed up the whole training process significantly. We note that depth multiplier will not benefit from this approach as any newly activated connections would require learning new filters from scratch."}, {"heading": "4 ANALYSIS", "text": "In this section, we approach a question of why sparse convolutions are frequently more efficient than the dense convolutions with the same number of parameters. Our main intuition is that the sparse convolutional networks promote diversity. It is much harder to learn equivalent set of channels as, at high sparsity, channels have distinct connection structure or even overlapping connections. This can be formalized with a simple observation that any dense network is in fact a part of an exponentially large equivalence class, which is guaranteed to produce the same output for every input.\nLemma 1 Any dense convolutional neural network with no cross-channel nonlinearities, distinct weights and biases, and with l hidden layers of sizes n1, n2, . . . , nl, has at least \u220fl i=1 ni! distinct equivalent networks which produce the same output.\nProof Let I denote the input to the network, Ci be the convolutional operator, \u03c3i denote the nonlinearity operator applied to the i-th convolution layer and S be a final transformation (e.g. softmax classifier). We assume that \u03c3i is a function that operates on each of the channels independently. We note that this is the case for almost any modern network. The output of the network can then be written as:\nN (I) \u2261 S \u25e6 \u03c3l \u25e6 Cl \u25e6 \u03c3l\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c31 \u25e6 C1(I)\nwhere we use \u25e6 to denote function composition to avoid numerous parentheses. The convolution operator Ci operates on input with ni\u22121 channels and produces an output with ni channels. Now, fix arbitrary set of permutation functions \u03c0i, where \u03c0i can permute depth of size ni. Since \u03c0i is a linear function, it follows that C \u2032i = \u03c0 \u22121 i Ci\u03c0i\u22121 is a valid convolutional operator, which can be obtained from Ci by permuting its bias according to \u03c0i and its weight matrix along input and output dimensions according to \u03c0i\u22121 and \u03c0i respectively. For a new network defined as:\nN \u2032(I) = S\u2032 \u25e6 \u03c3l \u25e6 C \u2032l \u25e6 \u03c3l\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c31 \u25e6 C \u20321(I),\nwhere \u03c00 is an identity operator and S\u2032 \u2261 S \u25e6 \u03c0l, we claim that N \u2032(I) \u2261 N (I). Indeed, since nonlinearities do not apply cross-depth we have \u03c0n\u03c3n\u03c0\u22121n \u2261 \u03c3n and thus:\nN \u2032(I) = S\u2032 \u25e6 \u03c3l \u25e6 C \u2032l \u25e6 \u03c3l\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c31 \u25e6 C \u20321(I) = = S \u25e6 \u03c0l \u25e6 \u03c3l \u25e6 \u03c0\u22121l \u25e6 Cl \u25e6 \u03c0l\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c01 \u25e6 \u03c31 \u25e6 \u03c0 \u22121 1 \u25e6 C1(I) = N (I).\nThus, any set of permutations on hidden units defines an equivalent network.\nIt is obvious that sparse networks are much more immune to parameter permutation \u2013 indeed every channel at layer l is likely to have a unique tree describing its connection matrix all the way down. Exploring this direction is an interesting open question."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we demonstrate the effectiveness of the sparse random approach by comparing it to the depth multiplier approach at different compression rates. Moreover, we examine several settings in the incremental training where connections gradually become active during the training process."}, {"heading": "5.1 SETUP", "text": "Networks and Datasets Our experiments are conducted on 4 networks for 3 different datasets. All our experiments use open-source TensorFlow networks Abadi et al. (2015).\nMNIST AND CIFAR-10 We use standard networks provided by TensorFlow. For MNIST, it has 3-layer convolutional layers and achieves 99.5% accuracy when fully trained. For CIFAR-10, it has 2 convolutional layers and achieves 87% accuracy.\nIMAGENET We use open source Inception-V3 (Szegedy et al., 2016) network and a slightly modified version of VGG-16 (Simonyan & Zisserman, 2015) called VGG-16n on ImageNet ILSVRC 2012 (Deng et al., 2009; Russakovsky et al., 2015).\nRandom connections Connections are activated according to their likelihood from the uniform distribution. In addition, they are activated in such a way that there are no connections going in or coming out of dead filters (i.e., any connection must have a path to input image and a path to the final prediction.). All connections in fully connected layers are retained.\nImplementation details All code is implemented in TensorFlow (Abadi et al., 2015). Deactivating connections is done by applying masks to parameter tensors. The Inception-v3 and VGG-16n networks are trained on 8 Tesla K80 GPUs, each with batch size 256 (32 per gpu) and batch normalization was used for all networks."}, {"heading": "5.2 COMPARISON BETWEEN SPARSE RANDOM AND DEPTH MULTIPLIER", "text": ""}, {"heading": "5.2.1 MNIST AND CIFAR-10", "text": "We first compare depth multiplier and sparse random for the two small networks on MNIST and CIFAR-10. We compare the accuracy of the two approaches when the numbers of connections are roughly the same, based on a hyperparameter \u03b1. For dense convolutions, we pick a multiplier \u03b1 and each filter depth is scaled down by \u221a \u03b1 and then rounded up. In sparse convolutions, a fraction \u03b1 of connections are randomly deactivated if those parameters connect at least two filters on each layer; otherwise, a fraction of \u221a \u03b1 is used instead if the parameters connect layers with only one filter left. The accuracy numbers are averaged over 5 rounds for MNIST and 2 rounds on CIFAR-10.\nWe show in Fig. 2 and Fig. 3 that the sparse networks have comparable or higher accuracy for the same number of parameters, with comparable accuracy at higher density. We note however that these networks are so small that at high compression rates most of operations are concentrated at the first layer, which is negligible for large networks. Moreover, in MNIST example, the size of network changes most dramatically from 2000 to 2 million parameters, while affecting accuracy only by 1%. This observation suggests that there might be benefits of maintaining the number of filters to be high and/or breaking the symmetry of connections. We explore this in the next section."}, {"heading": "5.2.2 INCEPTION-V3 ON IMAGENET", "text": "We consider different values of sparsity ranging from 0.003 to 1, and depth multiplier from 0.05 to 1. Our experiments show (see Table 1 and Fig. 4) significant advantage of sparse networks over equivalently sized dense networks. We note that due to time constraints the reported quantitative numbers are preliminary, as the networks have not finished converging. We expect the final numbers to match the reported number for Inception V3 Szegedy et al. (2016), and the smaller networks to have comparable improvement."}, {"heading": "5.2.3 VGG-16 ON IMAGENET", "text": "In our experiments with the VGG-16 network (Simonyan & Zisserman, 2015), we modify the model architecture (calling it VGG-16n) by removing the two fully-connected layers with depth 4096 and replacing them with a 2 \u00d7 2 maxpool layer followed by a 3 \u00d7 3 convolutional layer with the depth of 1024. This alone sped up our training significantly. The comparison between depth multiplier and sparse connection approaches is shown in Fig. 5. The modified VGG-16n network has about 7 times fewer parameters, but appears to have comparable precision."}, {"heading": "5.3 INCREMENTAL TRAINING", "text": "Finally, we show that incremental training is a promising direction. We start with a very sparse model and increase its density over time, using the approach described in Sect. 3.2.1. We note that a naive approach where we simply add filters results in training process basically equivalent to as if it started from scratch in every step. On the other hand, when the network densifies over time, all channels already possess some discriminative power and that information is utilized.\nIn our experiments, we initially start training Inception-V3 with only 1% or 0.1% of connections enabled. Then, we double the number of connections every T steps. We use T = 10, 000, T = 25, 000 and T = 50, 000. The results are presented in Fig. 6. We show that the networks trained with the incremental approach regardless of the doubling period can catch up with the full Inception-V3 network (in some cases with small gains). Moreover, they recover very quickly from adding more (untrained) connections. In fact, the recovery is so fast that it is shorter than our saving interval for all the networks except for the network with 10K doubling period (resulting in the sharp drop). We believe that incremental training is a promising direction to speeding up the training of large convolutional neural networks since early stages of the training require much less computation."}, {"heading": "6 CONCLUSION", "text": "We have proposed a new compression technique that uses a sparse random connection structure between input-output filters in convolutional layers of CNNs. We fix this structure before training and use the same structure across spatial dimensions to harvest savings from modern hardware. We show that this approach is especially useful at very high compression rates for large networks. For example, this simple method when applied to Inception V3 (Fig. 4), achieves AlexNet-level accuracy (Krizhevsky et al., 2012) with fewer than 400K parameters and VGG-level one (Fig. 5) with roughly 3.5M parameters. The simplicity of our approach is instructive in that it establishes a strong baseline to compare against when developing more advanced techniques. On the other hand, the uncanny match in performance of dense and equivalently-sized sparse networks with sparsity > 0.1 suggests that there might be some fundamental property of network architectures that is controlled by the number of parameters, regardless of how they are organized. Exploring this further might yield additional insights on understanding neural networks.\nIn addition, we show that our method leads to an interesting novel incremental training technique, where we take advantage of sparse (and smaller) models to build a dense network. One interesting open direction is to enable incremental training not to simply densify the network over time, but also increase the number of channels. This would allow us to grow the network without having to fix its original shape in place."}, {"heading": "A ADDITIONAL DETAILS ON DENSE VS. SPARSE CONVOLUTIONS", "text": "We contrast naive implementations of dense and sparse convolutions (cf. Sect. 3) in Algorithm. 1 and Algorithm 2. We emphasize that we do not use sparse matrices and only introduce sparsity from channel to channel. Thus, walltime will be mostly in terms of Multiply-Adds; the basic operation (convolving the entire image plane in Line 8 of both algorithms) is unchanged.\nAlgorithm 1 Naive implementation of dense convolution 1: Inputs: 2: - input: Data tensor 3: - W : Parameter tensor 4: - input channels: Array of input channel IDs 5: - output channels: Array of output channel IDs 6: for i in input channels do 7: for o in output channels do 8: output[o]\u2190 output[o] + convolve(input[i],W [i, o, . . .]) 9: end for 10: end for 11: return output\nAlgorithm 2 Naive implementation of sparse convolution 1: Inputs: 2: - input: Data tensor 3: - W : Parameter tensor 4: - input channels: Array of input channel IDs 5: - output channels connected to i: Array of array of output channel IDs specifying connec-\ntions to each input channel 6: for i in input channels do 7: for index, o in enumerate(output channels connected to i[i]) do 8: output[o]\u2190 output[o] + convolve(input[i],W [i, index, . . .]) 9: end for\n10: end for 11: return output"}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "on heterogeneous systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In ICASSP,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1512.08571,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In ICML,", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Highperformance neural networks for visual object classification", "author": ["Dan C. Cire\u015fan", "Ueli Meier", "Jonathan Masci", "Luca M. Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1102.0183,", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2011}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In NIPS,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Dynamic filter networks", "author": ["Bert De Brabandere", "Xu Jia", "Tinne Tuytelaars", "Luc Van Gool"], "venue": "In NIPS,", "citeRegEx": "Brabandere et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brabandere et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc Aurelio Ranzato", "Nando de Freitas"], "venue": "In NIPS,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Learning the structure of deep convolutional networks", "author": ["Jiashi Feng", "Trevor Darrell"], "venue": "In ICCV,", "citeRegEx": "Feng and Darrell.,? \\Q2015\\E", "shortCiteRegEx": "Feng and Darrell.", "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "The indian buffet process: An introduction and review", "author": ["Thomas L. Griffiths", "Zoubin Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Griffiths and Ghahramani.,? \\Q2011\\E", "shortCiteRegEx": "Griffiths and Ghahramani.", "year": 2011}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In NIPS,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Dsd: Regularizing deep neural networks with dense-sparse-dense training flow", "author": ["Song Han", "Jeff Pool", "Sharan Narang", "Huizi Mao", "Shijian Tang", "Erich Elsen", "Bryan Catanzaro", "John Tran", "William J. Dally"], "venue": "arXiv preprint arXiv:1607.04381,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Comparing biases for minimal network construction with backpropagation", "author": ["Stephen Jos\u00e9 Hanson", "Lorien Y. Pratt"], "venue": "In NIPS,", "citeRegEx": "Hanson and Pratt.,? \\Q1989\\E", "shortCiteRegEx": "Hanson and Pratt.", "year": 1989}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G. Stork"], "venue": "In NIPS,", "citeRegEx": "Hassibi and Stork.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi and Stork.", "year": 1993}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ECCV,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "author": ["Andrew G. Howard"], "venue": "In forthcoming,", "citeRegEx": "Howard.,? \\Q2017\\E", "shortCiteRegEx": "Howard.", "year": 2017}, {"title": "Network trimming: A data-driven neuron pruning approach towards efficient deep architectures", "author": ["Hengyuan Hu", "Rui Peng", "Yu-Wing Tai", "Chi-Keung Tang"], "venue": "arXiv preprint arXiv:1607.03250,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Deep roots: Improving cnn efficiency with hierarchical filter groups", "author": ["Yani Ioannou", "Duncan Robertson", "Roberto Cipolla", "Antonio Criminisi"], "venue": "arXiv preprint arXiv:1605.06489,", "citeRegEx": "Ioannou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2016}, {"title": "Training cnns with low-rank filters for efficient image classification", "author": ["Yani Ioannou", "Duncan Robertson", "Jamie Shotton", "Roberto Cipolla", "Antonio Criminisi"], "venue": "In ICLR,", "citeRegEx": "Ioannou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In BMVC,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Training skinny deep neural networks with iterative hard thresholding methods", "author": ["Xiaojie Jin", "Xiaotong Yuan", "Jiashi Feng", "Shuicheng Yan"], "venue": "arXiv preprint arXiv:1607.05423,", "citeRegEx": "Jin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"], "venue": "In ICLR,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "venue": "In ICLR,", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Pruning filters for efficient convnets", "author": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Online dictionary learning for sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In ICML,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Auto-sizing neural networks: With applications to n-gram language models", "author": ["Kenton Murray", "David Chiang"], "venue": "In EMNLP,", "citeRegEx": "Murray and Chiang.,? \\Q2015\\E", "shortCiteRegEx": "Murray and Chiang.", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["Alexander Novikov", "Dmitrii Podoprikhin", "Anton Osokin", "Dmitry P. Vetrov"], "venue": "In NIPS,", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Tensor-train decomposition", "author": ["Ivan V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "In ECCV,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Training very deep networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In NIPS,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Accelerating very deep convolutional networks", "author": ["Xiangyu Zhang", "Jianhua Zou", "Kaiming He", "Jian Sun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 24, "context": "Our approach is inspired by a very simple yet successful method called depth multiplier (Howard, 2017).", "startOffset": 88, "endOffset": 102}, {"referenceID": 11, "context": "For example, Denil et al. (2013) show that it is possible to learn less than 5% of the network parameters and predict the rest without losing predictive accuracy.", "startOffset": 13, "endOffset": 33}, {"referenceID": 49, "context": "For Inception V3 (Szegedy et al., 2016), we show that we can train a network with only about 300K of convolutional parameters1 and about 100M multiply-adds that achieves above 52% accuracy after it is fully trained.", "startOffset": 17, "endOffset": 39}, {"referenceID": 36, "context": "Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi & Stork, 1993) prune the networks based on second-order derivatives of the objectives.", "startOffset": 21, "endOffset": 41}, {"referenceID": 14, "context": "Recent work by Han et al. (2015; 2016a) alternates between pruning near-zero weights, which are encouraged by `1 or `2 regularization, and retraining the pruned networks. More complex regularizers have also been considered. Wen et al. (2016) and Li et al. (2016) put structured sparsity regularizers on the weights, while Murray & Chiang (2015) put them on the hidden units.", "startOffset": 15, "endOffset": 263}, {"referenceID": 14, "context": "Recent work by Han et al. (2015; 2016a) alternates between pruning near-zero weights, which are encouraged by `1 or `2 regularization, and retraining the pruned networks. More complex regularizers have also been considered. Wen et al. (2016) and Li et al. (2016) put structured sparsity regularizers on the weights, while Murray & Chiang (2015) put them on the hidden units.", "startOffset": 15, "endOffset": 345}, {"referenceID": 14, "context": "Recent work by Han et al. (2015; 2016a) alternates between pruning near-zero weights, which are encouraged by `1 or `2 regularization, and retraining the pruned networks. More complex regularizers have also been considered. Wen et al. (2016) and Li et al. (2016) put structured sparsity regularizers on the weights, while Murray & Chiang (2015) put them on the hidden units. Feng & Darrell (2015) explore a nonparametric prior based on the Indian buffet processes (Griffiths & Ghahramani, 2011) on layers.", "startOffset": 15, "endOffset": 397}, {"referenceID": 14, "context": "Recent work by Han et al. (2015; 2016a) alternates between pruning near-zero weights, which are encouraged by `1 or `2 regularization, and retraining the pruned networks. More complex regularizers have also been considered. Wen et al. (2016) and Li et al. (2016) put structured sparsity regularizers on the weights, while Murray & Chiang (2015) put them on the hidden units. Feng & Darrell (2015) explore a nonparametric prior based on the Indian buffet processes (Griffiths & Ghahramani, 2011) on layers. Hu et al. (2016) prune neurons based on the analysis of their outputs on a large dataset.", "startOffset": 15, "endOffset": 523}, {"referenceID": 1, "context": "Anwar et al. (2015b) consider special sparsity patterns: channel-wise (removing a feature map/channel from a layer), kernel-wise (removing all connections between two feature maps in consecutive layers), and intra-kernel-strided (removing connections between two features with particular stride and offset).", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Anwar et al. (2015b) consider special sparsity patterns: channel-wise (removing a feature map/channel from a layer), kernel-wise (removing all connections between two feature maps in consecutive layers), and intra-kernel-strided (removing connections between two features with particular stride and offset). They also propose to use particle filter to decide the importance of connections and paths during training. Another line of work explores fixed network architectures with some subsets of connections removed. For example, LeCun et al. (1998) remove connections between the first two convolutional feature maps in a completely uniform manner.", "startOffset": 0, "endOffset": 549}, {"referenceID": 34, "context": "These arrangements include columnar topologies inspired by AlexNet (Krizhevsky et al., 2012), tree-like topologies previously used by Ioannou et al.", "startOffset": 67, "endOffset": 92}, {"referenceID": 38, "context": "With the exception of (Anwar et al., 2015b; Li et al., 2016; Ioannou et al., 2016a) and depth multiplier (Howard, 2017), the above previous work performs connection pruning that leads to irregular network architectures.", "startOffset": 22, "endOffset": 83}, {"referenceID": 24, "context": ", 2016a) and depth multiplier (Howard, 2017), the above previous work performs connection pruning that leads to irregular network architectures.", "startOffset": 30, "endOffset": 44}, {"referenceID": 4, "context": "Along a similar vein, Cire\u015fan et al. (2011) remove random connections in their MNIST experiments.", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "Along a similar vein, Cire\u015fan et al. (2011) remove random connections in their MNIST experiments. However, they do not try to preserve the spatial convolutional density and it might be a challenge to harvest the savings on existing hardware. Ioannou et al. (2016a) explore three types of hierarchical arrangements of filter groups for CNNs, which depend on different assumptions about co-dependency of filters within each layer.", "startOffset": 22, "endOffset": 265}, {"referenceID": 4, "context": "Along a similar vein, Cire\u015fan et al. (2011) remove random connections in their MNIST experiments. However, they do not try to preserve the spatial convolutional density and it might be a challenge to harvest the savings on existing hardware. Ioannou et al. (2016a) explore three types of hierarchical arrangements of filter groups for CNNs, which depend on different assumptions about co-dependency of filters within each layer. These arrangements include columnar topologies inspired by AlexNet (Krizhevsky et al., 2012), tree-like topologies previously used by Ioannou et al. (2016b), and root-like topologies.", "startOffset": 22, "endOffset": 586}, {"referenceID": 4, "context": "Along a similar vein, Cire\u015fan et al. (2011) remove random connections in their MNIST experiments. However, they do not try to preserve the spatial convolutional density and it might be a challenge to harvest the savings on existing hardware. Ioannou et al. (2016a) explore three types of hierarchical arrangements of filter groups for CNNs, which depend on different assumptions about co-dependency of filters within each layer. These arrangements include columnar topologies inspired by AlexNet (Krizhevsky et al., 2012), tree-like topologies previously used by Ioannou et al. (2016b), and root-like topologies. Finally, Howard (2017) proposes the depth multiplier method to scale down the number of filters in each convolutional layer by a factor.", "startOffset": 22, "endOffset": 636}, {"referenceID": 16, "context": "(2016a) combine connection pruning in (Han et al., 2015) with quantization and Huffman coding.", "startOffset": 38, "endOffset": 56}, {"referenceID": 1, "context": "Hwang & Sung (2014); Arora et al. (2014) and Courbariaux et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 1, "context": "Hwang & Sung (2014); Arora et al. (2014) and Courbariaux et al. (2015; 2016); Rastegari et al. (2016) propose to train CNNs with ternary weights and binary weights, respectively.", "startOffset": 21, "endOffset": 102}, {"referenceID": 1, "context": "Hwang & Sung (2014); Arora et al. (2014) and Courbariaux et al. (2015; 2016); Rastegari et al. (2016) propose to train CNNs with ternary weights and binary weights, respectively. Gong et al. (2014) use vector quantization for parameters in fully connected layers.", "startOffset": 21, "endOffset": 198}, {"referenceID": 1, "context": "Anwar et al. (2015a) quantize a network with the squared error minimization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Anwar et al. (2015a) quantize a network with the squared error minimization. Chen et al. (2015) randomly group network parameters using a hash function.", "startOffset": 0, "endOffset": 96}, {"referenceID": 1, "context": "Anwar et al. (2015a) quantize a network with the squared error minimization. Chen et al. (2015) randomly group network parameters using a hash function. We note that this technique could be complementary to network pruning. For example, Han et al. (2016a) combine connection pruning in (Han et al.", "startOffset": 0, "endOffset": 256}, {"referenceID": 12, "context": "Decomposition methods include truncated SVD (Denton et al., 2014), decomposition to rank-1 bases (Jaderberg et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 31, "context": ", 2014), decomposition to rank-1 bases (Jaderberg et al., 2014), CP decomposition (PARAFAC or CANDECOMP) (Lebedev et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 35, "context": ", 2014), CP decomposition (PARAFAC or CANDECOMP) (Lebedev et al., 2015), Tensor-Train decomposition of Oseledets (2011) (Novikov et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 42, "context": ", 2015), Tensor-Train decomposition of Oseledets (2011) (Novikov et al., 2015), sparse dictionary learning of Mairal et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 39, "context": "(2009) and PCA (Liu et al., 2015), asymmetric (3D) decomposition using reconstruction loss of non-linear responses combined with a rank selection method based on PCA accumulated energy (Zhang et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 33, "context": ", 2015b;a), and Tucker decomposition using the kernel tensor reconstruction loss combined with a rank selection method based on global analytic variational Bayesian matrix factorization (Kim et al., 2016).", "startOffset": 186, "endOffset": 204}, {"referenceID": 12, "context": "Decomposition methods include truncated SVD (Denton et al., 2014), decomposition to rank-1 bases (Jaderberg et al., 2014), CP decomposition (PARAFAC or CANDECOMP) (Lebedev et al., 2015), Tensor-Train decomposition of Oseledets (2011) (Novikov et al.", "startOffset": 45, "endOffset": 234}, {"referenceID": 12, "context": "Decomposition methods include truncated SVD (Denton et al., 2014), decomposition to rank-1 bases (Jaderberg et al., 2014), CP decomposition (PARAFAC or CANDECOMP) (Lebedev et al., 2015), Tensor-Train decomposition of Oseledets (2011) (Novikov et al., 2015), sparse dictionary learning of Mairal et al. (2009) and PCA (Liu et al.", "startOffset": 45, "endOffset": 309}, {"referenceID": 16, "context": "Recently, Han et al. (2016b) and Jin et al. (2016) propose a form of regularization where dropped connections are unfrozen and the network is retrained.", "startOffset": 10, "endOffset": 51}, {"referenceID": 48, "context": "See, among others, highway networks (Srivastava et al., 2015), residual networks (He et al.", "startOffset": 36, "endOffset": 61}, {"referenceID": 24, "context": "Many large networks can be significantly reduced in size using this method with only a small loss of precision (Howard, 2017).", "startOffset": 111, "endOffset": 125}, {"referenceID": 24, "context": "We first give a description of the depth multiplier method used in Howard (2017). Given a hyperparameter \u03b1 \u2208 (0, 1], the depth multiplier approach scales down the number of filters in each convolutional layers by \u03b1.", "startOffset": 67, "endOffset": 81}, {"referenceID": 49, "context": "IMAGENET We use open source Inception-V3 (Szegedy et al., 2016) network and a slightly modified version of VGG-16 (Simonyan & Zisserman, 2015) called VGG-16n on ImageNet ILSVRC 2012 (Deng et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 10, "context": ", 2016) network and a slightly modified version of VGG-16 (Simonyan & Zisserman, 2015) called VGG-16n on ImageNet ILSVRC 2012 (Deng et al., 2009; Russakovsky et al., 2015).", "startOffset": 126, "endOffset": 171}, {"referenceID": 45, "context": ", 2016) network and a slightly modified version of VGG-16 (Simonyan & Zisserman, 2015) called VGG-16n on ImageNet ILSVRC 2012 (Deng et al., 2009; Russakovsky et al., 2015).", "startOffset": 126, "endOffset": 171}, {"referenceID": 49, "context": "We expect the final numbers to match the reported number for Inception V3 Szegedy et al. (2016), and the smaller networks to have comparable improvement.", "startOffset": 74, "endOffset": 96}, {"referenceID": 34, "context": "4), achieves AlexNet-level accuracy (Krizhevsky et al., 2012) with fewer than 400K parameters and VGG-level one (Fig.", "startOffset": 36, "endOffset": 61}], "year": 2016, "abstractText": "Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_335"}