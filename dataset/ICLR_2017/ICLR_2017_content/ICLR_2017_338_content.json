{"name": "ICLR_2017_338.pdf", "metadata": {"source": "CRF", "title": "TWO METHODS FOR WILD VARIATIONAL INFERENCE", "authors": ["Qiang Liu", "Yihao Feng"], "emails": ["yihao.feng.gr}@dartmouth.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Probabilistic modeling provides a principled approach for reasoning under uncertainty, and has been increasingly dominant in modern machine learning where highly complex, structured probabilistic models are often the essential components for solving complex problems with increasingly larger datasets. A key challenge, however, is to develop computationally efficient Bayesian inference methods to approximate, or draw samples from the posterior distributions. Variational inference (VI) provides a powerful tool for scaling Bayesian inference to complex models and big data. The basic idea of VI is to approximate the true distribution with a simpler distribution by minimizing the KL divergence, transforming the inference problem into an optimization problem, which is often then solved efficiently using stochastic optimization techniques (e.g., Hoffman et al., 2013; Kingma & Welling, 2013). However, the practical design and application of VI are still largely restricted by the requirement of using simple approximation families, as we explain in the sequel.\nLet p(z) be a distribution of interest, such as the posterior distribution in Bayesian inference. VI approximates p(z) with a simpler distribution q\u2217(z) found in a set Q = {q\u03b7(z)} of distributions indexed by parameter \u03b7 by minimizing the KL divergence objective:\nmin \u03b7\n{ KL(q\u03b7 || p) \u2261 Ez\u223cq\u03b7 [log(q\u03b7(z)/p(z))] } , (1)\nwhere we can get exact result p = q\u2217 if Q is chosen to be broad enough to actually include p. In practice, however, Q should be chosen carefully to make the optimization in (1) computationally tractable; this casts two constraints on Q: 1. A minimum requirement is that we should be able to sample from q\u03b7 efficiently, which allows us to make estimates and predictions based on q\u03b7 in placement of the more intractable p. The samples from q\u03b7 can also be used to approximate the expectation Eq[\u00b7] in (1) during optimization. This means that there should exist some computable function f(\u03b7; \u03be), called the inference network, which takes a random seed \u03be, whose distribution is denoted by q0, and outputs a random variable z = f(\u03b7; \u03be) whose distribution is q\u03b7 .\n2. We should also be able to calculate the density q\u03b7(z) or it is derivative in order to optimize the KL divergence in (1). This, however, casts a much more restrictive condition, since it requires us to use only simple inference network f(\u03b7; \u03be) and input distributions q0 to ensure a tractable form for the density q\u03b7 of the output z = f(\u03b7; \u03be).\nIn fact, it is this requirement of calculating q\u03b7(z) that has been the major constraint for the design of state-of-the-art variational inference methods. The traditional VI methods are often limited to\nusing simple mean field, or Gaussian-based distributions as q\u03b7 and do not perform well for approximating complex target distributions. There is a line of recent work on variational inference with rich approximation families (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only a few), all based on handcrafting special inference networks to ensure the computational tractability of q\u03b7(z) while simultaneously obtaining high approximation accuracy. These approaches require substantial mathematical insights and research effects, and can be difficult to understand or use for practitioners without a strong research background in VI. Methods that allow us to use arbitrary inference networks without substantial constraints can significantly simplify the design and applications of VI methods, allowing practical users to focus more on choosing proposals that work best with their specific tasks.\nWe use the term wild variational inference to refer to variants of variational methods working with general inference networks f(\u03b7, \u03be) without tractability constraints on its output density q\u03b7(z); this should be distinguished with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(z) without significant model-by-model consideration (but still require to calculate the proposal density q\u03b7(z)). Essentially, wild variational inference makes it possible to \u201clearn to draw samples\u201d, constructing black-box neural samplers for given distributions. This enables more adaptive and automatic design of efficient Bayesian inference procedures, replacing the hand-designed inference algorithms with more efficient ones that can improve their efficiency adaptively over time based on past tasks they performed.\nIn this work, we discuss two methods for wild variational inference, both based on recent works that combine kernel techniques with Stein\u2019s method (e.g., Liu & Wang, 2016; Liu et al., 2016). The first method, also discussed in Wang & Liu (2016), is based on iteratively adjusting parameter \u03b7 to make the random output z = f(\u03b7; \u03be) mimic a Stein variational gradient direction (SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution. The second method is based on minimizing a kernelized Stein discrepancy, which, unlike KL divergence, does not require to calculate density q\u03b7(z) for the optimization thanks to its special form.\nAnother critical problem is to design good network architectures well suited for Bayesian inference. Ideally, the network design should leverage the information of the target distribution p(z) in a convenient way. One useful perspective is that we can view the existing MC/MCMC methods as (hand-designed) stochastic neural networks which can be used to construct native inference networks for given target distributions. On the other hand, using existing MC/MCMC methods as inference networks also allow us to adaptively adjust the hyper-parameters of these algorithms; this enables amortized inference which leverages the experience on past tasks to accelerate the Bayesian computation, providing a powerful approach for designing efficient algorithms in settings when a large number of similar tasks are needed.\nAs an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as the inference network, which can be treated as a special deep residential network (He et al., 2016), in which important gradient information \u2207z log p(z) is fed into each layer to allow efficient approximation for the target distribution p(z). In our case, the network parameter \u03b7 are the step sizes of SGLD, and our method provides a way to adaptively improve the step sizes, providing speed-up on future tasks with similar structures. We show that the adaptively estimated step sizes significantly outperform the hand-designed schemes such as Adagrad.\nRelated Works The idea of amortized inference (Gershman & Goodman, 2014) has been recently applied in various domains of probabilistic reasoning, including both amortized variational inference\n(e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a) and date-driven designs of Monte Carlo based methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods, however, require to explicitly calculate q\u03b7(z) (or its gradient).\nOne well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating q\u03b7(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.2.\nThe auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative way when the variational distribution q\u03b7(z) can be represented as a hidden variable model. In particular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a variational approximation. These approaches, however, still require to write down the likelihood function on the augmented spaces, and need to introduce an additional inference network related to the auxiliary variables.\nThere is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008; Roberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of MCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the autocorrelation). Our method is simpler, more generic, and works efficiently in practice thanks to the use of gradient-based back-propagation. Finally, connections between stochastic gradient descent and variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al. (2015).\nOutline Section 2 introduces background on Stein discrepancy and Stein variational gradient descent. Section 3 discusses two methods for wild variational inference. Section 4 discuss using stochastic gradient Langevin dynamics (SGLD) as the inference network. Empirical results are shown in Section 5."}, {"heading": "2 STEIN\u2019S IDENTITY, STEIN DISCREPANCY, STEIN VARIATIONAL GRADIENT", "text": "Stein\u2019s identity Stein\u2019s identity plays a fundamental role in our framework. Let p(z) be a positive differentiable density on Rd, and \u03c6(z) = [\u03c61(z), \u00b7 \u00b7 \u00b7 , \u03c6d(z)]> is a differentiable vector-valued function. Define\u2207z \u00b7 \u03c6 = \u2211 i \u2202zi\u03c6. Stein\u2019s identity is\nEz\u223cp[\u3008\u2207z log p(z), \u03c6(z)\u3009+\u2207z \u00b7 \u03c6(z)] = \u222b X \u2207z \u00b7 (p(z)\u03c6(z))dx = 0, (2)\nwhich holds once p(z)\u03c6(z) vanishes on the boundary of X by integration by parts or Stokes\u2019 theorem; It is useful to rewrite Stein\u2019s identity in a more compact way:\nEz\u223cp[Tp\u03c6(z)] = 0, with Tp\u03c6 def = \u3008\u2207z log p, \u03c6\u3009+\u2207z \u00b7 \u03c6, (3)\nwhere Tp is called a Stein operator, which acts on function \u03c6 and returns a zero-mean function Tp\u03c6(z) under z \u223c p. A key computational advantage of Stein\u2019s identity and Stein operator is that they depend on p only through the derivative of the log-density \u2207z log p(z), which does not depend on the cumbersome normalization constant of p, that is, when p(z) = p\u0304(z)/Z, we have \u2207z log p(z) = \u2207z log p\u0304(z), independent of the normalization constant Z. This property makes Stein\u2019s identity a powerful practical tool for handling unnormalized distributions widely appeared in machine learning and statistics.\nStein Discrepancy Although Stein\u2019s identity ensures that Tp\u03c6 has zero expectation under p, its expectation is generally non-zero under a different distribution q. Instead, for p 6= q, there must exist a \u03c6 which distinguishes p and q in the sense that Ez\u223cq[Tp\u03c6(z)] 6= 0. Stein discrepancy leverages this fact to measure the difference between p and q by considering the \u201cmaximum violation of Stein\u2019s identity\u201d for \u03c6 in certain function set F :\nD(q || p) = max \u03c6\u2208F\n{ Ez\u223cq[Tp\u03c6(z)] } , (4)\nwhere F is the set of functions \u03c6 that we optimize over, and decides both the discriminative power and computational tractability of Stein discrepancy. Kernelized Stein discrepancy (KSD) is a special\nStein discrepancy that takes F to be the unit ball of vector-valued reproducing kernel Hilbert spaces (RKHS), that is,\nF = {\u03c6 \u2208 Hd : ||\u03c6||Hd \u2264 1}, (5)\nwhere H is a real-valued RKHS with kernel k(z, z\u2032). This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017):\nD(q || p) = max \u03c6\u2208Hd\n{ Ez\u223cq[Tp\u03c6(z)], s.t. ||\u03c6||Hd \u2264 1 } , (6)\n= \u221a Ez,z\u2032\u223cq[\u03bap(z, z\u2032)], (7)\nwhere \u03bap(z, z\u2032) is a positive definite kernel obtained by applying Stein operator on k(z, z\u2032) twice:\n\u03bap(z, z \u2032) = T z\n\u2032\np (T zp \u2297 k(z, z\u2032)), = sp(z)sp(z\n\u2032)k(z, z\u2032) + sp(z)\u2207z\u2032k(z, z\u2032) + sp(z\u2032)\u2207zk(z, z\u2032) +\u2207z \u00b7 (\u2207z\u2032k(z, z\u2032)), (8)\nwhere sp(z) = \u2207z log p(z) and T zp and T zp denote the Stein operator when treating k(z, z\u2032) as a function of z and z\u2032, respectively; here we defined T zp \u2297k(z, z\u2032) = \u2207x log p(x)k(z, z\u2032)+\u2207xk(z, z\u2032) which returns a d\u00d7 1 vector-valued function. It can be shown that D(q || p) = 0 if and only if q = p when k(z, z\u2032) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016). D(q || p) can treated as a variant of maximum mean discrepancy equipped with kernel \u03bap(z, z\n\u2032) which depends on p (which makes D(q || p) asymmetric on q and p). The form of KSD in (6) allows us to estimate the discrepancy between a set of sample {zi} (e.g., drawn from q) and a distribution p specified by\u2207z log p(z),\nD\u03022u({zi} || p) = 1 n(n\u2212 1) \u2211 i6=j [\u03bap(zi, zj)], D\u03022v({zi} || p) = 1 n2 \u2211 i,j [\u03bap(zi, zj)], (9)\nwhere D\u03022u(q || p) provides an unbiased estimator (hence called a U -statistic) for D2(q || p), and D\u03022v(q || p), called V -statistic, provides a biased estimator but is guaranteed to be always nonnegative: D\u03022v({zi} || p) \u2265 0.\nStein Variational Gradient Descent (SVGD) Stein operator and Stein discrepancy have a close connection with KL divergence, which is exploited in Liu & Wang (2016) to provide a general purpose deterministic approximate sampling method. Assume that {zi}ni=1 is a sample (or a set of particles) drawn from q, and we want to update {zi}ni=1 to make it \u201cmove closer\u201d to the target distribution p to improve the approximation quality. We consider updates of form\nzi \u2190 zi + \u03c6\u2217(zi), \u2200i = 1, . . . , n, (10)\nwhere \u03c6\u2217 is a perturbation direction, or velocity field, chosen to maximumly decrease the KL divergence between the distribution of updated particles and the target distribution, in the sense that\n\u03c6\u2217 = arg max \u03c6\u2208F { \u2212 d d KL(q[ \u03c6] || p) \u2223\u2223 =0 } , (11)\nwhere q[ \u03c6] denotes the density of the updated particle z\u2032 = z + \u03c6(z) when the density of the original particle z is q, and F is the set of perturbation directions that we optimize over. A key observation (Liu & Wang, 2016) is that the optimization in (11) is in fact equivalent to the optimization for KSD in (4); we have\n\u2212 d d KL(q[ \u03c6] || p) \u2223\u2223 =0 = Ez\u223cq[Tp\u03c6(z)], (12)\nthat is, the Stein operator transforms the perturbation \u03c6 on the random variable (the particles) to the change of the KL divergence. Taking F to be unit ball of Hd as in (5), the optimal solution \u03c6\u2217 of (11) equals that of (6), which is shown to be (e.g., Liu et al., 2016)\n\u03c6\u2217(z\u2032) \u221d Ez\u223cq[T zp k(z, z\u2032)] = Ez\u223cq[\u2207z log p(z)k(z, z\u2032) +\u2207zk(z, z\u2032)].\nAlgorithm 1 Amortized SVGD and KSD Minimization for Wild Variational Inference for iteration t do\n1. Draw random {\u03bei}ni=1, calculate zi = f(\u03b7; \u03bei), and the Stein variational gradient \u2206zi in (13). 2. Update parameter \u03b7 using (14) or (15) for amortized SVGD, or (17) for KSD minimization.\nend for\nBy approximating the expectation under q with the empirical mean of the current particles {zi}ni=1, SVGD admits a simple form of update that iteratively moves the particles towards the target distribution,\nzi \u2190 zi + \u2206zi, \u2200i = 1, . . . , n, \u2206zi = E\u0302z\u2208{zi}ni=1 [\u2207z log p(z)k(z, zi) +\u2207zk(z, zi)], (13)\nwhere E\u0302z\u223c{zi}ni=1 [f(z)] = \u2211 i f(zi)/n. The two terms in \u2206zi play two different roles: the term with the gradient \u2207z log p(z) drives the particles towards the high probability regions of p(z), while the term with \u2207zk(z, zi) serves as a repulsive force to encourage diversity; to see this, consider a stationary kernel k(z, z\u2032) = k(z \u2212 z\u2032), then the second term reduces to E\u0302z\u2207zk(z, zi) = \u2212E\u0302z\u2207zik(z, zi), which can be treated as the negative gradient for minimizing the average similarity E\u0302zk(z, zi) in terms of zi.\nIt is easy to see from (13) that \u2206zi reduces to the typical gradient \u2207z log p(zi) when there is only a single particle (n = 1) and \u2207zk(z, zi) when z = zi, in which case SVGD reduces to the standard gradient ascent for maximizing log p(z) (i.e., maximum a posteriori (MAP))."}, {"heading": "3 TWO METHODS FOR WILD VARIATIONAL INFERENCE", "text": "Since the direct parametric optimization of the KL divergence (1) requires calculating q\u03b7(z), there are two essential ways to avoid calculating q\u03b7(z): either using alternative (approximate) optimization approaches, or using different divergence objective functions. We discuss two possible approaches in this work: one based on \u201camortizing SVGD\u201d (Wang & Liu, 2016) which trains the inference network f(\u03b7, \u03be) so that its output mimic the SVGD dynamics in order to decrease the KL divergence; another based on minimizing the KSD objective (9) which does not require to evaluate q(z) thanks to its special form."}, {"heading": "3.1 AMORTIZED SVGD", "text": "SVGD provides an optimal updating direction to iteratively move a set of particles {zi} towards the target distribution p(z). We can leverage it to train an inference network f(\u03b7; \u03be) by iteratively adjusting \u03b7 so that the output of f(\u03b7; \u03be) changes along the Stein variational gradient direction in order to maximumly decrease its KL divergence with the target distribution. By doing this, we \u201camortize\u201d SVGD into a neural network, which allows us to leverage the past experience to adaptively improve the computational efficiency and generalize to new tasks with similar structures. Amortized SVGD is also presented in Wang & Liu (2016); here we present some additional discussion.\nTo be specific, assume {\u03bei} are drawn from q0 and zi = f(\u03b7; \u03bei) the corresponding random output based on the current estimation of \u03b7. We want to adjust \u03b7 so that zi changes along the Stein variational gradient direction \u2206zi in (13) so as to maximumly decrease the KL divergence with target distribution. This can be done by updating \u03b7 via\n\u03b7 \u2190 arg min \u03b7 n\u2211 i=1 ||f(\u03b7; \u03bei)\u2212 zi \u2212 \u2206zi||22. (14)\nEssentially, this projects the non-parametric perturbation direction \u2206zi to the change of the finite dimensional network parameter \u03b7. If we take the step size to be small, then the updated \u03b7 by (14) should be very close to the old value, and a single step of gradient descent of (14) can provide a\ngood approximation for (14). This gives a simpler update rule: \u03b7 \u2190 \u03b7 + \u2211 i \u2202\u03b7f(\u03b7; \u03bei)\u2206zi, (15)\nwhich can be intuitively interpreted as a form of chain rule that back-propagates the SVGD gradient to the network parameter \u03b7. In fact, when we have only one particle, (15) reduces to the standard gradient ascent for max\u03b7 log p(f(\u03b7; \u03be)), in which f\u03b7 is trained to \u201clearn to optimize\u201d (e.g., Andrychowicz et al., 2016), instead of \u201clearn to sample\u201d p(z). Importantly, as we have more than one particles, the repulsive term \u2207zk(z, zi) in \u2206zi becomes active, and enforces an amount of diversity on the network output that is consistent with the variation in p(z). The full algorithm is summarized in Algorithm 1.\nAmortized SVGD can be treated as minimizing the KL divergence using a rather special algorithm: it leverages the non-parametric SVGD which can be treated as approximately solving the infinite dimensional optimization minq KL(q || p) without explicitly assuming a parametric form on q, and iteratively projecting the non-parametric update back to the finite dimensional parameter space of \u03b7. It is an interesting direction to extend this idea to \u201camortize\u201d other MC/MCMC-based inference algorithms. For example, given a MCMC with transition probability T (z\u2032|z) whose stationary distribution is p(z), we may adjust \u03b7 to make the network output move towards the updated values z\u2032 drawn from the transition probability T (z\u2032|z). The advantage of using SVGD is that it provides a deterministic gradient direction which we can back-propagate conveniently and is particle efficient in that it reduces to \u201clearning to optimize\u201d with a single particle. We have been using the simple L2 loss in (14) mainly for convenience; it is possible to use other two-sample discrepancy measures such as maximum mean discrepancy."}, {"heading": "3.2 KSD VARIATIONAL INFERENCE", "text": "Amortized SVGD attends to minimize the KL divergence objective, but can not be interpreted as a typical finite dimensional optimization on parameter \u03b7. Here we provide an alternative method based on directly minimizing the kernelized Stein discrepancy (KSD) objective, for which, thanks to its special form, the typical gradient-based optimization can be performed without needing to estimate q(z) explicitly.\nTo be specific, take q\u03b7 to be the density of the random output z = f(\u03b7; \u03be) when \u03be \u223c q0, and we want to find \u03b7 to minimize D(q\u03b7 || p). Assuming {\u03bei} is i.i.d. drawn from q0, we can approximate D2(q\u03b7 || p) unbiasedly with a U-statistics:\nD2(q\u03b7 || p) \u2248 1 n(n\u2212 1) \u2211 i 6=j \u03bap(f(\u03b7; \u03bei), f(\u03b7; \u03bej)), (16)\nfor which a standard gradient descent can be derived for optimizing \u03b7:\n\u03b7 \u2190 \u03b7 \u2212 2 n(n\u2212 1) \u2211 i 6=j \u2202\u03b7f(\u03b7; \u03bei)\u2207zi\u03bap(zi, zj), where zi = f(\u03b7; \u03bei). (17)\nThis enables a wild variational inference method based on directly minimizing \u03b7 with standard (stochastic) gradient descent. See Algorithm 1. Note that (17) is similar to (15) in form, but replaces \u2206zi with a \u2206\u0303zi \u221d \u2212 \u2211 j : i 6=j \u2207zi\u03bap(zi, zj). It is also possible to use the V -statistic in (9), but we find that the U -statistic performs much better in practice, possibly because of its unbiasedness property.\nMinimizing KSD can be viewed as minimizing a constrastive divergence objective function. To see this, recall that q[ \u03c6] denotes the density of z\u2032 = z + \u03c6(z) when z \u223c q. Combining (11) and (6), we can show that\nD2(q || p) \u2248 1\n(KL(q || p)\u2212KL(q[ \u03c6] || p)).\nThat is, KSD measures the amount of decrease of KL divergence when we update the particles along the optimal SVGD perturbation direction \u03c6 given by (11). If q = p, then the decrease of KL\ndivergence equals zero and D2(q || p) equals zero. In fact, as shown in Liu & Wang (2016) KSD can be explicitly represented as the magnitude of a functional gradient of KL divergence:\nD(q || p) = \u2223\u2223\u2223\u2223\u2223\u2223 d d\u03c6 KL(q[\u03c6] || p) \u2223\u2223 \u03c6=0 \u2223\u2223\u2223\u2223\u2223\u2223 Hd ,\nwhere q[\u03c6] is the density of z = z +\u03c6(z) when z \u223c q, and dd\u03c6F (\u03c6) denotes the functional gradient of functional F (\u03c6) w.r.t. \u03c6 defined in RKHSHd, and dd\u03c6F (\u03c6) is also an element inH\nd. Therefore, KSD variational inference can be treated as explicitly minimizing the magnitude of the gradient of KL divergence, in contract with amortized SVGD which attends to minimize the KL divergence objective itself.\nThis idea is also similar to the contrastive divergence used for learning restricted Boltzmann machine (RBM) (Hinton, 2002) (which, however, optimizes p with fixed q). It is possible to extend this approach by replacing z\u2032 = z + \u03c6(z) with other transforms, such as these given by a transition probability of a Markov chain whose stationary distribution is p. In fact, according the so called generator method for constructing Stein operator (Barbour, 1988), any generator of a Markov process defines a Stein operator that can be used to define a corresponding Stein discrepancy.\nThis idea is related to a very recent work by Ranganath et al. (2016), which is based on directly minimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F consists of a neural network \u03c6\u03c4 (z) parametrized by \u03c4 , and find \u03b7 by solving the following min-max problem:\nmin \u03b7 max \u03c4 Ez\u223cq[Tp\u03c6\u03c4 (z)].\nIn contrast, our method leverages the closed form solution by taking F to be an RKHS and hence obtains an explicit optimization problem, instead of a min-max problem that can be computationally more expensive, or have difficulty in achieving convergence.\nBecause \u03bap(x, x\u2032) (defined in (8)) depends on the derivative \u2207x log p(x) of the target distribution, the gradient in (17) depends on the Hessian matrix \u22072x log p(x) and is hence less convenient to implement compared with amortized SVGD (the method by Ranganath et al. (2016) also has the same problem). However, this problem can be alleviated using automatic differentiation tools, which be used to directly take the derivative of the objective in (16) without manually deriving its derivatives."}, {"heading": "4 LANGEVIN INFERENCE NETWORK", "text": "With wild variational inference, we can choose more complex inference network structures to obtain better approximation accuracy. Ideally, the best network structure should leverage the special properties of the target distribution p(z) in a convenient way. One way to achieve this by viewing existing MC/MCMC methods as inference networks with hand-designed (and hence potentially suboptimal) parameters, but good architectures that take the information of the target distribution p(z) into account. By applying wild variational inference on networks constructed based on existing MCMC methods, we effectively provide an hyper-parameter optimization for these existing methods. This allows us to fully optimize the potential of existing Bayesian inference methods, significantly improving the result with less computation cost, and decreasing the need for hyper-parameter tuning by human experts. This is particularly useful when we need to solve a large number of similar tasks, where the computation cost spent on optimizing the hyper-parameters can significantly improve the performance on the future tasks.\nStochastic Gradient Langevin Dynamics We first take the original stochastic gradient Langevin dynamics (SGLD) algorithm (Welling & Teh, 2011) as an example. SGLD starts with a random initialization z0, and perform iterative update of form\nzt+1 \u2190 zt + \u03b7t \u2207z log p\u0302(zt; Mt) + \u221a 2\u03b7t \u03bet, \u2200t = 1, \u00b7 \u00b7 \u00b7T, (18)\nwhere log p\u0302(zt; Mt) denotes an approximation of log p(zt) based on, e.g., a random mini-batch Mt of observed data at t-th iteration, and \u03bet is a standard Gaussian random vector of the same size as z, and \u03b7t denotes a (vector) step-size at t-th iteration; here \u201c \u201d denotes element-wise product. When running SGLD for T iterations, we can treat zT as the output of a T -layer neural network\nparametrized by the collection of step sizes \u03b7 = {\u03b7t}Tt=1, whose random inputs include the random initialization z0, the mini-batch Mt and Gaussian noise \u03bet at each iteration t. We can see that this defines a rather complex network structure with several different types of random inputs (z0, Mt and \u03bet). This makes it intractable to explicitly calculate the density of zT and traditional variational inference methods can not be applied directly. But wild variational inference can still allow us to adaptively improve the optimal step-size \u03b7 in this case.\nGeneral Langevin Networks Based on the original formula of SGLD, we proposed a more general langevin network structure, and each layer of the network has a form\nzt+1 \u2190 Atzt + h(BtBt>\u2207z log p\u0302(zt; Mt) +Bt\u03bet +Dt), \u2200t = 1, \u00b7 \u00b7 \u00b7T, (19)\nwhere At, Bt and Dt are network parameters at t-th iteration(whose size is d\u00d7 d, and d is the size of zt), and h(\u00b7) denotes a smooth element-wise non-linearity function; here \u03bet is still a standard gaussian random vector with the same size as z. With this more complex network, we can use fewer layers to construct more powerful back-box samplers."}, {"heading": "5 EMPIRICAL RESULTS", "text": ""}, {"heading": "5.1 SGLD INFERENCE NETWORK", "text": "We first test our algorithm with SGLD inference network with (18) formula on both a toy Gaussian mixture model and a Bayesian logistic regression example. We find that we can adaptively learn step sizes that significantly outperform the existing hand-designed step size schemes, and hence save computational cost in the testing phase. In particular, we compare with the following step size schemes, for all of which we report the best results (testing accuracy in Figure 3(a); testing likelihood in Figure 3(b)) among a range of hyper-parameters:\n1. Constant Step Size. We select a best constant step size in {1, 2, 23, . . . , 229} \u00d7 10\u22126. 2. Power Decay Step Size. We consider t = 10a \u00d7 (b + t)\u2212\u03b3 where \u03b3 = 0.55, a \u2208 {\u22126,\u22125, . . . , 1, 2}, b \u2208 {0, 1, . . . , 9}. 3. Adagrad, Rmsprop, Adadelta, all with the master step size selected in {1, 2, 23, . . . , 229}\u00d7 10\u22126, with the other parameters chosen by default values.\nGaussian Mixture We start with a simple 1D Gaussian mixture example shown in Figure 2 where the target distribution p(z) is shown by the red dashed curve. We use amortized SVGD and KSD to optimize the step size parameter of the Langevin inference network in (18) with T = 20 layers (i.e., SGLD with T = 20 iterations), with an initial z0 drawn from a q0 far away from the target distribution (see the green curve in Figure 2(a)); this makes it critical to choose a proper step size to achieve close approximation within T = 20 iterations. We find that amortized SVGD and KSD allow us to achieve good performance with 20 steps of SGLD updates (Figure 2(b)-(c)), while the result of the best constant step size and power decay step-size are much worse (Figure 2(d)-(e)).\nBayesian Logistic Regression We consider Bayesian logistic regression for binary classification using the same setting as Gershman et al. (2012), which assigns the regression weights w with a Gaussian prior p0(w|\u03b1) = N (w,\u03b1\u22121) and p0(\u03b1) = Gamma(\u03b1, 1, 0.01). The inference is applied on the posterior of z = [w, log\u03b1]. We test this model on the binary Covertype dataset1 with 581,012 data points and 54 features.\nTo demonstrate that our estimated learning rate can work well on new datasets never seen by the algorithm. We partition the dataset into mini-datasets of size 50, 000, and use 80% of them for training and 20% for testing. We adapt our amortized SVGD/KSD to train on the whole population of the training mini-datasets by randomly selecting a mini-dataset at each iteration of Algorithm 1, and evaluate the performance of the estimated step sizes on the remaining 20% testing mini-datasets.\nFigure 3 reports the testing accuracy and likelihood on the 20% testing mini-datasets when we train the Langevin network with T = 10, 50, 100 layers, respectively. We find that our methods outperform all the hand-designed learning rates, and allow us to get performance closer to the fully converged SGLD and SVGD with a small number T of iterations.\nFigure 4 shows the testing accuracy and testing likelihood of all the intermediate results when training Langevin network with T = 100 layers. It is interesting to observe that amortized SVGD and KSD learn rather different behavior: KSD tends to increase the performance quickly at the first few iterations but saturate quickly, while amortized SVGD tends to increase slowly in the beginning and boost the performance quickly in the last few iterations. Note that both algorithms are set up to optimize the performance of the last layers, while need to decide how to make progress on the intermediate layers to achieve the best final performance."}, {"heading": "5.2 GENERAL LANGEVIN INFERENCE NETWORK", "text": "We further test our algorithm with general Langevin inference network. We firstly construct one single layer general Langevin network to approach the posterior of Bayesian logistic regression parameters and we can achieve 74.58% average accuracy and \u22120.5216 average testing log-likelihood in 100 repeat experiments. This result proves the proposed general Langevin Inference Network is quite competitive and worth to explore. Moreover, we use it as a black-box sampler to approach more complicate Gaussian Mixture distributions.\nGaussian Mixture We consider 10 components Gaussian Mixture Models with mean and covariance matrix of each component randomly drawed from a uniform distribution, and we test our methods on different dimensions models.\nWe construct 6 layers of general Langevin networks as a black-box sampler, and our proposed two methods to train the black-box sampler to approximate the target distribution. Figure 5 shows our\n1https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/binary.html\nresults on 50 dimension Gaussian Mixture case and figure 6 shows results of different dimensions of Gaussian Mixture. From the figures we can know that our proposed sampling structure is quite competive comparing with NUT sampler(Hoffman & Gelman, 2014), and these two variational inference methods can both train a good black-box sampler."}, {"heading": "6 CONCLUSION", "text": "We consider two methods for wild variational inference that allows us to train general inference networks with intractable density functions, and apply it to adaptively estimate step sizes of stochastic gradient Langevin dynamics. More studies are needed to develop better methods, more applications and theoretical understandings for wild variational inference, and we hope that the two methods we discussed in the paper can motivate more ideas and studies in the field."}], "references": [{"title": "An auxiliary variational method", "author": ["Agakov", "Felix V", "Barber", "David"], "venue": "In International Conference on Neural Information Processing,", "citeRegEx": "Agakov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Agakov et al\\.", "year": 2004}, {"title": "A tutorial on adaptive mcmc", "author": ["Andrieu", "Christophe", "Thoms", "Johannes"], "venue": "Statistics and Computing,", "citeRegEx": "Andrieu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2008}, {"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "arXiv preprint arXiv:1606.04474,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Stein\u2019s method and poisson process convergence", "author": ["Barbour", "Andrew D"], "venue": "Journal of Applied Probability,", "citeRegEx": "Barbour and D.,? \\Q1988\\E", "shortCiteRegEx": "Barbour and D.", "year": 1988}, {"title": "A kernel test of goodness of fit", "author": ["Chwialkowski", "Kacper", "Strathmann", "Heiko", "Gretton", "Arthur"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2016}, {"title": "Nonparametric variational inference", "author": ["Gershman", "Samuel", "Hoffman", "Matt", "Blei", "David"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Gershman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2012}, {"title": "Amortized inference in probabilistic reasoning", "author": ["Gershman", "Samuel J", "Goodman", "Noah D"], "venue": "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,", "citeRegEx": "Gershman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2014}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["Gorham", "Jack", "Mackey", "Lester"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gorham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gorham et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo", "author": ["Hoffman", "Matthew D", "Gelman", "Andrew"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Liu", "Qiang", "Wang", "Dilin"], "venue": "arXiv preprint arXiv:1608.04471,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests", "author": ["Liu", "Qiang", "Lee", "Jason D", "Jordan", "Michael I"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Early stopping is nonparametric variational inference", "author": ["Maclaurin", "Dougal", "Duvenaud", "David", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1504.01344,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "A variational analysis of stochastic gradient algorithms", "author": ["Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M"], "venue": "arXiv preprint arXiv:1602.02666,", "citeRegEx": "Mandt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mandt et al\\.", "year": 2016}, {"title": "Control functionals for Monte Carlo integration", "author": ["Oates", "Chris J", "Girolami", "Mark", "Chopin", "Nicolas"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Oates et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2017}, {"title": "Inference networks for sequential monte carlo in graphical models", "author": ["Paige", "Brooks", "Wood", "Frank"], "venue": "arXiv preprint arXiv:1602.06701,", "citeRegEx": "Paige et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paige et al\\.", "year": 2016}, {"title": "Operator variational inference", "author": ["R. Ranganath", "J. Altosaar", "D. Tran", "D.M. Blei"], "venue": null, "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Black box variational inference", "author": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Hierarchical variational models", "author": ["Ranganath", "Rajesh", "Tran", "Dustin", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.02386,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Examples of adaptive mcmc", "author": ["Roberts", "Gareth O", "Rosenthal", "Jeffrey S"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Roberts et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Roberts et al\\.", "year": 2009}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Salimans and Tim,? \\Q2015\\E", "shortCiteRegEx": "Salimans and Tim", "year": 2015}, {"title": "Variational gaussian process", "author": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "arXiv preprint arXiv:1511.06499,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["Wang", "Dilin", "Liu", "Qiang"], "venue": "Submitted to ICLR", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee W"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "We use the term wild variational inference to refer to variants of variational methods working with general inference networks f(\u03b7, \u03be) without tractability constraints on its output density q\u03b7(z); this should be distinguished with the black-box variational inference (Ranganath et al., 2014) which refers to methods that work for generic target distributions p(z) without significant model-by-model consideration (but still require to calculate the proposal density q\u03b7(z)).", "startOffset": 267, "endOffset": 291}, {"referenceID": 13, "context": "In this work, we discuss two methods for wild variational inference, both based on recent works that combine kernel techniques with Stein\u2019s method (e.g., Liu & Wang, 2016; Liu et al., 2016).", "startOffset": 147, "endOffset": 189}, {"referenceID": 8, "context": "As an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as the inference network, which can be treated as a special deep residential network (He et al., 2016), in which important gradient information \u2207z log p(z) is fed into each layer to allow efficient approximation for the target distribution p(z).", "startOffset": 179, "endOffset": 196}, {"referenceID": 19, "context": "One well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating q\u03b7(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 13, "context": ", 2016) that also avoids calculating q\u03b7(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.", "startOffset": 160, "endOffset": 248}, {"referenceID": 17, "context": ", 2016) that also avoids calculating q\u03b7(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.", "startOffset": 160, "endOffset": 248}, {"referenceID": 4, "context": ", 2016) that also avoids calculating q\u03b7(z) and hence works for general inference networks; their method is based on a similar idea related to Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham & Mackey, 2015), for which we provide a more detailed discussion in Section 3.", "startOffset": 160, "endOffset": 248}, {"referenceID": 13, "context": "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max \u03c6\u2208Hd { Ez\u223cq[Tp\u03c6(z)], s.", "startOffset": 93, "endOffset": 158}, {"referenceID": 4, "context": "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max \u03c6\u2208Hd { Ez\u223cq[Tp\u03c6(z)], s.", "startOffset": 93, "endOffset": 158}, {"referenceID": 17, "context": "This choice of F makes it possible to get a closed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2017): D(q || p) = max \u03c6\u2208Hd { Ez\u223cq[Tp\u03c6(z)], s.", "startOffset": 93, "endOffset": 158}, {"referenceID": 13, "context": "It can be shown that D(q || p) = 0 if and only if q = p when k(z, z\u2032) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016).", "startOffset": 118, "endOffset": 163}, {"referenceID": 4, "context": "It can be shown that D(q || p) = 0 if and only if q = p when k(z, z\u2032) is strictly positive definite in a proper sense (Liu et al., 2016; Chwialkowski et al., 2016).", "startOffset": 118, "endOffset": 163}], "year": 2017, "abstractText": "Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD, yielding significant improvement over the hand-designed step size schemes.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_338"}