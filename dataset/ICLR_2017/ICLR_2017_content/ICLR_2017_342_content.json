{"name": "ICLR_2017_342.pdf", "metadata": {"source": "CRF", "title": "DEEP GENERALIZED CANONICAL CORRELATION ANALYSIS", "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "emails": ["@jhu.edu,", "@cogsci.jhu.edu,", "@cs.jhu.edu"], "sections": [{"heading": null, "text": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques."}, {"heading": "1 INTRODUCTION", "text": "Multiview representation learning refers to settings where one has access to many \u201cviews\u201d of data, at train time. Views often correspond to different modalities or independent information about examples: a scene represented as a series of audio and image frames, a social media user characterized by the messages they post and who they friend, or a speech utterance and the configuration of the speaker\u2019s tongue. Multiview techniques learn a representation of data that captures the sources of variation common to all views.\nMultiview representation techniques are attractive for intuitive reasons. A representation that is able to explain many views of the data is more likely to capture meaningful variation than a representation that is a good fit for only one of the views. They are also attractive for the theoretical reasons. For example, Anandkumar et al. (2014) show that certain classes of latent variable models, such as Hidden Markov Models, Gaussian Mixture Models, and Latent Dirichlet Allocation models, can be optimally learned with multiview spectral techniques. Representations learned from many views will generalize better than one, since the learned representations are forced to accurately capture variation in all views at the same time (Sridharan & Kakade, 2008) \u2013 each view acts as a regularizer, constraining the possible representations that can be learned. These methods are often based on canonical correlation analysis (CCA), a classical statisical technique proposed by Hotelling (1936).\nIn spite of encouraging theoretical guarantees, multiview learning techniques cannot freely model nonlinear relationships between arbitrarily many views. Either they are able to model variation across many views, but can only learn linear mappings to the shared space (Horst, 1961), or they simply cannot be applied to data with more than two views using existing techniques based on kernel CCA (Hardoon et al., 2004) and deep CCA (Andrew et al., 2013).\nHere we present Deep Generalized Canonical Correlation Analysis (DGCCA). Unlike previous correlation-based multiview techniques, DGCCA learns a shared representation from data with arbitrarily many views and simultaneously learns nonlinear mappings from each view to this shared space. The only (mild) constraint is that these nonlinear mappings from views to shared space must be differentiable. Our main methodological contribution is the derivation of the gradient update for the Generalized Canonical Correlation Analysis (GCCA) objective (Horst, 1961). As a practical contribution, we have also released an implementation of DGCCA1.\nWe also evaluate DGCCA-learned representations on two distinct datasets and three downstream tasks: phonetic transcription from aligned speech and articulatory data, and Twitter hashtag and friend recommendation from six text and network feature views. We find that downstream performance of DGCCA representations is ultimately task-dependent. However, we find clear gains in performance from DGCCA for tasks previously shown to benefit from representation learning on more than two views, with up to 4% improvement in heldout accuracy for phonetic transcription.\nThe paper is organized as follows. We review prior work in Section 2. In Section 3 we describe DGCCA. Empirical results on a synthetic dataset, and three downstream tasks are presented in Section 4. In Section 5, we describe the differences between DGCCA and other non-CCA-based multiview learning work and conclude with future directions in Section 6."}, {"heading": "2 PRIOR WORK", "text": "Some of most successful techniques for multiview representation learning are based on canonical correlation analysis (Wang et al., 2015a;b) and its extension to the nonlinear and many view settings, which we describe in this section. For other related multiview learning techniques, see Section 5."}, {"heading": "2.1 CANONICAL CORRELATION ANALYSIS (CCA)", "text": "Canonical correlation analysis (CCA) (Hotelling, 1936) is a statistical method that finds maximally correlated linear projections of two random vectors and is a fundamental multiview learning technique. Given two input views, X1 \u2208 Rd1 and X2 \u2208 Rd2 , with covariance matrices, \u03a311 and \u03a322, respectively, and cross-covariance matrix, \u03a312, CCA finds directions that maximize the correlation between them:\n(u\u22171, u \u2217 2) = argmax u1\u2208Rd1 ,u2\u2208Rd2 corr(u>1 X1, u > 2 X2) = argmax u1\u2208Rd1 ,u2\u2208Rd2 u>1 \u03a312u2\u221a u>1 \u03a311u1u > 2 \u03a322u2\nSince this formulation is invariant to affine transformations of u1 and u2, we can write it as the following constrained optimization formulation:\n(u\u22171, u \u2217 2) = argmax\nu>1 \u03a311u1=u > 2 \u03a322u2=1\nu>1 \u03a312u2 (1)\nThis technique has two limitations that have led to significant extensions: First, it is limited to learning representations that are linear transformations of the data in each view, and second, it can only leverage two input views."}, {"heading": "2.2 DEEP CANONICAL CORRELATION ANALYSIS (DCCA)", "text": "Deep CCA (DCCA) (Andrew et al., 2013) is an extension of CCA that addresses the first limitation by finding maximally linearly correlated non-linear transformations of two vectors. It does this by passing each of the input views through stacked non-linear representations and performing CCA on the outputs.\nLet us use f1(X1) and f2(X2) to represent the network outputs. The weights, W1 and W2, of these networks are trained through standard backpropagation to maximize the CCA objective:\n(u\u22171, u \u2217 2,W \u2217 1 ,W \u2217 2 ) = argmax\nu1,u2\ncorr(u>1 f1(X1), u > 2 f2(X2))\nDCCA is still limited to only 2 input views. 1See https://bitbucket.org/adrianbenton/dgcca-py3 for implementation of DGCCA along with data from the synthetic experiments."}, {"heading": "2.3 GENERALIZED CANONICAL CORRELATION ANALYSIS (GCCA)", "text": "Another extension of CCA, which addresses the limitation on the number of views, is Generalized CCA (GCCA) (Horst, 1961). It corresponds to solving the optimization problem in Equation (2), of finding a shared representation G of J different views, where N is the number of data points, dj is the dimensionality of the jth view, r is the dimensionality of the learned representation, and Xj \u2208 Rdj\u00d7N is the data matrix for the jth view.2\nminimize Uj\u2208Rdj\u00d7r,G\u2208Rr\u00d7N J\u2211 j=1 \u2016G\u2212 U>j Xj\u20162F\nsubject to GG> = Ir\n(2)\nSolving GCCA requires finding an eigendecomposition of an N \u00d7N matrix, which scales quadratically with sample size and leads to memory constraints. Unlike CCA and DCCA, which only learn projections or transformations on each of the views, GCCA also learns a view-independent representation G that best reconstructs all of the view-specific representations simultaneously. The key limitation of GCCA is that it can only learn linear transformations of each view."}, {"heading": "3 DEEP GENERALIZED CANONICAL CORRELATION ANALYSIS (DGCCA)", "text": "In this section, we present deep GCCA (DGCCA): a multiview representation learning technique that benefits from the expressive power of deep neural networks and can also leverage statistical strength from more than two views in data, unlike Deep CCA which is limited to only two views. More fundamentally, deep CCA and deep GCCA have very different objectives and optimization problems, and it is not immediately clear how to extend deep CCA to more than two views.\nDGCCA learns a nonlinear map for each view in order to maximize the correlation between the learnt representations across views. In training, DGCCA passes the input vectors in each view through multiple layers of nonlinear transformations and backpropagates the gradient of the GCCA objective with respect to network parameters to tune each view\u2019s network, as illustrated in Figure 1. The objective is to train networks that reduce the GCCA reconstruction error among their outputs. At test time, new data can be projected by feeding them through the learned network for each view.\nWe now formally define the DGCCA problem. We consider J views in our data, and let Xj \u2208 Rdj\u00d7N denote the jth input matrix.3 The network for the jth view consists of Kj layers. Assume, for simplicity, that each layer in the jth view network has cj units with a final (output) layer of size oj . The output of the kth layer for the jth view is h j k = s(W j kh j k\u22121), where s : R \u2192 R is a nonlinear activation function and W jk \u2208 Rck\u00d7ck\u22121 is the weight matrix for the kth layer of the jth view network. We denote the output of the final layer as fj(Xj).\n2Horst\u2019s original presentation of GCCA is in a very different form from the one used here but has been shown to be equivalent (Kettenring, 1971).\n3Our notation for this section closely follows that of Andrew et al. (2013)\nDGCCA can be expressed as the following optimization problem: find weight matrices W j = {W j1 , . . . ,W j Kj } defining the functions fj , and linear transformations Uj (of the output of the jth network), for j = 1, . . . , J , that minimize\nUj\u2208Roj\u00d7r,G\u2208Rr\u00d7N J\u2211 j=1 \u2016G\u2212 U>j fj(Xj)\u20162F\nsubject to GG> = Ir\n, (3)\nwhere G \u2208 Rr\u00d7N is the shared representation we are interested in learning.\nOptimization: We solve the DGCCA optimization problem using stochastic gradient descent (SGD) with mini-batches. In particular, we estimate the gradient of the DGCCA objective in Problem 3 on a mini-batch of samples that is mapped through the network and use back-propagation to update the weight matrices, W j\u2019s. However, note that the DGCCA optimization problem is a constrained optimization problem. It is not immediately clear how to perform projected gradient descent with back-propagation. Instead, we characterize the objective function of the GCCA problem at an optimum, and compute its gradient with respect to the inputs to GCCA, i.e. with respect to the network outputs. These gradients are then back-propagated through the network to update W j\u2019s.\nAlthough the relationship between DGCCA and GCCA is analogous to the relationship between DCCA and CCA, derivation of the GCCA objective gradient with respect to the network output layers is non-trivial. The main difficulty stems from the fact that there is no natural extension of the correlation objective to more than two random variables. Instead, we consider correlations between every pair of views, stack them in a J \u00d7 J matrix and maximize a certain matrix norm for that matrix. For GCCA, this suggests an optimization problem that maximizes the sum of correlations between a shared representation and each view. Since the objective as well as the constraints of the generalized CCA problem are very different from that of the CCA problem, it is not immediately obvious how to extend Deep CCA to Deep GCCA.\nNext, we show a sketch of the gradient derivation, the full derivation is given in appendix A. It is straightforward to show that the solution to the GCCA problem is given by solving an eigenvalue problem. In particular, defineCjj = f(Xj)f(Xj)> \u2208 Roj\u00d7oj , to be the scaled empirical covariance matrix of the jth network output, and Pj = f(Xj)>C\u22121jj f(Xj) \u2208 RN\u00d7N be the corresponding projection matrix that whitens the data; note that Pj is symmetric and idempotent. We define M =\u2211J\nj=1 Pj . Since each Pj is positive semi-definite, so is M . Then, it is easy to check that the rows of G are the top r (orthonormal) eigenvectors of M , and Uj = C\u22121jj f(Xj)G\n>. Thus, at the minimum of the objective, we can rewrite the reconstruction error as follows:\nJ\u2211 j=1 \u2016G\u2212 U>j fj(Xj)\u20162F = J\u2211 j=1 \u2016G\u2212Gfj(Xj)>C\u22121jj fj(Xj)\u2016 2 F = rJ \u2212 Tr(GMG>)\nMinimizing the GCCA objective (w.r.t. the weights of the neural networks) means maximizing Tr(GMG>), which is the sum of eigenvalues L = \u2211r i=1 \u03bbi(M). Taking the derivative of L with respect to each output layer fj(Xj) we have: \u2202L\n\u2202fj(Xj) = 2UjG\u2212 2UjU>j fj(Xj)\nThus, the gradient is the difference between the r-dimensional auxiliary representation G embedded into the subspace spanned by the columns of Uj (the first term) and the projection of the actual data in fj(Xj) onto the said subspace (the second term). Intuitively, if the auxiliary representation G is far away from the view-specific representation U>j fj(Xj), then the network weights should receive a large update. Computing the gradient descent update has time complexity O(JNrd), where d = max(d1, d2, . . . , dJ) is the largest dimensionality of the input views."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 SYNTHETIC MULTIVIEW MIXTURE MODEL", "text": "In this section, we apply DGCCA to a small synthetic data set to show how it preserves the generative structure of data sampled from a multiview mixture model. The data we use for this experiment are\nplotted in Figure 2. Points that share the same color across different views are sampled from the same mixture component.\nImportantly, in each view, there is no linear transformation of the data that separates the two mixture components, in the sense that the generative structure of the data could not be exploited by a linear model. This point is reinforced by Figure 3(a), which shows the two-dimensional representation G learned by applying (linear) GCCA to the data in Figure 2. The learned representation completely loses the structure of the data.\nWe can contrast the failure of GCCA to preserve structure with the result of applying DGCCA; in this case, the input neural networks had three hidden layers with ten units each with weights randomly initialized. We plot the representation G learned by DGCCA in Figure 3 (b). In this representation, the mixture components are easily separated by a linear classifier; in fact, the structure is largely preserved even after projection onto the first coordinate of G.\nIt is also illustrative to consider the view-specific representations learned by DGCCA, that is, to consider the outputs of the neural networks that were trained to maximize the GCCA objective. We plot the representations in Figure 4. For each view, we have learned a nonlinear mapping that does remarkably well at making the mixture components linearly separable. Recall that absolutely no direct supervision was given about which mixture component each point was generated from. The only training signals available to the networks were the reconstruction errors between the network outputs and the learned representation G."}, {"heading": "4.2 PHONEME CLASSIFICATION", "text": "In this section, we discuss experiments on the University of Wisconsin X-ray Microbeam Database (XRMB) (Westbury, 1994). XRMB contains acoustic and articulatory recordings as well as phonemic labels. We present phoneme classification results on the acoustic vectors projected using DCCA,\nGCCA, and DGCCA. We set acoustic and articulatory data as the two views and phoneme labels as the third view for GCCA and DGCCA. For classification, we run K-nearest neighbor classification (Cover & Hart, 1967) on the projected result."}, {"heading": "4.2.1 DATA", "text": "We use the same train/tune/test split of the data as Arora & Livescu (2014). To limit experiment runtime, we use a subset of speakers for our experiments. We run a set of cross-speaker experiments using the male speaker JW11 for training and two splits of JW24 for tuning and testing. We also perform parameter tuning for the third view with 5-fold cross validation using a single speaker, JW11. For both experiments, we use acoustic and articulatory measurements as the two views in DCCA. Following the pre-processing in Andrew et al. (2013), we get 273 and 112 dimensional feature vectors for the first and second view respectively. Each speaker has \u223c50,000 frames. For the third view in GCCA and DGCCA, we use 39-dimensional one-hot vectors corresponding to the labels for each frame, following Arora & Livescu (2014)."}, {"heading": "4.2.2 PARAMETERS", "text": "We use a fixed network size and regularization for the first two views, each containing three hidden layers with sigmoid activation functions. Hidden layers for the acoustic view were all width 1024, and layers in the articulatory view all had width 512 units. L2 penalty constants of 0.0001 and 0.01 were used to train the acoustic and articulatory view networks, respectively. The output layer dimension of each network is set to 30 for DCCA and DGCCA. For the 5-fold speaker-dependent experiments, we performed a grid search for the network sizes in {128, 256, 512, 1024} and covariance matrix regularization in {10\u22122, 10\u22124, 10\u22126, 10\u22128} for the third view in each fold. We fix the hyperparameters for these experiments optimizing the networks with minibatch stochastic gradient descent with a step size of 0.005, batch size of 2000, and no learning decay or momentum. The third view neural network had an L2 penalty of 0.0005."}, {"heading": "4.2.3 RESULTS", "text": "As we show in Table 1, DGCCA improves upon both the linear multiview GCCA and the non-linear 2-view DCCA for both the cross-speaker and speaker-dependent cross-validated tasks.\nIn addition to accuracy, we examine the reconstruction error, i.e. the objective in Equation 3, obtained from the objective in GCCA and DGCCA.4 This sharp improvement in reconstruction error shows that a non-linear algorithm can better model the data.\nIn this experimental setup, DCCA under-performs the baseline of simply running KNN on the original acoustic view. Prior work considered the output of DCCA stacked on to the central frame of the original acoustic view (39 dimensions). This poor performance, in the absence of original features, indicates that it was not able to find a more informative projection than original acoustic features based on correlation with the articulatory view within the first 30 dimensions.\nTo highlight the improvements of DGCCA over GCCA, Figure 5 presents a subset of the the confusion matrices on speaker-dependent test data. In particular, we observe large improvements in the classification of D, F , K, SH , V and Y . GCCA outperforms DGCCA for UH and DH . These matrices also highlight the common misclassifications that DGCCA improves upon. For instance,\n4For 2-view experiments, correlation is a common metric to compare performance. Since that metric is unavailable in a multiview setting, reconstruction error is the analogue.\nDGCCA rectifies the frequent misclassification of V as P , R and B by GCCA. In addition, commonly incorrect classification of phonemes such as S and T is corrected by DGCCA, which enables better performance on other voiceless consonants such as like F , K and SH . Vowels are classified with almost equal accuracy by both the methods."}, {"heading": "4.3 TWITTER USER HASHTAG & FRIEND RECOMMENDATION", "text": "Linear multiview techniques are effective at recommending hashtag and friends for Twitter users (Benton et al., 2016). In this experiment, six views of a Twitter user were constructed by applying principal component analysis (PCA) to the bag-of-words representations of (1) tweets posted by the ego user, (2) other mentioned users, (3) their friends, and (4) their followers, as well as one-hot encodings of the local (5) friend and (6) follower networks. We learn and evaluate DGCCA models on identical training, development, and test sets as Benton et al. (2016), and evaluate the DGCCA representations on macro precision at 1000 (P@1000) and recall at 1000 (R@1000) for the hashtag and friend recommendation tasks described there.\nWe trained 40 different DGCCA model architectures, each with identical architectures across views, where the width of the hidden and output layers, c1 and c2, for each view are drawn uniformly from [10, 1000], and the auxiliary representation width r is drawn uniformly from [10, c2]5. All networks used ReLUs as activation functions, and were optimized with Adam (Kingma & Ba, 2014) for 200 epochs6. Networks were trained on 90% of 102,328 Twitter users, with 10% of users used as a tuning set to estimate heldout reconstruction error for model selection. We report development and test results for the best performing model on the downstream task development set. Learning rate was set to 10\u22124 with an L1 and L2 regularization constants of 0.01 and 0.001 for all weights 7.\nTable 2 displays the performance of DGCCA compared to PCA[text+net] (PCA applied to concatenation of view feature vectors), linear GCCA applied to the four text views, [text], and all\n5We chose to restrict ourselves to a single hidden layer with non-linear activation and identical architectures for each view, so as to avoid a fishing expedition. If DGCCA is appropriate for learning Twitter user representations, then a good architecture should require little exploration.\n6From preliminary experiments, we found that Adam pushed down reconstruction error more quickly than SGD with momentum, and that ReLUs were easier to optimize than sigmoid activations.\n7This setting of regularization constants led to low reconstruction error in preliminary experiments.\nviews, [text+net], along with a weighted GCCA variant (WGCCA). We learned PCA, GCCA, and WGCCA representations of width r \u2208 {10, 20, 50, 100, 200, 300, 400, 500, 750, 1000}, and report the best performing representations on the development set.\nThere are several points to note: First is that DGCCA outperforms linear methods at hashtag recommendation by a wide margin in terms of recall. This is exciting because this task was shown to benefit from incorporating more than just two views from Twitter users. These results suggest that a nonlinear transformation of the input views can yield additional gains in performance. In addition, WGCCA models sweep over every possible weighting of views with weights in {0, 0.25, 1.0}. WGCCA has a distinct advantage in that the model is allowed to discriminatively weight views to maximize downstream performance. The fact that DGCCA is able to outperform WGCCA at hashtag recommendation is encouraging, since WGCCA has much more freedom to discard uninformative views, whereas the DGCCA objective forces networks to minimize reconstruction error equally across all views. As noted in Benton et al. (2016), only the friend network view was useful for learning representations for friend recommendation (corroborated by performance of PCA applied to friend network view), so it is unsurprising that DGCCA when applied to all views cannot compete with WGCCA representations learned on the single useful friend network view8."}, {"heading": "5 OTHER MULTIVIEW LEARNING WORK", "text": "There has been strong work outside of CCA-related methods to combine nonlinear representation and learning from multiple views. Kumar et al. (2011) elegantly outlines two main approaches these methods take to learn a joint representation from many views: either by 1) explicitly maximizing pairwise similarity/correlation between views or by 2) alternately optimizing a shared, \u201cconsensus\u201d representation and view-specific transformations to maximize similarity. Models such as the siamese network proposed by Masci et al. (2014), fall in the former camp, minimizing the squared error between embeddings learned from each view, leading to a quadratic increase in the terms of the loss function size as the number of views increase. Rajendran et al. (2015) extend Correlational Neural Networks (Chandar et al., 2015) to many views and avoid this quadratic explosion in the loss function by only computing correlation between each view embedding and the embedding of a \u201cpivot\u201d view. Although this model may be appropriate for tasks such as multilingual image captioning, there are many datasets where there is no clear method of choosing a pivot view. The DGCCA objective does not suffer from this quadratic increase w.r.t. the number of views, nor does it require a privileged pivot view, since the shared representation is learned from the per-view representations.\nApproaches that estimate a \u201cconsensus\u201d representation, such as the multiview spectral clustering approach in Kumar et al. (2011), typically do so by an alternating optimization scheme which depends on a strong initialization to avoid bad local optima. The GCCA objective our work builds on is particularly attractive, since it admits a globally optimal solution for both the view-specific projections U1 . . . UJ , and the shared representation G by singular value decomposition of a single matrix: a sum of the per-view projection matrices. Local optima arise in the DGCCA objective only because we are also learning nonlinear transformations of the input views. Nonlinear multiview methods often avoid learning these nonlinear transformations by assuming that a kernel or graph Laplacian (e.g. in multiview clustering) is given (Kumar et al., 2011; Xiaowen, 2014; Sharma et al., 2012)."}, {"heading": "6 CONCLUSION", "text": "We present DGCCA, a method for non-linear multiview representation learning from an arbitrary number of views. We show that DGCCA clearly outperforms prior work when using labels as a third view (Andrew et al., 2013; Arora & Livescu, 2014; Wang et al., 2015c), and can successfully exploit multiple views to learn user representations useful for downstream tasks such as hashtag recommendation for Twitter users. To date, CCA-style multiview learning techniques were either restricted to learning representations from no more than two views, or strictly linear transformations of the input views. This work overcomes these limitations.\n8The performance of WGCCA suffers compared to PCA because whitening the friend network data ignores the fact that the spectrum of the decays quickly with a long tail \u2013 the first few principal components made up a large portion of the variance in the data, but it was also important to compare users based on other components."}, {"heading": "APPENDIX A DERIVING THE GCCA OBJECTIVE GRADIENT", "text": "In order to train the neural networks in DGCCA, we need to compute the gradient of the GCCA objective with respect to any one of its input views. This gradient can then be backpropagated through the input networks to derive updates for the network weights.\nLet N be the number of data points and J the number of views. Let Yj \u2208 Rc j K\u00d7N be the data matrix representing the output of the jth neural network, i.e. Yj = fj(Xj), where c j K is the number of neurons in the output layer of the jth network. Then, GCCA can be written as the following optimization problem, where r is the dimensionality of the learned auxiliary representation:\nminimize Uj\u2208Rc j K \u00d7r,G\u2208Rr\u00d7N J\u2211 j=1 \u2016G\u2212 U>j Yj\u20162F\nsubject to GG> = Ir It can be shown that the solution is found by solving a certain eigenvalue problem. In particular, define Cjj = YjY >j \u2208 Rc j K\u00d7c j K , Pj = Y >j C \u22121 jj Yj (note that Pj is symmetric and idempotent), and\nM = \u2211J\nj=1 Pj (since each Pj is psd, so is M ). Then the rows of G are the top r (orthonormal) eigenvectors of M , and Uj = C\u22121jj YjG\n>. Thus, at the minima of the objective, we can rewrite the reconstruction error as follows:\nJ\u2211 j=1 \u2016G\u2212 U>j Yj\u20162F = J\u2211 j=1 \u2016G\u2212GY >j C\u22121jj Yj\u2016 2 F\n= J\u2211 j=1 \u2016G(IN \u2212 Pj)\u20162F\n= J\u2211 j=1 Tr[G(IN \u2212 Pj)G>]\n= J\u2211 j=1 Tr(Ir)\u2212 Tr(GMG>)\n= Jr \u2212 Tr(GMG>) Note that we can write the rank-1 decomposition of M as \u2211N\nk=1 \u03bbkgkg > k . Furthermore, since the\nkth row of G is gk, and since the matrix product Ggk = e\u0302k,\nGMG> = N\u2211 k=1 \u03bbkGgk(Ggk) > = r\u2211 k=1 \u03bbke\u0302ke\u0302 > k\nBut this is just an N \u00d7 N diagonal matrix containing the top r eigenvalues of M , so we can write the GCCA objective as\nJr \u2212 r\u2211\ni=1\n\u03bbi(M)\nThus, minimizing the GCCA objective (w.r.t. the weights of the neural nets) means maximizing the sum of eigenvalues \u2211r i=1 \u03bbi(M), which we will henceforth denote by L.\nNow, we will derive an expression for \u2202L\u2202Yj for any view Yj . First, by the chain rule, and using the fact that \u2202L\u2202M = G >G (Petersen & Pedersen, 2012),\n\u2202L\n\u2202(Yj)ab = N\u2211 c,d=1 \u2202L \u2202Mcd \u2202Mcd \u2202(Yj)ab\n= N\u2211 c,d=1 (G>G)cd \u2202Mcd \u2202(Yj)ab\nSince M = \u2211J\nj\u2032=1 Pj\u2032 , and since the only projection matrix that depends on Yj is Pj , \u2202M \u2202Yj = \u2202Pj \u2202Yj .\nSince Pj = Y >j C \u22121 jj Yj ,\n(Pj)cd = cjK\u2211 k,`=1 (Yj)kc(C \u22121 jj )k`(Yj)`d\nThus, by the product rule,\n\u2202(Pj)cd \u2202(Yj)ab = \u03b4cb cjK\u2211 `=1 (Yj)`d(C \u22121 jj )a` +\n\u03b4db cjK\u2211 k=1 (Yj)kc(C \u22121 jj )ka +\ncjK\u2211 k,`=1 (Yj)kc(Yj)`d \u2202(C\u22121jj )k` \u2202(Yj)ab\n= \u03b4cb(C \u22121 jj Yj)ad + \u03b4db(C \u22121 jj Yj)ac\n+ cjK\u2211 k,`=1 (Yj)kc(Yj)`d \u2202(C\u22121jj )k` \u2202(Yj)ab\nThe derivative in the last term can also be computed using the chain rule:\n\u2202(C\u22121jj )k`\n\u2202(Yj)ab = N\u2211 m,n=1 \u2202(C\u22121jj )k` \u2202(Cjj)mn \u2202(Cjj)mn \u2202(Yj)ab\n= \u2212 N\u2211\nm,n=1\n{ (C\u22121jj )km(C \u22121 jj )n`\n[\u03b4am(Yj)nb + \u03b4an(Yj)mb] }\n= \u2212 N\u2211\nn=1\n(C\u22121jj )ka(C \u22121 jj )n`(Yj)nb\n\u2212 N\u2211\nm=1\n(C\u22121jj )km(C \u22121 jj )a`(Yj)mb\n= \u2212(C\u22121jj )ka(C \u22121 jj Yj)`b\n\u2212 (C\u22121jj )a`(C \u22121 jj Yj)kb\nSubstituting this into the expression for \u2202(Pj)cd\u2202(Yj)ab and simplifying matrix products, we find that\n\u2202(Pj)cd \u2202(Yj)ab = \u03b4cb(C \u22121 jj Yj)ad + \u03b4db(C \u22121 jj Yj)ac\n\u2212 (C\u22121jj Yj)ac(Y > j C \u22121 jj Yj)bd \u2212 (C\u22121jj Yj)ad(Y > j C \u22121 jj Yj)bc\n= (IN \u2212 Pj)cb(C\u22121jj Yj)ad + (IN \u2212 Pj)db(C\u22121jj Yj)ac\nFinally, substituting this into our expression for \u2202L\u2202(Yj)ab , we find that\n\u2202L\n\u2202(Yj)ab = N\u2211 c,d=1 (G>G)cd(IN \u2212 Pj)cb(C\u22121jj Yj)ad\n+ N\u2211 c,d=1 (G>G)cd(IN \u2212 Pj)db(C\u22121jj Yj)ac\n= 2[C\u22121jj YjG >G(IN \u2212 Pj)]ab\nTherefore, \u2202L\n\u2202Yj = 2C\u22121jj YjG >G(IN \u2212 Pj)\nBut recall that Uj = C\u22121jj YjG >. Using this, the gradient simplifies as follows:\n\u2202L \u2202Yj = 2UjG\u2212 2UjU>j Yj\nThus, the gradient is the difference between the r-dimensional auxiliary representation G embedded into the subspace spanned by the columns of Uj (the first term) and the projection of the network outputs in Yj = fj(Xj) onto said subspace (the second term). Intuitively, if the auxiliary representation G is far away from the view-specific representation U>j fj(Xj), then the network weights should receive a large update."}, {"heading": "APPENDIX B DGCCA OPTIMIZATION PSEUDOCODE", "text": "Algorithm 1 contains the pseudocode for the DGCCA optimization algorithm. In practice we use stocastic optimization with minibatches, following Wang et al. (2015c).\nAlgorithm 1 Deep Generalized CCA Input: multiview data: X1, X2, . . . , XJ ,\nnumber of iterations T , learning rate \u03b7 Output: O1, O2, . . . , OJ Initialize weights W1,W2, . . . ,WJ for iteration t = 1, 2, . . . , T do\nfor each view j = 1, 2, . . . , J do Oj \u2190 forward pass of Xj with weights Wj mean-center Oj end for U1, . . . , UJ , G\u2190 gcca(O1, . . . , OJ ) for each view j = 1, 2, . . . , J do \u2202F/\u2202Oj \u2190 UjU>j Oj \u2212 UjG \u2207Wj \u2190 backprop(\u2202F/\u2202Oj ,Wj) Wj \u2190Wj \u2212 \u03b7\u2207Wj\nend for end for for each view j = 1, 2, . . . , J do Oj \u2190 forward pass of Xj with weights Wj mean-center Oj end for U1, . . . , UJ , G\u2190 gcca(O1, . . . , OJ ) for each view j = 1...J do Oj \u2190 U>j Oj end for"}, {"heading": "APPENDIX C RECONSTRUCTION ERROR AND DOWNSTREAM", "text": "PERFORMANCE\nCCA methods are typically evaluated intrinsically by the amount of correlation captured, or reconstruction error. These measures are dependent on the width of the shared embeddings and viewspecific output layers, and do not necessarily predict downstream performance. Although reconstruction error cannot solely be relied on for model selection for a downstream task, we found that it was a useful as a signal to weed out very poor models. Figure 6 shows the reconstruction error against hashtag prediction Recall at 1000 for an initial grid search of DGCCA hyperparameters. Models with tuning reconstruction error greater than 103 can safely be ignored, while there is some variability in the performance of models with achieving lower error.\nSince a DGCCA model with high reconstruction error suggests that the views do not agree with each other at all, it makes sense that the shared embedding will likely be noisy, whereas a relatively lowly reconstruction error suggests that the transformed views have converged to a stable solution."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Multi-view learning with supervision for transformed bottleneck features", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Arora and Livescu.,? \\Q2014\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2014}, {"title": "Learning multiview embeddings of twitter users", "author": ["Adrian Benton", "Raman Arora", "Mark Dredze"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Benton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benton et al\\.", "year": 2016}, {"title": "Nearest neighbor pattern classification", "author": ["Thomas M Cover", "Peter E Hart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Generalized canonical correlations and their applications to experimental data", "author": ["Paul Horst"], "venue": "Journal of Clinical Psychology,", "citeRegEx": "Horst.,? \\Q1961\\E", "shortCiteRegEx": "Horst.", "year": 1961}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pp", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Canonical analysis of several sets of variables", "author": ["Jon R. Kettenring"], "venue": "Biometrika, 58(3):433\u2013451,", "citeRegEx": "Kettenring.,? \\Q1971\\E", "shortCiteRegEx": "Kettenring.", "year": 1971}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Co-regularized multi-view spectral clustering", "author": ["Abhishek Kumar", "Piyush Rai", "Hal Daume"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Multimodal similarity-preserving hashing", "author": ["Jonathan Masci", "Michael M Bronstein", "Alexander M Bronstein", "J\u00fcrgen Schmidhuber"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Masci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2014}, {"title": "URL http://www2", "author": ["Kaare Petersen", "Michael Pedersen. The matrix cookbook", "Nov"], "venue": "imm.dtu.dk/pubdb/p.php?3274. Version 20121115.", "citeRegEx": "Petersen et al\\.,? 2012", "shortCiteRegEx": "Petersen et al\\.", "year": 2012}, {"title": "Bridge correlational neural networks for multilingual multimodal representation learning", "author": ["Janarthanan Rajendran", "Mitesh M Khapra", "Sarath Chandar", "Balaraman Ravindran"], "venue": "arXiv preprint arXiv:1510.03519,", "citeRegEx": "Rajendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajendran et al\\.", "year": 2015}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["Abhishek Sharma", "Abhishek Kumar", "Hal Daume", "David W Jacobs"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sharma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2012}, {"title": "An information theoretic framework for multi-view learning", "author": ["Karthik Sridharan", "Sham M Kakade"], "venue": "In Proceedings of COLT,", "citeRegEx": "Sridharan and Kakade.,? \\Q2008\\E", "shortCiteRegEx": "Sridharan and Kakade.", "year": 2008}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the 32nd Int. Conf. Machine Learning (ICML 2015),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Nathan Srebro"], "venue": "In Proceedings of the 53rd Annual Allerton Conference on Communication, Control and Computing (ALLERTON),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "X-ray microbeam speech production database users handbook", "author": ["John R. Westbury"], "venue": "In Waisman Center on Mental Retardation & Human Development University of Wisconsin Madison, WI 537052280,", "citeRegEx": "Westbury.,? \\Q1994\\E", "shortCiteRegEx": "Westbury.", "year": 1994}, {"title": "Multi-View Signal Processing and Learning on Graphs", "author": ["Dong Xiaowen"], "venue": "PhD thesis, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne,", "citeRegEx": "Xiaowen.,? \\Q2014\\E", "shortCiteRegEx": "Xiaowen.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views.", "startOffset": 72, "endOffset": 93}, {"referenceID": 6, "context": ", 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views.", "startOffset": 71, "endOffset": 84}, {"referenceID": 6, "context": "Either they are able to model variation across many views, but can only learn linear mappings to the shared space (Horst, 1961), or they simply cannot be applied to data with more than two views using existing techniques based on kernel CCA (Hardoon et al.", "startOffset": 114, "endOffset": 127}, {"referenceID": 5, "context": "Either they are able to model variation across many views, but can only learn linear mappings to the shared space (Horst, 1961), or they simply cannot be applied to data with more than two views using existing techniques based on kernel CCA (Hardoon et al., 2004) and deep CCA (Andrew et al.", "startOffset": 241, "endOffset": 263}, {"referenceID": 6, "context": "Our main methodological contribution is the derivation of the gradient update for the Generalized Canonical Correlation Analysis (GCCA) objective (Horst, 1961).", "startOffset": 146, "endOffset": 159}, {"referenceID": 7, "context": "Canonical correlation analysis (CCA) (Hotelling, 1936) is a statistical method that finds maximally correlated linear projections of two random vectors and is a fundamental multiview learning technique.", "startOffset": 37, "endOffset": 54}, {"referenceID": 1, "context": "Deep CCA (DCCA) (Andrew et al., 2013) is an extension of CCA that addresses the first limitation by finding maximally linearly correlated non-linear transformations of two vectors.", "startOffset": 16, "endOffset": 37}, {"referenceID": 6, "context": "Another extension of CCA, which addresses the limitation on the number of views, is Generalized CCA (GCCA) (Horst, 1961).", "startOffset": 107, "endOffset": 120}, {"referenceID": 8, "context": "(2)Horst\u2019s original presentation of GCCA is in a very different form from the one used here but has been shown to be equivalent (Kettenring, 1971).", "startOffset": 128, "endOffset": 146}, {"referenceID": 19, "context": "In this section, we discuss experiments on the University of Wisconsin X-ray Microbeam Database (XRMB) (Westbury, 1994).", "startOffset": 103, "endOffset": 119}, {"referenceID": 3, "context": "3 TWITTER USER HASHTAG & FRIEND RECOMMENDATION Linear multiview techniques are effective at recommending hashtag and friends for Twitter users (Benton et al., 2016).", "startOffset": 143, "endOffset": 164}, {"referenceID": 10, "context": "in multiview clustering) is given (Kumar et al., 2011; Xiaowen, 2014; Sharma et al., 2012).", "startOffset": 34, "endOffset": 90}, {"referenceID": 20, "context": "in multiview clustering) is given (Kumar et al., 2011; Xiaowen, 2014; Sharma et al., 2012).", "startOffset": 34, "endOffset": 90}, {"referenceID": 14, "context": "in multiview clustering) is given (Kumar et al., 2011; Xiaowen, 2014; Sharma et al., 2012).", "startOffset": 34, "endOffset": 90}, {"referenceID": 1, "context": "We show that DGCCA clearly outperforms prior work when using labels as a third view (Andrew et al., 2013; Arora & Livescu, 2014; Wang et al., 2015c), and can successfully exploit multiple views to learn user representations useful for downstream tasks such as hashtag recommendation for Twitter users.", "startOffset": 84, "endOffset": 148}], "year": 2017, "abstractText": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_342"}