{"name": "ICLR_2017_350.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Onkar Bhardwaj", "Guojing Cong"], "emails": ["obhardw@us.ibm.com", "gcong@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Stochastic gradient descent (SGD) and its parallel variants form the backbone of most popular deep learning applications. Consequently, there has been a significant interest in investigating their convergence properties. SGD has been shown to satisfy an asymptotic convergence rate of O(1/S) for convex objective functions [Nemirovski et al. (2009)] and an asymptotic convergence rate of O(1/ \u221a S) for general non-convex objective functions with mini-batch size 1 in [Ghadimi & Lan (2013)] or with arbitrary mini-batch sizes in [Dekel et al. (2012)].\nAlthough SGD converges asymptotically with the same rate irrespective of mini-batch size, it has been reported that for large mini-batch sizes, often it is slower to converge - for example, see Wilson & Martinez (2003) for the detailed graphical illustrations therein for the effect of increasing batchsize or see Bottou (2010) for the comments on the tradeoffs of mini-batching. In this work, we are interested in using theoretical analysis for justifying such practical observations or comments. In particular, we show the following:\n\u2022 We consider general non-convex objective functions and show that prior to reaching asymptotic regime, SGD convergence could get much slower (inferred from the difference in the convergence rate guarantees from Theorem 2) with using higher mini-batch size, assuming a constant learning rate. Here, to evaluate the convergence rate guarantee we use the measure of average gradient norm since we are considering general non-convex objectives. As a consequence of slower convergence, the number of training samples required to attain a certain convergence guarantee (in terms of average gradient norm) increases as the mini-batch size increases. We build the analysis based on the framework in Ghadimi & Lan (2013).\n\u2022 Further, we investigate Asynchronous Stochastic Gradient Descent (ASGD) which is one of the most popular asynchronous variants of SGD [Dean et al. (2012); Li et al. (2014a);\nChilimbi et al. (2014)]. Recently, Lian et al. (2015) extended the results of SGD convergence to ASGD and showed that it converges asymptotically with a convergence rate of O(1/ \u221a S). In our analysis we show that prior to the asymptotic regime, with using higher\nnumber of learners ASGD convergence could get much slower in terms of average gradient norm attained after cumulatively processing a fixed number of training samples (this slow-down is inferred from the difference in the convergence guarantees from Theorem 4). \u2022 This suggests that there is an inherent limit on harnessing parallelism with SGD either\nby increasing mini-batch size or increasing the number of learners (even when we do not take into account overheads such as communication cost). The difference in convergence behavior caused by increasing mini-batch size for SGD and by increasing the number of learners with ASGD was found to be similar (See Theorem 2, Theorem 4 and the discussion at the end of Section 4).\nFor rest of the paper, we use the following notation: Let F (x, zi) denote a non-convex function of a parameter vector x and a training sample zi selected from the training set {z1, z2, . . . , zn}. Our aim is to find a parameter vector that minimizes the objective function f(x) = EzF (x, z). Towards this, we use mini-batch SGD, where in kth iteration, we select a random mini-batch zk = {z1k, z2k, . . . , zMk } of size M and perform the following update:\nxk+1 = xk \u2212 \u03b3 \u00b7G(x, zk) = xk \u2212 \u03b3 \u00b7 M\u2211 i=1 G(x, zik) (1)\nIn the above equation, \u03b3 denotes the learning rate and we have G(x, zk) = \u2211M i=1G(x, z i k) where G(x, zik) denotes the gradient of the objective function f(x) with respect to a training sample z i k \u2208 zk. We define Df := f(x1) \u2212 f(x\u2217) where x1 is the initial parameter vector and x\u2217 is a local optima towards which SGD proceeds. We also denote by S the total number of training samples to be processed.\nAdditionally, we make the following standard assumptions, see e.g., [Lian et al. (2015), Ghadimi & Lan (2013)]:\nA.1 Unbiased estimator: We assume that the expectation of G(x, z) equals to the true value of the gradient, i.e., EzG(x, z) = \u2207f(x) \u2200x.\nA.2 Bounded variance: We assume that there exists a constant \u03c32 such that Ez(\u2016G(x, z)\u2212\u2207f(x)\u20162) \u2264 \u03c32 \u2200x.\nA.3 Lipschitzian Gradient: We assume f(x) to satisfy Lipschitzian smoothness, i.e., there exists a constant L such that \u2016\u2207f(x)\u2212\u2207f(y)\u2016 \u2264 L \u00b7 \u2016x\u2212 y\u2016 \u2200x, y.\nThe paper is organized as follows: Section 2 discussed some related work. We follow it up by analyzing impact of mini-batch size on SGD in Section 3. Later we extend the analysis to ASGD in Section 4. We provide experimental evidence regarding our analysis in Section 5 and conclude by discussing future directions in Section 6."}, {"heading": "2 RELATED WORK", "text": "In recent years, there have been several works analyzing convergence properties of SGD and its variants. In Nemirovski et al. (2009), SGD has been shown to have a convergence rate of O(1/S) for convex objective functions, where S is the number of samples seen, and this rate is in terms of distance of objective function from the optimal value. When the objective cost functions are non-convex, as is the case with most deep learning applications, the rate of convergence of SGD in terms of average gradient norm has been shown to be O(1/ \u221a S) asymptotically by Ghadimi & Lan (2013). The results in Dekel et al. (2012) can be interpreted as showing the applicability of this convergence rate also for the mini-batches of size in M , where now S takes form of MK with K being the number of mini-batches processed.\nAmong the distributed variants of SGD, ASGD has been the most popular variant Dean et al. (2012); Li et al. (2014a); Chilimbi et al. (2014). Practically it has been observed that ASGD is often slower to converge with increasing number of learners [Seide et al. (2014); Chan & Lane (2014); Dean et al.\n(2012)]. Although these works did not ignore communication overhead, in Section 4 we investigate the inherent inefficiency in ASGD without communication overhead costs. In Lian et al. (2015), it was proved that in the asymptotic regime the convergence rate of O(1/ \u221a S) can be extended to ASGD when the \u201cage\u201d of updates was bounded by the number of learners.\nThere exist several other sequential and distributed variants of SGD. For example, SGD with variance reduction techniques to mitigate the effects of gradient variance have been discussed in [Johnson & Zhang (2013); Xiao & Zhang (2014)]. SGD with co-ordinate descent/ascent and its distributed variants has been studied in [Hsieh et al. (2008); Richta\u0301rik & Taka\u0301c (2013); Fercoq et al. (2014); Konec\u030cny\u0300 et al. (2014); Qu & Richta\u0301rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al. (2016) have studied combining variance reduction techniques, randomized block co-ordinate descent [Richta\u0301rik & Taka\u0301c\u030c (2014)] and Nesterov acceleration methods [Nesterov (2013)] and analyzed its theoretical properties.\nThere have been several recent works which attempt to mitigate the effect of degrading convergence for large mini-batches (e.g., see [Li et al. (2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al. (2012); Tan et al. (2016); De et al. (2016)], or there have been works which attempt to improve SGD performance by intelligent selection of training samples, e.g., [Needell et al. (2014); Bouchard et al. (2015)]."}, {"heading": "3 THE IMPACT OF MINIBATCH SIZE ON SGD", "text": "In this section, we build on the SGD convergence analysis in Ghadimi & Lan (2013). In particular, we consider the convergence guarantees from Theorem 2.1 in Ghadimi & Lan (2013) restricted to constant learning rate. However, we first modify their analysis to allow mini-batches of arbitrary sizes. Building on, we show that SGD with smaller mini-batches can have better convergence guarantees than with using larger mini-batches. As a consequence, we observe that for larger mini-batches, a larger number of samples is needed for the convergence rate to fall below a certain threshold.\nLemma 1. With mini-batches of size M and a constant learning rate \u03b3, after K iterations of SGD, we have \u2211K\nk=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n1 \u03b3 \u2212 LM\u03b322 \u00b7 ( Df S + L\u03c32\u03b32 2 ) (2)\nProof outline. Using the property of Lipschitzian gradient, we get\nf(xk+1) \u2264 f(xk) + \u3008\u2207f(xk), xk+1 \u2212 xk\u3009+ L\n2 \u2016xk+1 \u2212 xk\u20162\n= f(xk)\u2212 \u03b3k \u00b7 \u2329 \u2207f(xk),\nM\u2211 i=1 G(xk, z i k)\n\u232a + \u03b32kL\n2 \u2225\u2225\u2225\u2225\u2225 M\u2211 i=1 G(xk, z i k) \u2225\u2225\u2225\u2225\u2225 2\n(3)\nLet us define \u03b4ik = G(xk, z i k) \u2212 \u2207f(xk) and \u03b4k = \u2211M i=1 \u03b4 i k. Using it, the above equation can be rewritten as\nf(xk+1)\u2212 f(xk) \u2264 \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k +M \u00b7 \u2207f(xk)\u3009+ \u03b32kL\n2 \u2016\u03b4k +M \u00b7 \u2207f(xk)\u20162\n\u21d2 f(xk+1)\u2212 f(xk) \u2264 \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k\u3009 \u2212M\u03b3k \u00b7 \u2016\u2207f(xk)\u20162\n+ \u03b32kL\n2\n( \u2016\u03b4k\u20162 + 2M\u3008\u03b4k,\u2207f(xk)\u3009+M2\u2016\u2207f(xk)\u20162 ) Rest of the proof involves adding such inequalities over first K \u2212 1 updates and bounding the \u2016\u03b4k\u20162 using assumption A.2 and \u3008\u03b4k,\u2207f(xk)\u3009 using assumption A.1 from Section 1. Finally, we use f(xK)\u2212 f(x\u2217) \u2264 Df and rearrange the terms to get the desired bound.\nIn the following theorem, we justify that given a fixed number of samples S to be processed, SGD with smaller mini-batches SGD can have better convergence guarantees than with using larger minibatches. Note that \u03b1 used in the theorem statement is a measure of (the square root of) the number of training samples processed.\nTheorem 2. Let \u03b1 := \u221a S\u03c32\nLDf . Let 4Ml \u2264 \u03b1 \u2264 Mh/4. Then the convergence guarantee for SGD\nfor mini-batch size Mh after processing S training samples can be worse than the convergence guarantee for mini-batch size Ml by a factor of 2Mh/(\u03b1 \u221a 2 +Ml).\nProof outline. For a fixed number S of training samples to be processed, we now minimize the right hand side of Equation 2 to find the best convergence guarantee supported by Lemma 1. Let \u03b3 = c \u00b7 \u221a Df/(SL\u03c32), where c is a scalar multiplier. Substituting it in Equation 2 and after some algebraic manipulations, we get\u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n( 1 c + c 2\n1\u2212 cM2\u03b1\n) \u00b7 \u221a DfL\u03c3 2\nS (4)\nBy applying simple calculus, it can be shown that the value c\u2217 of c which minimizes right hand side of the above equation is given by\nc\u2217 = M\n\u03b1 \u00b7\n( \u22121 + \u221a 1 + 2\u03b12\nM2\n) (5)\nIn appendix, we show that with M = Ml \u2264 \u03b1/4, we have c\u2217 \u2248 \u221a 2 \u2212Ml/\u03b1 and consequently the coefficient of \u221a DfL\u03c32/S from Equation 4 evaluates to approximately \u221a 2 +Ml/\u03b1. Whereas\nfor M = Mh \u2265 4\u03b1, we show that in the appendix using that c\u2217 = \u03b1/Mh and the coefficient of\u221a DfL\u03c32/S from Equation 4 evaluates to approximately 2Mh/\u03b1. Combining these observations, we get that the convergence guarantees for Ml and Mh can differ by a factor of 2Mh/(\u03b1 \u221a 2 +Ml) after processing S training samples. See the appendix for complete proof.\nNote that while it could be possible to show that in general smaller mini-batches converge faster than larger mini-batches in theory, we used 4Ml \u2264 \u03b1 \u2264 Mh/4 in Theorem 2 for the sake of simplicity. Also, although we theoretically justified faster convergence smaller mini-batches in Theorem 2, the exact factors by which bigger mini-batches can be worse can vary in practice. Here our purpose is to give theoretical support to the practical observations of smaller mini-batches being faster. Theorem 3. The number of samples needs to be processed in order for SGD achieve the same convergence guarantee increases as the mini-batch size increases.\nProof. For the same values of \u03b3 and S, the value of the bound from Equation 2 becomes worse (i.e., increases) as M increases. This is because for a fixed \u03b3 the quantity \u03b3 \u2212 LM\u03b32/2 must decrease as M increases. Consequently for given S, the best value of convergence guarantee (i.e., smallest average gradient norm) attained by varying \u03b3 must become worse (i.e., higher) asM increases. Thus in order to reach the same convergence guarantee, SGD must process more training samples with increasing mini-batch size.\nNow we will proceed to the next section, where we will show that with ASGD (which is one of the most popular distributed variants of SGD) increasing number of learners can lead to slower convergence given a fixed total number of samples to be processed, and the effect is similar to that of increasing mini-batch size for SGD."}, {"heading": "4 ASYNCHRONOUS STOCHASTIC GRADIENT DESCENT", "text": "ASGD typically has a parameter server maintaining the parameters (i.e., the weights in the neural network) and multiple learners. Each learner asynchronously repeats the following:\n\u2022 Pull: Get the parameters from the server.\n\u2022 Compute: Compute the gradient with respect to randomly selected mini-batch (i.e., a certain number of samples from the dataset). \u2022 Push and update: Communicate the gradient to the server. Server then updates the pa-\nrameters by subtracting this newly communicated gradient multiplied by the learning rate.\nWe assume that the update performed by the server is atomic, i.e., the server does not send or receive parameters while it updates the parameters. Now we express kth update step of the ASGD algorithm in terms of our notation. Note that for kth update, the partial gradient computed by a learner can be with respect to an older parameter vector. This is because while computing the partial gradient, the parameter vector could have been updated because of the partial gradients sent in by other learners. Let xk\u2212\u03c4 be the parameter vector used by a learner to compute the partial gradient to be used in kth update. Then the equation for kth update of ASGD becomes:\nxk+1 = xk \u2212 \u03b3 \u00b7G(xk\u2212\u03c4 , z) (6)\nLian et al. (2015) showed that when the age of the updates \u03c4 is bounded by the number of learners N , then ASGD asymptotically converges with a rate ofO(1/ \u221a S) where S is the cumulative number of training samples processed. From Theorem 1 in Lian et al. (2015), the convergence rate guarantee (expressed in the terms of average gradient norm) for ASGD with N learners after processing K updates becomes \u2211K\nk=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264 2Df MK\u03b3 + \u03c32L\u03b3 + 2\u03c32L2MN\u03b32 (7)\ns.t. LM\u03b3 + 2L2M2N2\u03b32 \u2264 1 (8)\nThe terms independent of the number of updates K in Equation 7 indicate that with a constant learning rate, there is a limit on how close the algorithm can reach to the optimum without lowering the learning rate. Although asymptotically, it can be shown that Equation 7-8 lead to O(1/ \u221a S) convergence (see Lian et al. (2015)), we now investigate the convergence behavior prior to such a regime. We have the following theorem about the effect of increasing the number of learners on ASGD convergence guarantee:\nTheorem 4. Let N > 1 be the number of learners and let \u03b1 = \u221a K\u03c32\nMLDf \u2264 N , then the optimal\nASGD convergence rate guarantee for 1 learner and N learners can differ by a factor of approximately N\u03b1 .\nThe proof of above theorem is in the same spirit as that of Theorem 2 and can be found in the appendix. Note that without asynchronous nature of ASGD, the analysis for synchronous distributed SGD would be the same as the analysis for SGD from Section 3. This is because synchronous SGD, where each of the N learners compute the gradient for a random mini-batch of size M , equivalently represents SGD with mini-batch size MN . The asynchronous nature of ASGD introduces extra factors to be taken into account such as the \u201cage\u201d of the updates (i.e., the situation where the gradient returned by a learner may have been computed by an older parameter vector). Theorem 5. For a constant mini-batch size M , the number of samples needs to be processed in order to achieve the same convergence guarantee increases as the number of learners increases.\nProof outline. The range of \u03b3 permissible by Equation 8 becomes smaller as N increases. Rest of the proof combines the observations that the minimum attained by the convergence guarantee by Equation 7 must become worse if the range of \u03b3 decreases and N increases. For complete proof, please see the appendix.\nDiscussion: From Theorem 2, for sequential SGD, there could be a difference of 2Mh/(\u03b1 \u221a 2+Ml) between the convergence guarantee of mini-batch sizesMl, Mh with 4Ml \u2264 \u03b1 \u2264Mh/4. Assuming Ml to be far smaller than \u03b1, this factor becomes approximately \u221a 2Mh/\u03b1. This is comparable with the difference N/\u03b1 between ASGD convergence guarantee of 1 learner and N learners. Although exact numerical multipliers may differ depending on the tightness of the original convergence bound, it points to the similarities between slow-down caused by bigger mini-batch sizes with SGD and larger number of learners with ASGD. At a high level, ASGD with higher number of learners (with\nbounded age/staleness of updates) can be thought of SGD with some effective mini-batch size. This effective mini-batch size could be dependent on the number of learners as well as the age/staleness of updates."}, {"heading": "5 EXPERIMENTS", "text": "Experiment setup: We carry our experiments with CIFAR-10 dataset [cif (accessed January 11, 2016a)] which contains 50, 000 training samples and 10, 000 test samples, each associated with 1 out of 10 possible labels For the CIFAR-10 dataset, our aim is to predict the correct label the input images. For our experiments, we train convolutional neural network shown in Table 1, which is taken from [cif (accessed January 11, 2016b)]. It is a fairly standard convolutional network design consisting of a series of convolutional layers interspersed by max-pooling layers. The convolutional layers outputs are filtered with rectified linear unit before applying max-pooling. Additionally, it also uses Dropout layers which act as regularization [Srivastava et al. (2014)]. At the end, it has a fully connected layer with 10 outputs (equal to the number of labels). The number of parameters to be learned for CIFAR-10 network is \u2248 0.5 million. We use cross-entropy between the input labels and the predicted labels in the final output layer, i.e., F (x, z) (see Section 1 for the notation) is the cross-entropy error and f(x) is the average crossentropy error over all training samples. The implementation of neural network was done using Torch. The target platform for our experiments is a Magma system with 16 GPUs connected to an IBM Power8 host. In our ASGD Downpour implementation, the learners are run on the GPUs, while the parameter server is run on the CPU.\nWe carried our experiments for 100 epochs, here by an epoch we mean a complete pass of the training data. We chose the learning rate to be 0.01 as it was seen to be performing well at the end\nof our experiments with respect to test accuracy (i.e., classification accuracy on the test data). For ASGD part of experiments, we randomly partitioned the training data between all N learners in the beginning of each epoch. At the end of each epoch we measured the test accuracy. See Figure 1 for the results of our SGD experiments. We can observe that as the mini-batch size increases, the test error converges slower with respect to the number of epochs. See Figure 2 for the result of our experiments with ASGD experiments. Again, we can observe that as the number of learners increases, the convergence of test error becomes slower.\nThese observations agree with our justifications from Section 3 and 4. Moreover, they show that there are similarities between the slow-down caused by increasing mini-batch size with SGD and increasing the number of learners with ASGD. Thus, exploiting parallelism, either by increasing mini-batch size or by increasing the number of learners introduces an inherent inefficiency in the convergence behavior, even after disregarding other overheads such as communication time when we increase the number of learners."}, {"heading": "6 CONCLUSION AND FUTURE DIRECTIONS", "text": "In this paper, we theoretically justified faster convergence (in terms of average gradient norm attained after processing a fixed number of samples) of SGD with small mini-batches or that of ASGD with smaller number of learners. This indicates that there is an inherent inefficiency in the speed-up obtained with parallelizing gradient descent methods by taking advantage of hardware. It would be interesting to see if such a conclusion holds for more advanced update methods than vanilla SGD, for example methods using momentum and its variants."}, {"heading": "A APPENDIX", "text": "Lemma 1. With mini-batches of size M and a constant learning rate \u03b3, after K iterations of SGD, we have \u2211K\nk=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n1 \u03b3 \u2212 LM\u03b322 \u00b7 ( Df S + L\u03c32\u03b32 2 ) (2)\nProof. Using the property of Lipschitzian gradient, we get\nf(xk+1) \u2264 f(xk) + \u3008\u2207f(xk), xk+1 \u2212 xk\u3009+ L\n2 \u2016xk+1 \u2212 xk\u20162\n= f(xk)\u2212 \u03b3k \u00b7 \u2329 \u2207f(xk),\nM\u2211 i=1 G(xk, z i k)\n\u232a + \u03b32kL\n2 \u2225\u2225\u2225\u2225\u2225 M\u2211 i=1 G(xk, z i k) \u2225\u2225\u2225\u2225\u2225 2\n(9)\nLet us define \u03b4ik = G(xk, z i k) \u2212 \u2207f(xk) and \u03b4k = \u2211M i=1 \u03b4 i k. Using it, the above equation can be rewritten as\nf(xk+1)\u2212 f(xk) \u2264 \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k +M \u00b7 \u2207f(xk)\u3009+ \u03b32kL\n2 \u2016\u03b4k +M \u00b7 \u2207f(xk)\u20162\n\u21d2 f(xk+1)\u2212 f(xk) \u2264 \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k\u3009 \u2212M\u03b3k \u00b7 \u2016\u2207f(xk)\u20162\n+ \u03b32kL\n2\n( \u2016\u03b4k\u20162 + 2M\u3008\u03b4k,\u2207f(xk)\u3009+M2\u2016\u2207f(xk)\u20162 ) Generating such inequalities over K mini-batches and adding them, we get\n\u2212Df \u2264 f(xK+1)\u2212 f(x1)\n\u2264 K\u2211 k=1 ( \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k\u3009 \u2212M\u03b3k\u2016\u2207f(xk)\u20162\n+ \u03b32kL\n2\n( \u2016\u03b4k\u20162 + 2M\u3008\u03b4k,\u2207f(xk)\u3009+M2\u2016\u2207f(xk)\u20162 )) Simple rearrangement of terms gives us\nK\u2211 k=1 ( M\u03b3k \u2212 \u03b32kLM 2 2 ) \u00b7 \u2016\u2207f(xk)\u20162\n\u2264 Df + K\u2211 k=1 ( \u2212\u03b3k \u00b7 \u3008\u2207f(xk), \u03b4k\u3009+ \u03b32kL 2 ( \u2016\u03b4k\u20162 + 2M \u00b7 \u3008\u03b4k,\u2207f(xk)\u3009 )) (10)\nNow we observe that E(\u2016\u03b4k\u20162) = E(\u2016 \u2211M i=1 \u03b4 i k\u2016 2 ) \u2264 M \u00b7 \u2211M i=1 E(\u2016\u03b4ik\u2016 2 ) \u2264 M\u03c32, using Assumption A.2. From the assumption of the stochastic gradient being unbiased estimator of the true gradient (Assumption A.1), we also know that E(\u3008\u03b4k,\u2207f(xk)\u3009) = \u2211M i=1 E( \u2329 \u03b4ik,\u2207f(xk)\n\u232a ) =\u2329\u2211M\ni=1 E(\u03b4ik),\u2207f(xk) \u232a = 0. Taking the expectation of both sides in Equation 10 with respect to\nrandomly selected samples and using these observations we get the following equation: K\u2211 k=1 ( M\u03b3k \u2212 LM2\u03b32k 2 ) \u00b7 E(\u2016\u2207f(xk)\u20162) \u2264 Df + LM\u03c32 2 K\u2211 k=1 \u03b32k\nThe above equation is equivalent to Equation 2.11 from Ghadimi & Lan (2013) modified to allow arbitrary mini-batch sizes. Restricting ourselves to allow constant learning rate and after simplifying the above equation, we get\u2211K\nk=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n1 \u03b3 \u2212 LM\u03b322 \u00b7 ( Df S + L\u03c32\u03b32 2 ) (11)\nTheorem 2. Let \u03b1 := \u221a S\u03c32\nLDf . Let 4Ml \u2264 \u03b1 \u2264 Mh/4. Then the convergence guarantee for SGD\nfor mini-batch size Mh after processing S training samples can be worse than the convergence guarantee for mini-batch size Ml by a factor of 2Mh/(\u03b1 \u221a 2 +Ml).\nProof outline. For a fixed number S of training samples to be processed, we now minimize the right hand side of Equation 2 to find the best convergence guarantee supported by Lemma 1. Let \u03b3 = c \u00b7 \u221a Df/(SL\u03c32), where c is a scalar multiplier. Substituting it in Equation 2 and after some algebraic manipulations, we get\u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n( 1 c + c 2\n1\u2212 cM2\u03b1\n) \u00b7 \u221a DfL\u03c3 2\nS (12)\nBy applying simple calculus, it can be shown that the value c\u2217 of c which minimizes right hand side of the above equation is given by\nc\u2217 = M\n\u03b1 \u00b7\n( \u22121 + \u221a 1 + 2\u03b12\nM2\n) (13)\n\u2022 Consider the case ofM =Ml \u2264 \u03b1/4. Denote c\u2217 by c\u2217l for this case. WithM =Ml \u2264 \u03b1/4, we have \u221a 1 + 2\u03b12/M2l \u2248 \u221a 2\u03b1/Ml. Thus we get c\u2217l \u2248 \u221a 2\u2212Ml/\u03b1. Further, we can write\nthe following using Ml is little compared to \u03b1 and other simple known approximations: 1\n1\u2212 c \u2217 lMl 2\u03b1\n= 1\n1\u2212 ( \u221a 2\u2212 Ml\u03b1 ) Ml 2\u03b1 \u2248 1 1\u2212 Ml\u221a\n2\u03b1\n\u2248 1 + Ml\u221a 2\u03b1\n(14)\n1 c\u2217l = 1\u221a 2\u2212 Ml\u03b1 = 1\u221a 2 \u00b7 1 1\u2212 Ml\u221a\n2\u03b1\n\u2248 1\u221a 2 \u00b7 ( 1 + Ml\u221a 2\u03b1 ) \u2248 1\u221a 2 + Ml 2\u03b1 (15)\nc\u2217l 2 = 1\u221a 2 \u2212 Ml 2\u03b1 (16)\nUsing Equation 14, 15, 16, we get that the coefficient of \u221a DfL\u03c32/S from Equation 12 evaluates to approximately \u221a 2 +Ml/\u03b1. \u2022 Consider Mh \u2265 4\u03b1. Denote c\u2217 by c\u2217h for this case. With M = Ml \u2264 \u03b1/4, we have\u221a 1 + 2\u03b12/M2h \u2248 1+\u03b12/M2h , using the approximation \u221a 1 + \u2248 1+ /2 for small . Thus\nc\u2217h \u2248 \u03b1/Mh. Since \u03b1 is much smaller thanMh, we can approximate 1/c\u2217h+c\u2217h/2 \u2248 1/c\u2217h \u2248 Mh/\u03b1 and we also have 1\u2212 c\u2217hM/(2\u03b1) = 1/2. Thus the coefficient of \u221a DfL\u03c32/S from Equation 12 evaluates to approximately 2Mh/\u03b1.\nCombining the above observations, we get that the convergence guarantees forMl andMh can differ by a factor of 2Mh/(\u03b1 \u221a 2 +Ml) after processing S training samples.\nTheorem 4. Let N > 1 be the number of learners and let \u03b1 = \u221a K\u03c32\nMLDf \u2264 N , then the optimal\nASGD convergence rate guarantee for 1 learner and N learners can differ by a factor of approximately N\u03b1 .\nProof. We have \u03b3 = c \u00b7 \u221a Df/(MKL\u03c32) = c \u03b1ML from the definition of \u03b1. Substituting this in Equation 7, we get\u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n2Df MKc \u00b7 \u221a\nDf MKL\u03c32\n+ \u03c32L \u00b7 c \u00b7 \u221a\nDf MKL\u03c32\n+ 2\u03c32L2MN \u00b7 c \u03b1ML\n\u00b7 c \u00b7 \u221a\nDf MKL\u03c32\n=\n( 2\nc + c+\n2Nc2\n\u03b1\n) \u00b7 \u221a DfL\u03c32\nMK (17)\nFrom the definition of \u03b1, we have K = \u03b12MLDf/\u03c32. Using it in the above equation, we get\u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK \u2264\n{( 2\nc + c+\n2Nc2\n\u03b1 ) \u00b7 1 \u03b1 } \u00b7 \u03c3 2 M (18)\nSimilarly, given \u03b3 = c \u00b7 \u221a\nDf MKL\u03c32 = c \u03b1ML , the condition in Equation 8 can also be expressed as:\nc \u03b1 +\n2N2c2\n\u03b12 \u2264 1 \u21d2 2N2c2 + \u03b1c\u2212 \u03b12 \u2264 0\nSince learning rate (and hence c) is always positive, the above equation gives us\n0 \u2264 c \u2264 \u03b1 4N2\n\u00b7 (\u22121 + \u221a 1 + 8N2)\nThus finding the optimal learning rate (within the regime of Equation 7 and 8) is equivalent to solving the following:\nminimize ( 2\nc + c+\n2Nc2\n\u03b1 ) \u00b7 1 \u03b1 \u00b7 \u03c3 2 M (19)\ns.t. 0 \u2264 c \u2264 \u03b1 4N2\n\u00b7 (\u22121 + \u221a 1 + 8N2) (20)\nNow, by means of solving the above optimization, we will investigate how much the convergence guarantee can differ as the number of learners increase. In particular, we will look at the difference in the guarantee for 1 learner and N \u2032 learners where N \u2032 \u2265 16. Taking the derivative of Equation 19 with respect to c and setting it to 0, we get the following:\n4Nc3 + \u03b1c2 \u2212 2\u03b1 = 0 (21)\nLet c\u22171 and c \u2217 N \u2032 denote the solutions to the above equation for 1 and N \u2032 learners respectively. Notice that for N = 1 and for \u03b1 \u2265 16, the square term dominates in Equation 21 and c\u22171 \u2248 \u221a 2 (which also satisfies the constraint in Equation 20). Thus we get\nFor N = 1: \u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK . 2 \u221a 2 \u00b7 \u03c3 2 \u03b1M (22)\nHowever, for N = N \u2032 and 16 \u2264 \u03b1 \u2264 N \u2032, the cubic term dominates in Equation 21. Thus the value of c which satisfies Equation 21 is approximately 3 \u221a \u03b1/(2N \u2032). However, the upper bound in Equation 20 for large N \u2032 becomes c . \u03b1/( \u221a 2N \u2032). For the range of \u03b1 under consideration, this\nis smaller than 3 \u221a \u03b1/(2N \u2032), thus we get c\u2217N \u2032 = \u03b1/( \u221a 2N \u2032). Thus for 16 \u2264 \u03b1 \u2264 N \u2032, Equation 18 becomes\nFor N = N \u2032: \u2211K k=1 E(\u2016\u2207f(xk)\u2016 2 )\nK .\n2 \u221a 2N \u2032\n\u03b1 \u00b7 \u03c3\n2\n\u03b1M (23)\nThus comparing Equation 22 and 23, we see that the ASGD convergence guarantee for N = 1 and N = N \u2032 learners can differ by a factor of N \u2032\n\u03b1 for 16 \u2264 \u03b1 \u2264 N \u2032."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Accelerating stochastic gradient descent via online learning to sample", "author": ["Guillaume Bouchard", "Th\u00e9o Trouillon", "Julien Perez", "Adrien Gaidon"], "venue": "arXiv preprint arXiv:1506.09016,", "citeRegEx": "Bouchard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2015}, {"title": "Sample size selection in optimization methods for machine learning", "author": ["Richard H Byrd", "Gillian M Chin", "Jorge Nocedal", "Yuchen Wu"], "venue": "Mathematical programming,", "citeRegEx": "Byrd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2012}, {"title": "Distributed asynchronous optimization of convolutional neural networks", "author": ["William Chan", "Ian Lane"], "venue": "In INTERSPEECH,", "citeRegEx": "Chan and Lane.,? \\Q2014\\E", "shortCiteRegEx": "Chan and Lane.", "year": 2014}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "Chilimbi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Big batch sgd: Automated inference using adaptive batch sizes", "author": ["Soham De", "Abhay Yadav", "David Jacobs", "Tom Goldstein"], "venue": "arXiv preprint arXiv:1610.05792,", "citeRegEx": "De et al\\.,? \\Q2016\\E", "shortCiteRegEx": "De et al\\.", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Fast distributed coordinate descent for non-strongly convex losses", "author": ["Olivier Fercoq", "Zheng Qu", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "In 2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),", "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2013}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin", "S Sathiya Keerthi", "Sellamanickam Sundararajan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Takac", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Semi-stochastic coordinate descent", "author": ["Jakub Kone\u010dn\u1ef3", "Zheng Qu", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1412.6293,", "citeRegEx": "Kone\u010dn\u1ef3 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kone\u010dn\u1ef3 et al\\.", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Jun Woo Park", "Alexander J Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J Shekita", "Bor-Yiing Su"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Ji Liu", "Stephen J Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Liu and Wright.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Wright.", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher Re", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Asynchronous accelerated stochastic gradient descent", "author": ["Qi Meng", "Wei Chen", "Jingcheng Yu", "Taifeng Wang", "Zhi-Ming Ma", "Tie-Yan Liu"], "venue": "In Proceedings of the 25th international joint conference on Artificial Intelligence,", "citeRegEx": "Meng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["Deanna Needell", "Rachel Ward", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Needell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Needell et al\\.", "year": 2014}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov.", "year": 2012}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2013\\E", "shortCiteRegEx": "Nesterov.", "year": 2013}, {"title": "Coordinate descent with arbitrary sampling i: Algorithms and complexity", "author": ["Zheng Qu", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1412.8060,", "citeRegEx": "Qu and Richt\u00e1rik.,? \\Q2014\\E", "shortCiteRegEx": "Qu and Richt\u00e1rik.", "year": 2014}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1c"], "venue": null, "citeRegEx": "Richt\u00e1rik and Tak\u00e1c.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1c.", "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2014\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2014}, {"title": "On parallelizability of stochastic gradient descent for speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Barzilai-borwein step size for stochastic gradient descent", "author": ["Conghui Tan", "Shiqian Ma", "Yu-Hong Dai", "Yuqiu Qian"], "venue": "arXiv preprint arXiv:1605.04131,", "citeRegEx": "Tan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2016}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D Randall Wilson", "Tony R Martinez"], "venue": "Neural Networks,", "citeRegEx": "Wilson and Martinez.,? \\Q2003\\E", "shortCiteRegEx": "Wilson and Martinez.", "year": 2003}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Zhang.", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "SGD has been shown to satisfy an asymptotic convergence rate of O(1/S) for convex objective functions [Nemirovski et al. (2009)] and an asymptotic convergence rate of O(1/ \u221a S) for general non-convex objective functions with mini-batch size 1 in [Ghadimi & Lan (2013)] or with arbitrary mini-batch sizes in [Dekel et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 19, "context": "SGD has been shown to satisfy an asymptotic convergence rate of O(1/S) for convex objective functions [Nemirovski et al. (2009)] and an asymptotic convergence rate of O(1/ \u221a S) for general non-convex objective functions with mini-batch size 1 in [Ghadimi & Lan (2013)] or with arbitrary mini-batch sizes in [Dekel et al.", "startOffset": 103, "endOffset": 268}, {"referenceID": 6, "context": "(2009)] and an asymptotic convergence rate of O(1/ \u221a S) for general non-convex objective functions with mini-batch size 1 in [Ghadimi & Lan (2013)] or with arbitrary mini-batch sizes in [Dekel et al. (2012)].", "startOffset": 187, "endOffset": 207}, {"referenceID": 6, "context": "(2009)] and an asymptotic convergence rate of O(1/ \u221a S) for general non-convex objective functions with mini-batch size 1 in [Ghadimi & Lan (2013)] or with arbitrary mini-batch sizes in [Dekel et al. (2012)]. Although SGD converges asymptotically with the same rate irrespective of mini-batch size, it has been reported that for large mini-batch sizes, often it is slower to converge - for example, see Wilson & Martinez (2003) for the detailed graphical illustrations therein for the effect of increasing batchsize or see Bottou (2010) for the comments on the tradeoffs of mini-batching.", "startOffset": 187, "endOffset": 428}, {"referenceID": 0, "context": "Although SGD converges asymptotically with the same rate irrespective of mini-batch size, it has been reported that for large mini-batch sizes, often it is slower to converge - for example, see Wilson & Martinez (2003) for the detailed graphical illustrations therein for the effect of increasing batchsize or see Bottou (2010) for the comments on the tradeoffs of mini-batching.", "startOffset": 314, "endOffset": 328}, {"referenceID": 6, "context": "\u2022 Further, we investigate Asynchronous Stochastic Gradient Descent (ASGD) which is one of the most popular asynchronous variants of SGD [Dean et al. (2012); Li et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 6, "context": "\u2022 Further, we investigate Asynchronous Stochastic Gradient Descent (ASGD) which is one of the most popular asynchronous variants of SGD [Dean et al. (2012); Li et al. (2014a);", "startOffset": 137, "endOffset": 175}, {"referenceID": 16, "context": "In Nemirovski et al. (2009), SGD has been shown to have a convergence rate of O(1/S) for convex objective functions, where S is the number of samples seen, and this rate is in terms of distance of objective function from the optimal value.", "startOffset": 3, "endOffset": 28}, {"referenceID": 5, "context": "The results in Dekel et al. (2012) can be interpreted as showing the applicability of this convergence rate also for the mini-batches of size in M , where now S takes form of MK with K being the number of mini-batches processed.", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "Among the distributed variants of SGD, ASGD has been the most popular variant Dean et al. (2012); Li et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 5, "context": "Among the distributed variants of SGD, ASGD has been the most popular variant Dean et al. (2012); Li et al. (2014a); Chilimbi et al.", "startOffset": 78, "endOffset": 116}, {"referenceID": 4, "context": "(2014a); Chilimbi et al. (2014). Practically it has been observed that ASGD is often slower to converge with increasing number of learners [Seide et al.", "startOffset": 9, "endOffset": 32}, {"referenceID": 4, "context": "(2014a); Chilimbi et al. (2014). Practically it has been observed that ASGD is often slower to converge with increasing number of learners [Seide et al. (2014); Chan & Lane (2014); Dean et al.", "startOffset": 9, "endOffset": 160}, {"referenceID": 7, "context": "In Lian et al. (2015), it was proved that in the asymptotic regime the convergence rate of O(1/ \u221a S) can be extended to ASGD when the \u201cage\u201d of updates was bounded by the number of learners.", "startOffset": 3, "endOffset": 22}, {"referenceID": 6, "context": "SGD with co-ordinate descent/ascent and its distributed variants has been studied in [Hsieh et al. (2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "SGD with co-ordinate descent/ascent and its distributed variants has been studied in [Hsieh et al. (2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al.", "startOffset": 86, "endOffset": 132}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al.", "startOffset": 34, "endOffset": 78}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al.", "startOffset": 34, "endOffset": 101}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al.", "startOffset": 34, "endOffset": 120}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)].", "startOffset": 34, "endOffset": 141}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)].", "startOffset": 34, "endOffset": 158}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al.", "startOffset": 34, "endOffset": 275}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al. (2016) have studied combining variance reduction techniques, randomized block co-ordinate descent [Richt\u00e1rik & Tak\u00e1\u010d (2014)] and Nesterov acceleration methods [Nesterov (2013)] and analyzed its theoretical properties.", "startOffset": 34, "endOffset": 325}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al. (2016) have studied combining variance reduction techniques, randomized block co-ordinate descent [Richt\u00e1rik & Tak\u00e1\u010d (2014)] and Nesterov acceleration methods [Nesterov (2013)] and analyzed its theoretical properties.", "startOffset": 34, "endOffset": 442}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al. (2016) have studied combining variance reduction techniques, randomized block co-ordinate descent [Richt\u00e1rik & Tak\u00e1\u010d (2014)] and Nesterov acceleration methods [Nesterov (2013)] and analyzed its theoretical properties.", "startOffset": 34, "endOffset": 494}, {"referenceID": 5, "context": "(2008); Richt\u00e1rik & Tak\u00e1c (2013); Fercoq et al. (2014); Kone\u010dn\u1ef3 et al. (2014); Qu & Richt\u00e1rik (2014); Liu et al. (2015); Jaggi et al. (2014); Nesterov (2012)]. The convergence properties of asynchronous stochastic coordinate descent have been analyzed in Liu & Wright (2015). More recently, the authors in Meng et al. (2016) have studied combining variance reduction techniques, randomized block co-ordinate descent [Richt\u00e1rik & Tak\u00e1\u010d (2014)] and Nesterov acceleration methods [Nesterov (2013)] and analyzed its theoretical properties. There have been several recent works which attempt to mitigate the effect of degrading convergence for large mini-batches (e.g., see [Li et al. (2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al.", "startOffset": 34, "endOffset": 688}, {"referenceID": 1, "context": "(2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al. (2012); Tan et al.", "startOffset": 225, "endOffset": 244}, {"referenceID": 1, "context": "(2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al. (2012); Tan et al. (2016); De et al.", "startOffset": 225, "endOffset": 263}, {"referenceID": 1, "context": "(2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al. (2012); Tan et al. (2016); De et al. (2016)], or there have been works which attempt to improve SGD performance by intelligent selection of training samples, e.", "startOffset": 225, "endOffset": 281}, {"referenceID": 1, "context": "(2014b)] where in each mini-batch a regularized objective function is optimized to compute updated parameter vector), or there are works which attempt to select mini-batch dynamically for better performance, for example see [Byrd et al. (2012); Tan et al. (2016); De et al. (2016)], or there have been works which attempt to improve SGD performance by intelligent selection of training samples, e.g., [Needell et al. (2014); Bouchard et al.", "startOffset": 225, "endOffset": 424}, {"referenceID": 16, "context": "Although asymptotically, it can be shown that Equation 7-8 lead to O(1/ \u221a S) convergence (see Lian et al. (2015)), we now investigate the convergence behavior prior to such a regime.", "startOffset": 94, "endOffset": 113}, {"referenceID": 28, "context": "Additionally, it also uses Dropout layers which act as regularization [Srivastava et al. (2014)].", "startOffset": 71, "endOffset": 96}], "year": 2016, "abstractText": "Stochastic Gradient Descent (SGD) and its variants are the most important optimization algorithms used in large scale machine learning. Mini-batch version of stochastic gradient is often used in practice for taking advantage of hardware parallelism. In this work, we analyze the effect of mini-batch size over SGD convergence for the case of general non-convex objective functions. Building on the past analyses, we justify mathematically that there can often be a large difference between the convergence guarantees provided by small and large mini-batches (given each instance processes equal number of training samples), while providing experimental evidence for the same. Going further to distributed settings, we show that an analogous effect holds with popular Asynchronous Gradient Descent (ASGD): there can be a large difference between convergence guarantees with increasing number of learners given that the cumulative number of training samples processed remains the same. Thus there is an inherent (and similar) inefficiency introduced in the convergence behavior when we attempt to take advantage of parallelism, either by increasing mini-batch size or by increase the number of learners.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_350"}