{"name": "ICLR_2017_369.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP GENERA", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "emails": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "We investigate deep generative models that can exchange multiple modalities bidirectionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE\u2019s encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bidirectionally."}, {"heading": "1 INTRODUCTION", "text": "In our world, information is represented through various modalities. While images are represented by pixel information, these can also be described with text or tag information. People often exchange such information bi-directionally. For instance, we can not only imagine what \u201ca young female with a smile who does not wear glasses\u201d looks like, but also add this caption to a corresponding photograph. To do so, it is important to extract a joint representation that captures high-level concepts among all modalities. Then we can bi-directionally generate modalities through the joint representations. However, each modality typically has a different kind of dimension and structure, e.g., images (real-valued and dense) and texts (discrete and sparse). Therefore, the relations between each modality and the joint representations might become high nonlinearity. To discover such relations, deep neural network architectures have been used widely for multimodal learning (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012). The common approach with these models to learn joint representations is to share the top of hidden layers in modality specific networks. Among them, generative approaches using deep Boltzmann machines (DBMs) (Srivastava & Salakhutdinov, 2012; Sohn et al., 2014) offer the important advantage that these can generate modalities bi-directionally.\nRecently, variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) have been proposed to estimate flexible deep generative models by variational inference methods. These models use back-propagation during training, so that it can be trained on large-scale and highdimensional dataset compared with DBMs with MCMC training. Some studies have addressed to handle such large-scale and high-dimensional modalities on VAEs, but they are forced to model conditional distribution (Kingma et al., 2014; Sohn et al., 2015; Pandey & Dukkipati, 2016). Therefore, it can only generate modalities in one direction. For example, we cannot obtain generated images from texts if we train the likelihood of texts given images. To generate modalities bi-directionally,\nall modalities should be treated equally under the learned joint representations, which is the same as previous multimodal learning models before VAEs.\nAs described in this paper, we develop a novel multimodal learning model with VAEs, which we call a joint multimodal variational autoencoder (JMVAE). The most significant feature of our model is that all modalities, x and w (e.g., images and texts), are conditioned independently on a latent variable z corresponding to joint representation, i.e., the JMVAE models a joint distribution of all modalities, p(x,w). Therefore, we can extract a high-level representation that contains all information of modalities. Moreover, since it models a joint distribution, we can draw samples from both p(x|w) and p(w|x). Because, at this time, modalities that we want to generate are usually missing, the inferred latent variable becomes incomplete and generated samples might be collapsed in the testing time when missing modalities are high-dimensional and complicated. To prevent this issue, we propose a method of preparing the new encoders for each modality, p(z|x) and p(z|w), and reducing the divergence between the multimodal encoder p(z|x,w), which we call JMVAE-kl. This contributes to more effective bi-directional generation of modalities, e.g., from face images to texts (attributes) and vice versa (see Figure 1).\nThe main contributions of this paper are as follows:\n\u2022 We introduce a joint multimodal variational autoencoder (JMVAE), which is the first study to train joint distribution of modalities with VAEs.\n\u2022 We propose an additional method (JMVAE-kl), which prevents generated samples from being collapsed when some modalities are missing. We experimentally confirm that this method solves this issue.\n\u2022 We show qualitatively and quantitatively that JMVAE can extract appropriate joint distribution and that it can generate and reconstruct modalities similarly or more properly than conventional VAEs.\n\u2022 We demonstrate that the JMVAE can generate multiple modalities bi-directionally even if these modalities have completely different kinds of dimensions and structures, e.g., highdimentional color face images and low-dimentional binary attributes."}, {"heading": "2 RELATED WORK", "text": "The common approach of multimodal learning with deep neural networks is to share the top of hidden layers in modality specific networks. Ngiam et al. (2011) proposed this approach with deep autoencoders (AEs) and found that it can extract better representations than single modality settings. Srivastava & Salakhutdinov (2012) also took this idea but used deep Boltzmann machines (DBMs) (Salakhutdinov & Hinton, 2009). DBMs are generative models with undirected connections based on maximum joint likelihood learning of all modalities. Therefore, this model can generate modalities bi-directionally. Sohn et al. (2014) improved this model to exchange multiple modalities effectively, which are based on minimizing the variation of information and JMVAE-kl in ours can be regarded as minimizing it with variational learning on parameterized distributions (see Section 3.3\nand Appendix A). However, it is computationally difficult for DBMs to train high-dimensional data such as natural images because of MCMC training.\nRecently, VAEs (Kingma & Welling, 2013; Rezende et al., 2014) are used to train such highdimensional modalities. Kingma et al. (2014); Sohn et al. (2015) propose conditional VAEs (CVAEs), which maximize a conditional log-likelihood by variational methods. Many studies are based on CVAEs to train various multiple modalities such as handwriting digits and labels (Kingma et al., 2014; Sohn et al., 2015), object images and degrees of rotation (Kulkarni et al., 2015), face images and attributes (Larsen et al., 2015; Yan et al., 2015), and natural images and captions (Mansimov et al., 2015). The main features of CVAEs are that the relation between modalities is one-way and a latent variable does not contain the information of a conditioned modality1, which are unsuitable for our objective.\nPandey & Dukkipati (2016) proposed a conditional multimodal autoencoder (CMMA), which also maximizes the conditional log-likelihood. The difference between CVAEs is that a latent variable is connected directly from a conditional variable, i.e., these variables are not independent. Moreover, this model forces the latent representation from an input to be close to the joint representation from multiple inputs, which is similar to JMVAE-kl. However, the CMMA still considers that modalities are generated in fixed direction. This is the most different part from ours."}, {"heading": "3 METHODS", "text": "This section first introduces the algorithm of VAEs briefly and then proposes a novel multimodal learning model with VAEs, which we call the joint multimodal variational autoencoder (JMVAE)."}, {"heading": "3.1 VARIATIONAL AUTOENCODERS", "text": "Given observation variables x and corresponding latent variables z, their generating processes are definable as z \u223c p(z) = N (0, I) and x \u223c p\u03b8(x|z), where \u03b8 is the model parameter of p. The objective of VAEs is maximization of the marginal distribution p(x) = \u222b p\u03b8(x|z)p(z)dx. Because this distribution is intractable, we instead train the model to maximize the following lower bound of the marginal distribution LV AE(x) as\nlog p(x) \u2265 \u2212DKL(q\u03c6(z|x)||p(z)) + Eq\u03c6(z|x)[log p\u03b8(x|z)] = LV AE(x), (1)\nwhere q\u03c6(z|x) is an approximate distribution of posterior p(z|x) and \u03c6 is the model parameter of q. We designate q\u03c6(z|x) as encoder and p\u03b8(x|z) as decoder. Moreover, in Equation 1, the first term represents a regularization. The second one represents a negative reconstruction error.\nTo optimize the lower bound L(x) with respect to parameters \u03b8, \u03c6, we estimate gradients of Equation 1 using stochastic gradient variational Bayes (SGVB). If we consider q\u03c6(z|x) as Gaussian distribution N (z;\u00b5, diag(\u03c32)), where \u03c6 = {\u00b5,\u03c32}, then we can reparameterize z \u223c q\u03c6(z|x) to z = \u00b5 + \u03c3 \u2299 \u01eb, where \u01eb \u223c N (0, I). Therefore, we can estimate the gradients of the negative reconstruction term in Equation 1 with respect to \u03b8 and \u03c6 as \u2207\u03b8,\u03c6Eq\u03c6(z|x)[log p\u03b8(x|z)] = EN (\u01eb;0,I)[\u2207\u03b8,\u03c6 log p\u03b8(z|\u00b5 + \u03c3 \u2299 \u01eb)]. Because the gradients of the regularization term are solvable analytically, we can optimize Equation 1 with standard stochastic optimization methods."}, {"heading": "3.2 JOINT MULTIMODAL VARIATIONAL AUTOENCODERS", "text": "Next, we consider i.i.d. dataset (X,W) = {(x1,w1), ..., (xN ,wN )}, where two modalities x and w have different kinds of dimensions and structures2. Our objective is to generate two modalities bi-directionally. For that reason, we assume that these are conditioned independently on the same latent concept z: joint representation. Therefore, we assume their generating processes as z \u223c p(z) and x,w \u223c p(x,w|z) = p\u03b8x(x|z)p\u03b8w (w|z), where \u03b8x and \u03b8w represent the model parameters of each independent p. Figure 2(a) shows a graphical model that represents generative processes. One can see that this models joint distribution of all modalities, p(x,w). Therefore, we designate this model as a joint multimodal variational autoencoder (JMVAE).\n1According to Louizos et al. (2015), this independence might not be satisfied strictly because the encoder in CVAEs still has the dependence.\n2In our experiment, these depend on dataset, see Section 4.2.\nConsidering an approximate posterior distribution as q\u03c6(z|x,w), we can estimate a lower bound of the log-likelihood log p(x,w) as follows:\nLJM (x,w) = Eq\u03c6(z|x,w)[log p\u03b8(x,w, z)\nq\u03c6(z|x,w) ] (2)\n= \u2212DKL(q\u03c6(z|x,w)||p(z))\n+Eq\u03c6(z|x,w)[log p\u03b8x(x|z)] + Eq\u03c6(z|x,w)[log p\u03b8w(w|z)]. (3)\nEquation 3 has two negative reconstruction terms which are correspondent to each modality. As with VAEs, we designate q\u03c6(z|x,w) as the encoder and both p\u03b8x(x|z) and p\u03b8w(w|z) as decoders.\nWe can apply the SGVB to Equation 3 just as Equation 1, so that we can parameterize the encoder and decoder as deterministic deep neural networks and optimize them with respect to their parameters, \u03b8x, \u03b8w, and \u03c6. Because each modality has different feature representation, we should set different networks for each decoder, p\u03b8x(x|z) and p\u03b8w(w|z). The type of distribution and corresponding network architecture depends on the representation of each modality, e.g., Gaussian when the representation of modality is continuous, and a Bernoulli when it is a binary value.\nUnlike original VAEs and CVAEs, the JMVAE models joint distribution of all modalities. In this model, modalities are conditioned independently on a joint latent variable. Therefore, we can extract better representation that includes all information of modalities. Moreover, we can estimate both marginal distribution and conditional distribution in bi-directional, so that we can not only obtain images reconstructed themselves but also draw texts from corresponding images and vice versa. Additionally, we can extend JMVAEs to handle more than two modalities such as p(x,w1,w2, ...) in the same learning framework."}, {"heading": "3.3 INFERENCE MISSING MODALITIES", "text": "In the JMVAE, we can extract joint latent features by sampling from the encoder q\u03c6(z|x,w) at testing time. Our objective is to exchange modalities bi-directionally, e.g., images to texts and vice versa. In this setting, modalities that we want to sample are missing, so that inputs of such modalities are set to zero (the left panel of Figure 2(b)). The same is true of reconstructing a modality only from itself. This is a natural way in discriminative multimodal settings to estimate samples from unimodal information (Ngiam et al., 2011). However, if missing modalities are high-dimensional and complicated such as natural images, then the inferred latent variable becomes incomplete and generated samples might collapse.\nWe propose a method to solve this issue, which we designate as JMVAE-kl. Moreover, we describe the former way as JMVAE-zero to distinguish it. Suppose that we have encoders with a single input, q\u03c6x(z|x) and q\u03c6w(z|w), where \u03c6x and \u03c6w are parameters. We would like to train them by bringing their encoders close to an encoder q\u03c6(z|x,w) (the right panel of Figure 2(b)). Therefore, the object function of JMVAE-kl becomes\nLJMkl(\u03b1)(x,w) = LJM (x,w) \u2212 \u03b1 \u00b7 [DKL(q\u03c6(z|x,w)||q\u03c6x (z|x)) +DKL(q\u03c6(z|x,w)||q\u03c6w (z|w))], (4)\nwhere \u03b1 is a factor that regulates the KL divergence terms.\nFrom another viewpoint, maximizing Equation 4 can be regarded as minimizing the variation of information with variational learning on parameterized distributions (proven and derived in Appendix A). The variation of information, a measure of the distance between two variables, is written as\n\u2212EpD(x,w)[log p(x|w) + log p(w|x)], where pD is the data distribution. It is apparent that the variation of information is the sum of two negative conditional log-likelihoods. Therefore, minimizing the variation of information contributes to appropriate bi-directional exchange of modalities. Sohn et al. (2014) also train their model to minimize the VI for the same objective as ours. However, they use DBMs with MCMC training."}, {"heading": "4 EXPERIMENTS", "text": "This section presents evaluation of the qualitative and quantitative performance and confirms the JMVAE functionality in practice."}, {"heading": "4.1 DATASETS", "text": "As described herein, we used two datasets: MNIST and CelebA (Liu et al., 2015).\nMNIST is not a dataset for multimodal setting. In this work, we used this dataset for toy problem of multimodal learning. We consider handwriting images and corresponding digit labels as two different modalities. We used 50,000 as training set and the remaining 10,000 as a test set.\nCelebA consists of 202,599 color face images and corresponding 40 binary attributes such as male, eyeglasses, and mustache. In this work, we regard them as two modalities. This dataset is challenging because these have completely different kinds of dimensions and structures. Beforehand, we cropped the images to squares and resized to 64 \u00d7 64 and normalized. From the dataset, we chose 191,899 images that are identifiable face by OpenCV and used them for our experiment. We used 90% out of all the dataset contains as training set and the remaining 10% of them as test set."}, {"heading": "4.2 MODEL ARCHITECTURES", "text": "For MNIST, we considered images as x \u2208 R28\u00d728 and corresponding labels as w \u2208 {0, 1}10. We prepared two networks each with two dense layers of 512 hidden units and using leaky rectifiers and shared the top of each layers and mapped them into 64 hidden units. Moreover, we prepared two networks each with three dense layers of 512 units and set p(x|z) as Bernoulli and p(w|z) as categorical distribution whose output layer is softmax. We used warm-up (Bowman et al., 2015; S\u00f8nderby et al., 2016), which first forces training only of the term of the negative reconstruction error and then gradually increases the effect of the regularization term to prevent local minima during early training. We increased this term linearly during the first Nt epochs as with S\u00f8nderby et al. (2016). We set Nt = 200 and trained for 500 epochs on MNIST. Moreover, same as Burda et al. (2015); S\u00f8nderby et al. (2016), we resampled the binarized training values randomly from MNIST for each epoch to prevent over-fitting.\nFor CelebA, we considered face images as x \u2208 R32\u00d732\u00d73 and corresponding attributes as w \u2208 {\u22121, 1}40. We prepared two networks with layers (four convolutional and a flattened layers for x and two dense layers for w) with ReLU and shared the top of each layers and mapped them into 128 units. For the decoder, we prepared two networks, with a dense and four deconvolutional layers for x and three dense layers for w, and set Gaussian distribution for decoder of both modalities, where the variance of Gaussian was fixed to 1 for the decoder of w. In CelebA settings, we combined JMVAE with generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate clearer images. We considered the network of p(x|z) as generator in GAN, then we optimized the GAN loss with the lower bound of the JMVAE, which is the same way as a VAE-GAN model (Larsen et al., 2015). As presented herein, we describe this model as JMVAE-GAN. We set Nt = 20 and trained for 100 epochs on CelebA.\nWe used the Adam optimization algorithm (Kingma & Ba, 2014) with a learning rate of 10\u22123 on MNIST and 10\u22124 on CelebA. The models were implemented using Theano (Team et al., 2016) and Lasagne (Dieleman et al., 2015)."}, {"heading": "4.3 QUANTITATIVE EVALUATION", "text": ""}, {"heading": "4.3.1 EVALUATION METHOD", "text": "For this experiment, we estimated test log-likelihood to evaluate the performance of model. This estimate roughly corresponds to negative reconstruction error. Therefore, higher is better. From this performance, we can find that not only whether the JMVAE can generate samples properly but also whether it can obtain joint representation properly. If the log-likelihood of a modality is low, representation for this modality might be hurt by other modalities. By contrast, if it is the same or higher than model trained on a single modality, then other modalities contribute to obtaining appropriate representation.\nWe estimate the test marginal log-likelihood and test conditional log-likelihood on JMVAE. We compare the test marginal log-likelihood against VAEs (Kingma & Welling, 2013; Rezende et al., 2014) and the test conditional log-likelihood against CVAEs (Kingma et al., 2014; Sohn et al., 2015) and CMMAs (Pandey & Dukkipati, 2016). On CelebA, we combine all competitive models with GAN and describe them as VAE-GAN, CVAE-GAN, and CMMA-GAN. For fairness, architectures and parameters of these competitive models were set to be as close as possible to those of JMVAE.\nWe calculate the importance weighted estimator (Burda et al., 2015) from lower bounds at testing time because we would like to estimate the true test log-likelihood from lower bounds. To estimate the test marginal log-likelihood p(w) of the JMVAE, we use two possible lower bounds: sampling from q\u03c6(z|x,w) or q\u03c6x(z|x). We describe the former lower bound as the multiple lower bound and the latter one as the single lower bound. When we estimate the test conditional loglikelihood log p(x|w), we also use two lower bounds, each of which is estimated by sampling from q\u03c6(z|x,w) (multiple) or q\u03c6w(z|w) (single) (see Appendix B for more details). To estimate the single lower bound, we should approximate the single encoder (q\u03c6x(z|x) or q\u03c6w(z|w)) by JMVAE-zero or JMVAE-kl. When the value of log-likelihood with the single lower bound is the same or larger than that with the multiple lower bound, the approximation of the single encoder is good. Note that original VAEs use a single lower bound and that CVAEs and CMMAs use a multiple lower bound."}, {"heading": "4.3.2 MNIST", "text": "Our first experiment evaluated the test marginal log-likelihood and compared it with that of the VAE on MNIST dataset. We trained the model with both JMVAE-zero and JMVAE-kl and confirmed these differences. As described in Section 4.3.1, we have two possible ways of estimating the marginal log-likelihood of the JMVAE, i.e., multiple and single lower bounds. The left of Table 1 shows the test marginal log-likelihoods of the VAE and JMVAE. It is apparent that log-likelihood of the JMVAE-zero is the same or slightly better than that of the VAE. In the case of the loglikelihood of JMVAE-kl, the log-likelihood becomes better as \u03b1 is small. Especially, JMVAE-kl with \u03b1 = 0.01 and single lower bound archives the highest log-likelihood in Table 1. If \u03b1 is 1, however, then the test log-likelihood on JMVAE-kl becomes much lower. This is because the influence of the regularization term becomes strong as \u03b1 is large.\nNext, we evaluated the test conditional log-likelihood and compared it with that of the CVAE and CMMA conditioned on w. As in the case of the marginal log-likelihood, we can estimate the JMVAE\u2019s conditional log-likelihood by both the single and multiple lower bound. The single bound can be estimated using JMVAE-zero or JMVAE-kl. The right of Table 1 shows the test conditional log-likelihoods of the JMVAE, CVAE, and CMMA. It is apparent that the CVAE achieves the highest log-likelihood. Even so, in the case of multiple bound, log-likelihoods with both JMVAE-zero and JMVAE-kl (except \u03b1 = 1) outperform that of the CMMA.\nIt should be noted that the log-likelihood with JMVAE-zero and single bound is significantly low. As described in Section 3.3, this is because a modality w is missing as input. By contrast, it is apparent that the log-likelihood with JMVAE-kl is improved significantly from that with JMVAEzero. It shows that JMVAE-kl solves the issue of missing modalities (we can also find this result in generated images, see Appendix E). Moreover, we find that this log-likelihood becomes better as \u03b1 is large, which is opposite to the other results. Therefore, there is a trade-off between whether each modality can be reconstructed properly and whether multiple modalities can be exchanged properly and it can be regulated by \u03b1."}, {"heading": "4.3.3 CELEBA", "text": "In this section, we used CelebA dataset to evaluate the JMVAE. Table 2 presents the evaluations of marginal and conditional log-likelihood. From this table, it is apparent that values of both marginal and conditional log-likelihood with JMVAEs are larger than those with other competitive methods. Moreover, comparison with Table 1 shows that the improvement on CelebA is greater than that on MNIST, which suggests that joint representation with multiple modalities contributes to improvement of the quality of the reconstruction and generation in the case in which an input modality is large-dimensioned and complicated."}, {"heading": "4.4 QUALITATIVE EVALUATION", "text": ""}, {"heading": "4.4.1 JOINT REPRESENTATION ON MNIST", "text": "In this section, we first evaluated that the JMVAE can obtain joint representation that includes the information of modalities. Figure 3 shows the visualization of latent representation with the VAE, CVAE, and JMVAE on MNIST. It is apparent that the JMVAE obtains more discriminable latent representation by adding digit label information. Figure 3(b) shows that, in spite of using multimodal information as with the JMVAE, points in CVAE are distributed irrespective of labels because CVAEs force latent representation to be independent of label information, i.e., it is not objective for CVAEs to obtain joint representation."}, {"heading": "4.4.2 GENERATING FACES FROM ATTRIBUTES AND JOINT REPRESENTATION ON CELEBA", "text": "Next, we confirm that JMVAE-GAN on CelebA can generate images from attributes. Figure 4(a) portrays generated faces conditioned on various attributes. We find that we can generate an average face of each attribute and various random faces conditioned on a certain attributes. Figure 4(b) shows that samples are gathered for each attribute and that locations of each variation are the same irrespective of attributes. From these results, we find that manifold learning of joint representation with images and attributes works well."}, {"heading": "4.4.3 BI-DIRECTIONAL GENERATION BETWEEN FACES AND ATTRIBUTES ON CELEBA", "text": "Finally, we demonstrate that JMVAE-GAN can generate bi-directionally between faces and attributes. Figure 5 shows that MVAE-GAN can generate both attributes and changed images conditioned on various attributes from images which had no attribute information. This way of generating an image by varying attributes is similar to the way of the CMMA (Pandey & Dukkipati, 2016). However, the CMMA cannot generate attributes from an image because it only generates images from attributes in one direction."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this paper, we introduced a novel multimodal learning model with VAEs, the joint multimodal variational autoencoders (JMVAE). In this model, modalities are conditioned independently on joint representation, i.e., it models a joint distribution of all modalities. We further proposed the method (JMVAE-kl) of reducing the divergence between JMVAE\u2019s encoder and a prepared encoder of each modality to prevent generated samples from collapsing when modalities are missing. We confirmed that the JMVAE can obtain appropriate joint representations and high log-likelihoods on MNIST\n4https://en.wikipedia.org/wiki/Mona Lisa 4https://en.wikipedia.org/wiki/Wolfgang Amadeus Mozart\nand CelebA datasets. Moreover, we demonstrated that the JMVAE can generate multiple modalities bi-directionally on the CelebA dataset.\nIn future work, we would like to evaluate the multimodal learning performance of JMVAEs using various multimodal datasets such as containing three or more modalities."}, {"heading": "A RELATION BETWEEN THE OBJECTIVE OF JMVAE-KL AND THE VARIATION", "text": "OF INFORMATION\nThe variation of information can be expressed as \u2212EpD(x,w)[log p(x|w) + log p(w|x)], where pD is the data distribution. In this equation, we specifically examine the sum of two negative loglikelihoods and do not consider the expectation in this derivation. We can calculate the lower bounds of these log-likelihoods as follows:\nlog p(x|w) + log p(w|x) \u2265 Eq(z|x,w)[log p(x|z)p(z|w)\nq(z|x,w) ] + Eq(z|x,w)[log\np(w|z)p(z|x)\nq(z|x,w) ]\n= Eq(z|x,w)[log p(x|z)] + Eq(z|x,w)[log p(w|z)]\n\u2212DKL(q(z|x,w)||p(z|x)) \u2212DKL(q(z|x,w)||p(z|w))\n= LJM (x,w)\u2212 [DKL(q(z|x,w)||p(z|x)) +DKL(q(z|x,w)||p(z|w))]\n+DKL(q(z|x,w)||p(z)). (5)\nIf all the probability distributions are parameterized in neural networks, we can consider each p(z|x) and p(z|w) as q(z|x) and q(z|w), respectively. This is because both p(z|x) and q(z|x) (or both p(z|w) and q(z|w)) can be expressed as same network architectures. Therefore, the replaced Equation 5 can be calculated as follows:\nLJM (x,w) \u2212 [DKL(q(z|x,w)||q(z|x)) +DKL(q(z|x,w)||q(z|w))] +DKL(q(z|x,w)||p(z))\n= LJMkl(1)(x,w) +DKL(q(z|x,w)||p(z)) \u2265 LJMkl(1)(x,w), (6)\nwhere LJMkl(1) is Equation 4 with \u03b1 = 1. Therefore, maximizing Equation 4 is regarded as minimizing the variation of information with variational learning on parameterized distributions, i.e., maximizing the lower bounds of the negative variation of information."}, {"heading": "B TEST LOWER BOUNDS", "text": "Two lower bounds used to estimate test marginal log-likelihood p(x) of the JMVAE are as follows:\nLsingle(x) = Eq\u03c6x (z|x)[log p\u03b8x(x|z)p(z)\nq\u03c6x(z|x) ], (7)\nLmultiple(x) = Eq\u03c6(z|x,w)[log p\u03b8x(x|z)p(z)\nq\u03c6(z|x,w) ]. (8)\nWe can also estimate test conditional log-likelihood p(x|w) from these two lower bounds as\nLsingle(x|w) = Eq\u03c6w (z|w)[log p(x, z|w)\nq\u03c6w (z|w) ] = Eq\u03c6w (z|w)[log\np\u03b8x(x|z)p\u03b8w (w|z)p(z)\nq\u03c6w(z|w) ]\u2212 log p(w), (9)\nLmultiple(x|w) = Eq\u03c6(z|x,w)[log p\u03b8x(x|z)p\u03b8w (w|z)p(z)\nq\u03c6(z|x,w) ]\u2212 log p(w), (10)\nwhere log p(w) = logEp(z)[p\u03b8w(w|z)] = log 1\nNw\n\u2211Nw i p\u03b8w(w|z\n(i)) and z(i) \u223c p(z). In this paper, we set Nw = 5, 000 on MNIST and Nw = 10 on CelebA.\nWe can obtain a tighter bound on the log-likelihood by k-fold importance weighted sampling. For example, we obtain an importance weighted bound on log p(x) from Equation 11 as follows:\nlog p(x) \u2265 Ez1,...,zk\u223cq\u03c6x (z|x)[log 1\nk\nk\u2211\ni=1\np\u03b8x(x|z)p(z)\nq\u03c6x(z|x) ] = Lksingle(x). (11)\nStrictly speaking, these two lower bounds are not equal. However, if the number of importance samples is extremely large, the difference of these two lower bounds converges to 0.\nProof. Let the multiple and single k-hold importance weighted lower bounds as Lksingle and L k single. From the theorem of the importance weighted bound, both Lksingle and L k multiple converge to log p(x) as k \u2192 \u221e.\nTherefore,\nlimk\u2192\u221e|L k multiple \u2212 L k single| \u2264 |limk\u2192\u221eL k multiple \u2212 limk\u2192\u221eL k single| = 0"}, {"heading": "C RECONSTRUCTED IMAGES", "text": "Figure 6 presents a comparison of the original image and reconstructed image by the JMVAE on both MNIST and CelebA datasets. It is apparent that the JMVAE can reconstruct the original image properly with either a multiple or single encoder."}, {"heading": "D TEST JOINT LOG-LIKELIHOOD ON MNIST", "text": "Table 3 shows the joint log-likelihood of the JMVAE on MNIST dataset by both JMVAE-zero and JMVAE-kl. It is apparent that the log-likelihood test on both approaches is almost identical (strictly, JMVAE-zero is slightly lower). The test log-likelihood on JMVAE-kl becomes much lower if \u03b1 is large.\nE IMAGE GENERATION FROM CONDITIONAL DISTRIBUTION ON MNIST\nFigure 7 presents generation samples of x conditioned on single input w. It is apparent that the JMVAE with JMVAE-kl generates conditioned digit images properly, although that with JMVAEzero cannot generate them. As results showed, we also confirmed qualitatively that JMVAE-kl can model q\u03c6x(z|x) properly compared to JMVAE-zero."}], "references": [{"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schlter", "Colin Raffel", "Eben Olson", "Sren Kaae Snderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly", "Jeffrey De Fauw", "Michael Heilman", "Diogo Moitinho de Almeida", "Brian McFee", "Hendrik Weideman", "Gbor Takcs", "Peter de Rivaz", "Jon Crall", "Gregory Sanders", "Kashif Rasul", "Cong Liu", "Geoffrey French", "Jonas Degrave"], "venue": "URL http://dx.doi.org/10.5281/zenodo.27878", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep convolutional inverse graphics network", "author": ["Tejas D Kulkarni", "William F Whitney", "Pushmeet Kohli", "Josh Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1512.09300,", "citeRegEx": "Larsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2015}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The variational fair auto encoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": "arXiv preprint arXiv:1511.00830,", "citeRegEx": "Louizos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2015}, {"title": "Generating images from captions with attention", "author": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Variational methods for conditional multimodal learning: Generating human faces from attributes", "author": ["Gaurav Pandey", "Ambedkar Dukkipati"], "venue": "arXiv preprint arXiv:1603.01801,", "citeRegEx": "Pandey and Dukkipati.,? \\Q2016\\E", "shortCiteRegEx": "Pandey and Dukkipati.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Improved multimodal deep learning with variation of information", "author": ["Kihyuk Sohn", "Wenling Shang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Ladder variational autoencoders", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.02282,", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava and Salakhutdinov.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2012}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "Team et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Team et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "arXiv preprint arXiv:1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "To discover such relations, deep neural network architectures have been used widely for multimodal learning (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).", "startOffset": 108, "endOffset": 162}, {"referenceID": 16, "context": "Among them, generative approaches using deep Boltzmann machines (DBMs) (Srivastava & Salakhutdinov, 2012; Sohn et al., 2014) offer the important advantage that these can generate modalities bi-directionally.", "startOffset": 71, "endOffset": 124}, {"referenceID": 14, "context": "Recently, variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) have been proposed to estimate flexible deep generative models by variational inference methods.", "startOffset": 42, "endOffset": 88}, {"referenceID": 6, "context": "Some studies have addressed to handle such large-scale and high-dimensional modalities on VAEs, but they are forced to model conditional distribution (Kingma et al., 2014; Sohn et al., 2015; Pandey & Dukkipati, 2016).", "startOffset": 150, "endOffset": 216}, {"referenceID": 17, "context": "Some studies have addressed to handle such large-scale and high-dimensional modalities on VAEs, but they are forced to model conditional distribution (Kingma et al., 2014; Sohn et al., 2015; Pandey & Dukkipati, 2016).", "startOffset": 150, "endOffset": 216}, {"referenceID": 9, "context": "We used the CelebA dataset (Liu et al., 2015) to train and test models in this example.", "startOffset": 27, "endOffset": 45}, {"referenceID": 14, "context": "Recently, VAEs (Kingma & Welling, 2013; Rezende et al., 2014) are used to train such highdimensional modalities.", "startOffset": 15, "endOffset": 61}, {"referenceID": 6, "context": "Many studies are based on CVAEs to train various multiple modalities such as handwriting digits and labels (Kingma et al., 2014; Sohn et al., 2015), object images and degrees of rotation (Kulkarni et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 17, "context": "Many studies are based on CVAEs to train various multiple modalities such as handwriting digits and labels (Kingma et al., 2014; Sohn et al., 2015), object images and degrees of rotation (Kulkarni et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 7, "context": ", 2015), object images and degrees of rotation (Kulkarni et al., 2015), face images and attributes (Larsen et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 8, "context": ", 2015), face images and attributes (Larsen et al., 2015; Yan et al., 2015), and natural images and captions (Mansimov et al.", "startOffset": 36, "endOffset": 75}, {"referenceID": 21, "context": ", 2015), face images and attributes (Larsen et al., 2015; Yan et al., 2015), and natural images and captions (Mansimov et al.", "startOffset": 36, "endOffset": 75}, {"referenceID": 11, "context": ", 2015), and natural images and captions (Mansimov et al., 2015).", "startOffset": 41, "endOffset": 64}, {"referenceID": 12, "context": "This is a natural way in discriminative multimodal settings to estimate samples from unimodal information (Ngiam et al., 2011).", "startOffset": 106, "endOffset": 126}, {"referenceID": 9, "context": "As described herein, we used two datasets: MNIST and CelebA (Liu et al., 2015).", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "We used warm-up (Bowman et al., 2015; S\u00f8nderby et al., 2016), which first forces training only of the term of the negative reconstruction error and then gradually increases the effect of the regularization term to prevent local minima during early training.", "startOffset": 16, "endOffset": 60}, {"referenceID": 18, "context": "We used warm-up (Bowman et al., 2015; S\u00f8nderby et al., 2016), which first forces training only of the term of the negative reconstruction error and then gradually increases the effect of the regularization term to prevent local minima during early training.", "startOffset": 16, "endOffset": 60}, {"referenceID": 3, "context": "In CelebA settings, we combined JMVAE with generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate clearer images.", "startOffset": 82, "endOffset": 107}, {"referenceID": 8, "context": "We considered the network of p(x|z) as generator in GAN, then we optimized the GAN loss with the lower bound of the JMVAE, which is the same way as a VAE-GAN model (Larsen et al., 2015).", "startOffset": 164, "endOffset": 185}, {"referenceID": 20, "context": "The models were implemented using Theano (Team et al., 2016) and Lasagne (Dieleman et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 14, "context": "We compare the test marginal log-likelihood against VAEs (Kingma & Welling, 2013; Rezende et al., 2014) and the test conditional log-likelihood against CVAEs (Kingma et al.", "startOffset": 57, "endOffset": 103}, {"referenceID": 6, "context": ", 2014) and the test conditional log-likelihood against CVAEs (Kingma et al., 2014; Sohn et al., 2015) and CMMAs (Pandey & Dukkipati, 2016).", "startOffset": 62, "endOffset": 102}, {"referenceID": 17, "context": ", 2014) and the test conditional log-likelihood against CVAEs (Kingma et al., 2014; Sohn et al., 2015) and CMMAs (Pandey & Dukkipati, 2016).", "startOffset": 62, "endOffset": 102}, {"referenceID": 1, "context": "We calculate the importance weighted estimator (Burda et al., 2015) from lower bounds at testing time because we would like to estimate the true test log-likelihood from lower bounds.", "startOffset": 47, "endOffset": 67}], "year": 2016, "abstractText": "We investigate deep generative models that can exchange multiple modalities bidirectionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE\u2019s encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bidirectionally.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_369"}