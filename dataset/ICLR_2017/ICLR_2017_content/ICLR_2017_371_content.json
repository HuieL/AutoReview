{"name": "ICLR_2017_371.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "emails": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning has achieved remarkable success in many technological areas (Bengio et al., 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al., 2012; Hannun et al., 2014), natural language processing (Collobert et al., 2011; Mikolov et al., 2013; Cho et al., 2014), bioinformatics (Chicco et al., 2014), even high energy particle physics (Baldi et al., 2014). In particular, deep Convolutional Neural Networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan and Zisserman, 2015) have been a critical enabling technique for analyzing images and sequential data.\nFollowing the unprecedented success of deep networks, there has been some theoretical work (e.g., Arora et al. (2014; 2015); Paul and Venkatasubramanian (2014)) that suggest several mathematical models for different deep learning architectures. However, theoretical analysis and understanding lag behind the very rapid evolution and empirical success of deep architectures, and more theoretical analysis is needed to better understand the state-of-the-art deep architectures, and possibly to improve them further.\nIn this paper, we attempt to address the gap between the empirical success and theoretical understanding of the Convolutional Neural Nets, in particular its invertibility (i.e., reconstructing the input from the hidden activations), by analyzing a simplified mathematical model using random weights.1\nThis property is intriguing because convolutional neural networks are typically trained with discriminative objectives (i.e., unrelated to reconstruction) with a large amount of labels, such as the ImageNet dataset. For example, Dosovitskiy and Brox (2016) used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain. In other related work, Zhao et al. (2016) proposed stacked a what-where network via a (deconvolutional) decoder and demonstrate its promise in unsupervised and semi-supervised settings. Bruna et al. (2014) studied signal discovery from generalized pooling operators using image patches on non-convolutional small scale networks and datasets. Zhang et al. (2016) showed that CNNs discriminately trained for image classification (e.g., VGG Net (Simonyan and Zisserman, 2015)) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.\nWe introduce three new concepts that, coupled with the accepted notion that images have sparse representations, guide our understanding of CNNs:\n1For more discussion about random filters, see Sections 2.1 and 4.1.\n1. we provide a particular model of sparse linear combinations of the learned filters that are consistent with natural images; also, this model of sparsity is itself consistent with the feedforward network;\n2. we show that the effective matrices that capture explicitly the convolution of multiple filters exhibit a model-Restricted Isometry Property (model-RIP) (Baraniuk et al., 2010); and\n3. our model can explain each layer of the feedforward CNN algorithm as one iteration of Iterative Hard Thresholding (IHT) (Blumensath and Davies, 2009) for model-based compressive sensing and, hence, we can reconstruct the input simply and accurately.\nIn other words, we give a theoretical connection to a particular model of model-based compressive sensing (and its recovery algorithms) and CNNs. We show empirically that large-scale deep convolution networks are consistent with our mathematical analysis. We then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images, using filters from trained networks. Finally, we observe that it makes a significant difference which filters one uses for encoding and decoding, whether they are trained specifically for reconstruction, or random, or the same for both procedures. This paper explores these properties and elucidate specific empirical aspects that any more sophisticated mathematical model should take into account.2"}, {"heading": "2 PRELIMINARIES", "text": "In this section, we set the stage for our mathematical analysis in Section 3. We begin with discussion on the use of random weights in (convolutional) neural networks, and then provide the definitions and models for CNNs. Then, we discuss compressive sensing and sparse signal recovery. We define a particular model of sparsity that we will use throughout our analysis and detail the Iterative Hard Thresholding (IHT) algorithm which is the basis of our reconstruction analysis.\nIn order to simplify our notation and to make clear our analysis, we focus on a single layer in the analysis instead of multiple layers.3 Also, we assume that all of our input signals are vectors rather than matrices and that any operations we would ordinarily carry out on images (e.g., convolving with a filter bank, dividing into regions over which we pool coefficients), we do on vectors with the appropriate modifications for a simplified structure. While these assumptions ease our exposition, they do not change the nature of our arguments nor their implications for images. Furthermore, we demonstrate the validity of our results in two-dimensional natural images."}, {"heading": "2.1 EFFECTIVENESS OF GAUSSIAN RANDOM FILTERS", "text": "We analyze theoretically CNNs with Gaussian random filters, which have been surprisingly effective in unsupervised and supervised deep learning tasks. Jarrett et al. (2009) showed that random filters in 2-layer CNNs work well for image classification. In addition, Saxe et al. (2011) observed that convolutional layer followed by pooling layer is frequency selective and translation invariant, even with random filters, and these properties lead to good performance for object recognition tasks. On the other hand, Giryes et al. (2016) proved that CNNs with random Gaussian filters have metric preservation property, and they argued that the role of training is to select better hyperplanes discriminating classes by distorting boundary points among classes. According to their observation, random filters are in fact a good choice if training data are initially well-separated. Also, He et al. (2016) empirically showed that random weight CNNs can do image reconstruction well.\nTo better demonstrate the effectiveness of Gaussian random CNNs, we evaluate their classification performance on CIFAR-10; see Section 4.1 for details. We find that a 3-layer Gaussian random CNN is able to achieve \u223c 75% accuracy on the test set, with only the last classifier layer optimized, (see Table 1 for more details). Even though this number is far from the state-of-the-art results, it is surprisingly good considering the networks are almost untrained. Our theoretical results may provide another new perspective on explaining these phenomena.\n2We note that our model may not be an exact replica of a real setting, but for mathematical analysis, it is a simplified but representative abstraction of practical settings. A number of works show that random weight CNNs still achieve surprisingly good classification accuracy although they may not match the state-of-the-art results; see Sections 2.1 and 4.1 for more discussion.\n3We can extend the equivalency on a single layer of CNNs to multiple layer CNNs simply by using the output on one layer as the input to another, still using the steps of the inner loop of IHT."}, {"heading": "2.2 CONVOLUTIONAL NEURAL NETS", "text": "We define a single layer of our CNN as follows. We assume that the input signal x consists of M channels, each of length D, and we write x \u2208 RMD. For each of the input channels, m = 1, . . . ,M , let wi,m, i = 1, . . . ,K denote one of K filters, each of length `. Let t be the stride length, the number of indices by which we shift each filter. Note that t can be larger than 1.\nWe assume that the number of shifts, n = (D\u2212 `)/t+ 1, is an integer. Let wji,m be a vector of length D that consists of the (i,m)-th filter shifted by jt, j = 0, . . . , n\u22121 (i.e., wji,m has at most ` non-zero entries). We will concatenate over the M channels each of these vectors (as row vectors) to form a large matrix, W , which is the Kn\u00d7MD matrix made up of K blocks of the n shifts of each filter in each of M channels. We assume that Kn \u2265MD. We also assume that the Kn row vectors of W span RMD and that we have normalized the rows so that they have unit `2 norm. We assume that the hidden units of the feed-forward CNN are computed by multiplying an input signal x \u2208 RMD by the matrix W (i.e., convolving, in each channel, by a filter bank of size K, and summing over the channels to obtain Kn outputs), applying the ReLU function to the Kn outputs, and then selecting the value with maximum absolute value in each of the K blocks; i.e., we perform max pooling over each of the convolved filters and sum over the channels.4 We use h = Wx for the hidden activation computed by a single layer CNN without pooling. Figure 1 illustrates the architecture."}, {"heading": "2.3 COMPRESSIVE SENSING", "text": "Let \u03a6 be a i\u00d7j matrix with j > i. We say that \u03a6 satisfies the Restricted Isometry Property RIP(k, \u03b4k) (or, just RIP) if there is a distortion factor \u03b4k > 0 such that for all z \u2208 Rj with exactly k non-zero entries, (1\u2212 \u03b4k)\u2016z\u201622 \u2264 \u2016\u03a6z\u201622 \u2264 (1 + \u03b4k)\u2016z\u201622. If \u03a6 satisfies RIP (for appropriate sparsity level k and sufficiently small \u03b4k) and if z \u2208 Rj is k-sparse, then, given the vector x = \u03a6z \u2208 Ri, we can efficiently recover z (see Cand\u00e9s (2008) for more details)5. There are many efficient algorithms for doing so, including `1 sparse coding (e.g., `2 minimization with `1 regularization) and greedy, iterative algorithms (such as Iterative Hard Thresholding or IHT).\nModel-based compressive sensing. While sparse signals are a natural model for some applications, they are less realistic for CNNs. We consider a vector z \u2208 RKn as the true sparse code for generating the CNN input x with a particular model of sparsity. Rather than permitting k non-zero entries anywhere in the vector z, we divide the support of z into K contiguous blocks of size n and we stipulate that from each block there is at most one non-zero entry in z with a total of k non-zero\n4The convolution can be computed more efficiently than a straight-forward matrix multiplication, but they are mathematically equivalent.\n5We note that this is a sufficient condition and that there are other, less restrictive sufficient conditions, as well as more complicated necessary conditions. Furthermore, we have not given the exact, quantitative relations amongst the parameters. For simplicity, we stick with this definition.\nentries. We call a vector with this sparsity model model-k-sparse and denote the union of all ksparse subspaces with this structure Mk. It is clear that Mk contains nk ( K k ) subspaces. In our analysis, we consider linear combinations of two model-k-sparse signals. To be precise, suppose that z = \u03b11z1 + \u03b12z2 is the linear combination of two elements inMk. Then, we say that z lies in the linear subspaceM2k that consists of all linear combinations of vectors fromMk. We say that a matrix \u03a6 satisfies the model-RIP condition for parameter k if, there is a distortion factor \u03b4k > 0 such that, for all z \u2208Mk,\n(1\u2212 \u03b4k)\u2016z\u201622 \u2264 \u2016\u03a6z\u201622 \u2264 (1 + \u03b4k)\u2016z\u201622. (1)\nSee Baraniuk et al. (2010) for the definitions of model sparse and model-RIP, as well as the necessary modifications to account for signal noise and compressible (as opposed to exactly sparse) signals (which we have neglected to consider to keep our analysis simple). Intuitively speaking, a matrix that satisfies the model-RIP is a nearly an orthonormal matrix for a particular set of sparse vectors with a particular sparsity model or pattern.\nFor our analysis, we also need matrices \u03a6 that satisfy the model-RIP condition for vectors z \u2208M2k. We denote the distortion factor \u03b42k for such matrices. Note that \u03b4k \u2264 \u03b42k < 1.\nAlgorithm 1 Model-based IHT Input: model-RIP matrix \u03a6, measure-\nments x = \u03a6z, structured sparse approximation algorithm M\nOutput: k-sparse approximation z 1: Initialize z0 = 0, d = x, i = 0 2: while stopping criteria not met do 3: i\u2190 i+ 1 4: b\u2190 zi\u22121 + \u03a6Td 5: zi \u2190M(b, k) 6: d\u2190 x\u2212\u03a6zi 7: end while 8: return z \u2190 zi\nMany efficient algorithms have been proposed for sparse coding and compressive sensing (Olshausen et al., 1996; Mallat and Zhang, 1993; Beck and Teboulle, 2009). As with traditional compressive sensing, there are efficient algorithms for recovering model-k-sparse signals from measurements (see Baraniuk et al. (2010)), assuming the existence of an efficient structured sparse approximation algorithm M, that given an input vector and the sparsity parameter, returns the vector closest to the input with the specified sparsity structure.\nIn convolutional neural networks, the max pooling operator finds the downsampled activations that are closest to the activations of the original size by retaining the most significant values. The max pooling can be viewed as two steps: 1) zeroing out the locally non-maximum values;\n2) downsampling the activations with the locally maximum values retained. To study the pooled activations with sparsity structures, we can recover dimension loss from the second step (downsampling step) by an unsampling operator. This procedure defines our structured sparse approximation algorithm M(z, k), where z is the original (unpooled) code, and k is the sparsity parameter for further sparsification, which guarantees that M(z, k) is a model-k-sparse signal. With the standard layered formulation for neural networks, we have\nM(z, k) = block-sparsify(upsample(max-pool(z), s), k), (2)\nwhere s denotes the upsampling switches that indicate where to place the non-zero values in the upsampled activations. Taking the pooling switches known from the max pooling operation as s, we specifically define M as the nesting of the max pooling and the unpooling with known switch. We define this special case as\nMknown(z, k) = block-sparsify(upsample(max-pool(z),max-pool-switch(z)), k). (3)\nAlternatively, using the fixed uniform switches as s, we specifically define M as the nesting of the max pooling and the naive unsampling, denoted by Mfixed. In the rest of this paper, our theoretical analysis are generic to any type of valid upsampling switches6, so we use M(z, k) to denote the structured sparse approximation algorithm without worrying about s. The two special cases Mknown and Mfixed are used in the empirical analysis when we need to specify M(z, k) as a fully concrete operator.\nThe main recovery algorithm that we focus on is a model-sparse version of Iterative Hard Thresholding (IHT) (see Blumensath and Davies (2009)), not because we are interested in recovering modelsparse signals, per se, but because one iteration of IHT for our model of sparsity captures exactly\n6Valid switches should place a non-zero value at exactly one location.\na feedforward CNN.7 Algorithm 1 describes the model-based IHT algorithm. In particular, the sequence of steps 4\u20136 in the middle IHT (without the outer iterative loop) is exactly one layer of a feedforward CNN. As a result, the theoretical analysis of IHT for model-based sparse signal recovery serves as a guide for how to analyze the approximation activations of a CNN."}, {"heading": "3 ANALYSIS", "text": "To motivate our more formal analysis, we begin with a simple example. Suppose that the matrix W is an orthonormal basis for RMD and define \u03a8 = [ W T \u2212W T ] .\nProposition 1. A one-layer CNN using the matrix \u03a8T , with no pooling, gives perfect reconstruction (with the matrix \u03a8) for any input vector x \u2208 RMD.\nProof. Because we have both the positive and the negative dot products of the signal with the basis vectors in ReLU(\u03a8Tx) = ReLU ([\nWx \u2212Wx\n]) , we have positive and negative versions of the hidden\nunits h+ = ReLU(Wx) and h\u2212 = ReLU(\u2212Wx) where we decompose h = Wx = h+ \u2212 h\u2212 into the difference of two non-negative vectors, the positive and the negative entries of h. From this decomposition, we can easily reconstruct the original signal via\n\u03a8 [ h+ h\u2212 ] = [ W T \u2212W T ] [h+ h\u2212 ] = W T (h+ \u2212 h\u2212) = W Th = W TWx = x.\nIn the example above, we have pairs of vectors (w,\u2212w) in our matrix \u03a8. This settings allow us to turn what would ordinarily be a nonlinear function, ReLU, into a linear one. In fact, the assumption that trained CNN filters come in positive and negative is validated by Shang et al. (2016), which makes a CNN much easier to analyze within the model compressed sensing framework.\nSuppose that we have a vector z that we split into positive and negative components, z = z+ \u2212 z\u2212, and that we synthesize (or construct) a signal x from z using the matrix [ W T \u2212W T ] . Then, we have [ W T \u2212W T ] [z+ z\u2212 ] = W T (z+ \u2212 z\u2212) = W Tz = x.\nNext, suppose that we multiply x = W Tz by the transpose of the same matrix, we find [ W \u2212W ] x =[\nWW Tz \u2212WW Tz\n] and, if we apply ReLU to this vector, we produce [ (WW Tz)+ (WW Tz)\u2212 ] a vector that is split\ninto its positive and negative components. To determine whether or not we have \u201creconstructed\u201d the vector z, the structure of the product WW T is crucial. In addition, this calculation shows that if we have both positive and negative pairs of filters or vectors, then the ReLU function applied to both the positive and negative dot products simply splits the vector into the positive and negative components. These components are then reassembled in the next computation. For this reason, in the analysis in the following sections, it is sufficient to consider W Tz = x and Wx = h with max pooling alone applied to h, assuming that all of the entries in the vectors are real numbers, rather than only non-negative."}, {"heading": "3.1 MODEL-RIP AND RANDOM FILTERS", "text": "Our first main result says that if we use Gaussian random filters in our CNN, then, with high probability, the transpose of the matrix W formed by the convolutions with these filters has the model-RIP property. In other words, Gaussian random filters generate a matrix whose transpose W T is almost an orthonormal transform for sparse signals with a particular sparsity pattern (that is consistent with our pooling procedure). The bounds in the theorem tell us that we must balance the size of the filters ` and the number of channels M against the sparsity of the hidden units k, the number of the filter banks K, the number of shifts n, the distortion parameter \u03b4k, and the failure probability . The proof is in Appendix A.\n7Multiple iterations of IHT can improve the quality of signal recovery. However, it is rather equivalent to the recurrent version of CNNs and does not fit to the scope of this work.\nTheorem 3.1. Assume that we have MK vectors wi,m of length ` in which each entry is a scaled i.i.d. (sub-)Gaussian random variable with mean zero and variance 1 (the scaling factor is 1/ \u221a M`). Let t be the stride length (where n = (D \u2212 `)/t+ 1) and build the structured random matrix W as the weight matrix in a single layer CNN for M -channel input dimension D. If\nM`2\nD \u2265 Ck\n\u03b42k\n( log(K) + log(n)\u2212 log( ) ) ,\nthen, with probability 1\u2212 , the MD \u00d7Kn matrix W T satisfies the model-RIP for modelMk with parameter \u03b4k.\nWe also note that the same analysis can be applied to the sum of two model-k-sparse signals, with changes in the constants (that we do not track here). Corollary 3.2. Random matrices with the CNN structure have, with high probability, the model-RIP property forM2k.\nOther examples of matrices that satisfy model-RIP (both empirically and via a less sophisticated analysis on the dot products between any two columns) include wavelets and localized Fourier bases; both examples that can be easily and efficiently implemented via convolutions in a CNN."}, {"heading": "3.2 RECONSTRUCTION BOUNDS", "text": "To distinguish the true sparse code z and its reconstruction, we use z\u0302 = M(h, k) = M(Wx, k) for the reconstruction by CNN. Our next result tells us that if we compute the hidden units h from an input signal x using a weight matrix W whose transpose has the model-RIP and using max pooling over each filter (z\u0302), then we can reconstruct (approximately) the input signal x simply by multiplying the hidden units by W . This result bounds the relative error between the approximate reconstruction x\u0302 and the input as a function of the distortion for the model-RIP. In our analysis, we assume that the input signal x = W Tz is a sparse linear combination of hidden activations, captured approximately by the filters in W . See Appendix B for the detailed proofs. Part of our analysis also shows that the hidden units z\u0302 are approximately the putative coefficient vector z in the sparse linear representation for the input signal. Theorem 3.3. We assume that W T satisfies theM2k-RIP with constant \u03b4k \u2264 \u03b42k < 1. If we use W in a single layer CNN both to compute the hidden units z\u0302 and to reconstruct the input x from these hidden units as x\u0302 so that x\u0302 = W TM(Wx, k), the error in our reconstruction is\n\u2016x\u0302\u2212 x\u20162 \u2264 5\u03b42k\n1\u2212 \u03b4k \u221a 1 + \u03b42k\u221a 1\u2212 \u03b42k \u2016x\u20162.\nRecall that the structured sparsity approximation algorithm M includes the downsampling caused by pooling and an unsampling operator. Theorem 3.3 is applicable to any type of upsampling switches, so our reconstruction bound is generic to the particular design choice on how to recover the activation size in a decoding neural network."}, {"heading": "4 EXPERIMENTAL EVIDENCE AND ANALYSIS", "text": "In this section, we provide experimental validation of our theoretical model and analysis. We first validate experimentally the relevance of our assumption by examining the effectiveness of random filter CNNs. We then provide an experimental validation of our theoretical analysis on the synthetic 1D case, then we provide experimental results on more realistic scenarios. In particular, we study popular deep neural networks trained for image classification on the ImageNet ILSVRC 2012 dataset (Deng et al., 2009). We calculate empirical model-RIP bounds for W T , showing that they are consistent with theory. Our results are also consistent with a long line of research shows that it is reasonable to model real, natural images as sparse linear combinations over learned dictionaries (e.g., Boureau et al. (2008); Le et al. (2013); Lee et al. (2008); Olshausen et al. (1996); Ranzato et al. (2007); Yang et al. (2010)). In addition, we verify our theoretical bounds for the reconstruction error \u2016x\u2212W T z\u0302\u20162/\u2016x\u20162 on real images. (This is the relative `2 distance between the original image and the reconstruction.) We investigate both randomly sampled filters and empirically learned filters in these experiments. Our implementation is based on the Caffe (Jia et al., 2014) and MatConvNet (Vedaldi and Lenc, 2015) toolboxes."}, {"heading": "4.1 EVALUATION OF GAUSSIAN RANDOM CNNS ON CIFAR-10", "text": "To show the practical relevance of our theoretical assumptions on using random filters for CNNs as stated in Section 2.1, we evaluate simple CNNs with Gaussian random filters (with i.i.d. zeromean unit-variance entries) on the CIFAR-10 dataset. The goal of this experiment is not to achieve state-of-the-art results, but to examine practical relevance of our assumption on random filter CNNs. Once the CNNs weights are initialized (randomly), they are fixed during the training of the classifiers. Specifically, we test random CNNs with 1, 2, and 3 convolutional layers, where we use ReLU as the activation. A 2\u00d72 max pooling layer follows each convolutional layer to down-sample the feature map.8 We experiment with different filter sizes (3, 5, 7) and numbers of channels (64, 128, 256, 1024, 2048) and report the classification accuracy of the best-performing architectures based on cross-validation in Table 1. We also report the best performance using learnable filters for comparison. More details about the architectures can be found in Section C.1 of the supplementary materials. We observe the CNNs with Gaussian random filters achieve surprisingly good classification performance (implying that they serve as reasonable representation of input data), although fully learnable CNN counterparts perform better. Our experimental results are also consistent with the observations made by Jarrett et al. (2009) and Saxe et al. (2011). Overall, these results seem to suggest that the CNNs with Gaussian random filters might be a reasonable setup which is amenable to mathematical analysis while not being too far off in terms of practical relevance."}, {"heading": "4.2 EXPERIMENTAL VALIDATION OF THE ANALYSIS IN 1D SYNTHETIC DATA", "text": "We use 1-D synthetic data to empirically show the basic validity of our theory in terms of the modelRIP condition in Equation (1) and reconstruction bound in Theorem 3.3. We plot the histograms of the empirical model-RIP values of 1D Gaussian random filters W ( scaled by 1/ \u221a lM ) with size l \u00d7 1\u00d7M \u00d7K = 5\u00d7 1\u00d7 32\u00d7 96 on 1DMk sparse signal z with size D = 32 and sparsity k = 10, whose non-zero elements are drawn from a uniform distribution on [\u22121, 1]. The histograms in Figure 2a and 2b are tightly centered around 1, suggesting that W T satisfies the model-RIP condition in Equation (1) and its corollary from Lemma B.1 in the supplementary materials. We also empirically show the reconstruction bound in Theorem 3.3 on synthetic vectors x = W Tz (Figure 2c). The reconstruction error is concentrated at around 0.1\u20130.2 and bound under 0.5. Results in Figure 2 suggests the practical validity of our theory when the model assumptions hold."}, {"heading": "4.3 ARCHITECTURES AND DATASET", "text": "We conduct the rest of our experimental evaluations on the 16-layer VGGNet (Model D in Simonyan and Zisserman (2015)),9 where the computation is carried out on images; e.g., convolution with a 2-D filter bank and pooling on square regions. In contrast to the theory, the realistic network does not pool activations over all the possible shifts for each filter, but rather on non-overlapping patches. The networks are trained for the large-scale ImageNet classification task, which is important for extending to other supervised tasks in vision. The main findings on VGGNet are presented in the rest of this section; we also provide some analysis on AlexNet (Krizhevsky et al., 2012) in the supplementary materials.\n8Implementation detail: We add a batch normalization layer together with a learnable scale and bias before the activation so that we do not need to tune the scale of the filters. The filter weights of the intermediate layers in the CNNs are not trained after random initialization. On top of the network, we use an optional average pooling layer to reduce the feature map size to 4\u00d7 4 and a dropout layer for better regularization before feeding the feature to a learnable soft-max classifier for image classification.\n9VGGNet is practically important as it is popularly used in the community and is one of the best-performing \u201csingle-pathway\u201d networks (i.e., no skip connections). We expect that the ResNet (e.g., trained from ImagNet) can also reconstruct images from its activations well in practice. However, the ResNet architectures are too complicated to be in the scope of our theory without further nontrivial customization.\nVGGNet contains five groups of convolution and pooling layers, each group has 2~3 convolutional layers followed by a pooling layer. We denote the j-th convolutional layer in the i-th group \u201cconv(i, j),\u201d and the pooling layer \u201cpool(i).\u201d When we say the activations/features are from i-th layer, we mean they are the output of pool(i). Our analysis is for single convolutional layers. When evaluating the i-th layer, we take the activations from the (i\u2212 1)-th layer, and investigate the filters and output of conv(i, 1)."}, {"heading": "4.4 2D MODEL-RIP", "text": "The key to our reconstruction bound is Theorem 3.3 is the model-RIP condition for our particular model of sparsity in Equation (1). We empirically evaluate the model-RIP property, i.e., \u2016W Tz\u2016/\u2016z\u2016, for real CNN filters of the pretrained VGGNet. We use two-dimensional coefficients (or hidden units) z (each block of coefficients is of size D \u00d7D), K filters of size `\u00d7 `, and pool the coefficients over smaller pooling regions (i.e., not over all possible shifts of each filter). The following experimental evidence suggest that the sparsity model and the model-RIP property of the filters are consistent with what we conclude from the mathematical analysis on the simpler one-dimensional case.\nTo check the significance of the model-RIP property (i.e., how close \u2016W Tz\u2016/\u2016z\u2016 is to 1) in controlled settings, we first synthesize the hidden activations z with sparse uniform random variables, which fully agree with our model assumptions. The sparsity of z is constrained to the average level of the real CNN activations (refer to Table 2). Given the filters of a certain convolutional layer, we use the synthetic z (in equal position to this layer\u2019s output activations) to get statistics for the model-RIP property. To be consistent with succeeding experiments, we choose conv(5, 2), while other layers\nshow similar results. Figure 3 (a) summarizes the distribution of empirical model-RIP values, which is clearly centered around 1 and satisfies Equation (1) with a short tail roughly bounded by \u03b4k < 1. For more details of the algorithm, we normalize the filters from the conv(5, 2) layer, which are `\u00d7 ` (` = 3). All K = 512 filters with M = 512 input channels are used.10 We set D = 15 (the same as the output activations of conv(5, 2)) and use 2\u00d7 2 pooling regions11 (commonly used in recent deep networks). We generate 1000Mk randomly sampled sparse activation (z) maps by first sampling their non-zero supports and then filling elements on the supports uniformly from [\u22121, 1]. The sparsity is the same as that in conv(5, 1) activations.\nAlgorithm 2 Sparse hidden activation recovery Input: convolution matrix W , input activation/image x Output: hidden code z, satisfying our model-RIP assump-\ntion withMk and reconstructing x with W 1: zinit = argminz \u2016x\u2212W Tz\u2016 2\n2 + \u03bb\u2016z\u20161 2: zmodel = Mknown(zinit, k) 3: z = argminz \u2016x\u2212W Tz\u2016 2\n2 + \u03bb\u2016z\u20161, s.t. zi = 0 if zmodeli = 0 To conduct more realistic experiments, we observe the actual conv(5, 2) activations from VGGNet are not necessarily drawn from a model-sparse uniform distribution. This motivates us to evaluate the empirical model-RIP property on the hidden activations z that reconstruct the actual input activations x from conv(5, 1) by W Tz. Per theory, the x is given by a max pooling layer, so we constrain the sparsity (i.e., the size of the support set is no more than 1 in a pooling region for a single channel). We use a simple and efficient algorithm to recover z from x in Algorithm 2. The algorithm is inspired by \u201c`1 heuristic\" method that are commonly used in practice (e.g. Boyd (2015)). As shown in Algorithm 2, we first do `1-regularized least squares without constraining the support set. Max pooling is then applied to figure out the support set for each pooling region. In particular, we use Mknown, defined in (3), to zero out the locally non-maximum values without messing up the support structures. We perform `1-regularized least squares again on the fixed support set to recover the hidden activations satisfying the model sparsity. As shown in Figures 3 (b)\u2013(c), the empirical model-RIP property values for visual activations x from conv(5, 1) with/without ReLU are both close to 1. The center offset to 1 is less than 0.05 and the range bound \u03b4k is rough less then 0.05, which agrees with the theoretical bound (1) quite well.\nTo gain more insight, we summarize the learned filter coherence in Table 4 for all the convolutional layers in VGGNet.12 This measures the correlation or similarity between the columns of WT and is a proxy for the value of the model-RIP parameter \u03b4k (which we can only estimate computationally). The smaller the coherence, the smaller \u03b4k is, and the better the reconstruction. The coherence of the learned filters is not low, which is inconsistent with our theoretical assumptions. However, the model-RIP property turns out to be robust to this mismatch. It also demonstrates the strong invertibility of CNN in practice."}, {"heading": "4.5 RECONSTRUCTION BOUNDS", "text": "With model-RIP as a sufficient condition, Theorem 3.3 provides theoretical bounds for layer-wise reconstruction via x\u0302 = W TM(Wx, k). This operator consists of the projection and reconstruction in one IHT iteration. Without confusion, we refer to it as IHT for notational convenience. We investigate the practical reconstruction errors on Layer 1~4 activations (i.e., pool(1)~(4)) of VGGNet.\nTo encode and reconstruct intermediate activations of CNNs, we employ IHT with sparsity estimated from the real CNN activations on ILSVRC-2012 validation set (see Table 2). We also reconstruct input images, since CNN inversion is not limited to a single layer, and images are easier to visualize than hidden activations. To implement image reconstruction, we project the reconstructed activations into the image space via a pretrained decoding network as in (Zhang et al., 2016), which extends a similar autoencoder architecture as in (Dosovitskiy and Brox, 2016) to a stacked \u201cwhat-where\u201d autoencoder (Zhao et al., 2016). The reconstructed activations were scaled to have the same norm as the original activations so that we can feed them into the decoding network.\n10We do not remove any filters including those in approximate positive/negative pairs (refer to 3). 11In VGGNet, no pooling layer follows conv(5, 2). Here, we just use it in this way to analyze the convolutionpooling pair targeted by our theory. 12The coherence is defined as the maximum (in absolute value) dot product between distinct pairs of columns of the matrix WT , i.e. \u00b5 = maxi 6=j |WiWTj |, where Wi denote the i-th row of matrix W .\nAs an example, Figure 4 illustrates the image reconstruction results for the hidden activations of the 4-th layer, the ground truth of which is obtained by feeding natural images to the CNNs. Interestingly, the decoding network itself is powerful, since it can reconstruct the glimpse of images with Gaussian random input, as shown in Figure 4 (e). Object shapes are recovered by using the pooling switches only in the \u201cwhat-where\u201d autoencoder. This result suggests that it is important to determine which pooling units are active and then to estimate these values accurately. These steps are consistent with the steps in the inner loop of any iterative sparse signal reconstruction algorithm.\nIn Figure 4 (c), we take the pretrained conv(5, 1) filters for IHT. The images recovered from the IHT reconstructed 4-th layer activations are reasonable and the reconstruction quality is significantly better than the random input baseline. We also try Gaussian random filters (Figure 4 (d)), which agree more with the model assumptions (e.g., lower coherence, see Table 4). The learned filters from VGGNet perform equally well visually. IHT ties the encoder and decoder weights (no filter learning for the decoder), so it does not perform as well as the decoding network trained with a huge batch of data (Figure 4 (b)). Nevertheless, we show both theoretically and experimentally decent reconstruction bounds for these simple reconstruction methods on real CNNs. More visualization results for more layers are in the supplementary materials (Figure 5 in Section C.3).\nIn Table 3, we summarize reconstruction performance for all 4 layers. With random filters, the model assumptions hold and the IHT reconstruction is the best quantitatively. IHT with real CNN filters performs comparable to the best case and much better than the baseline established by the randomly sampled activations.\nAdditionally, reconstruction performance of IHT is strongly related to the filter coherence, summarized in Table 4. Lower coherence agrees more closely with the model assumptions and leads to higher reconstruction quality. Higher coherence yields worse recovery of the hidden activation (i.e., large \u2016z\u0302 \u2212 z\u2016, where z\u0302 is the hidden activations recovered by IHT, z is the true activation). Compared to Algorithm 2, (one-step) IHT is not so robust to high coherence.\nIn summary, when the assumption of i.i.d Gaussian randomness of the CNN filters holds, our theoretical reconstruction bound strictly match with the empirical observations. More importantly, we demonstrate that the bound can still reasonably hold in practice for discriminatively learned CNN layers, which is particularly true for layers with relatively lower coherence."}, {"heading": "5 CONCLUSION", "text": "We introduce three concepts that tie together a particular model of compressive sensing (and the associated recovery algorithms), the properties of learned filters, and the empirical observation\n13The relative error in activation space of random activations (the last column) are identical (1.414) for all layers because \u2016f \u2212 f\u0302\u2016/\u2016f\u2016 = \u221a 2 on average for Gaussian random f\u0302 provided \u2016f\u2016 = \u2016f\u0302\u2016.\nthat CNNs are (approximately) invertible. Our experiments show that filters in trained CNNs are consistent with the mathematical properties we present while the hidden units exhibit a much richer structure than mathematical analysis suggests. Perhaps simply moving towards a compressive, rather than exactly sparse, model for the hidden units will capture the sophisticated structure in these layers of a CNN or, perhaps, we need a more sophisticated model. Our experiments also demonstrate that there is considerable information captured in the switch units (or the identities of the non-zeros in the hidden units after pooling) that no mathematical model has yet expressed or explored thoroughly."}, {"heading": "A MATHEMATICAL ANALYSIS: MODEL-RIP AND RANDOM FILTERS", "text": "Theorem 3.1(Restated) Assume that we have MK vectors wi,m of length ` in which each entry is a scaled i.i.d. (sub-)Gaussian random variable with mean zero and variance 1 (the scaling factor is 1/ \u221a M`). Let t be the stride length (where n = (D \u2212 `)/t+ 1) and build the structured random matrix W as the weight matrix in a single layer CNN for M -channel input dimension D. If\nM`2\nD \u2265 Ck\n\u03b42k\n( log(K) + log(n)\u2212 log( ) ) ,\nthen, with probability 1\u2212 , the MD \u00d7Kn matrix W T satisfies the model-RIP for modelMk with parameter \u03b4k.\nProof. We note that this result follows the same structure of that for many proofs of the RIP for (structured) random matrices (see Park et al. (2011); Vershynin (2010) for details) although we make minor tweaks to account for the particular structure of W T .\nSuppose that z \u2208Mk which means that z consists of at most k non-zero entries that each appear in a distinct block of size n (there are a total of K blocks). First, we observe that the norm of W Tz is preserved in expectation.\nLemma A.1. E(\u2016W Tz\u201622) = \u2016z\u201622\nProof. Note that each entry of W T is either zero or Gaussian random variable w \u223c N(0, 1) (suitably normalized). Therefore, it is obvious that E(WW T ) = I since each row of W satisfies E ((\nwj1i,m1 )T ( wj2i,m2 )) = 0 if j1 6= j2 or m1 6= m2, and we normalized the random variables so\nthat E (\u2225\u2225\u2225\u2225[(wji,1)T , . . . ,(wji,M)T]\u2225\u2225\u2225\u2225\n2\n) = 1 for all j. Finally, we have\nE ( \u2016W Tz\u201622 ) = E ( zTWW Tz ) = zTE ( WW T ) z = zTz = \u2016z\u201622.\nLet y = W Tz. We aim to show that the square norm of the random variable \u2016y\u201622 concentrates tightly about its mean; i.e., with exceedingly low probability\u2223\u2223\u2223\u2016y\u201622 \u2212 \u2016z\u201622\u2223\u2223\u2223 > \u03b4\u2016z\u201622. To do so, we need several properties of sub-Gaussian and sub-exponential random variables. A mean-zero sub-Gaussian random variable Z has a moment generating function that satisfies\nE(exp(tZ)) \u2264 exp(t2C2) for all t \u2208 R and some constant C. The sub-Gaussian norm of Z, denoted \u2016Z\u2016\u03c82 is\n\u2016Z\u2016\u03c82 = sup p\u22651 1 \u221a p\n( E|Z|p )1/p .\nIf Z \u223c N(0, \u03c32), then \u2016Z\u2016\u03c82 \u2264 c\u03c3.\nA sub-exponential random variable X satisfies14 P ( |X| > t ) \u2264 exp(1\u2212 t/C)\n14There are two other equivalent properties. See Vershynin (2010) for details.\nfor all t \u2265 0. Let yi denote the ith entry of the vector y = W Tz. We can write\nyi = Kn\u2211 j=1 W Ti,jzj\nand observe that yi is a linear combination of i.i.d. sub-Gaussian random variables (or it is identically equal to 0) and, as such, is itself a sub-Gaussian random variable with mean zero and sub-Gaussian norm \u2016yi\u2016\u03c82 \u2264 C/ \u221a M`\u2016w\u2016\u03c82\u2016z\u20162 (see Vershynin (2010), Lemma 5.9). The structure of the random matrix and how many non-zero entries are in row i of W do enter the more refined bound on the sub-Gaussian norm of \u2016yi\u2016\u03c82 (again, see Vershynin (2010), Lemma 5.9 for details) but we ignore such details for this estimate as they are not necessary for the next estimate.\nTo obtain a concentration bound for \u2016yi\u201622, we recall from Park et al. (2011); Vershynin (2010) that the sum of squares of sub-Gaussian random variables tightly concentrate.\nTheorem A.2. Let Y1, . . . , YMD be independent sub-Gaussian random variables with sub-Gaussian norms \u2016Yi\u2016\u03c82 for all i = 1, . . . ,MD. Let T = maxi \u2016Yi\u2016\u03c82 . The for every t \u2265 0 and every a \u2208 RMD,\nP (\u2223\u2223\u2223MD\u2211 i=1 ai(Yi \u2212 EY 2i ) \u2223\u2223\u2223 \u2265 t) \u2264 2 exp(\u2212 C min( Ct2 T 2\u2016a\u201622 , Ct T\u2016a\u2016\u221e )) .\nWe note that although some entries yi may be identically zero, depending on the sparsity pattern of z, not all entries are. Let us define y\u0303i = yi\u2016yi\u2016\u03c82 so that \u2016y\u0303i\u2016\u03c82 = 1 and observe that\nP (\u2223\u2223\u2223\u2016y\u201622 \u2212 \u2016z\u201622\u2223\u2223\u2223 > \u03b4\u2016z\u201622) = P (\u2223\u2223\u2223MD\u2211 i=1 \u2016yi\u20162\u03c82(y\u0303 2 i \u2212 Ey\u03032i ) \u2223\u2223\u2223 > \u03b4\u2016z\u201622 ) .\nWe apply Theorem A.2 to the sub-Gaussian random variables y\u0303i with the weights \u2016yi\u20162\u03c82 . We have\n\u2016a\u201622 = MD\u2211 i=1 \u2016yi\u20164\u03c82 \u2264 CD\u2016w\u20164\u03c82\u2016z\u2016 4 2 M`2 and \u2016a\u2016\u221e \u2264 C\u2016w\u20162\u03c82\u2016z\u2016 2 2 M` .\nIf we set T = 1, t = \u03b4\u2016z\u201622, and use the above estimates for the norms of a, we have P (\u2223\u2223\u2223\u2016y\u201622 \u2212 \u2016z\u201622\u2223\u2223\u2223 > \u03b4\u2016z\u201622) \u2264 2 exp ( \u2212 C min (C\u03b42M`2 D\u2016w\u20164\u03c82 , C\u03b4M` \u2016w\u20162\u03c82 )) . (4)\nFinally, we use the concentration of measure result in a crude union bound to bound the failure probability over all vectors z \u2208Mk. We take nk ( K k ) \u2248 (nK)k and for a desired constant failure probability. Using the smaller term in Equation (4), (note that \u03b4 < 1, `/D < 1, and \u2016w\u2016\u03c82 \u2265 1) we have\nexp ( \u2212 C M` 2\u03b42\nD\u2016w\u20164\u03c82\n) exp ( k(log(K) + log(n)) ) \u2264 exp(log( ))\nwhich implies\nM`2\nD \u2265 k \u03b42 \u2016w\u20164\u03c82\n( log(K) + log(n)\u2212 log( ) ) = C k\n\u03b42\n( log(K) + log(n)\u2212 log( ) ) .\nTherefore, if design our matrix W as described and with the parameter relationship as above, the matrix W T with satisfy the model-RIP forMk and parameter \u03b4 with probability 1\u2212 .\nLet us discuss the relationship amongst the parameters in our result. First, if we have only one channel M = 1 and the filter length ` = D, then our bound on the number of measurements D matches those of traditional (model-based) compressive sensing; namely,\nD \u2265 C k \u03b42\n(log(K) + log(n)\u2212 log( )) . If ` < D (i.e., the filters are much shorter than the length of the input signal as in a CNN), then we can compensate by adding more channels; i.e., the filter length ` needs to be larger than \u221a D, or, if\nadd more channels, \u221a D/M ."}, {"heading": "B MATHEMATICAL ANALYSIS: RECONSTRUCTION BOUNDS", "text": "The consequences of having model-RIP are two-fold. The first is that if we assume that an input image is the structured sparse linear combination of filters, x = W Tz (where z \u2208 Mk and W T satisfies the model-RIP property), then we know an upper and lower bound on the norm of x in terms of the norm of its sparse coefficients, \u2016x\u20162 \u2264 (1\u00b1 \u03b4)\u2016z\u20162. Additionally,\n\u2016z\u20162 \u2264 1\u221a\n1\u2212 \u03b4 \u2016x\u20162.\nMore importantly, when we calculate the hidden units of x,\nh = ReLU(Wx) = ReLU(WW Tz)\nwe can see that the computation of h is nothing other than the first step of a reconstruction algorithm analogous to that of model-based compressed sensing. As a result, we have a bound on the error between h and z and we see that we can analyze the approximation properties of a feedfoward CNN and its linear reconstruction algorithm. In particular, we can conclude that a feedforward CNN and a linear reconstruction algorithm provide a good approximation to the original input image.\nTheorem 3.3(Restated) We assume that W T satisfies theM2k-RIP with constant \u03b4k \u2264 \u03b42k < 1. If we use W in a single layer CNN both to compute the hidden units z\u0302 and to reconstruct the input x from these hidden units as x\u0302 so that x\u0302 = W TM(Wx, k), the error in our reconstruction is\n\u2016x\u0302\u2212 x\u20162 \u2264 5\u03b42k\n1\u2212 \u03b4k \u221a 1 + \u03b42k\u221a 1\u2212 \u03b42k \u2016x\u20162.\nProof. To show this result, we recall the two following lemmas from Baraniuk et al. (2010) and rephrase them in the setting of a feedforward CNN.\nLemma B.1. Suppose W T hasMk-RIP with constant \u03b4k. Let \u2126 be a support corresponding to a subspace inMk. Then we have the following bounds:\n\u2016W\u2126x\u20162 \u2264 \u221a\n1 + \u03b4k\u2016x\u20162 (5) \u2016W\u2126W T\u2126 z\u20162 \u2264 (1 + \u03b4k)\u2016z\u20162 (6) \u2016W\u2126W T\u2126 z\u20162 \u2265 (1\u2212 \u03b4k)\u2016z\u20162 (7)\nLemma B.2. Suppose that W T hasM2k-RIP with constant \u03b42k. Let \u2126 be a support corresponding to a subspace ofMk and suppose that z \u2208Mk (not necessarily supported on \u2126). Then\n\u2016W\u2126W Tz|\u2126c\u20162 \u2264 \u03b42k\u2016z|\u2126c\u20162.\nLet \u03a0 denote the support of theMk sparse vector z. Set h = Wx and set z\u0302 to be the result of max pooling applied to the vector h, or the best fit (with respect to the `2 norm) to h in the modelMk. Let \u2126 denote the support set of z\u0302 \u2208Mk. For simplicity, we assume |\u03a0| = k = |\u2126|. Lemma B.3 (Identification). The support set, \u2126, of the switch units captures a significant fraction of the total energy in the coefficient vector z\n\u2016z|\u2126c\u20162 \u2264 2\u03b42k\n1\u2212 \u03b4k \u2016z\u20162.\nProof. Let h\u2126 and h\u03a0 be the vector h restricted to the support sets \u2126 and \u03a0, respectively. Since both are support sets forMk and since \u2126 is the best support set for h,\n\u2016h\u2212 h\u2126\u20162 \u2264 \u2016h\u2212 h\u03a0\u20162, and, after several calculations, we have\n\u2016h|\u2126\\\u03a0\u201622 \u2265 \u2016h|\u03a0\\\u2126\u201622.\nUsing Lemma B.2 and the size |(\u2126 \\\u03a0) \u22c3 \u03a0| \u2264 2k, we have\n\u2016h\u2126\\\u03a0\u20162 = \u2016W\u2126\\\u03a0W Tz\u20162 \u2264 \u03b42k\u2016z\u20162.\nWe can bound the other side of the inequality as\n\u2016h\u03a0\\\u2126\u20162 \u2265 \u2016W\u03a0\\\u2126(W Tz|\u03a0\\\u2126)\u20162 \u2212 \u2016W\u03a0\\\u2126(W Tz|\u2126)\u20162 \u2265 (1\u2212 \u03b4k)\u2016z|\u03a0\\\u2126\u20162 \u2212 \u03b42k\u2016z|\u2126\u20162.\nSince the support of z is the set \u03a0, \u03a0 \\ \u2126 = \u2126c and we can conclude that\n\u03b42k\u2016z\u20162 \u2265 (1\u2212 \u03b4k)\u2016z|\u2126c\u20162 \u2212 \u03b42k\u2016z|\u2126\u20162,\nand with some rearrangement, we have\n\u2016z|\u2126c\u20162 \u2264 2\u03b42k\n1\u2212 \u03b4k \u2016z\u20162.\nTo set the value of z\u0302 on its support set \u2126, we simply set z\u0302 = h|\u2126 and z\u0302|\u2126c = 0. Then Lemma B.4 (Estimation).\n\u2016z \u2212 z\u0302\u20162 \u2264 5\u03b42k\n1\u2212 \u03b4k \u2016z\u20162\nProof. First, note that \u2016I \u2212W\u2126W T\u2126 \u20162 \u2264 \u03b4k since\n(1\u2212 \u03b4k) \u2264 sup \u2016z\u20166=0 \u2016W T\u2126 z\u201622 \u2016z\u201622\n( = \u03c32max(W T \u2126 ) = \u03c3max(W\u2126W T \u2126 ) ) \u2264 (1 + \u03b4k),\nwhere \u03c3max is the maximum singular value. Therefore,\n\u2016z \u2212 z\u0302\u20162 \u2264 \u2016z|\u2126c\u20162 + \u2016z|\u2126 \u2212 z\u0302|\u2126\u20162 = \u2016z|\u2126c\u20162 + \u2016z|\u2126 \u2212W\u2126(W Tz|\u2126 + W Tz|\u2126c)\u20162 \u2264 \u2016z|\u2126c\u20162 + \u2016(I \u2212W\u2126W T\u2126 )z|\u2126\u20162 + \u2016W\u2126W Tz|\u2126c\u20162 \u2264 \u2016z|\u2126c\u20162 + \u2016I \u2212W\u2126W T\u2126 \u20162\u2016z|\u2126\u20162 + \u03b42k\u2016z|\u2126c\u20162 \u2264 \u2016z|\u2126c\u20162 + \u03b4k\u2016z|\u2126\u20162 + \u03b42k\u2016z|\u2126c\u20162\n\u2264 ( (1 + \u03b42k) 2\u03b42k\n1\u2212 \u03b4k + \u03b4k\n) \u2016z\u20162\n\u2264 5\u03b42k 1\u2212 \u03b4k \u2016z\u20162.\nFinally, if we use the autoencoder formulation to reconstruct the original image x by setting x\u0302 = W T z\u0302, we can estimate the reconstruction error. We note that z\u0302 isMk-sparse by construction and remind the reader that W T satisfiesM2k-model-RIP with constants \u03b4k \u2264 \u03b42k 1. Then, using Lemma B.4 as well as theM2k-sparse properties of W T ,\n\u2016x\u2212 x\u0302\u20162 = \u2016W T (z \u2212 z\u0302)\u20162 \u2264 \u221a 1 + \u03b42k\u2016z \u2212 z\u0302\u20162\n\u2264 5\u03b42k 1\u2212 \u03b4k\n\u221a 1 + \u03b42k\u2016z\u20162\n\u2264 5\u03b42k 1\u2212 \u03b4k \u221a 1 + \u03b42k\u221a 1\u2212 \u03b42k \u2016x\u20162.\nThis proves that a feedforward CNN with a linear reconstruction algorithm is an approximate autoencoder and bounds the reconstruction error of the input image in terms of the geometric properties of the filters."}, {"heading": "C MORE EXPERIMENTAL RESULTS", "text": "C.1 MORE DETAILS ON EVALUATION OF CNNS WITH GAUSSIAN RANDOM FILTERS\nIn this section, we provide more details on the network architectures that we used in Table 1. In particular, we describe the best performing architectures for all cases in Table 5.\nC.2 LAYER-WISE COHERENCE AND SPARSITY FOR ALEXNET\nWe present coherence (see Table 6) and sparsity level (see Table 7) for each layer in AlexNet.\nC.3 VISUALIZATION OF IMAGE RECONSTRUCTION FOR VGGNET\nIn Figure 5, we show reconstructed images from each layer using different reconstruction methods via a pretrained decoding network."}], "references": [{"title": "Provable Bounds for Learning Some Deep Representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Why are deep nets reversible: A simple theory, with implications for training", "author": ["S. Arora", "Y. Liang", "T. Ma"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature communications,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Model-Based Compressive Sensing", "author": ["R.G. Baraniuk", "V. Cevher", "M.F. Duarte", "C. Hegde"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Baraniuk et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Baraniuk et al\\.", "year": 1982}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Imaging Science,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Sparse feature learning for deep belief networks", "author": ["Y.-l. Boureau", "Y.L. Cun"], "venue": "In NIPS,", "citeRegEx": "Boureau and Cun,? \\Q2008\\E", "shortCiteRegEx": "Boureau and Cun", "year": 2008}, {"title": "l1-norm methods for convex-cardinality problems, ee364b: Convex optimization ii lecture notes, 2014-2015", "author": ["S. Boyd"], "venue": null, "citeRegEx": "Boyd.,? \\Q2015\\E", "shortCiteRegEx": "Boyd.", "year": 2015}, {"title": "Signal recovery from pooling representations", "author": ["J. Bruna", "A. Szlam", "Y. LeCun"], "venue": "In ICML,", "citeRegEx": "Bruna et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2014}, {"title": "Cand\u00e9s. The restricted isometry property and its implications for compressed sensing", "author": ["J. E"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "E.,? \\Q2008\\E", "shortCiteRegEx": "E.", "year": 2008}, {"title": "Deep autoencoder neural networks for gene ontology annotation predictions", "author": ["D. Chicco", "P. Sadowski", "P. Baldi"], "venue": "In Proceedings of the 5th ACM Conference Bioinformatics, Computational Biology, and Health Informatics,", "citeRegEx": "Chicco et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chicco et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Inverting visual representations with convolutional networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": null, "citeRegEx": "Dosovitskiy and Brox.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2016}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Giryes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Giryes et al\\.", "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "A powerful generative model using random weights for the deep image representation", "author": ["K. He", "Y. Wang", "J. Hopcroft"], "venue": "arXiv preprint arXiv:1606.04801,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat and Zhang.,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang.", "year": 1993}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": null, "citeRegEx": "Olshausen,? \\Q1996\\E", "shortCiteRegEx": "Olshausen", "year": 1996}, {"title": "Concentration of Measure for Block Diagonal Matrices With Applications to Compressive Signal Processing", "author": ["J.Y. Park", "H.L. Yap", "C. Rozell", "M.B. Wakin"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Park et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Park et al\\.", "year": 2011}, {"title": "Why does Deep Learning work? - A perspective from Group Theory", "author": ["A. Paul", "S. Venkatasubramanian"], "venue": null, "citeRegEx": "Paul and Venkatasubramanian.,? \\Q2014\\E", "shortCiteRegEx": "Paul and Venkatasubramanian.", "year": 2014}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["W. Shang", "K. Sohn", "D. Almeida", "H. Lee"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In Proceeding of the ACM Int. Conf. on Multimedia,", "citeRegEx": "Vedaldi and Lenc.,? \\Q2015\\E", "shortCiteRegEx": "Vedaldi and Lenc.", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Augmenting neural networks with reconstructive decoding pathways for large-scale image classification", "author": ["Y. Zhang", "K. Lee", "H. Lee"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Vershynin (2010) that the sum of squares of sub-Gaussian random variables tightly concentrate", "author": ["Park"], "venue": "Theorem A.2. Let Y1,", "citeRegEx": "Park,? \\Q2011\\E", "shortCiteRegEx": "Park", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "1 INTRODUCTION Deep learning has achieved remarkable success in many technological areas (Bengio et al., 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al.", "startOffset": 89, "endOffset": 129}, {"referenceID": 33, "context": "1 INTRODUCTION Deep learning has achieved remarkable success in many technological areas (Bengio et al., 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al.", "startOffset": 89, "endOffset": 129}, {"referenceID": 22, "context": ", 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al.", "startOffset": 54, "endOffset": 131}, {"referenceID": 36, "context": ", 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al.", "startOffset": 54, "endOffset": 131}, {"referenceID": 35, "context": ", 2013; Schmidhuber, 2015), including computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al.", "startOffset": 54, "endOffset": 131}, {"referenceID": 19, "context": ", 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al., 2012; Hannun et al., 2014), natural language processing (Collobert et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 17, "context": ", 2015; Simonyan and Zisserman, 2015), automatic speech recognition (Hinton et al., 2012; Hannun et al., 2014), natural language processing (Collobert et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 13, "context": ", 2014), natural language processing (Collobert et al., 2011; Mikolov et al., 2013; Cho et al., 2014), bioinformatics (Chicco et al.", "startOffset": 37, "endOffset": 101}, {"referenceID": 27, "context": ", 2014), natural language processing (Collobert et al., 2011; Mikolov et al., 2013; Cho et al., 2014), bioinformatics (Chicco et al.", "startOffset": 37, "endOffset": 101}, {"referenceID": 12, "context": ", 2014), natural language processing (Collobert et al., 2011; Mikolov et al., 2013; Cho et al., 2014), bioinformatics (Chicco et al.", "startOffset": 37, "endOffset": 101}, {"referenceID": 11, "context": ", 2014), bioinformatics (Chicco et al., 2014), even high energy particle physics (Baldi et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 2, "context": ", 2014), even high energy particle physics (Baldi et al., 2014).", "startOffset": 43, "endOffset": 63}, {"referenceID": 24, "context": "In particular, deep Convolutional Neural Networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan and Zisserman, 2015) have been a critical enabling technique for analyzing images and sequential data.", "startOffset": 57, "endOffset": 132}, {"referenceID": 22, "context": "In particular, deep Convolutional Neural Networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan and Zisserman, 2015) have been a critical enabling technique for analyzing images and sequential data.", "startOffset": 57, "endOffset": 132}, {"referenceID": 35, "context": "In particular, deep Convolutional Neural Networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan and Zisserman, 2015) have been a critical enabling technique for analyzing images and sequential data.", "startOffset": 57, "endOffset": 132}, {"referenceID": 35, "context": ", VGG Net (Simonyan and Zisserman, 2015)) are almost fully invertible using pooling switches.", "startOffset": 10, "endOffset": 40}, {"referenceID": 6, "context": "our model can explain each layer of the feedforward CNN algorithm as one iteration of Iterative Hard Thresholding (IHT) (Blumensath and Davies, 2009) for model-based compressive sensing and, hence, we can reconstruct the input simply and accurately.", "startOffset": 120, "endOffset": 149}, {"referenceID": 26, "context": "Algorithm 1 Model-based IHT Input: model-RIP matrix \u03a6, measurements x = \u03a6z, structured sparse approximation algorithm M Output: k-sparse approximation z 1: Initialize z0 = 0, d = x, i = 0 2: while stopping criteria not met do 3: i\u2190 i+ 1 4: b\u2190 zi\u22121 + \u03a6d 5: zi \u2190M(b, k) 6: d\u2190 x\u2212\u03a6zi 7: end while 8: return z \u2190 zi Many efficient algorithms have been proposed for sparse coding and compressive sensing (Olshausen et al., 1996; Mallat and Zhang, 1993; Beck and Teboulle, 2009).", "startOffset": 397, "endOffset": 470}, {"referenceID": 4, "context": "Algorithm 1 Model-based IHT Input: model-RIP matrix \u03a6, measurements x = \u03a6z, structured sparse approximation algorithm M Output: k-sparse approximation z 1: Initialize z0 = 0, d = x, i = 0 2: while stopping criteria not met do 3: i\u2190 i+ 1 4: b\u2190 zi\u22121 + \u03a6d 5: zi \u2190M(b, k) 6: d\u2190 x\u2212\u03a6zi 7: end while 8: return z \u2190 zi Many efficient algorithms have been proposed for sparse coding and compressive sensing (Olshausen et al., 1996; Mallat and Zhang, 1993; Beck and Teboulle, 2009).", "startOffset": 397, "endOffset": 470}, {"referenceID": 14, "context": "In particular, we study popular deep neural networks trained for image classification on the ImageNet ILSVRC 2012 dataset (Deng et al., 2009).", "startOffset": 122, "endOffset": 141}, {"referenceID": 21, "context": "Our implementation is based on the Caffe (Jia et al., 2014) and MatConvNet (Vedaldi and Lenc, 2015) toolboxes.", "startOffset": 41, "endOffset": 59}, {"referenceID": 22, "context": "The main findings on VGGNet are presented in the rest of this section; we also provide some analysis on AlexNet (Krizhevsky et al., 2012) in the supplementary materials.", "startOffset": 112, "endOffset": 137}, {"referenceID": 40, "context": "To implement image reconstruction, we project the reconstructed activations into the image space via a pretrained decoding network as in (Zhang et al., 2016), which extends a similar autoencoder architecture as in (Dosovitskiy and Brox, 2016) to a stacked \u201cwhat-where\u201d autoencoder (Zhao et al.", "startOffset": 137, "endOffset": 157}, {"referenceID": 15, "context": ", 2016), which extends a similar autoencoder architecture as in (Dosovitskiy and Brox, 2016) to a stacked \u201cwhat-where\u201d autoencoder (Zhao et al.", "startOffset": 64, "endOffset": 92}], "year": 2017, "abstractText": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_371"}