{"name": "ICLR_2017_373.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei"], "emails": ["tmei}@microsoft.com"], "sections": [{"heading": null, "text": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard."}, {"heading": "1 INTRODUCTION", "text": "Accelerated by tremendous increase in Internet bandwidth and proliferation of sensor-rich mobile devices, image data has been generated, published and spread explosively, becoming an indispensable part of today\u2019s big data. This has encouraged the development of advanced techniques for a broad range of image understanding applications. A fundamental issue that underlies the success of these technological advances is the recognition (Szegedy et al., 2015; Simonyan & Zisserman, 2015; He et al., 2016). Recently, researchers have strived to automatically describe the content of an image with a complete and natural sentence, which has a great potential impact for instance on robotic vision or helping visually impaired people. Nevertheless, this problem is very challenging, as description generation model should capture not only the objects or scenes presented in the image, but also be capable of expressing how the objects/scenes relate to each other in a nature sentence.\nThe main inspiration of recent attempts on this problem (Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; You et al., 2016) are from the advances by using RNNs in machine translation (Sutskever et al., 2014), which is to translate a text from one language (e.g., English) to another (e.g., Chinese). The basic idea is to perform a sequence to sequence learning for translation, where an encoder RNN reads the input sequential sentence, one word at a time till the end of the sentence and then a decoder RNN is exploited to generate the sentence in target language, one word at each time step. Following this philosophy, it is natural to employ a CNN instead of the encoder RNN for image captioning, which is regarded as an image encoder to produce image representations.\nWhile encouraging performances are reported, these CNN plus RNN image captioning methods translate directly from image representations to language, without explicitly taking more high-level semantic information from images into account. Furthermore, attributes are properties observed in images with rich semantic cues and have been proved to be effective in visual recognition (Parikh & Grauman, 2011). A valid question is how to incorporate high-level image attributes into CNN plus RNN image captioning architecture as complementary knowledge in addition to image representations. We investigate particularly in this paper the architectures by exploiting the mutual relationship between image representations and attributes for enhancing image description generation. Specifically, to better demonstrate the impact of simultaneously utilizing the two kinds of representations, we devise variants of architectures by feeding them into RNN in different placements and moments,\ne.g., leveraging only attributes, inserting image representations first and then attributes or vice versa, and inputting image representations/attributes once or at each time step.\nThe main contribution of this work is the proposal of attribute augmented architectures by integrating the attributes into CNN plus RNN image captioning framework, which is a problem not yet fully understood in the literature. By leveraging more knowledge for building richer representations and description models, our work takes a further step forward to enhance image captioning and could have a direct impact of indicating a new direction of vision and language research. More importantly, the utilization of attributes also has a great potential to be an elegant solution of generating openvocabulary sentences, making image captioning system really practical."}, {"heading": "2 RELATED WORK", "text": "The research on image captioning has proceeded along three different dimensions: template-based methods (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012), search-based approaches (Farhadi et al., 2010; Ordonez et al., 2011; Devlin et al., 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).\nThe first direction, template-based methods, predefine the template for sentence generation which follows some specific rules of language grammar and split sentence into several parts (e.g., subject, verb, and object). With such sentence fragments, many works align each part with image content and then generate the sentence for the image. Obviously, most of them highly depend on the templates of sentence and always generate sentence with syntactical structure. For example, Kulkarni et al. employ Conditional Random Field (CRF) model to predict labeling based on the detected objects, attributes, and prepositions, and then generate sentence with a template by filling in slots with the most likely labeling (Kulkarni et al., 2013). Similar in spirit, Yang et al. utilize Hidden Markov Model (HMM) to select the best objects, scenes, verbs, and prepositions with the highest log-likelihood ratio for template-based sentence generation in (Yang et al., 2011). Furthermore, the traditional simple template is extended to syntactic trees in (Mitchell et al., 2012) which also starts from detecting attributes from image as description anchors and then connecting ordered objects with a syntactically well-formed tree, followed by adding necessary descriptive information.\nSearch-based approaches \u201cgenerate\u201d sentence for an image by selecting the most semantically similar sentences from sentence pool or directly copying sentences from other visually similar images. This direction indeed can achieve human-level descriptions as all sentences are from existing humangenerated sentences. The need to collect human-generated sentences, however, makes the sentence pool hard to be scaled up. Moreover, the approaches in this dimension cannot generate novel descriptions. For instance, in (Farhadi et al., 2010), an intermediate meaning space based on the triplet of object, action, and scene is proposed to measure the similarity between image and sentence, where the top sentences are regarded as the generated sentences for the target image. Ordonez et al. (Ordonez et al., 2011) search images in a large captioned photo collection by using the combination of object, stuff, people, and scene information and transfer the associated sentences to the query image. Recently, a simple k-nearest neighbor retrieval model is utilized in (Devlin et al., 2015) and the best or consensus caption is selected from the returned candidate captions, which even performs as well as several state-of-the-art language-based models.\nDifferent from template-based and search-based models, language-based models aim to learn the probability distribution in the common space of visual content and textual sentence to generate novel sentences with more flexible syntactical structures. In this direction, recent works explore such probability distribution mainly using neural networks for image captioning. For instance, in (Vinyals et al., 2015), Vinyals et al. propose an end-to-end neural networks architecture by utilizing LSTM to generate sentence for an image, which is further incorporated with attention mechanism in (Xu et al., 2015) to automatically focus on salient objects when generating corresponding words. More recently, in (Wu et al., 2016), high-level concepts/attributes are shown to obtain clear improvements on image captioning when injected into existing state-of-the-art RNN-based model and such visual attributes are further utilized as semantic attention in (You et al., 2016) to enhance image captioning.\nIn short, our work in this paper belongs to the language-based models. Different from most of the aforementioned language-based models which mainly focus on sentence generation by solely\ndepending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al., 2016), our work contributes by studying not only jointly exploiting image representations and attributes for image captioning, but also how the architecture can be better devised by exploring mutual relationship in between. It is also worth noting that (You et al., 2016) also additionally involve attributes for image captioning. Ours is fundamentally different in the way that (You et al., 2016) is as a result of utilizing attributes to model semantic attention to the locally previous words, as opposed to holistically employing attributes as a kind of complementary representations in this work."}, {"heading": "3 BOOSTING IMAGE CAPTIONING WITH ATTRIBUTES", "text": "In this paper, we devise our CNN plus RNN architectures to generate descriptions for images under the umbrella of additionally incorporating the detected high-level attributes. Specifically, we begin this section by presenting the problem formulation and followed by five variants of our image captioning frameworks with attributes."}, {"heading": "3.1 PROBLEM FORMULATION", "text": "Suppose we have an image I to be described by a textual sentence S, where S = {w1, w2, ..., wNs} consisting of Ns words. Let I \u2208 RDv and wt \u2208 RDs denote the Dv-dimensional image representations of the image I and the Ds-dimensional textual features of the t-th word in sentence S, respectively. Furthermore, we have feature vector A \u2208 RDa to represent the probability distribution over the high-level attributes for image I . Specifically, we train the attribute detectors by using the weakly-supervised approach of Multiple Instance Learning (MIL) in (Fang et al., 2015) on image captioning benchmarks. For an attribute wa, one image I is regarded as a positive bag of regions (instances) if wa exists in image I\u2019s ground-truth sentences, and negative bag otherwise. By inputting all the bags into a noisy-OR MIL model, the probability of the bag bI which contains attribute wa is measured on the probabilities of all the regions in the bag as\nPrwaI = 1\u2212 \u220f\nri\u2208bI\n(1\u2212 pwai ), (1)\nwhere pwai is the probability of the attribute wa predicted by region ri and can be calculated through a sigmoid layer after the last convolutional layer in the fully convolutional network. In particular, the dimension of convolutional activations from the last convolutional layer is x\u00d7x\u00d7h and h represents the representation dimension of each region, resulting in x \u00d7 x response map which preserves the spatial dependency of the image. Then, a cross entropy loss is calculated based on the probabilities of all the attributes at the top of the whole architecture to optimize MIL model. With the learnt MIL model on image captioning dataset, we treat the final image-level response probabilities of all the attributes as A.\nInspired by the recent successes of probabilistic sequence models leveraged in statistical machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), we aim to formulate our image captioning models in an end-to-end fashion based on RNNs which encode the given image and/or its detected attributes into a fixed dimensional vector and then decode it to the target output sentence. Hence, the sentence generation problem we explore here can be formulated by minimizing the following energy loss function as E(I,A,S) = \u2212 log Pr (S|I,A), (2) which is the negative log probability of the correct textual sentence given the image representations and detected attributes.\nSince the model produces one word in the sentence at each time step, it is natural to apply chain rule to model the joint probability over the sequential words. Thus, the log probability of the sentence is given by the sum of the log probabilities over the word and can be expressed as\nlog Pr (S|I,A) = Ns\u2211 t=1 log Pr (wt| I,A,w0, . . . ,wt\u22121). (3)\nBy minimizing this loss, the contextual relationship among the words in the sentence can be guaranteed given the image and its detected attributes.\nWe formulate this task as a variable-length sequence to sequence problem and model the parametric distribution Pr (wt| I,A,w0, . . . ,wt\u22121) in Eq.(3) with Long Short-Term Memory (LSTM), which is a widely used type of RNN. The vector formulas for a LSTM layer forward pass are summarized as below. For time step t, xt and ht are the input and output vector respectively, T are input weights matrices, R are recurrent weight matrices and b are bias vectors. Sigmoid \u03c3 and hyperbolic tangent \u03c6 are element-wise non-linear activation functions. The dot product of two vectors is denoted with . Given inputs xt, ht\u22121 and ct\u22121, the LSTM unit updates for time step t are:\ngt = \u03c6(Tgx t +Rgh t\u22121 + bg), i t = \u03c3(Tix t +Rih t\u22121 + bi),\nf t = \u03c3(Tfx t +Rfh t\u22121 + bf ), c t = gt it + ct\u22121 f t,\not = \u03c3(Tox t +Roh t\u22121 + bo), h t = \u03c6(ct) ot,\nwhere gt, it, f t, ct, ot, and ht are cell input, input gate, forget gate, cell state, output gate, and cell output of the LSTM, respectively."}, {"heading": "3.2 LONG SHORT-TERM MEMORY WITH ATTRIBUTES", "text": "Unlike the existing image captioning models in (Donahue et al., 2015; Vinyals et al., 2015) which solely encode image representations for sentence generation, our proposed Long Short-Term Memory with Attributes (LSTM-A) model additionally integrates the detected high-level attributes into LSTM. We devise five variants of LSTM-A for involvement of two design purposes. The first purpose is about where to feed attributes into LSTM and three architectures, i.e., LSTM-A1 (leveraging only attributes), LSTM-A2 (inserting image representations first) and LSTM-A3 (feeding attributes first), are derived from this view. The second is about when to input attributes or image representations into LSTM and we design LSTM-A4 (inputting image representations at each time step) and LSTM-A5 (inputting attributes at each time step) for this purpose. An overview of LSTM-A architectures is depicted in Figure 1.\n3.2.1 LSTM-A1 (LEVERAGING ONLY ATTRIBUTES)\nGiven the detected attributes, one natural way is to directly inject the attributes as representations at the initial time to inform the LSTM about the high-level attributes. This kind of architecture with only attributes input is named as LSTM-A1. It is also worth noting that the attributes-based model in (Wu et al., 2016) is similar to LSTM-A1 and can be regarded as one special case of our LSTM-A. Given the attribute representations A and the corresponding sentence W \u2261 [w0,w1, ...,wNs ], the LSTM updating procedure in LSTM-A1 is as\nx\u22121 = TaA, xt = Tswt, t \u2208 {0, . . . , Ns \u2212 1} and ht = f ( xt ) , t \u2208 {0, . . . , Ns \u2212 1} ,\nwhere De is the dimensionality of LSTM input, Ta \u2208 RDe\u00d7Da and Ts \u2208 RDe\u00d7Ds is the transformation matrix for attribute representations and textual features of word, respectively, and f is the\nupdating function within LSTM unit. Please note that for the input sentence W \u2261 [w0, . . . ,wNs ], we take w0 as the start sign word to inform the beginning of sentence and wNs as the end sign word which indicates the end of sentence. Both of the special sign words are included in our vocabulary. Most specifically, at the initial time step, the attribute representations are transformed as the input to LSTM, and then in the next steps, word embedding xt will be input into the LSTM along with the previous step\u2019s hidden state ht\u22121. In each time step (except the initial step), we use the LSTM cell output ht to predict the next word. Here a softmax layer is applied after the LSTM layer to produce a probability distribution over all the Ds words in the vocabulary as\nPrt+1 (wt+1) = exp\n{ T (wt+1) h h t } \u2211\nw\u2208W exp { T (w) h h t } , whereW is the word vocabulary space and T(w)h is the parame-\nter matrix in softmax layer.\n3.2.2 LSTM-A2 (INSERTING IMAGE REPRESENTATIONS FIRST)\nTo further leverage both image representations and high-level attributes in the encoding stage of our LSTM-A, we design the second architecture LSTM-A2 by treating both of them as atoms in the input sequence to LSTM. Specifically, at the initial step, the image representations I are firstly transformed into LSTM to inform the LSTM about the image content, followed by the attribute representations A which are encoded into LSTM at the next time step to inform the high-level attributes. Then, LSTM decodes each output word based on previous word xt and previous step\u2019s hidden state ht\u22121, which is similar to the decoding stage in LSTM-A1. The LSTM updating procedure in LSTM-A2 is designed as\nx\u22122 = TvI and x \u22121 = TaA, xt = Tswt, t \u2208 {0, . . . , Ns \u2212 1} and ht = f ( xt ) , t \u2208 {0, . . . , Ns \u2212 1} ,\nwhere Tv \u2208 RDe\u00d7Dv is the transformation matrix for image representations.\n3.2.3 LSTM-A3 (FEEDING ATTRIBUTES FIRST)\nThe third design LSTM-A3 is similar to LSTM-A2 as both designs utilize image representations and high-level attributes to form the input sequence to LSTM in the encoding stage, except that the orders of encoding are different. In LSTM-A3, the attribute representations are firstly encoded into LSTM and then the image representations are transformed into LSTM at the second time step. The whole LSTM updating procedure in LSTM-A3 is as\nx\u22122 = TaA and x \u22121 = TvI, xt = Tswt, t \u2208 {0, . . . , Ns \u2212 1} and ht = f ( xt ) , t \u2208 {0, . . . , Ns \u2212 1} .\n3.2.4 LSTM-A4 (INPUTTING IMAGE REPRESENTATIONS AT EACH TIME STEP)\nDifferent from the former three designed architectures which mainly inject high-level attributes and image representations at the encoding stage of LSTM, we next modify the decoding stage in our LSTM-A by additionally incorporating image representations or high-level attributes. More specifically, in LSTM-A4, the attribute representations are injected once at the initial step to inform the LSTM about the high-level attributes, and then image representations are fed at each time step as an extra input to LSTM to emphasize the image content frequently among memory cells in LSTM. Hence, the LSTM updating procedure in LSTM-A4 is:\nx\u22121 = TaA, xt = Tswt +TvI, t \u2208 {0, . . . , Ns \u2212 1} and ht = f ( xt ) , t \u2208 {0, . . . , Ns \u2212 1} .\n3.2.5 LSTM-A5 (INPUTTING ATTRIBUTES AT EACH TIME STEP)\nThe last design LSTM-A5 is similar to LSTM-A4 except that it firstly encodes image representations and then feeds attribute representations as an additional input to LSTM at each step in decoding stage to emphasize the high-level attributes frequently. Accordingly, the LSTM updating procedure in LSTM-A5 is as\nx\u22121 = TvI, xt = Tswt +TaA, t \u2208 {0, . . . , Ns \u2212 1} and ht = f ( xt ) , t \u2208 {0, . . . , Ns \u2212 1} ."}, {"heading": "4 EXPERIMENTS", "text": "We conducted our experiments on COCO captioning dataset (COCO) (Lin et al., 2014) and evaluated our approaches for image captioning."}, {"heading": "4.1 DATASET", "text": "The dataset, COCO, is the most popular benchmark for image captioning, which contains 82,783 training images and 40,504 validation images. There are 5 human-annotated descriptions per image. As the annotations of the official testing set are not publicly available, we follow the widely used settings in prior works (You et al., 2016; Zhou et al., 2016) and take 82,783 images for training, 5,000 for validation and 5,000 for testing."}, {"heading": "4.2 EXPERIMENTAL SETTINGS", "text": "Data Preprocessing. Following (Karpathy & Fei-Fei, 2015), we convert all the descriptions in training set to lower case and discard rare words which occur less than 5 times, resulting in the final vocabulary with 8,791 unique words in COCO dataset.\nFeatures and Parameter Settings. Each word in the sentence is represented as \u201cone-hot\u201d vector (binary index vector in a vocabulary). For image representations, we take the output of 1,024-way pool5/7 \u00d7 7 s1 layer from GoogleNet (Szegedy et al., 2015) pre-trained on Imagenet ILSVRC12 dataset (Russakovsky et al., 2015). For attribute representations, we select 1,000 most common words on COCO as the high-level attributes and train the attribute detectors with MIL model (Fang et al., 2015) purely on the training data of COCO, resulting in the final 1,000-way vector of probabilities of attributes. The dimension of the input and hidden layers in LSTM are both set to 1,024.\nImplementation Details. We mainly implement our image captioning models based on Caffe (Jia et al., 2014), which is one of widely adopted deep learning frameworks. Specifically, with an initial learning rate 0.01 and mini-batch size set 1,024, the objective value can decrease to 25% of the initial loss and reach a reasonable result after 50,000 iterations (about 123 epochs).\nTesting Strategies. For sentence generation in testing stage, there are two common strategies. One is to choose the word with maximum probability at each time step and set it as LSTM input for next time step until the end sign word is emitted or the maximum length of sentence is reached. The other strategy is beam search which selects the top-k best sentences at each time step and considers them as the candidates to generate new top-k best sentences at the next time step. We adopt the second strategy and the beam size k is empirically set to 3.\nMoreover, to avoid model-level overfitting, we utilize ensembling strategy to fuse the prediction results of 5 identical models as previous works (Vinyals et al., 2015; You et al., 2016). Please note that all the 5 identical models are trained with different initializations separately.\nEvaluation Metrics. For the evaluation of our proposed models, we adopt five metrics: BLEU@N (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr-D (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). All the metrics are computed by using the codes1 released by COCO Evaluation Server (Chen et al., 2015)."}, {"heading": "4.3 COMPARED APPROACHES", "text": "To verify the merit of our LSTM-A models, we compared the following state-of-the-art methods.\n(1) NIC & LSTM (Vinyals et al., 2015): NIC attempts to directly translate from image pixels to natural language with a single deep neural network. The image representations are only injected into LSTM at the initial time step. We directly extract the results reported in (You et al., 2016) and name this run as NIC. Furthermore, for fair comparison, we also include one run LSTM which is our implementation of NIC.\n1https://github.com/tylin/coco-caption\n(2) LRCN (Donahue et al., 2015): LRCN inputs both image representations and previous word into LSTM at each time step for sentence generation.\n(3) Hard-Attention (HA) & Soft-Attention (SA) (Xu et al., 2015): Spatial attention on convolutional features of an image is incorporated into the encoder-decoder framework through two kinds of mechanisms: 1) \u201chard\u201d stochastic attention mechanism equivalently by reinforce learning and 2) \u201csoft\u201d deterministic attention mechanism with standard back-propagation.\n(4) ATT (You et al., 2016): ATT utilizes attributes as semantic attention to combine image representations and attributes in RNN for image captioning.\n(5) Sentence-Condition (SC) (Zhou et al., 2016): Sentence-condition is proposed most recently and exploits text-conditional semantic attention to generate semantic guidance for sentence generation by conditioning image features on current text content.\n(6) MSR Captivator (Devlin et al., 2015): MSR Captivator employs both Multimodal Recurrent Neural Network (MRNN) and Maximum Entropy Language Model (MELM) (Fang et al., 2015) for sentence generation. Deep Multimodal Similarity Model (DMSM) (Fang et al., 2015) is further exploited for sentence re-ranking.\n(7) CaptionBot (Tran et al., 2016): CaptionBot is a publicly image captioning system2 which is mainly built on vision models by using Deep residual networks (ResNets) (He et al., 2016) to detect visual concepts, MELM (Fang et al., 2015) language model for sentence generation and DMSM (Fang et al., 2015) for caption ranking. Entity recognition model for celebrities and landmarks is further incorporated to enrich captions and the confidence scoring model is finally utilized to select the output caption.\n(8) LSTM-A: LSTM-A1, LSTM-A2, LSTM-A3, LSTM-A4, and LSTM-A5 are five variants derived from our proposed LSTM-A framework. In addition, LSTM-A\u2217 is an oracle run that inputs groundtruth attributes in the LSTM-A3 architecture."}, {"heading": "4.4 PERFORMANCE COMPARISON", "text": "Performance on COCO Table 1 shows the performances of different models on COCO image captioning dataset. It is worth noting that the performances of different approaches here are based on different image representations. Specifically, VGG architecture (Simonyan & Zisserman, 2015) is utilized as image feature extractor in the methods of Hard-Attention & Soft-Attention and SentenceCondition, while GoogleNet (Szegedy et al., 2015) is exploited in NIC, LRCN, ATT, LSTM and our LSTM-A. In view that the GoogleNet and VGG features are comparable, we compare directly with results. Overall, the results across eight evaluation metrics consistently indicate that our proposed LSTM-A exhibits better performance than all the state-of-the-art techniques including non-attention models (NIC, LSTM, LRCN) and attention-based methods (Hard-Attention, Soft-Attention, ATT, Sentence-Condition). In particular, the CIDEr-D can achieve 98.6%, which is to date the high-\n2https://www.captionbot.ai\nest performance reported on COCO dataset when extracting image representations by GoogleNet. LSTM-A1 inputting only high-level attributes as representations makes the relative improvement over LSTM which feeds into image representations instead by 11.6%, 7.8%, 5.1%, 13.9% and 11.25% in BLEU@4, METEOR, ROUGR-L, CIDEr-D and SPICE, respectively. The results basically indicate the advantage of exploiting high-level attributes than image representations for image captioning. Furthermore, by additionally incorporating attributes to LSTM model, LSTM-A2, LSTM-A3 and LSTM-A5 lead to a performance boost, indicating that image representations and attributes are complementary and thus have mutual reinforcement for image captioning. Similar in spirit, LSTM-A4 improves LRCN by further taking attributes into account. There is a significant performance gap between ATT and LSTM-A5. Though both runs involve the utilization of image representations and attributes, they are fundamentally different in the way that the performance of ATT is as a result of modulating the strength of attention on attributes to the previous words, and LSTM-A5 is by employing attributes as auxiliary knowledge to complement image representations. This somewhat reveals the weakness of semantic attention model, where the prediction errors will accumulate along the generated sequence.\nCompared to LSTM-A1, LSTM-A2 which is augmented by integrating image representations performs better, but the performances are lower than LSTM-A3. The results indicate that LSTM-A3, in comparison, benefits from the mechanism of first feeding high-level attributes into LSTM instead of starting from inserting image representations in LSTM-A2. The chance that a good start point can be attained and lead to performance gain is better. LSTM-A4 feeding the image representations at each time step yields inferior performances to LSTM-A3, which only inputs image representations once. We speculate that this may because the noise in the image can be explicitly accumulated and thus the network overfits more easily. In contrast, the performances of LSTM-A5 which feeds attributes at each time step show the improvements on LSTM-A3. The results demonstrate that the high-level attributes are more accurate and easily translated into human understandable sentence. Among the five proposed LSTM-A architectures, LSTM-A3 achieves the best performances in terms of BLEU@1 and METEOR, while LSTM-A5 performs the best in other six evaluation metrics. The performances of the oracle run LSTM-A\u2217 could be regarded as the upper bound of employing attributes in our framework and lead to large performance gain against LSTM-A3. Such an upper bound enables us to obtain more insights on the factor accounting for the success of the current attribute augmented architecture and also provides guidance to future research in this direction. More specifically, the results, on one hand, indicate the advantage and great potential of leveraging attributes for boosting image captioning, and on the other, suggest that more efforts are further required towards mining and representing attributes more effectively.\nPerformance on COCO online testing server We also submitted our best run in terms of METEOR, i.e., LSTM-A3, to online COCO testing server and evaluated the performance on official testing set. Table 2 shows the performance Leaderboard on official testing image set with 5 reference captions (c5) and 40 reference captions (c40). Please note that here we utilize the outputs of 2,048-way pool5 layer from ResNet-152 as image representations and train the attribute detectors by ResNet-152 in our final submission. Only the latest top-3 performing methods which have been officially published are included in the table. Compared to the top performing methods, our proposed LSTM-A3 achieves the best performance across all the evaluation metrics on both c5 and c40 testing sets, and ranks the first on the Leaderboard."}, {"heading": "4.5 HUMAN EVALUATION", "text": "To better understand how satisfactory are the sentences generated from different methods, we also conducted a human study to compare our LSTM-A3 against three approaches, i.e., CaptionBot, LRCN and LSTM. A total number of 12 evaluators (6 females and 6 males) from different education backgrounds, including computer science (4), business (2), linguistics (2) and engineering (4), are invited and a subset of 1,000 images is randomly selected from testing set for the subjective evaluation. The evaluation process is as follows. All the evaluators are organized into two groups. We show the first group all the four sentences generated by each approach plus the five human-annotated sentences and ask them the question: Do the systems produce captions resembling human-generated sentences? In contrast, we show the second group once only one sentence generated by different approach or human annotation and they are asked: Can you determine whether the given sentence has been generated by a system or by a human being? From evaluators\u2019 responses, we calculate two metrics: 1) M1: percentage of captions that are evaluated as better or equal to human caption; 2) M2: percentage of captions that pass the Turing Test. Table 3 lists the result of the user study. Overall, our LSTM-A3 is clearly the winner for all two criteria. In particular, the percentage achieves 62.8% and 72.2% in terms of M1 and M2, respectively, making the absolute improvement over the best competitor CaptionBot by 4.6% and 5.9%."}, {"heading": "4.6 QUALITATIVE ANALYSIS", "text": "Visualization of prediction changes w.r.t. the additional attribute inputs. Figure 2 shows two image examples to illustrate the word prediction changes with respect to the additional attribute\ninputs. Take the first image as an example, the predicted subject is \u201ca cake\u201d in LSTM model. By additionally incorporating the detected attributes, e.g., \u201ccandles\u201d and \u201cbirthday,\u201d the output subject in the sentence by our LSTM-A3 changes into \u201ca birthday cake with candles,\u201d demonstrating the advantage of the auxiliary attribute inputs.\nSentence generation comparison between five LSTM-A architectures. The examples of sentence generated by our five LSTM-A architectures are further illustrated in Figure 3. In general, the sentences generated by LSTM-A3 and LSTM-A5 are very comparable and more accurate than those by LSTM-A1, LSTM-A2 and LSTM-A4. For instance, LSTM-A3 and LSTM-A5 produce the sentence of \u201ca bunch of stuffed animals hanging from a ceiling,\u201d which describes the first image very precisely and finely.\nSentence generation comparison across different approaches. Figure 4 showcases a few sentence examples generated by different methods, the detected high-level attributes, and humanannotated ground truth sentences. From these exemplar results, it is easy to see that all of these automatic methods can generate somewhat relevant sentences, while our proposed LSTM-A3 can predict more relevant keywords by jointly exploiting high-level attributes and image representations for image captioning. For example, compared to subject term \u201ca group of people\u201d and \u201ca man\u201d in the sentence generated by LSTM and CaptionBot respectively, \u201ca man and a dog\u201d in our LSTM-A3 is more precise to describe the image content in the first image, since the keyword \u201cdog\u201d is one of the detected attributes and directly injected into LSTM to guide the sentence generation. Similarly, verb term \u201cholding\u201d which is also detected as one high-level attribute presents the fourth image more exactly. Moreover, our LSTM-A3 can generate more descriptive sentence by enriching the semantics with high-level attributes. For instance, with the detected adjective \u201cred,\u201d the generated sentence \u201ca red and white plane flying over a body of water\u201d of the fifth image depicts the image content more comprehensive.\n4.7 ANALYSIS OF THE BEAM SIZE k\nIn order to analyze the effect of the beam size k in testing stage, we illustrate the performances of our two top performing architectures LSTM-A3 and LSTM-A5 with the beam size in the range of {1, 2, 3, 4, 5} in Figure 5. To make all performances fall into a comparable scale, all scores are normalized by the highest score of each evaluation metric. As shown in Figure 5, we can see that\nalmost all performances in terms of each evaluation metric are like the \u201c\u2227\u201d shapes when beam size k varies from 1 to 5. Hence, we set the beam size k as 3 in our experiments, which can achieve the best performance with a relatively small beam size."}, {"heading": "5 DISCUSSIONS AND CONCLUSIONS", "text": "We have presented Long Short-Term Memory with Attributes (LSTM-A) architectures which explores both image representations and high-level attributes for image captioning. Particularly, we study the problem of augmenting high-level attributes from images to complement image representations for enhancing sentence generation. To verify our claim, we have devised variants of architectures by modifying the placement and moment, where and when to feed into the two kinds of representations. Experiments conducted on COCO image captioning dataset validate our proposal and analysis. Performance improvements are clearly observed when comparing to other captioning techniques and more remarkably, the performance of our LSTM-A to date ranks the first on COCO image captioning Leaderboard.\nOur future works are as follows. First, more attributes will be learnt from large-scale image benchmarks, e.g., YFCC-100M dataset, and integrated into image captioning. We will further analyze the impact of different number of attributes involved. Second, how to generate free-form and openvocabulary sentences with the learnt attributes is also expected."}], "references": [{"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould"], "venue": "In ECCV,", "citeRegEx": "Anderson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": null, "citeRegEx": "Banerjee and Lavie.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell"], "venue": "In ACL,", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt", "C. Lawrence Zitnick", "Geoffrey Zweig"], "venue": "In CVPR,", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": "In ECCV,", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In MM,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Vicente Ordonez", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "IEEE Trans. on PAMI,", "citeRegEx": "Kulkarni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In ACL Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille"], "venue": "In NIPS Workshop on Deep Learning,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg", "Kota Yamaguchi", "Tamara Berg", "Karl Stratos", "Hal Daum\u00e9 III"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In NIPS,", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rich image captioning in the wild", "author": ["Kenneth Tran", "Xiaodong He", "Lei Zhang", "Jian Sun", "Cornelia Carapcea", "Chris Thrasher", "Chris Buehler", "Chris Sienkiewicz"], "venue": "arXiv preprint arXiv:1603.09016,", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR,", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Qi Wu", "Chunhua Shen", "Lingqiao Liu", "Anthony Dick", "Anton van den Hengel"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": null, "citeRegEx": "You et al\\.,? \\Q2016\\E", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "Image caption generation with text-conditional semantic attention", "author": ["Luowei Zhou", "Chenliang Xu", "Parker Koch", "Jason J Corso"], "venue": "arXiv preprint arXiv:1606.04621,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "A fundamental issue that underlies the success of these technological advances is the recognition (Szegedy et al., 2015; Simonyan & Zisserman, 2015; He et al., 2016).", "startOffset": 98, "endOffset": 165}, {"referenceID": 8, "context": "A fundamental issue that underlies the success of these technological advances is the recognition (Szegedy et al., 2015; Simonyan & Zisserman, 2015; He et al., 2016).", "startOffset": 98, "endOffset": 165}, {"referenceID": 5, "context": "The main inspiration of recent attempts on this problem (Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; You et al., 2016) are from the advances by using RNNs in machine translation (Sutskever et al.", "startOffset": 56, "endOffset": 135}, {"referenceID": 24, "context": "The main inspiration of recent attempts on this problem (Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; You et al., 2016) are from the advances by using RNNs in machine translation (Sutskever et al.", "startOffset": 56, "endOffset": 135}, {"referenceID": 26, "context": "The main inspiration of recent attempts on this problem (Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; You et al., 2016) are from the advances by using RNNs in machine translation (Sutskever et al.", "startOffset": 56, "endOffset": 135}, {"referenceID": 28, "context": "The main inspiration of recent attempts on this problem (Donahue et al., 2015; Vinyals et al., 2015; Xu et al., 2015; You et al., 2016) are from the advances by using RNNs in machine translation (Sutskever et al.", "startOffset": 56, "endOffset": 135}, {"referenceID": 20, "context": ", 2016) are from the advances by using RNNs in machine translation (Sutskever et al., 2014), which is to translate a text from one language (e.", "startOffset": 67, "endOffset": 91}, {"referenceID": 12, "context": "The research on image captioning has proceeded along three different dimensions: template-based methods (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012), search-based approaches (Farhadi et al.", "startOffset": 104, "endOffset": 169}, {"referenceID": 27, "context": "The research on image captioning has proceeded along three different dimensions: template-based methods (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012), search-based approaches (Farhadi et al.", "startOffset": 104, "endOffset": 169}, {"referenceID": 16, "context": "The research on image captioning has proceeded along three different dimensions: template-based methods (Kulkarni et al., 2013; Yang et al., 2011; Mitchell et al., 2012), search-based approaches (Farhadi et al.", "startOffset": 104, "endOffset": 169}, {"referenceID": 7, "context": ", 2012), search-based approaches (Farhadi et al., 2010; Ordonez et al., 2011; Devlin et al., 2015), and language-based models (Donahue et al.", "startOffset": 33, "endOffset": 98}, {"referenceID": 17, "context": ", 2012), search-based approaches (Farhadi et al., 2010; Ordonez et al., 2011; Devlin et al., 2015), and language-based models (Donahue et al.", "startOffset": 33, "endOffset": 98}, {"referenceID": 4, "context": ", 2012), search-based approaches (Farhadi et al., 2010; Ordonez et al., 2011; Devlin et al., 2015), and language-based models (Donahue et al.", "startOffset": 33, "endOffset": 98}, {"referenceID": 5, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 11, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 15, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 24, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 26, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 25, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 28, "context": ", 2015), and language-based models (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015; Wu et al., 2016; You et al., 2016).", "startOffset": 35, "endOffset": 169}, {"referenceID": 12, "context": "employ Conditional Random Field (CRF) model to predict labeling based on the detected objects, attributes, and prepositions, and then generate sentence with a template by filling in slots with the most likely labeling (Kulkarni et al., 2013).", "startOffset": 218, "endOffset": 241}, {"referenceID": 27, "context": "utilize Hidden Markov Model (HMM) to select the best objects, scenes, verbs, and prepositions with the highest log-likelihood ratio for template-based sentence generation in (Yang et al., 2011).", "startOffset": 174, "endOffset": 193}, {"referenceID": 16, "context": "Furthermore, the traditional simple template is extended to syntactic trees in (Mitchell et al., 2012) which also starts from detecting attributes from image as description anchors and then connecting ordered objects with a syntactically well-formed tree, followed by adding necessary descriptive information.", "startOffset": 79, "endOffset": 102}, {"referenceID": 7, "context": "For instance, in (Farhadi et al., 2010), an intermediate meaning space based on the triplet of object, action, and scene is proposed to measure the similarity between image and sentence, where the top sentences are regarded as the generated sentences for the target image.", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "(Ordonez et al., 2011) search images in a large captioned photo collection by using the combination of object, stuff, people, and scene information and transfer the associated sentences to the query image.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Recently, a simple k-nearest neighbor retrieval model is utilized in (Devlin et al., 2015) and the best or consensus caption is selected from the returned candidate captions, which even performs as well as several state-of-the-art language-based models.", "startOffset": 69, "endOffset": 90}, {"referenceID": 26, "context": "propose an end-to-end neural networks architecture by utilizing LSTM to generate sentence for an image, which is further incorporated with attention mechanism in (Xu et al., 2015) to automatically focus on salient objects when generating corresponding words.", "startOffset": 162, "endOffset": 179}, {"referenceID": 25, "context": "More recently, in (Wu et al., 2016), high-level concepts/attributes are shown to obtain clear improvements on image captioning when injected into existing state-of-the-art RNN-based model and such visual attributes are further utilized as semantic attention in (You et al.", "startOffset": 18, "endOffset": 35}, {"referenceID": 28, "context": ", 2016), high-level concepts/attributes are shown to obtain clear improvements on image captioning when injected into existing state-of-the-art RNN-based model and such visual attributes are further utilized as semantic attention in (You et al., 2016) to enhance image captioning.", "startOffset": 233, "endOffset": 251}, {"referenceID": 5, "context": "depending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al.", "startOffset": 35, "endOffset": 134}, {"referenceID": 11, "context": "depending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al.", "startOffset": 35, "endOffset": 134}, {"referenceID": 15, "context": "depending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al.", "startOffset": 35, "endOffset": 134}, {"referenceID": 24, "context": "depending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al.", "startOffset": 35, "endOffset": 134}, {"referenceID": 26, "context": "depending on image representations (Donahue et al., 2015; Kiros et al., 2014; Mao et al., 2014; Vinyals et al., 2015; Xu et al., 2015) or high-level attributes (Wu et al.", "startOffset": 35, "endOffset": 134}, {"referenceID": 25, "context": ", 2015) or high-level attributes (Wu et al., 2016), our work contributes by studying not only jointly exploiting image representations and attributes for image captioning, but also how the architecture can be better devised by exploring mutual relationship in between.", "startOffset": 33, "endOffset": 50}, {"referenceID": 28, "context": "It is also worth noting that (You et al., 2016) also additionally involve attributes for image captioning.", "startOffset": 29, "endOffset": 47}, {"referenceID": 28, "context": "Ours is fundamentally different in the way that (You et al., 2016) is as a result of utilizing attributes to model semantic attention to the locally previous words, as opposed to holistically employing attributes as a kind of complementary representations in this work.", "startOffset": 48, "endOffset": 66}, {"referenceID": 6, "context": "Specifically, we train the attribute detectors by using the weakly-supervised approach of Multiple Instance Learning (MIL) in (Fang et al., 2015) on image captioning benchmarks.", "startOffset": 126, "endOffset": 145}, {"referenceID": 1, "context": "Inspired by the recent successes of probabilistic sequence models leveraged in statistical machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), we aim to formulate our image captioning models in an end-to-end fashion based on RNNs which encode the given image and/or its detected attributes into a fixed dimensional vector and then decode it to the target output sentence.", "startOffset": 111, "endOffset": 158}, {"referenceID": 20, "context": "Inspired by the recent successes of probabilistic sequence models leveraged in statistical machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), we aim to formulate our image captioning models in an end-to-end fashion based on RNNs which encode the given image and/or its detected attributes into a fixed dimensional vector and then decode it to the target output sentence.", "startOffset": 111, "endOffset": 158}, {"referenceID": 5, "context": "Unlike the existing image captioning models in (Donahue et al., 2015; Vinyals et al., 2015) which solely encode image representations for sentence generation, our proposed Long Short-Term Memory with Attributes (LSTM-A) model additionally integrates the detected high-level attributes into LSTM.", "startOffset": 47, "endOffset": 91}, {"referenceID": 24, "context": "Unlike the existing image captioning models in (Donahue et al., 2015; Vinyals et al., 2015) which solely encode image representations for sentence generation, our proposed Long Short-Term Memory with Attributes (LSTM-A) model additionally integrates the detected high-level attributes into LSTM.", "startOffset": 47, "endOffset": 91}, {"referenceID": 25, "context": "It is also worth noting that the attributes-based model in (Wu et al., 2016) is similar to LSTM-A1 and can be regarded as one special case of our LSTM-A.", "startOffset": 59, "endOffset": 76}, {"referenceID": 14, "context": "We conducted our experiments on COCO captioning dataset (COCO) (Lin et al., 2014) and evaluated our approaches for image captioning.", "startOffset": 63, "endOffset": 81}, {"referenceID": 28, "context": "As the annotations of the official testing set are not publicly available, we follow the widely used settings in prior works (You et al., 2016; Zhou et al., 2016) and take 82,783 images for training, 5,000 for validation and 5,000 for testing.", "startOffset": 125, "endOffset": 162}, {"referenceID": 29, "context": "As the annotations of the official testing set are not publicly available, we follow the widely used settings in prior works (You et al., 2016; Zhou et al., 2016) and take 82,783 images for training, 5,000 for validation and 5,000 for testing.", "startOffset": 125, "endOffset": 162}, {"referenceID": 21, "context": "For image representations, we take the output of 1,024-way pool5/7 \u00d7 7 s1 layer from GoogleNet (Szegedy et al., 2015) pre-trained on Imagenet ILSVRC12 dataset (Russakovsky et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 6, "context": "For attribute representations, we select 1,000 most common words on COCO as the high-level attributes and train the attribute detectors with MIL model (Fang et al., 2015) purely on the training data of COCO, resulting in the final 1,000-way vector of probabilities of attributes.", "startOffset": 151, "endOffset": 170}, {"referenceID": 9, "context": "We mainly implement our image captioning models based on Caffe (Jia et al., 2014), which is one of widely adopted deep learning frameworks.", "startOffset": 63, "endOffset": 81}, {"referenceID": 24, "context": "Moreover, to avoid model-level overfitting, we utilize ensembling strategy to fuse the prediction results of 5 identical models as previous works (Vinyals et al., 2015; You et al., 2016).", "startOffset": 146, "endOffset": 186}, {"referenceID": 28, "context": "Moreover, to avoid model-level overfitting, we utilize ensembling strategy to fuse the prediction results of 5 identical models as previous works (Vinyals et al., 2015; You et al., 2016).", "startOffset": 146, "endOffset": 186}, {"referenceID": 18, "context": "For the evaluation of our proposed models, we adopt five metrics: BLEU@N (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr-D (Vedantam et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 13, "context": ", 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr-D (Vedantam et al.", "startOffset": 50, "endOffset": 61}, {"referenceID": 23, "context": ", 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr-D (Vedantam et al., 2015) and SPICE (Anderson et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 3, "context": "All the metrics are computed by using the codes1 released by COCO Evaluation Server (Chen et al., 2015).", "startOffset": 84, "endOffset": 103}, {"referenceID": 24, "context": "(1) NIC & LSTM (Vinyals et al., 2015): NIC attempts to directly translate from image pixels to natural language with a single deep neural network.", "startOffset": 15, "endOffset": 37}, {"referenceID": 28, "context": "We directly extract the results reported in (You et al., 2016) and name this run as NIC.", "startOffset": 44, "endOffset": 62}, {"referenceID": 24, "context": "Model B@1 B@2 B@3 B@4 M R C S NIC (Vinyals et al., 2015) 66.", "startOffset": 34, "endOffset": 56}, {"referenceID": 5, "context": "(2) LRCN (Donahue et al., 2015): LRCN inputs both image representations and previous word into LSTM at each time step for sentence generation.", "startOffset": 9, "endOffset": 31}, {"referenceID": 26, "context": "(3) Hard-Attention (HA) & Soft-Attention (SA) (Xu et al., 2015): Spatial attention on convolutional features of an image is incorporated into the encoder-decoder framework through two kinds of mechanisms: 1) \u201chard\u201d stochastic attention mechanism equivalently by reinforce learning and 2) \u201csoft\u201d deterministic attention mechanism with standard back-propagation.", "startOffset": 46, "endOffset": 63}, {"referenceID": 28, "context": "(4) ATT (You et al., 2016): ATT utilizes attributes as semantic attention to combine image representations and attributes in RNN for image captioning.", "startOffset": 8, "endOffset": 26}, {"referenceID": 29, "context": "(5) Sentence-Condition (SC) (Zhou et al., 2016): Sentence-condition is proposed most recently and exploits text-conditional semantic attention to generate semantic guidance for sentence generation by conditioning image features on current text content.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "(6) MSR Captivator (Devlin et al., 2015): MSR Captivator employs both Multimodal Recurrent Neural Network (MRNN) and Maximum Entropy Language Model (MELM) (Fang et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 6, "context": ", 2015): MSR Captivator employs both Multimodal Recurrent Neural Network (MRNN) and Maximum Entropy Language Model (MELM) (Fang et al., 2015) for sentence generation.", "startOffset": 122, "endOffset": 141}, {"referenceID": 6, "context": "Deep Multimodal Similarity Model (DMSM) (Fang et al., 2015) is further exploited for sentence re-ranking.", "startOffset": 40, "endOffset": 59}, {"referenceID": 22, "context": "(7) CaptionBot (Tran et al., 2016): CaptionBot is a publicly image captioning system2 which is mainly built on vision models by using Deep residual networks (ResNets) (He et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 8, "context": ", 2016): CaptionBot is a publicly image captioning system2 which is mainly built on vision models by using Deep residual networks (ResNets) (He et al., 2016) to detect visual concepts, MELM (Fang et al.", "startOffset": 140, "endOffset": 157}, {"referenceID": 6, "context": ", 2016) to detect visual concepts, MELM (Fang et al., 2015) language model for sentence generation and DMSM (Fang et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 6, "context": ", 2015) language model for sentence generation and DMSM (Fang et al., 2015) for caption ranking.", "startOffset": 56, "endOffset": 75}, {"referenceID": 21, "context": "Specifically, VGG architecture (Simonyan & Zisserman, 2015) is utilized as image feature extractor in the methods of Hard-Attention & Soft-Attention and SentenceCondition, while GoogleNet (Szegedy et al., 2015) is exploited in NIC, LRCN, ATT, LSTM and our LSTM-A.", "startOffset": 188, "endOffset": 210}, {"referenceID": 6, "context": "The attributes are predicted by MIL method in (Fang et al., 2015) and the output sentences are generated by LSTM and LSTM-A3.", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "The attributes are predicted by MIL method in (Fang et al., 2015) and the output sentences are generated by 1) LSTM, 2) CaptionBot(2), 3) our LSTM-A3, and 4) Ground Truth: randomly selected three ground truth sentences.", "startOffset": 46, "endOffset": 65}], "year": 2017, "abstractText": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_373"}