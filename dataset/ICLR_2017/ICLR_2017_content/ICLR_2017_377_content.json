{"name": "ICLR_2017_377.pdf", "metadata": {"source": "CRF", "title": "DEEP CHARACTER-LEVEL NEURAL MACHINE TRANSLATION BY LEARNING MORPHOLOGY", "authors": ["Shenjian Zhao", "Zhihua Zhang"], "emails": ["sword.york@gmail.com", "zhzhang@math.pku.edu.cn"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural machine translation (NMT) attempts to build a single large neural network that reads a sentence and outputs a translation (Sutskever et al., 2014). Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014). Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).\nChung et al. (2016a) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology.\nThere are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016); Luong et al. (2015); Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation. However, these approaches ignore several important properties of languages such as monolinguality and crosslinguality as pointed out by Luong and\nManning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation model which leverages the power of both words and characters to achieve the goal of open vocabulary neural machine translation.\nIntuitively, it is elegant to directly model pure characters. However, as the length of sequence grows significantly, character-level translation models have failed to produce competitive results compared with word-based models. In addition, they require more memory and computation resource. Especially, it is much difficult to train the attention component. For example, Ling et al. (2015a) proposed a compositional character to word (C2W) model and applied it to machine translation (Ling et al., 2015b). They also used a hierarchical decoder which has been explored before in other context (Serban et al., 2015). However, they found it slow and difficult to train the character-level models, and one has to resort to layer-wise training the neural network and applying supervision for the attention component. In fact, such RNNs often struggle with separating words that have similar morphologies but very different meanings.\nIn order to address the issues mentioned earlier, we introduce a novel architecture by exploiting the structure of words. It is built on two recurrent neural networks: one for learning the representation of preceding characters and another for learning the weight of this representation of the whole word. Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al., 2016), we learn the subword unit automatically. Compared with CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word. To decode at character level, we devise a hierarchical decoder which sets the state of the second-level RNN (character-level decoder) to the output of the first-level RNN (word-level decoder), which will generate a character sequence until generating a delimiter. In this way, our model almost keeps the same encoding length for encoder as word-based models but eliminates the use of a large vocabulary. Furthermore, we are able to efficiently train the deep model which consists of six recurrent networks, achieving higher performance.\nIn summary, we propose a hierarchical architecture (character -> subword -> word -> source sentence -> target word -> target character) to train a deep character-level neural machine translator. We show that the model achieves a high translation performance which is comparable to the state-of-the-art neural machine translation model on the task of En-Fr, En-Cs and Cs-En translation. The experiments and analyses further support the statement that our model is able to learn the morphology."}, {"heading": "2 NEURAL MACHINE TRANSLATION", "text": "Neural machine translation is often implemented as an encoder-decoder architecture. The encoder usually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) to encode the input sentence x = {x1, . . . , xTx} into a sequence of hidden states h = {h1, . . . ,hTx}:\nht = f1(e(xt),ht\u22121),\nwhere e(xt) \u2208 Rm is an m-dimensional embedding of xt. The decoder, another RNN, is often trained to predict next word yt given previous predicted words {y1, . . . , yt\u22121} and the context vector ct; that is, p(yt | {y1, . . . , yt\u22121}) = g(e(yt\u22121), st, ct), where\nst = f2(e(yt\u22121), st\u22121, ct) (1) and g is a nonlinear and potentially multi-layered function that computes the probability of yt. The context ct depends on the sequence of {h1, . . . ,hTx}. Sutskever et al. (2014) encoded all information in the source sentence into a fixed-length vector, i.e., ct = hTx . Bahdanau et al. (2015) computed ct by the alignment model which handles the bottleneck that the former approach meets.\nThe whole model is jointly trained by maximizing the conditional log-probability of the correct translation given a source sentence with respect to the parameters of the model \u03b8:\n\u03b8\u2217 = argmax \u03b8 Ty\u2211 t=1 log p(yt | {y1, . . . , yt\u22121},x,\u03b8).\nFor the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015)."}, {"heading": "3 DEEP CHARACTER-LEVEL NEURAL MACHINE TRANSLATION", "text": "We consider two problems in the word-level neural machine translation models. First, how can we map a word to a vector? It is usually done by a lookup table (embedding matrix) where the size of vocabulary is limited. Second, how do we map a vector to a word when predicting? It is usually done via a softmax function. However, the large vocabulary will make the softmax intractable computationally.\nWe correspondingly devise two novel architectures, a word encoder which utilizes the morphology and a hierarchical decoder which decodes at character level. Accordingly, we propose a deep character-level neural machine translation model (DCNMT)."}, {"heading": "3.1 LEARNING MORPHOLOGY IN A WORD ENCODER", "text": "Many words can be subdivided into smaller meaningful units called morphemes, such as \u201cany-one\u201d, \u201cany-thing\u201d and \u201cevery-one.\u201d At the basic level, words are made of morphemes which are recognized as grammatically significant or meaningful. Different combinations of morphemes lead to different meanings. Based on these facts, we introduce a word encoder to learn the morphemes and the rules of how they are combined. Even if the word encoder had never seen \u201ceverything\u201d before, with a understanding of English morphology, the word encoder could gather the meaning easily. Thus learning morphology in a word encoder might speedup training.\nThe word encoder is based on two recurrent neural networks, as illustrated in Figure 1. We compute the representation of the word \u2018anyone\u2019 as\nranyone = tanh( 6\u2211 t=1 wtrt),\nwhere rt is an RNN hidden state at time t, computed by\nrt = f(e(xt), rt\u22121).\nEach rt contains information about the preceding characters. The weight wt of each representation rt is computed by\nwt = exp(aff(ht)),\nwhere ht is another RNN hidden state at time t and aff() is an affine function which maps ht to a scalar. Here, we use a BiRNN to compute ht as shown in Figure 1. Instead of normalizing it by \u2211 t exp(aff(ht)), we use an activation function tanh as it performs best in experiments.\nWe can regard the weight wi as the energy that determines whether ri is a representation of a morpheme and how it contributes to the representation of the word. Compared with an embedding lookup table, the decoupled RNNs learn the representation of morphemes and the rules of how they are combined respectively, which may be viewed as learning distributed representations of words explicitly. For example, we are able to translate \u201cconvenienter\u201d correctly which validates our idea.\nAfter obtaining the representation of the word, we could encode the sentence using a bidirectional RNN as RNNsearch (Bahdanau et al., 2015). The detailed architecture is shown in Figure 2."}, {"heading": "3.2 HIERARCHICAL DECODER", "text": "To decode at the character level, we introduce a hierarchical decoder. The first-level decoder is similar to RNNsearch which contains the information of the target word. Specifically, st in Eqn. (1) contains the information of target word at time t. Instead of using a multi-layer network following a softmax function to compute the probability of each target word using st, we employ a second-level decoder which generates a character sequence based on st.\nWe proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the second-level decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter\nand Schmidhuber, 1997) units instead of the GRU described here). HGRU has a settable state and generates character sequence based on the given state until generating a delimiter. In our model, the state is initialized by the output of the first-level decoder. Once HGRU generates a delimiter, it will set the state to the next output of the first-level decoder. Given the previous output character sequence {y0, y1, . . . , yt\u22121} where y0 is a token representing the start of sentence, and the auxiliary sequence {a0, a1, . . . , at\u22121} which only contains 0 and 1 to indicate whether yi is a delimiter (a0 is set to 1), HGRU updates the state as follows:\ngt\u22121 = (1\u2212 at\u22121)gt\u22121 + at\u22121sit , (2) qjt = \u03c3([Wqe(yt\u22121)] j + [Uqgt\u22121] j), (3)\nzjt = \u03c3([Wze(yt\u22121)] j + [Uzgt\u22121] j), (4)\ng\u0303jt = \u03c6([We(yt\u22121)] j + [U(qt gt\u22121)]j), (5)\ngjt = z j tg j t\u22121 + (1\u2212 z j t )g\u0303 j t , (6)\nwhere sit is the output of the first-level decoder which calculated as Eqn. (8). We can compute the probability of each target character yt based on gt with a softmax function:\np(yt | {y1, . . . , yt\u22121},x) = softmax(gt). (7)\nThe current problem is that the number of outputs of the first-level decoder is much fewer than the target character sequence. It will be intractable to conditionally pick outputs from the the first-level decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other symbolic deep learning frameworks to build symbolic expressions). Luong and Manning (2016) uses two forward passes (one for word-level and another for character-level) in batch training which is less efficient. However, in our model, we use a matrix to unfold the outputs of the first-level decoder, which makes the batch training process more efficient. It is a Ty \u00d7 T matrix R, where Ty is the number of delimiter (number of words) in the target character sequence and T is the length of the target character sequence. R[i, j1 + 1] to R[i, j2] are set as 1 if j1 is the index of the (i\u22121)-th delimiter and j2 is the index of the i-th delimiter in the target character sequence. The index of the 0-th delimiter is set as 0. For example, when the target output is \u201cg o ! \u201d and the output of the first-level decoder is [s1, s2], the unfolding step will be:\n[s1, s2] [ 1 1 1 0 0 0 0 0 1 1 ] = [s1, s1, s1, s2, s2],\ntherefore {si1 , si2 , si3 , si4 , si5} is correspondingly set to {s1, s1, s1, s2, s2} in HGRU iterations. After this procedure, we can compute the probability of each target character by the second-level decoder according to Eqns. (2) to (7)."}, {"heading": "3.3 MODEL ARCHITECTURES", "text": "There are totally six recurrent neural networks in our model, which can be divided into four layers as shown in Figure 2. Figure 2 illustrates the training procedure of a basic deep character-level neural machine translation. It is possible to use multi-layer recurrent neural networks to make the model deeper. The first layer is a source word encoder which contains two RNNs as shown in Figure 1. The second layer is a bidirectional RNN sentence encoder which is identical to that of (Bahdanau et al., 2015). The third layer is the first-level decoder. It takes the representation of previous target word as a feedback, which is produced by the target word encoder in our model. As the feedback is less important, we use an ordinary RNN to encode the target word. The feedback rYt\u22121 then combines the previous hidden state ut\u22121 and the context ct from the sentence encoder to generate the vector st:\nst = W1ct +W2rYt\u22121 +W3ut\u22121 + b. (8)\nWith the state of HGRU in the second-level decoder setting to st and the information of previous generated character, the second-level decoder generates the next character until generating an end of sentence token (denoted as </s> in Figure 2). With such a hierarchical architecture, we can train our character-level neural translation model perfectly well in an end-to-end fashion."}, {"heading": "3.4 GENERATION PROCEDURE", "text": "We first encode the source sequence as in the training procedure, then we generate the target sequence character by character based on the output st of the first-level decoder. Once we generate a delimiter, we should compute next vector st+1 according to Eqn. (8) by combining feedback rYt from the target word encoder, the context ct+1 from the sentence encoder and the hidden state ut. The generation procedure will terminate once an end of sentence (EOS) token is produced."}, {"heading": "4 EXPERIMENTS", "text": "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merri\u00ebnboer et al., 2015), the source code and the trained models are available at github 1. We train our model on a single GTX Titan X with 12GB RAM. First we evaluate our model on English-toFrench translation task where the languages are morphologically poor. For fair comparison, we use the same dataset as in RNNsearch which is the bilingual, parallel corpora provided by ACL WMT\u201914. In order to show the strengths of our model, we conduct on the English-to-Czech and Czech-to-English translation tasks where Czech is a morphologically rich language. We use the same dataset as (Chung et al., 2016a; Lee et al., 2016) which is provided by ACL WMT\u2019152."}, {"heading": "4.1 DATASET", "text": "We use the parallel corpora for two language pairs from WMT: En-Cs and En-Fr. They consist of 15.8M and 12.1M sentence pairs, respectively. In terms of preprocessing, we only apply the usual tokenization. We choose a list of 120 most frequent characters for each language which coveres nearly 100% of the training data. Those characters not included in the list are mapped to a special token\n1https://github.com/SwordYork/DCNMT 2http://www.statmt.org/wmt15/translation-task.html\n(<unk>). We use newstest2013(Dev) as the development set and evaluate the models on newstest2015 (Test). We do not use any monolingual corpus."}, {"heading": "4.2 TRAINING DETAILS", "text": "We follow (Bahdanau et al., 2015) to use similar hyperparameters. The bidirectional RNN sentence encoder and the hierarchical decoder both consists of two-layer RNNs, each has 1024 hidden units; We choose 120 most frequent characters for DCNMT and the character embedding dimensionality is 64. The source word is encoded into a 600-dimensional vector. The other GRUs in our model have 512 hidden units.\nWe use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each model (for En-Fr we use a minibatch of 72 examples). The learning rate is first set to 10\u22123 and then annealed to 10\u22124.\nWe use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). In our DCNMT model, it is reasonable to search directly on character level to generate a translation."}, {"heading": "5 RESULT AND ANALYSIS", "text": "We conduct comparison of quantitative results on the En-Fr, En-Cs and Cs-En translation tasks in Section 5.1. Apart from measuring translation quality, we analyze the efficiency of our model and effects of character-level modeling in more details."}, {"heading": "5.1 QUANTITATIVE RESULTS", "text": "We illustrate the efficiency of the deep character-level neural machine translation by comparing with the bpe-based subword model (Sennrich et al., 2016) and other character-level models. We measure the performance by BLEU score (Papineni et al., 2002).\nIn Table 1, \u201cLength\u201d indicates the maximum sentence length in training (based on the number of words or characters), \u201cSize\u201d is the total number of parameters in the models. We report the BLEU\nscores of DCNMT when trained after one epoch in the above line and the final scores in the following line. The results of other models are taken from (1)Firat et al. (2016), (3)Chung et al. (2016a), (4)Lee et al. (2016) and (5)Luong and Manning (2016) respectively, except (2) is trained according to Ling et al. (2015b). The only difference between CNMT and DCNMT is CNMT uses an ordinary RNN to encode source words (takes the last hidden state). The training time for (3) and (4) is calculated based on the training speed in (Lee et al., 2016). For each test set, the best scores among the models per language pair are bold-faced. Obviously, character-level models are better than the subword-level models, and our model is comparable to the start-of-the-art character-level models. Note that, the purely character model of (5)(Luong and Manning, 2016) took 3 months to train and yielded +0.5 BLEU points compared to our result. We have analyzed the efficiency of our decoder in Section 3.2. Besides, our model is the simplest and the smallest one in terms of the model size."}, {"heading": "5.2 LEARNING MORPHOLOGY", "text": "In this section, we investigate whether our model could learn morphology. First we want to figure out the difference between an ordinary RNN word encoder and our word encoder. We choose some words with similar meaning but different in morphology as shown in Figure 3. We could find in Figure 3(a) that the words ending with \u201cability\u201d, which are encoded by the ordinary RNN word encoder, are jammed together. In contrast, the representations produced by our encoder are more reasonable and the words with similar meaning are closer.\nThen we analyze how our word encoder learns morphemes and the rules of how they are combined. We demonstrate the encoding details on \u201cany*\u201d and \u201cevery*\u201d. Figure 4(a) shows the energy of each character, more precisely, the energy of preceding characters. We could see that the last character of a morpheme will result a relative large energy (weight) like \u201cany\u201d and \u201cevery\u201d in these words. Moreover, even the preceding characters are different, it will produce a similar weight for the same morpheme like \u201cway\u201d in \u201canyway\u201d and \u201ceveryway\u201d. The two-dimensional PCA projection in Figure\n4(b) further validates our idea. The word encoder may be able to guess the meaning of \u201ceverything\u201d even it had never seen \u201ceverything\u201d before, thus speedup learning. More interestingly, we find that not only the ending letter has high energy, but also the beginning letter is important. It matches the behavior of human perception (White et al., 2008).\nMoreover, we apply our trained word encoder to Penn Treebank Line 1. Unlike Chung et al. (2016b), we are able to detect the boundary of the subword units. As shown in Figure 5, \u201cconsumers\u201d, \u201cmonday\u201d, \u201cfootball\u201d and \u201cgreatest\u201d are segmented into \u201cconsum-er-s\u201d,\u201cmon-day\u201d, \u201cfoot-ball\u201d and \u201cgreat-est\u201d respectively. Since there are no explicit delimiters, it may be more difficult to detect the subword units."}, {"heading": "5.3 BENEFITING FROM LEARNING MORPHOLOGY", "text": "As analyzed in Section 5.2, learning morphology could speedup learning. This has also been shown in Table 1 (En-Fr and En-Cs task) from which we see that when we train our model just for one epoch, the obtained result even outperforms the final result with bpe baseline.\nAnother advantage of our model is the ability to translate the misspelled words or the nonce words. The character-level model has a much better chance recovering the original word or sentence. In Table 2, we list some examples where the source sentences are taken from newstest2013 but we change some words to misspelled words or nonce words. We also list the translations from Google translate 3 and online demo of neural machine translation by LISA.\nAs listed in Table 2(a), DCNMT is able to translate out the misspelled words correctly. For a word-based translator, it is never possible because the misspelled words are mapped into <unk>\n3The translations by Google translate were made on Nov 4, 2016.\ntoken before translating. Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015). More interestingly, DCNMT could translate \u201cconvenienter\u201d correctly as shown in Table 2(b). By concatenating \u201cconvenient\u201d and \u201cer\u201d, we get the comparative adjective form of \u201cconvenient\u201d which never appears in the training set; however, our model guessed it correctly based on the morphemes and the rules."}, {"heading": "6 CONCLUSION", "text": "In this paper we have proposed an hierarchical architecture to train the deep character-level neural machine translation model by introducing a novel word encoder and a multi-leveled decoder. We have demonstrated the efficiency of the training process and the effectiveness of the model in comparison with the word-level and other character-level models. The BLEU score implies that our deep characterlevel neural machine translation model likely outperforms the word-level models and is competitive with the state-of-the-art character-based models. It is possible to further improve performance by using deeper recurrent networks (Wu et al., 2016), training for more epochs and training with longer sentence pairs.\nAs a result of the character-level modeling, we have solved the out-of-vocabulary (OOV) issue that word-level models suffer from, and we have obtained a new functionality to translate the misspelled or the nonce words. More importantly, the deep character-level is able to learn the similar embedding of the words with similar meanings like the word-level models. Finally, it would be potentially possible that the idea behind our approach could be applied to many other tasks such as speech recognition and text summarization."}, {"heading": "A DETAILED DESCRIPTION OF THE MODEL", "text": "Here we describe the implementation using Theano, it should be applicable to other symbolic deep learning frameworks. We use f to denote the transition of the recurrent network.\nA.1 SOURCE WORD ENCODER\nAs illustrated in Section 3.1, the word encoder is based on two recurrent neural networks. We compute the representation of the word \u2018anyone\u2019 as\nranyone = tanh( 6\u2211 t=1 wtrt),\nwhere rt \u2208 Rn is an RNN hidden state at time t, computed by\nrt = f(e(xt), rt\u22121).\nEach rt contains information about the preceding characters. The weight wt of each representation rt is computed by\nwt = exp(Wwht + bw),\nwhere Ww \u2208 R1\u00d72l maps the vector ht \u2208 R2l to a scalar and ht is the state of the BiRNN at time t:\nht = [\u2212\u2192 h t\u2190\u2212 h t ] . (9)\n\u2212\u2192 h t \u2208 Rl is the forward state of the BiRNN which is computed by\n\u2212\u2192 h t = f(e(xt), \u2212\u2192 h t\u22121). (10)\nThe backward state \u2190\u2212 h t \u2208 Rl is computed similarly, however in a reverse order.\nA.2 SOURCE SENTENCE ENCODER\nAfter encoding the words by the source word encoder, we feed the representations to the source sentence encoder. For example, the source \u201cHello world </s>\u201d is encoded into a vector [rHello, rworld, r</s>], then the BiRNN sentence encoder encodes this vector into [v1,v2,v3]. The computation is the same as Eqn. (9) and Eqn. (10), however the input now changes to the representation of the words.\nA.3 FIRST-LEVEL DECODER\nThe first-level decoder is similar to Bahdanau et al. (2015) which utilizes the attention mechanism. Given the context vector ct from encoder, the hidden state ut \u2208 Rm of the GRU is computed by\nut = (1\u2212 zt) \u25e6 ut\u22121 + zt \u25e6 u\u0303t,\nwhere\nu\u0303t = tanh(WrYt\u22121 +U[qt \u25e6 ut\u22121] +Cct) zt = \u03c3(WzrYt\u22121 +Uzut\u22121 +Czct)\nqt = \u03c3(WqrYt\u22121 +Uqut\u22121 +Cqct).\nrYt\u22121 is the representation of the target word which is produced by an ordinary RNN (take the last state). The context vector ct is computed by the attention mechanism at each step:\nct = Tx\u2211 j=1 \u03b1tjvj ,\nwhere\n\u03b1tj = exp(etj)\u2211Tx k=1 exp(etk)\netj = E tanh(Weut\u22121 +Hehj).\nE \u2208 R1\u00d7m which maps the vector into a scalar. Then the hidden state ut is further processed as Eqn. (8) before feeding to the second-level decoder:\nst+1 = W1ct+1 +W2rYt +W3ut + b.\nA.4 SECOND-LEVEL DECODER\nAs described in Section 3.2, the number of outputs of the first-level decoder is much fewer than the target character sequence. It will be intractable to conditionally pick outputs from the the first-level decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other symbolic deep learning frameworks to build symbolic expressions). We use a matrix R \u2208 RTy\u00d7T to unfold the outputs [s1, . . . , sTy ] of the first-level decoder (Ty is the number of words in the target sentence and T is the number of characters). R is a symbolic matrix in the final loss, it is constructed according the delimiters in the target sentences when training (see Section 3.2 for the detailed construction, note that R is a tensor in batch training. ). After unfolding, the input of HGRU becomes [si1 , . . . , siT ], that is\n[si1 , . . . , siT ] = [s1, . . . , sTy ]R.\nAccording to Eqns.(2) to (7), we can compute the probability of each target character :\np(yt | {y1, . . . , yt\u22121},x) = softmax(gt).\nFinally, we could compute the cross-entroy loss and train with SGD algorithm."}, {"heading": "B SAMPLE TRANSLATIONS", "text": "We show additional sample translations in the following Tables."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representation,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representation,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation. pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.,", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Eye movements when reading transposed text: the importance of word-beginning letters", "author": ["Sarah J White", "Rebecca L Johnson", "Simon P Liversedge", "Keith Rayner"], "venue": "Journal of Experimental Psychology: Human Perception and Performance,", "citeRegEx": "White et al\\.,? \\Q2008\\E", "shortCiteRegEx": "White et al\\.", "year": 2008}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT) attempts to build a single large neural network that reads a sentence and outputs a translation (Sutskever et al., 2014).", "startOffset": 129, "endOffset": 153}, {"referenceID": 0, "context": "Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 104, "endOffset": 146}, {"referenceID": 1, "context": "Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 104, "endOffset": 146}, {"referenceID": 0, "context": "However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).", "startOffset": 131, "endOffset": 173}, {"referenceID": 3, "context": "However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).", "startOffset": 131, "endOffset": 173}, {"referenceID": 0, "context": "In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015).", "startOffset": 64, "endOffset": 106}, {"referenceID": 3, "context": "In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015).", "startOffset": 64, "endOffset": 106}, {"referenceID": 10, "context": "They also used a hierarchical decoder which has been explored before in other context (Serban et al., 2015).", "startOffset": 86, "endOffset": 107}, {"referenceID": 11, "context": "Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al., 2016), we learn the subword unit automatically.", "startOffset": 75, "endOffset": 98}, {"referenceID": 12, "context": "Compared with CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word.", "startOffset": 31, "endOffset": 67}, {"referenceID": 13, "context": "Compared with CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word.", "startOffset": 31, "endOffset": 67}, {"referenceID": 14, "context": "The encoder usually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) to encode the input sentence x = {x1, .", "startOffset": 110, "endOffset": 138}, {"referenceID": 0, "context": "For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 86, "endOffset": 133}, {"referenceID": 2, "context": "For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 86, "endOffset": 133}, {"referenceID": 2, "context": "After obtaining the representation of the word, we could encode the sentence using a bidirectional RNN as RNNsearch (Bahdanau et al., 2015).", "startOffset": 116, "endOffset": 139}, {"referenceID": 1, "context": "We proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the second-level decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter", "startOffset": 55, "endOffset": 93}, {"referenceID": 15, "context": "We proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the second-level decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter", "startOffset": 55, "endOffset": 93}, {"referenceID": 17, "context": "It will be intractable to conditionally pick outputs from the the first-level decoder when training in batch manner (at least intractable for Theano (Bastien et al., 2012) and other symbolic deep learning frameworks to build symbolic expressions).", "startOffset": 149, "endOffset": 171}, {"referenceID": 2, "context": "The second layer is a bidirectional RNN sentence encoder which is identical to that of (Bahdanau et al., 2015).", "startOffset": 87, "endOffset": 110}, {"referenceID": 18, "context": "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merri\u00ebnboer et al.", "startOffset": 36, "endOffset": 81}, {"referenceID": 17, "context": "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merri\u00ebnboer et al.", "startOffset": 36, "endOffset": 81}, {"referenceID": 13, "context": "We use the same dataset as (Chung et al., 2016a; Lee et al., 2016) which is provided by ACL WMT\u2019152.", "startOffset": 27, "endOffset": 66}, {"referenceID": 2, "context": "We follow (Bahdanau et al., 2015) to use similar hyperparameters.", "startOffset": 10, "endOffset": 33}, {"referenceID": 20, "context": "We use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each model (for En-Fr we use a minibatch of 72 examples).", "startOffset": 26, "endOffset": 47}, {"referenceID": 0, "context": "We use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 167, "endOffset": 214}, {"referenceID": 2, "context": "We use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 167, "endOffset": 214}, {"referenceID": 11, "context": "We illustrate the efficiency of the deep character-level neural machine translation by comparing with the bpe-based subword model (Sennrich et al., 2016) and other character-level models.", "startOffset": 130, "endOffset": 153}, {"referenceID": 21, "context": "We measure the performance by BLEU score (Papineni et al., 2002).", "startOffset": 41, "endOffset": 64}, {"referenceID": 13, "context": "The training time for (3) and (4) is calculated based on the training speed in (Lee et al., 2016).", "startOffset": 79, "endOffset": 97}, {"referenceID": 7, "context": "Note that, the purely character model of (5)(Luong and Manning, 2016) took 3 months to train and yielded +0.", "startOffset": 44, "endOffset": 69}, {"referenceID": 23, "context": "It matches the behavior of human perception (White et al., 2008).", "startOffset": 44, "endOffset": 64}, {"referenceID": 5, "context": "Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015).", "startOffset": 80, "endOffset": 123}, {"referenceID": 6, "context": "Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015).", "startOffset": 80, "endOffset": 123}], "year": 2016, "abstractText": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_377"}