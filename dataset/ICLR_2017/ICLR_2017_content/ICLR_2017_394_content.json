{"name": "ICLR_2017_394.pdf", "metadata": {"source": "CRF", "title": "SKIP-GRAM NEGATIVE SAMPLING", "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "emails": ["newo@newo.su,", "oleksii.hrinchuk@skolkovotech.ru,", "gleb57@yandex-team.ru,", "pavser@yandex-team.ru,", "ioseledets@skoltech.ru"], "sections": [{"heading": "1 INTRODUCTION", "text": "In this paper, we consider the problem of embedding words into a low-dimensional space in order to measure the semantic similarity between them. As an example, how to find whether the word \u201ctable\u201d is semantically more similar to the word \u201cstool\u201d than to the word \u201csky\u201d? That is achieved by constructing a low-dimensional vector representation for each word and measuring similarity between the words as the similarity between the corresponding vectors.\nOne of the most popular word embedding models by Mikolov et al. (2013) is a discriminative neural network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3). It aims at predicting whether two words can be found close to each other within a text. As shown in Section 2, the process of word embeddings training using SGNS can be divided into two general steps with clear objectives:\nStep 1. Search for a low-rank matrix X that provides a good SGNS objective value; Step 2. Search for a good low-rank representation X = WC\u22a4 in terms of linguistic metrics,\nwhere W is a matrix of word embeddings and C is a matrix of so-called context embeddings.\nUnfortunately, most previous approaches mixed these two steps into a single one, what entails a not completely correct formulation of the optimization problem. For example, popular approaches to train embeddings (including the original \u201cword2vec\u201d implementation) do not take into account that the objective from Step 1 depends only on the product X = WC\u22a4: instead of straightforward computing of the derivative w.r.t. X , these methods are explicitly based on the derivatives w.r.t. W and C, what complicates the optimization procedure. Moreover, such approaches do not take into account that parametrization WC\u22a4 of matrix X is non-unique and Step 2 is required. Indeed, for any invertible matrix S, we have X = W1C\u22a41 = W1SS \u22121C\u22a41 = W2C \u22a4 2 , therefore, solutions W1C1 and W2C2 are equally good in terms of the SGNS objective but entail different cosine similarities between embeddings and, as a result, different performance in terms of linguistic metrics (see Section 4.2 for details).\nA successful attempt to follow the above described steps, which outperforms the original SGNS optimization approach in terms of various linguistic tasks, was proposed by Levy & Goldberg (2014). In order to obtain a low-rank matrix X on Step 1, the method reduces the dimensionality of Shifted Positive Pointwise Mutual Information (SPPMI) matrix via Singular Value Decomposition (SVD). On Step 2, it computes embeddings W and C via a simple formula that depends on the factors obtained by SVD. However, this method has one important limitation: SVD provides a solution to a surrogate optimization problem, which has no direct relation to the SGNS objective. In fact, SVD minimizes the Mean Squared Error (MSE) between X and SPPMI matrix, what does not lead to minimization of SGNS objective in general (see Section 6.1 and Section 4.2 in Levy & Goldberg (2014) for details).\nThese issues bring us to the main idea of our paper: while keeping the low-rank matrix search setup on Step 1, optimize the original SGNS objective directly. This leads to an optimization problem over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future work.\nAs a result, our approach achieves the significant improvement in terms of SGNS optimization on Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense, what, most importantly, opens the way to applying even more advanced approaches based on it (e.g., more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of Step 2).\nTo summarize, the main contributions of our paper are:\n\u2022 We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives;\n\u2022 For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly;\n\u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013); Schnabel et al. (2015))."}, {"heading": "2 PROBLEM SETTING", "text": ""}, {"heading": "2.1 SKIP-GRAM NEGATIVE SAMPLING", "text": "In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model (Mikolov et al. (2013)), which is a probabilistic discriminative model. Assume we have a text corpus given as a sequence of words w1, . . . , wn, where n may be larger than 1012 and wi \u2208 VW belongs to a vocabulary of words VW . A context c \u2208 VC of the word wi is a word from set {wi\u2212L, ..., wi\u22121, wi+1, ..., wi+L} for some fixed window size L. Let w, c \u2208 Rd be the word embeddings of word w and context c, respectively. Assume they are specified by the following mappings:\nW : VW \u2192 Rd, C : VC \u2192 Rd. The ultimate goal of SGNS word embedding training is to fit good mappingsW and C. In the SGNS model, the probability that pair (w, c) is observed in the corpus is modeled as a following function:\nP ((w, c) \u2208 D|w, c) = \u03c3(\u27e8w, c\u27e9) = 1 1 + exp(\u2212\u27e8w, c\u27e9) , (1)\nwhere D is the multiset of all word-context pairs (w, c) observed in the corpus and \u27e8x,y\u27e9 is the scalar product of vectors x and y. Number d is a hyperparameter that adjusts the flexibility of the model. It usually takes values from tens to hundreds.\nIn order to collect a training set, we take all pairs (w, c) from D as positive examples and k randomly generated pairs (w, c) as negative ones. Let #(w, c) be the number of times the pair (w, c) appears\nin D. Thereby the number of times the word w and the context c appear in D can be computed as #(w) = \u2211 c\u2208Vc #(w, c) and #(c) = \u2211 w\u2208Vw #(w, c) accordingly. Then negative examples are generated from the distribution defined by #(c) counters: PD(c) = #(c) |D| . In this way, we have a model maximizing the following logarithmic likelihood objective for each word pair (w, c):\n#(w, c)(log \u03c3(\u27e8w, c\u27e9) + k \u00b7 Ec\u2032\u223cPD log \u03c3(\u2212\u27e8w, c\u2032\u27e9)). (2) In order to maximize the objective over all observations for each pair (w, c), we arrive at the following SGNS optimization problem over all possible mappingsW and C:\nl = \u2211\nw\u2208VW \u2211 c\u2208VC #(w, c)(log \u03c3(\u27e8w, c\u27e9) + k \u00b7 Ec\u2032\u223cPD log \u03c3(\u2212\u27e8w, c\u2032\u27e9))\u2192 maxW, . (3)\nUsually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014))."}, {"heading": "2.2 OPTIMIZATION OVER LOW-RANK MATRICES", "text": "Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1)."}, {"heading": "2.2.1 SGNS LOSS FUNCTION", "text": "As shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form:\nlw,c(w, c) =#(w, c) log \u03c3(\u27e8w, c\u27e9) + k #(w)#(c)\n|D| log \u03c3(\u2212\u27e8w, c\u27e9). (4)\nA crucial observation is that this loss function depends only on the scalar product \u27e8w, c\u27e9 but not on embeddings w and c separately:\nlw,c(w, c) = fw,c(xw,c),\nfw,c(xw,c) = aw,c log \u03c3(xw,c) + bw,c log \u03c3(\u2212xw,c), where xw,c is the scalar product \u27e8w, c\u27e9 and aw,c = #(w, c), bw,c = k#(w)#(c)|D| are constants."}, {"heading": "2.2.2 MATRIX NOTATION", "text": "Denote |VW | as n and |VC | as m. Let W \u2208 Rn\u00d7d and C \u2208 Rm\u00d7d be matrices, where each row w \u2208 Rd of matrix W is the word embedding of the corresponding word w and each row c \u2208 Rd of matrix C is the context embedding of the corresponding context c. Then the elements of the product of these matrices\nX = WC\u22a4\nare the scalar products xw,c of all pairs (w, c):\nX = (xw,c), w \u2208 VW , c \u2208 VC . Note that this matrix has rank d, because X equals to the product of two matrices with sizes (n\u00d7 d) and (d\u00d7m). Now we can write SGNS objective given by (3) as a function of X:\nF (X) = \u2211\nw\u2208VW \u2211 c\u2208VC fw,c(xw,c), F : Rn\u00d7m \u2192 R. (5)\nThis arrives us at the following proposition:\nProposition 1 SGNS optimization problem given by (3) can be rewritten in the following constrained form:\nmaximize X\u2208Rn\u00d7m F (X), subject to X \u2208Md, (6)\nwhereMd is the manifold (Udriste (1994)) of all matrices in Rn\u00d7m with rank d: Md = {X \u2208 Rn\u00d7m : rank(X) = d}.\nThe key idea of this paper is to solve the optimization problem given by (6) via the framework of Riemannian optimization, which we introduce in Section 3.\nImportant to note that this prospect does not suppose the optimization over parameters W and C directly. This entails the optimization in the space with ((n + m \u2212 d) \u00b7 d) degrees of freedom (Mukherjee et al. (2015)) instead of ((n + m) \u00b7 d), what simplifies the optimization process (see Section 5 for the experimental results)."}, {"heading": "2.3 COMPUTING EMBEDDINGS FROM A LOW-RANK SOLUTION", "text": "Once X is found, we need to recover W and C such that X = WC\u22a4 (Step 2 in the scheme described in Section 1). This problem does not have a unique solution, since if (W,C) satisfy this equation, then WS\u22121 and CS\u22a4 satisfy it as well for any non-singular matrix S. Moreover, different solutions may achieve different values of the linguistic metrics (see Section 4.2 for details). While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed by Levy et al. (2015) and it shows good results in practice. We compute SVD of X in the form X = U\u03a3V \u22a4, where U and V have orthonormal columns, and \u03a3 is the diagonal matrix, and use\nW = U \u221a \u03a3, C = V \u221a \u03a3\nas matrices of embeddings.\nA simple justification of this solution is the following: we need to map words into vectors in a way that similar words would have similar embeddings in terms of cosine similarities:\ncos(w1,w2) = \u27e8w1,w2\u27e9 \u2225w1\u2225 \u00b7 \u2225w2\u2225 .\nIt is reasonable to assume that two words are similar, if they share contexts. Therefore, we can estimate the similarity of two words w1, w2 as s(w1, w2) = \u2211 c\u2208VC xw1,c \u00b7 xw2,c, what is the element of the matrix XX\u22a4 with indices (w1, w2). Note that XX\u22a4 = U\u03a3V \u22a4V \u03a3U\u22a4 = U\u03a32U\u22a4. If we choose W = U\u03a3, we exactly obtain \u27e8w1,w2\u27e9 = s(w1, w2), since WW\u22a4 = XX\u22a4 in this case. That is, the cosine similarity of the embeddings w1,w2 coincides with the intuitive similarity s(w1, w2). However, scaling by \u221a \u03a3 instead of \u03a3 was shown by Levy et al. (2015) to be a better solution in experiments."}, {"heading": "3 PROPOSED METHOD", "text": ""}, {"heading": "3.1 RIEMANNIAN OPTIMIZATION", "text": ""}, {"heading": "3.1.1 GENERAL SCHEME", "text": "The main idea of Riemannian optimization (Udriste (1994)) is to consider (6) as a constrained optimization problem. Assume we have an approximated solution Xi on a current step of the optimization process, where i is the step number. In order to improve Xi, the next step of the standard gradient ascent outputs Xi + \u2207F (Xi), where \u2207F (Xi) is the gradient of objective F at the point Xi. Note that the gradient \u2207F (Xi) can be naturally considered as a matrix in Rn\u00d7m. Point Xi + \u2207F (Xi) leaves the manifoldMd, because its rank is generally greater than d. That is why Riemannian optimization methods map point Xi + \u2207F (Xi) back to manifoldMd. The standard Riemannian gradient method first projects the gradient step onto the tangent space at the current point Xi and then retracts it back to the manifold:\nXi+1 = R (PTM (Xi +\u2207F (Xi))),\nwhere R is the retraction operator, and PTM is the projection onto the tangent space."}, {"heading": "3.1.2 PROJECTOR-SPLITTING ALGORITHM", "text": "In our paper, we use a much simpler version of such approach that retracts point Xi + \u2207F (Xi) directly to the manifold, as illustrated on Figure 1: Xi+1 = R(Xi +\u2207F (Xi)).\nUnder review as a conference paper at ICLR 2017\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrFi\nXi = UiSiV T i\nXi+1 = Ui+1Si+1V T i+1\nretraction\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah Keywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrFi\nXi = UiSiV T i\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx\nABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE"}, {"heading": "1. INTRODUCTION", "text": "sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xxxxx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx\nABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INTRODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classro m us is r nted without f e provided tha copies are not made or distributed for profit or commercial advantage an that copies bear this notice and the full citation n the first page. To copy otherwise, to republish, to post on servers or to redistribute o lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFine-tuning word embeddings xxxxx xx xxxxx xxxx xxxx xxxx xxx xxxxx xxxx xxxxx ABSTRACT Blah-blah\nKeywords word embeddings, SGNS, word2vec, GLOVE\n1. INT ODUCTION sdfdsf\n2. CONCLUSIONS\n3. RELATED WORK Mikolov main [?] Levi main [?]\nrF (Xi)\nXi +rF (Xi)\nXi = UiSiV T i\nXi\nXi+1\nXi+1 = Ui+1Si+1V T i+1\nretraction\nMd\n4. CONCLUSIONS\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. WOODSTOCK \u201997 El Paso, Texas USA Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nFigure 1: Geometric interpretation of one step of projector-splitting optimization procedure: the gradient step an the retraction of the high-rank atr x Xi + \u2207F (Xi) to the manifold of low-rank matrices Md.\nIntuitively, retractor R finds rank-d matrix on t e manifoldMd tha is similar to high-rank matrix Xi+\u2207F (Xi) in terms o Frobeniu orm. How can e do it? The most straightforward way to reduce the rank of Xi +\u2207F (Xi) is to perform the SVD, which keeps d largest singular values of it:\n1: Ui+1, Si+1, V \u22a4i+1 \u2190 SVD(Xi +\u2207F (Xi)), 2: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1.\n(7)\nHowever, it is computationally expensive. Instead of this approach, we use the projector-splitting method (Lubich & Oseledets (2014)), which is a second-order retraction onto the manifold (for details, see the review by Absil & Oseledets (2015)). Its practical implementation is also quite intuitive: instead of computing the full SVD of Xi +\u2207F (Xi) according to the gradient projection method, we use just one step of the block power numerical method (Bentbib & Kanber (2015)) which computes the SVD, what reduces the computational complexity.\nLet us keep the current point in the following factorized form: Xi = UiSiV \u22a4 i , (8) where matrices Ui \u2208 Rn\u00d7d and Vi \u2208 Rm\u00d7d have d orthonormal columns and Si \u2208 Rd\u00d7d. Then we need to perform two QR-decompositions to retract point Xi +\u2207F (Xi) back to the manifold:\n1: Ui+1, Si+1 \u2190 QR ((Xi +\u2207F (Xi))Vi) , 2: Vi+1, S\u22a4i+1 \u2190 QR ( (Xi +\u2207F (Xi))\u22a4Ui+1 ) ,\n3: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1. In this way, we always keep the solution Xi+1 = Ui+1Si+1V \u22a4i+1 on the manifold Md and in the form (8).\nWhat is important, we only need to compute \u2207F (Xi), so the gradients with respect to U , S and V are never computed explicitly, thus avoiding the subtle case where S is close to singular (so-called singular (critical) point on the manifold). Indeed, the gradient with respect to U (while keeping the orthogonality constraints) can be written (Koch & Lubich (2007)) as:\n\u2202F \u2202U = \u2202F \u2202X V S\u22121,\nwhich means that the gradient will be large if S is close to singular. The projector-splitting scheme is free from this problem."}, {"heading": "3.2 ALGORITHM", "text": "In case of SGNS objective given by (5), an element of gradient \u2207F has the form:\n(\u2207F (X))w,c = \u2202fw,c(xw,c)\n\u2202xw,c = #(w, c) \u00b7 \u03c3 (\u2212xw,c)\u2212 k\n#(w)#(c)\n|D| \u00b7 \u03c3 (xw,c) .\nTo make the method more flexible in terms of convergence properties, we additionally use \u03bb \u2208 R, which is a step size parameter. In this case, retractor R returns Xi + \u03bb\u2207F (Xi) instead of Xi +\u2207F (Xi) onto the manifold. The whole optimization procedure is summarized in Algorithm 1.\nAlgorithm 1 Riemannian Optimization for SGNS Require: Dimentionality d, initialization W0 and C0, step size \u03bb, gradient function\u2207F : Rn\u00d7m \u2192\nRn\u00d7m, number of iterations K Ensure: Factor W \u2208 Rn\u00d7d\n1: X0 \u2190W0C\u22a40 # get an initial point at the manifold 2: U0, S0, V \u22a40 \u2190 SVD(X0) # compute the first point satisfying the low-rank constraint 3: i\u2190 0 4: while i < K do 5: Ui+1, Si+1 \u2190 QR ((Xi + \u03bb\u2207F (Xi))Vi) # perform one step of the block power method with two QR-decompositions 6: Vi+1, S\u22a4i+1 \u2190 QR ( (Xi + \u03bb\u2207F (Xi))\u22a4Ui+1\n) 7: Xi+1 \u2190 Ui+1Si+1V \u22a4i+1 # update the point at the manifold 8: i\u2190 i+ 1 9: end while\n10: U,\u03a3, V \u22a4 \u2190 SVD(XK) 11: W \u2190 U \u221a \u03a3 # compute word embeddings 12: return W"}, {"heading": "4 EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1 TRAINING MODELS", "text": "We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1.\nThe models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result, we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are equal). The size of the context window was set to 5 for all experiments, as it was done by Levy & Goldberg (2014); Mikolov et al. (2013). We conduct two series of experiments: for dimensionality d = 100 and d = 200.\nOptimization step size is chosen to be small enough to avoid huge gradient values. However, thorough choice of \u03bb does not result in a significant difference in performance (this parameter was tuned on the training data only, the exact values used in experiments are reported below)."}, {"heading": "4.2 EVALUATION", "text": "We evaluate word embeddings via the word similarity task. We use the following popular datasets for this purpose: \u201cwordsim-353\u201d (Finkelstein et al. (2001); 3 datasets), \u201csimlex-999\u201d (Hill et al. (2016)) and \u201cmen\u201d (Bruni et al. (2014)). Original \u201cwordsim-353\u201d dataset is a mixture of the word pairs for both word similarity and word relatedness tasks. This dataset was split (Agirre et al. (2009)) into two intersecting parts: \u201cwordsim-sim\u201d (\u201cws-sim\u201d in the tables) and \u201cwordsim-rel\u201d (\u201cws-rel\u201d in the tables) to separate the words from different tasks. In our experiments, we use both of them on a par with the full version of \u201cwordsim-353\u201d (\u201cws-full\u201d in the tables). Each dataset contains word pairs together with assessor-assigned similarity scores for each pair. As a quality measure, we use Spearman\u2019s correlation between these human ratings and cosine similarities for each pair. We call this quality metric linguistic in our paper.\n1https://github.com/newozz/riemannian_sgns 2Enwik9 corpus can be found here: http://mattmahoney.net/dc/textdata"}, {"heading": "5 RESULTS OF EXPERIMENTS", "text": "First of all, we compare the value of SGNS objective obtained by the methods. The comparison is demonstrated in Table 1.\nWe see that SGD-SGNS and SVD-SPPMI methods provide quite similar results, however, the proposed method obtains significantly better SGNS values, what proves the feasibility of using Riemannian optimization framework in SGNS optimization problem. It is interesting to note that SVDSPPMI method, which does not optimize SGNS objective directly, obtains better results than SGDSGNS method, which aims at optimizing SGNS. This fact additionally confirms the idea described in Section 2.2.2 that the independent optimization over parameters W and C may decrease the performance.\nHowever, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods in terms of it. We see that our method outperforms the competitors on all datasets except for \u201cmen\u201d dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.\nIn order to understand how exactly our model improves or degrades the performance in comparison to the baseline, we found several words, whose neighbors in terms of cosine distance change significantly. Table 3 demonstrates neighbors of words \u201cfive\u201d, \u201che\u201d and \u201cmain\u201d in terms of our model and its nearest competitor according to the similarity task \u2014 SVD-SPPMI. These words were chosen as representative examples whose neighborhoods in terms of SVD-SPPMI and RO-SGNS models are strikingly different. A neighbour of a source word is bold if we suppose that it has a similar semantic meaning to the source word. First of all, we notice that our model produces much better neighbors of the words describing digits or numbers (see word \u201cfive\u201d as an example). The similar situation happens for many other words, e.g. in case of word \u201cmain\u201d \u2014 the nearest neighbors contain 4 similar words in case of our model instead of 2 in case of SVD-SPPMI. The neighbourhood of word \u201che\u201d contains less semantically similar words in case of our model. However, it filters out completely irrelevant words, such as \u201cpromptly\u201d and \u201cdumbledore\u201d.\nTalking about the optimal number K of iterations in the optimization procedure and step size \u03bb, we found that they depend on the particular value of dimensionality d. For d = 100, we have K = 25, \u03bb \u2248 5 \u00b7 10\u22125, and for d = 200, we have K = 13, \u03bb = 10\u22124. Moreover, it is interesting that the best results were obtained when SVD-SPPMI embeddings were used as an initialization of Riemannian optimization process."}, {"heading": "6 RELATED WORK", "text": ""}, {"heading": "6.1 WORD EMBEDDINGS", "text": "Skip-Gram Negative Sampling was introduced by Mikolov et al. (2013). The \u201cnegative sampling\u201d approach was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-\nplained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as \u201cword2vec\u201d 34.\nAs shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function. There are two general assumptions made in their algorithm that distinguish it from the SGNS optimization:\n1. SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function.\n2. In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner (SPPMI matrix) before applying SVD.\nThis makes the objective not interpretable in terms of the original task (3). As mentioned by Levy & Goldberg (2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs, what may entail the performance fall. The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al. (2015); Levy et al. (2015) give a good overview of highly practical methods to improve these word embedding models."}, {"heading": "6.2 RIEMANNIAN OPTIMIZATION", "text": "An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d.\nRiemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al. (2014)), and tensor completion (Kressner et al. (2014))."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In our paper, we proposed the general two-step scheme of training SGNS word embedding model and introduced the algorithm that performs the search of a solution in the low-rank form via Riemannian optimization framework. We also demonstrated the superiority of the proposed method, by providing the experimental comparison to the existing state-of-the-art approaches.\nIt seems to be an interesting direction of future work to apply more advanced optimization techniques to Step 1 of the scheme proposed in Section 1 and to explore the Step 2 \u2014 obtaining embeddings with a given low-rank matrix.\n3Original Google word2vec: https://code.google.com/archive/p/word2vec/ 4Gensim word2vec: https://radimrehurek.com/gensim/models/word2vec.html"}], "references": [{"title": "Low-rank retractions: a survey and new results", "author": ["P-A Absil", "Ivan V Oseledets"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Absil and Oseledets.,? \\Q2015\\E", "shortCiteRegEx": "Absil and Oseledets.", "year": 2015}, {"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In NAACL, pp", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Block power method for svd decomposition", "author": ["AH Bentbib", "A Kanber"], "venue": "Analele Stiintifice Ale Unversitatii Ovidius Constanta-Seria Matematica,", "citeRegEx": "Bentbib and Kanber.,? \\Q2015\\E", "shortCiteRegEx": "Bentbib and Kanber.", "year": 2015}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In WWW,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722,", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Towards a better understanding of predict and count models", "author": ["S Sathiya Keerthi", "Tobias Schnabel", "Rajiv Khanna"], "venue": "arXiv preprint arXiv:1511.02024,", "citeRegEx": "Keerthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2015}, {"title": "Dynamical low-rank approximation", "author": ["Othmar Koch", "Christian Lubich"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Koch and Lubich.,? \\Q2007\\E", "shortCiteRegEx": "Koch and Lubich.", "year": 2007}, {"title": "Low-rank tensor completion by riemannian optimization", "author": ["Daniel Kressner", "Michael Steinlechner", "Bart Vandereycken"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Kressner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "How to generate a good word embedding", "author": ["Siwei Lai", "Kang Liu", "Shi He", "Jun Zhao"], "venue": "arXiv preprint arXiv:1507.05523,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In NIPS, pp", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "A projector-splitting integrator for dynamical low-rank approximation", "author": ["Christian Lubich", "Ivan V Oseledets"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Lubich and Oseledets.,? \\Q2014\\E", "shortCiteRegEx": "Lubich and Oseledets.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fixed-rank matrix factorizations and riemannian low-rank optimization", "author": ["Bamdev Mishra", "Gilles Meyer", "Silv\u00e8re Bonnabel", "Rodolphe Sepulchre"], "venue": "Computational Statistics,", "citeRegEx": "Mishra et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mishra et al\\.", "year": 2014}, {"title": "On the degrees of freedom of reduced-rank estimators in multivariate regression", "author": ["A Mukherjee", "K Chen", "N Wang", "J Zhu"], "venue": null, "citeRegEx": "Mukherjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2015}, {"title": "word2vec parameter learning explained", "author": ["Xin Rong"], "venue": "arXiv preprint arXiv:1411.2738,", "citeRegEx": "Rong.,? \\Q2014\\E", "shortCiteRegEx": "Rong.", "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In EMNLP,", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang", "Bart Vandereycken", "Sinno Jialin Pan"], "venue": "In ICML,", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Convex functions and optimization methods on Riemannian manifolds, volume 297", "author": ["Constantin Udriste"], "venue": "Springer Science & Business Media,", "citeRegEx": "Udriste.,? \\Q1994\\E", "shortCiteRegEx": "Udriste.", "year": 1994}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["Bart Vandereycken"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Vandereycken.,? \\Q2013\\E", "shortCiteRegEx": "Vandereycken.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "One of the most popular word embedding models by Mikolov et al. (2013) is a discriminative neural network that optimizes Skip-Gram Negative Sampling (SGNS) objective (see Equation 3).", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "This leads to an optimization problem over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)).", "startOffset": 98, "endOffset": 119}, {"referenceID": 14, "context": "This leads to an optimization problem over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)).", "startOffset": 98, "endOffset": 189}, {"referenceID": 14, "context": "This leads to an optimization problem over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational complexity.", "startOffset": 98, "endOffset": 273}, {"referenceID": 14, "context": "This leads to an optimization problem over matrix X with the low-rank constraint, which is often (Mishra et al. (2014)) solved by applying Riemannian optimization framework (Udriste (1994)). In our paper, we use the projector-splitting algorithm (Lubich & Oseledets (2014)), which is easy to implement and has low computational complexity. Of course, Step 2 may be improved as well, but we regard this as a direction of future work. As a result, our approach achieves the significant improvement in terms of SGNS optimization on Step 1 and, moreover, the improvement on Step 1 entails the improvement on Step 2 in terms of linguistic metrics. That is why, the proposed two-step decomposition of the problem makes sense, what, most importantly, opens the way to applying even more advanced approaches based on it (e.g., more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of Step 2). To summarize, the main contributions of our paper are: \u2022 We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives; \u2022 For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly; \u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al.", "startOffset": 98, "endOffset": 1389}, {"referenceID": 14, "context": "To summarize, the main contributions of our paper are: \u2022 We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives; \u2022 For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly; \u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013); Schnabel et al.", "startOffset": 465, "endOffset": 487}, {"referenceID": 14, "context": "To summarize, the main contributions of our paper are: \u2022 We reformulated the problem of SGNS word embedding learning as a two-step procedure with clear objectives; \u2022 For Step 1, we developed an algorithm based on Riemannian optimization framework that optimizes SGNS objective over low-rank matrix X directly; \u2022 Our algorithm outperforms state-of-the-art competitors in terms of SGNS objective and the semantic similarity linguistic metric (Levy & Goldberg (2014); Mikolov et al. (2013); Schnabel et al. (2015)).", "startOffset": 465, "endOffset": 511}, {"referenceID": 14, "context": "1 SKIP-GRAM NEGATIVE SAMPLING In this paper, we consider the Skip-Gram Negative Sampling (SGNS) word embedding model (Mikolov et al. (2013)), which is a probabilistic discriminative model.", "startOffset": 118, "endOffset": 140}, {"referenceID": 14, "context": "(3) Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)).", "startOffset": 138, "endOffset": 160}, {"referenceID": 14, "context": "(3) Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)).", "startOffset": 138, "endOffset": 173}, {"referenceID": 14, "context": "(3) Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)). 2.2 OPTIMIZATION OVER LOW-RANK MATRICES Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1).", "startOffset": 138, "endOffset": 274}, {"referenceID": 14, "context": "(3) Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)). 2.2 OPTIMIZATION OVER LOW-RANK MATRICES Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1). 2.2.1 SGNS LOSS FUNCTION As shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form: lw,c(w, c) =#(w, c) log \u03c3(\u27e8w, c\u27e9) + k #(w)#(c) |D| log \u03c3(\u2212\u27e8w, c\u27e9).", "startOffset": 138, "endOffset": 568}, {"referenceID": 14, "context": "(3) Usually, this optimization is done via the stochastic gradient descent procedure that is performed during passing through the corpus (Mikolov et al. (2013); Rong (2014)). 2.2 OPTIMIZATION OVER LOW-RANK MATRICES Relying on the prospect proposed by Levy & Goldberg (2014), let us show that the optimization problem given by (3) can be considered as a problem of searching for a matrix that maximizes a certain objective function and has the rank-d constraint (Step 1 in the scheme described in Section 1). 2.2.1 SGNS LOSS FUNCTION As shown by Levy & Goldberg (2014), the logarithmic likelihood (3) can be represented as the sum of lw,c(w, c) over all pairs (w, c), where lw,c(w, c) has the following form: lw,c(w, c) =#(w, c) log \u03c3(\u27e8w, c\u27e9) + k #(w)#(c) |D| log \u03c3(\u2212\u27e8w, c\u27e9). (4) A crucial observation is that this loss function depends only on the scalar product \u27e8w, c\u27e9 but not on embeddings w and c separately: lw,c(w, c) = fw,c(xw,c), fw,c(xw,c) = aw,c log \u03c3(xw,c) + bw,c log \u03c3(\u2212xw,c), where xw,c is the scalar product \u27e8w, c\u27e9 and aw,c = #(w, c), bw,c = k |D| are constants. 2.2.2 MATRIX NOTATION Denote |VW | as n and |VC | as m. Let W \u2208 Rn\u00d7d and C \u2208 Rm\u00d7d be matrices, where each row w \u2208 R of matrix W is the word embedding of the corresponding word w and each row c \u2208 R of matrix C is the context embedding of the corresponding context c. Then the elements of the product of these matrices X = WC\u22a4 are the scalar products xw,c of all pairs (w, c): X = (xw,c), w \u2208 VW , c \u2208 VC . Note that this matrix has rank d, because X equals to the product of two matrices with sizes (n\u00d7 d) and (d\u00d7m). Now we can write SGNS objective given by (3) as a function of X: F (X) = \u2211 w\u2208VW \u2211 c\u2208VC fw,c(xw,c), F : Rn\u00d7m \u2192 R. (5) This arrives us at the following proposition: Proposition 1 SGNS optimization problem given by (3) can be rewritten in the following constrained form: maximize X\u2208Rn\u00d7m F (X), subject to X \u2208Md, (6) whereMd is the manifold (Udriste (1994)) of all matrices in Rn\u00d7m with rank d: Md = {X \u2208 Rn\u00d7m : rank(X) = d}.", "startOffset": 138, "endOffset": 1945}, {"referenceID": 16, "context": "This entails the optimization in the space with ((n + m \u2212 d) \u00b7 d) degrees of freedom (Mukherjee et al. (2015)) instead of ((n + m) \u00b7 d), what simplifies the optimization process (see Section 5 for the experimental results).", "startOffset": 86, "endOffset": 110}, {"referenceID": 12, "context": "While our paper focuses on Step 1, we use, for Step 2, a heuristic approach that was proposed by Levy et al. (2015) and it shows good results in practice.", "startOffset": 97, "endOffset": 116}, {"referenceID": 12, "context": "However, scaling by \u221a \u03a3 instead of \u03a3 was shown by Levy et al. (2015) to be a better solution in experiments.", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "1 GENERAL SCHEME The main idea of Riemannian optimization (Udriste (1994)) is to consider (6) as a constrained optimization problem.", "startOffset": 59, "endOffset": 74}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014).", "startOffset": 224, "endOffset": 246}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper.", "startOffset": 224, "endOffset": 349}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1. The models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al.", "startOffset": 224, "endOffset": 855}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1. The models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)).", "startOffset": 224, "endOffset": 878}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1. The models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result, we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are equal). The size of the context window was set to 5 for all experiments, as it was done by Levy & Goldberg (2014); Mikolov et al.", "startOffset": 224, "endOffset": 1099}, {"referenceID": 14, "context": "1 TRAINING MODELS We compare our method (\u201cRO-SGNS\u201d in the tables) performance to two baselines: SGNS embeddings optimized via Stochastic Gradient Descent, implemented in the original \u201cword2vec\u201d, (\u201cSGDSGNS\u201d in the tables) by Mikolov et al. (2013) and embeddings obtained by SVD over SPPMI matrix (\u201cSVD-SPPMI\u201d in the tables) by Levy & Goldberg (2014). We have also experimented with the blockwise alternating optimization over factors W and C, but the results are almost the same to SGD results, that is why we do not to include them into the paper. The source code of our experiments is available online1. The models were trained on English Wikipedia \u201cenwik9\u201d corpus2, which was previously used in most papers on this topic. Like in previous studies, we counted only the words which occur more than 200 times in the training corpus (Levy & Goldberg (2014); Mikolov et al. (2013)). As a result, we obtained a vocabulary of 24292 unique tokens (set of words VW and set of contexts VC are equal). The size of the context window was set to 5 for all experiments, as it was done by Levy & Goldberg (2014); Mikolov et al. (2013). We conduct two series of experiments: for dimensionality d = 100 and d = 200.", "startOffset": 224, "endOffset": 1122}, {"referenceID": 2, "context": "We use the following popular datasets for this purpose: \u201cwordsim-353\u201d (Finkelstein et al. (2001); 3 datasets), \u201csimlex-999\u201d (Hill et al.", "startOffset": 71, "endOffset": 97}, {"referenceID": 2, "context": "We use the following popular datasets for this purpose: \u201cwordsim-353\u201d (Finkelstein et al. (2001); 3 datasets), \u201csimlex-999\u201d (Hill et al. (2016)) and \u201cmen\u201d (Bruni et al.", "startOffset": 71, "endOffset": 144}, {"referenceID": 1, "context": "This dataset was split (Agirre et al. (2009)) into two intersecting parts: \u201cwordsim-sim\u201d (\u201cws-sim\u201d in the tables) and \u201cwordsim-rel\u201d (\u201cws-rel\u201d in the tables) to separate the words from different tasks.", "startOffset": 24, "endOffset": 45}, {"referenceID": 14, "context": "1 WORD EMBEDDINGS Skip-Gram Negative Sampling was introduced by Mikolov et al. (2013). The \u201cnegative sampling\u201d approach was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-", "startOffset": 64, "endOffset": 86}, {"referenceID": 14, "context": "1 WORD EMBEDDINGS Skip-Gram Negative Sampling was introduced by Mikolov et al. (2013). The \u201cnegative sampling\u201d approach was thoroughly described by Goldberg & Levy (2014), and the learning method is ex-", "startOffset": 64, "endOffset": 171}, {"referenceID": 14, "context": "plained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as \u201cword2vec\u201d 34.", "startOffset": 11, "endOffset": 23}, {"referenceID": 14, "context": "plained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as \u201cword2vec\u201d 34. As shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function.", "startOffset": 11, "endOffset": 353}, {"referenceID": 14, "context": "plained by Rong (2014). There are several open-source implementations of SGNS neural network, which is widely known as \u201cword2vec\u201d 34. As shown in Section 2.2, Skip-Gram Negative Sampling optimization can be reformulated as a problem of searching for a low-rank matrix. In order to be able to use out-of-the-box SVD for this task, Levy & Goldberg (2014) used the surrogate version of SGNS as the objective function. There are two general assumptions made in their algorithm that distinguish it from the SGNS optimization: 1. SVD optimizes Mean Squared Error (MSE) objective instead of SGNS loss function. 2. In order to avoid infinite elements in SPMI matrix, it is transformed in ad-hoc manner (SPPMI matrix) before applying SVD. This makes the objective not interpretable in terms of the original task (3). As mentioned by Levy & Goldberg (2014), SGNS objective weighs different (w, c) pairs differently, unlike the SVD, which works with the same weight for all pairs, what may entail the performance fall.", "startOffset": 11, "endOffset": 847}, {"referenceID": 7, "context": "The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 7, "context": "The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al. (2015); Levy et al.", "startOffset": 105, "endOffset": 146}, {"referenceID": 7, "context": "The comprehensive explanation of the relation between SGNS, SPPMI, SVD-over-SPPMI methods is provided by Keerthi et al. (2015). Lai et al. (2015); Levy et al. (2015) give a good overview of highly practical methods to improve these word embedding models.", "startOffset": 105, "endOffset": 166}, {"referenceID": 18, "context": "2 RIEMANNIAN OPTIMIZATION An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015).", "startOffset": 113, "endOffset": 128}, {"referenceID": 18, "context": "2 RIEMANNIAN OPTIMIZATION An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d.", "startOffset": 113, "endOffset": 241}, {"referenceID": 18, "context": "2 RIEMANNIAN OPTIMIZATION An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d.", "startOffset": 113, "endOffset": 320}, {"referenceID": 18, "context": "2 RIEMANNIAN OPTIMIZATION An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d.", "startOffset": 113, "endOffset": 372}, {"referenceID": 18, "context": "2 RIEMANNIAN OPTIMIZATION An introduction to optimization over Riemannian manifolds can be found in the paper of Udriste (1994). The overview of retractions of high rank matrices to low-rank manifolds is provided by Absil & Oseledets (2015). The projector-splitting algorithm was introduced by Lubich & Oseledets (2014), and also was mentioned by Absil & Oseledets (2015) as \u201cLie-Trotter retraction\u201d. Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al.", "startOffset": 113, "endOffset": 534}, {"referenceID": 18, "context": "Riemannian optimization is succesfully applied to various data science problems: for example, matrix completion (Vandereycken (2013)), large-scale recommender systems (Tan et al. (2014)), and tensor completion (Kressner et al.", "startOffset": 168, "endOffset": 186}], "year": 2016, "abstractText": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \u201cword2vec\u201d software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_394"}