{"name": "ICLR_2017_395.pdf", "metadata": {"source": "CRF", "title": "RL: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING", "authors": ["Yan Duan", "John Schulman", "Xi Chen", "Peter L. Bartlett", "Ilya Sutskever", "Pieter Abbeel"], "emails": ["rocky@openai.com,", "joschu@openai.com,", "peter@openai.com,", "peter@berkeley.edu,", "ilyasu@openai.com", "pieter@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), and acquiring advanced manipulation and locomotion skills (Levine et al., 2016; Lillicrap et al., 2015; Watter et al., 2015; Heess et al., 2015b; Schulman et al., 2015; 2016). However, many of the successes come at the expense of high sample complexity. For example, the state-of-the-art Atari results require tens of thousands of episodes of experience (Mnih et al., 2015) per game. To master a game, one would need to spend nearly 40 days playing it with no rest. In contrast, humans and animals are capable of learning a new task in a very small number of trials. Continuing the previous example, the human player in Mnih et al. (2015) only needed 2 hours of experience before mastering a game. We argue that the reason for this sharp contrast is largely due to the lack of a good prior, which results in these deep RL agents needing to rebuild their knowledge about the world from scratch.\nAlthough Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process (Strens, 2000; Ghavamzadeh et al., 2015; Kolter & Ng, 2009), exact computation of the Bayesian update is intractable in all but the simplest cases. Thus, practical reinforcement learning algorithms often incorporate a mixture of Bayesian and domain-specific ideas to bring down sample complexity and computational burden. Notable examples include guided policy search with unknown dynamics (Levine & Abbeel, 2014) and PILCO (Deisenroth & Rasmussen, 2011). These methods can learn a task using a few minutes to a few hours of real experience, compared to days or even weeks required by previous methods (Schulman et al., 2015; 2016; Lillicrap et al., 2015). However, these methods tend to make assumptions about the environment (e.g., instrumentation for access to the state at learning time), or become computationally intractable in high-dimensional settings (Wahlstro\u0308m et al., 2015).\nRather than hand-designing domain-specific reinforcement learning algorithms, we take a different approach in this paper: we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent. We structure the agent as a recurrent neural network, which receives past rewards, actions, and termination flags as inputs in addition to the normally received observations. Furthermore, its internal state is preserved across episodes, so that it has the capacity to perform learning in its own hidden activations. The learned agent thus also acts as the learning algorithm, and can adapt to the task at hand when deployed.\nWe evaluate this approach on two sets of classical problems, multi-armed bandits and tabular MDPs. These problems have been extensively studied, and there exist algorithms that achieve asymptotically optimal performance. We demonstrate that our method, named RL2, can achieve performance comparable with these theoretically justified algorithms. Next, we evaluate RL2 on a vision-based navigation task implemented using the ViZDoom environment (Kempka et al., 2016), showing that RL2 can also scale to high-dimensional problems."}, {"heading": "2 METHOD", "text": ""}, {"heading": "2.1 PRELIMINARIES", "text": "We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tupleM = (S,A,P, r, \u03c10, \u03b3, T ), in which S is a state set, A an action set, P : S \u00d7 A \u00d7 S \u2192 R+ a transition probability distribution, r : S \u00d7A \u2192 [\u2212Rmax, Rmax] a bounded reward function, \u03c10 : S \u2192 R+ an initial state distribution, \u03b3 \u2208 [0, 1] a discount factor, and T the horizon. In policy search methods, we typically optimize a stochastic policy \u03c0\u03b8 : S \u00d7 A \u2192 R+ parametrized by \u03b8. The objective is to maximize its expected discounted return, \u03b7(\u03c0\u03b8) = E\u03c4 [ \u2211T t=0 \u03b3\ntr(st, at)], where \u03c4 = (s0, a0, . . .) denotes the whole trajectory, s0 \u223c \u03c10(s0), at \u223c \u03c0\u03b8(at|st), and st+1 \u223c P(st+1|st, at)."}, {"heading": "2.2 FORMULATION", "text": "We now describe our formulation, which casts learning an RL algorithm as a reinforcement learning problem, and hence the name RL2.\nWe assume knowledge of a set of MDPs, denoted byM, and a distribution over them: \u03c1M :M\u2192 R+. We only need to sample from this distribution. We use n to denote the total number of episodes allowed to spend with a specific MDP. We define a trial to be such a series of episodes of interaction with a fixed MDP.\nThis process of interaction between an agent and the environment is illustrated in Figure 1. Here, each trial happens to consist of two episodes, hence n = 2. For each trial, a separate MDP is drawn from \u03c1M, and for each episode, a fresh s0 is drawn from the initial state distribution specific to the corresponding MDP. Upon receiving an action at produced by the agent, the environment computes reward rt, steps forward, and computes the next state st+1. If the episode has terminated, it sets termination flag dt to 1, which otherwise defaults to 0. Together, the next state st+1, action\nat, reward rt, and termination flag dt, are concatenated to form the input to the policy1, which, conditioned on the hidden state ht+1, generates the next hidden state ht+2 and action at+1. At the end of an episode, the hidden state of the policy is preserved to the next episode, but not preserved between trials.\nThe objective under this formulation is to maximize the expected total discounted reward accumulated during a single trial rather than a single episode. Maximizing this objective is equivalent to minimizing the cumulative pseudo-regret (Bubeck & Cesa-Bianchi, 2012). Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a \u201cfast\u201d reinforcement learning algorithm.\nFor clarity of exposition, we have defined the \u201cinner\u201d problem (of which the agent sees n each trials) to be an MDP rather than a POMDP. However, the method can also be applied in the partiallyobserved setting without any conceptual changes. In the partially observed setting, the agent is faced with a sequence of POMDPs, and it receives an observation ot instead of state st at time t. The visual navigation experiment in Section 3.3, is actually an instance of the this POMDP setting."}, {"heading": "2.3 POLICY REPRESENTATION", "text": "We represent the policy as a general recurrent neural network. Each timestep, it receives the tuple (s, a, r, d) as input, which is embedded using a function \u03c6(s, a, r, d) and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients (Bengio et al., 1994), we use Gated Recurrent Units (GRUs) (Cho et al., 2014) which have been demonstrated to have good empirical performance (Chung et al., 2014; Jo\u0301zefowicz et al., 2015). The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.\nWe have also experimented with alternative architectures which explicitly reset part of the hidden state each episode of the sampled MDP, but we did not find any improvement over the simple architecture described above."}, {"heading": "2.4 POLICY OPTIMIZATION", "text": "After formulating the task as a reinforcement learning problem, we can readily use standard off-theshelf RL algorithms to optimize the policy. We use a first-order implementation of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), because of its excellent empirical performance, and because it does not require excessive hyperparameter tuning. For more details, we refer the reader to the original paper. To reduce variance in the stochastic gradient estimation, we use a baseline which is also represented as an RNN using GRUs as building blocks. We optionally apply Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to further reduce the variance."}, {"heading": "3 EVALUATION", "text": "We designed experiments to answer the following questions:\n\u2022 Can RL2 learn algorithms that achieve good performance on MDP classes with special structure, relative to existing algorithms tailored to this structure that have been proposed in the literature?\n\u2022 Can RL2 scale to high-dimensional tasks?\nFor the first question, we evaluate RL2 on two sets of tasks, multi-armed bandits (MAB) and tabular MDPs. These problems have been studied extensively in the reinforcement learning literature, and this body of work includes algorithms with guarantees of asymptotic optimality. We demonstrate that our approach achieves comparable performance to these theoretically justified algorithms.\n1To make sure that the inputs have a consistent dimension, we use placeholder values for the initial input to the policy.\nFor the second question, we evaluate RL2 on a vision-based navigation task. Our experiments show that the learned policy makes effective use of the learned visual information and also short-term information acquired from previous episodes."}, {"heading": "3.1 MULTI-ARMED BANDITS", "text": "Multi-armed bandit problems are a subset of MDPs where the agent\u2019s environment is stateless. Specifically, there are k arms (actions), and at every time step, the agent pulls one of the arms, say i, and receives a reward drawn from an unknown distribution: our experiments take each arm to be a Bernoulli distribution with parameter pi. The goal is to maximize the total reward obtained over a fixed number of time steps. The key challenge is balancing exploration and exploitation\u2014 \u201cexploring\u201d each arm enough times to estimate its distribution (pi), but eventually switching over to \u201cexploitation\u201d of the best arm. Despite the simplicity of multi-arm bandit problems, their study has led to a rich theory and a collection of algorithms with optimality guarantees.\nUsing RL2, we can train an RNN policy to solve bandit problems by training it on a given distribution \u03c1M. If the learning is successful, the resulting policy should be able to perform competitively with the theoretically optimal algorithms. We randomly generated bandit problems by sampling each parameter pi from the uniform distribution on [0, 1]. After training the RNN policy with RL2, we compared it against the following strategies:\n\u2022 Random: this is a baseline strategy, where the agent pulls a random arm each time.\n\u2022 Gittins index (Gittins, 1979): this method gives the Bayes optimal solution in the discounted infinite-horizon case, by computing an index separately for each arm, and taking the arm with the largest index. While this work shows it is sufficient to independently compute an index for each arm (hence avoiding combinatorial explosion with the number of arms), it doesn\u2019t show how to tractably compute these individual indices exactly. We follow the practical approximations described in Gittins et al. (2011), Chakravorty & Mahajan (2013), and Whittle (1982), and choose the best-performing approximation for each setup.\n\u2022 UCB1 (Auer, 2002): this method estimates an upper-confidence bound, and pulls the arm with the largest value of ucbi(t) = \u00b5\u0302i(t\u22121)+c \u221a 2 log t Ti(t\u22121) , where \u00b5\u0302i(t\u22121) is the estimated\nmean parameter for the ith arm, Ti(t\u22121) is the number of times the ith arm has been pulled, and c is a tunable hyperparameter (Audibert & Munos, 2011). We initialize the statistics with exactly one success and one failure, which corresponds to a Beta(1, 1) prior.\n\u2022 Thompson sampling (TS) (Thompson, 1933): this is a simple method which, at each time step, samples a list of arm means from the posterior distribution, and choose the best arm according to this sample. It has been demonstrated to compare favorably to UCB1 empirically (Chapelle & Li, 2011). We also experiment with an optimistic variant (OTS) (May et al., 2012), which samples N times from the posterior, and takes the one with the highest probability.\n\u2022 -Greedy: in this strategy, the agent chooses the arm with the best empirical mean with probability 1 \u2212 , and chooses a random arm with probability . We use the same initialization as UCB1.\n\u2022 Greedy: this is a special case of -Greedy with = 0.\nThe Bayesian methods, Gittins index and Thompson sampling, take advantage of the distribution \u03c1M; and we provide these methods with the true distribution. For each method with hyperparameters, we maximize the score with a separate grid search for each of the experimental settings. The hyperparameters used for TRPO are shown in the appendix.\nThe results are summarized in Table 1. Learning curves for various settings are shown in Figure 2. We observe that our approach achieves performance that is almost as good as the the reference methods, which were (human) designed specifically to perform well on multi-armed bandit problems. It is worth noting that the published algorithms are mostly designed to minimize asymptotic regret (rather than finite horizon regret), hence there tends to be a little bit of room to outperform them in the finite horizon settings.\nWe observe that there is a noticeable gap between Gittins index and RL2 in the most challenging scenario, with 50 arms and 500 episodes. This raises the question whether better architectures or better (slow) RL algorithms should be explored. To determine the bottleneck, we trained the same policy architecture using supervised learning, using the trajectories generated by the Gittins index approach as training data. We found that the learned policy, when executed in test domains, achieved the same level of performance as the Gittins index approach, suggesting that there is room for improvement by using better RL algorithms."}, {"heading": "3.2 TABULAR MDPS", "text": "The bandit problem provides a natural and simple setting to investigate whether the policy learns to trade off between exploration and exploitation. However, the problem itself involves no sequential decision making, and does not fully characterize the challenges in solving MDPs. Hence, we perform further experiments using randomly generated tabular MDPs, where there is a finite number of possible states and actions\u2014small enough that the transition probability distribution can be explicitly given as a table. We compare our approach with the following methods:\n\u2022 Random: the agent chooses an action uniformly at random for each time step; \u2022 PSRL (Strens, 2000; Osband et al., 2013): this is a direct generalization of Thompson sam-\npling to MDPs, where at the beginning of each episode, we sample an MDP from the posterior distribution, and take actions according to the optimal policy for the entire episode. Similarly, we include an optimistic variant (OPSRL), which has also been explored in Osband & Van Roy (2016). \u2022 BEB (Kolter & Ng, 2009): this is a model-based optimistic algorithm that adds an explo-\nration bonus to (thus far) infrequently visited states and actions.\nSetup Random PSRL OPSRL UCRL2 BEB -Greedy Greedy RL2\nn = 10 100.1 138.1 144.1 146.6 150.2 132.8 134.8 156.2 n = 25 250.2 408.8 425.2 424.1 427.8 377.3 368.8 445.7 n = 50 499.7 904.4 930.7 918.9 917.8 823.3 769.3 936.1 n = 75 749.9 1417.1 1449.2 1427.6 1422.6 1293.9 1172.9 1428.8 n = 100 999.4 1939.5 1973.9 1942.1 1935.1 1778.2 1578.5 1913.7\nThe distribution over MDPs is constructed with |S| = 10, |A| = 5. The rewards follow a Gaussian distribution with unit variance, and the mean parameters are sampled independently from Normal(1, 1). The transitions are sampled from a flat Dirichlet distribution. This construction matches the commonly used prior in Bayesian RL methods. We set the horizon for each episode to be T = 10, and an episode always starts on the first state.\nThe results are summarized in Table 2, and the learning curves are shown in Figure 3. We follow the same evaluation procedure as in the bandit case. We experiment with n \u2208 {10, 25, 50, 75, 100}. For fewer episodes, our approach surprisingly outperforms existing methods by a large margin. The advantage is reversed as n increases, suggesting that the reinforcement learning problem in the outer loop becomes more challenging to solve. We think that the advantage for small n comes from the need for more aggressive exploitation: since there are 140 degrees of freedom to estimate in order to characterize the MDP, and by the 10th episode, we will not have enough samples to form a good estimate of the entire dynamics. By directly optimizing the RNN in this setting, our approach should be able to cope with this shortage of samples, and decides to exploit sooner compared to the reference algorithms."}, {"heading": "3.3 VISUAL NAVIGATION", "text": "The previous two tasks both only involve very low-dimensional state spaces. To evaluate the feasibility of scaling up RL2, we further experiment with a challenging vision-based task, where the\nagent is asked to navigate a randomly generated maze to find a randomly placed target2. The agent receives a +1 reward when it reaches the target, \u22120.001 when it hits the wall, and \u22120.04 per time step to encourage it to reach targets faster. It can interact with the maze for multiple episodes, during which the maze structure and target position are held fixed. The optimal strategy is to explore the maze efficiently during the first episode, and after locating the target, act optimally against the current maze and target based on the collected information. An illustration of the task is given in Figure 4.\nVisual navigation alone is a challenging task for reinforcement learning. The agent only receives very sparse rewards during training, and does not have the primitives for efficient exploration at the beginning of training. It also needs to make efficient use of memory to decide how it should explore the space, without forgetting about where it has already explored. Previously, Oh et al. (2016) have studied similar vision-based navigation tasks in Minecraft. However, they use higher-level actions for efficient navigation. Similar high-level actions in our task would each require around 5 low-level actions combined in the right way. In contrast, our RL2 agent needs to learn these higher-level actions from scratch.\nWe use a simple training setup, where we use small mazes of size 5\u00d7 5, with 2 episodes of interaction, each with horizon up to 250. Here the size of the maze is measured by the number of grid cells along each wall in a discrete representation of the maze. During each trial, we sample 1 out of 1000 randomly generated configurations of map layout and target positions. During testing, we evaluate on 1000 separately generated configurations. In addition, we also study its extrapolation behavior along two axes, by (1) testing on large mazes of size 9\u00d7 9 (see Figure 4c) and (2) running the agent for up to 5 episodes in both small and large mazes. For the large maze, we also increase the horizon per episode by 4x due to the increased size of the maze.\nThe results are summarized in Table 3, and the learning curves are shown in Figure 5. We observe that there is a significant reduction in trajectory lengths between the first two episodes in both the smaller and larger mazes, suggesting that the agent has learned how to use information from past episodes. It also achieves reasonable extrapolation behavior in further episodes by maintaining its performance, although there is a small drop in the rate of success in the larger mazes. We also observe that on larger mazes, the ratio of improved trajectories is lower, likely because the agent has not learned how to act optimally in the larger mazes.\nStill, even on the small mazes, the agent does not learn to perfectly reuse prior information. An illustration of the agent\u2019s behavior is shown in Figure 6. The intended behavior, which occurs most frequently, as shown in 6a and 6b, is that the agent should remember the target\u2019s location, and utilize it to act optimally in the second episode. However, occasionally the agent forgets about where the target was, and continues to explore in the second episode, as shown in 6c and 6d. We believe that better reinforcement learning techniques used as the outer-loop algorithm will improve these results in the future."}, {"heading": "4 RELATED WORK", "text": "The concept of using prior experience to speed up reinforcement learning algorithms has been explored in the past in various forms. Earlier studies have investigated automatic tuning of hyperparameters, such as learning rate and temperature (Ishii et al., 2002; Schweighofer & Doya, 2003), as a form of meta-learning. Wilson et al. (2007) use hierarchical Bayesian methods to maintain a posterior over possible models of dynamics, and apply optimistic Thompson sampling according to the posterior. Many works in hierarchical reinforcement learning propose to extract reusable skills from previous tasks to speed up exploration in new tasks (Singh, 1992; Perkins et al., 1999). We\nrefer the reader to Taylor & Stone (2009) for a more thorough survey on the multi-task and transfer learning aspects.\nThe formulation of searching for a best-performing algorithm, whose performance is averaged over a given distribution over MDPs, have been investigated in the past in more limited forms (Maes et al., 2011; Castronovo et al., 2012). There, they propose to learn an algorithm to solve multiarmed bandits using program search, where the search space consists of simple formulas composed from hand-specified primitives, which needs to be tuned for each specific distribution over MDPs. In comparison, our approach allows for entirely end-to-end training without requiring such domain knowledge.\nMore recently, Fu et al. (2015) propose a model-based approach on top of iLQG with unknown dynamics (Levine & Abbeel, 2014), which uses samples collected from previous tasks to build a neural network prior for the dynamics, and can perform one-shot learning on new, but related tasks thanks to reduced sample complexity. There has been a growing interest in using deep neural networks for multi-task learning and transfer learning (Parisotto et al., 2015; Rusu et al., 2015; 2016a; Devin et al., 2016; Rusu et al., 2016b).\nIn the broader context of machine learning, there has been a lot of interest in one-shot learning for object classification (Vilalta & Drissi, 2002; Fei-Fei et al., 2006; Larochelle et al., 2008; Lake et al., 2011; Koch, 2015). Our work draws inspiration from a particular line of work (Younger et al., 2001; Santoro et al., 2016; Vinyals et al., 2016), which formulates meta-learning as an optimization problem, and can thus be optimized end-to-end via gradient descent. While these work applies to the supervised learning setting, our work applies in the more general reinforcement learning setting. Although the reinforcement learning setting is more challenging, the resulting behavior is far richer: our agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning. Another line of work (Hochreiter et al., 2001; Younger et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2016) studies meta-learning over the optimization process. There, the meta-learner makes explicit updates to a parametrized model. In comparison, we do not use a directly parametrized policy; instead, the recurrent neural network agent acts as the meta-learner and the resulting policy simultaneously.\nOur formulation essentially constructs a partially observable MDP (POMDP) which is solved in the outer loop, where the underlying MDP is unobserved by the agent. This reduction of an unknown MDP to a POMDP can be traced back to dual control theory (Feldbaum, 1960), where \u201cdual\u201d refers to the fact that one is controlling both the state and the state estimate. Feldbaum pointed out that the solution can in principle be computed with dynamic programming, but doing so is usually impractical. POMDPs with such structure have also been studied under the name \u201cmixed observability MDPs\u201d (Ong et al., 2010). However, the method proposed there suffers from the usual challenges of solving POMDPs in high dimensions.\nApart from the various multiple-episode tasks we investigate in this work, previous literature on training RNN policies have used similar tasks that require memory to test if long-term dependency can be learned. Recent examples include the Labyrinth experiment in the A3C paper (Mnih et al., 2016), and the water maze experiment in the Recurrent DDPG paper (Heess et al., 2015a). Although these tasks can be reformulated under the RL2 framework, the key difference is that they focus on the memory aspect instead of the fast RL aspect."}, {"heading": "5 DISCUSSION", "text": "This paper suggests a different approach for designing better reinforcement learning algorithms: instead of acting as the designers ourselves, learn the algorithm end-to-end using standard reinforcement learning techniques. That is, the \u201cfast\u201d RL algorithm is a computation whose state is stored in the RNN activations, and the RNN\u2019s weights are learned by a general-purpose \u201cslow\u201d reinforcement learning algorithm. Our method, RL2, has demonstrated competence comparable with theoretically optimal algorithms in small-scale settings. We have further shown its potential to scale to high-dimensional tasks.\nIn the experiments, we have identified opportunities to improve upon RL2: the outer-loop reinforcement learning algorithm was shown to be an immediate bottleneck, and we believe that for settings with extremely long horizons, better architecture may also be required for the policy. Although we\nhave used generic methods and architectures for the outer-loop algorithm and the policy, doing this also ignores the underlying episodic structure. We expect algorithms and policy architectures that exploit the problem structure to significantly boost the performance."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers."}, {"heading": "A DETAILED EXPERIMENT SETUP", "text": "Common to all experiments: as mentioned in Section 2.2, we use placeholder values when necessary. For example, at t = 0 there is no previous action, reward, or termination flag. Since all of our experiments use discrete actions, we use the embedding of the action 0 as a placeholder for actions, and 0 for both the rewards and termination flags. To form the input to the GRU, we use the values for the rewards and termination flags as-is, and embed the states and actions as described separately below for each experiments. These values are then concatenated together to form the joint embedding.\nFor the neural network architecture, We use rectified linear units throughout the experiments as the hidden activation, and we apply weight normalization without data-dependent initialization (Salimans & Kingma, 2016) to all weight matrices. The hidden-to-hidden weight matrix uses an orthogonal initialization (Saxe et al., 2013), and all other weight matrices use Xavier initialization (Glorot & Bengio, 2010). We initialize all bias vectors to 0. Unless otherwise mentioned, the policy and the baseline uses separate neural networks with the same architecture until the final layer, where the number of outputs differ.\nAll experiments are implemented using TensorFlow (Abadi et al., 2016) and rllab (Duan et al., 2016). We use the implementations of classic algorithms provided by the TabulaRL package (Osband, 2016).\nA.1 MULTI-ARMED BANDITS\nThe parameters for TRPO are shown in Table 1. Since the environment is stateless, we use a constant embedding 0 as a placeholder in place of the states, and a one-hot embedding for the actions.\nA.2 TABULAR MDPS\nThe parameters for TRPO are shown in Table 2. We use a one-hot embedding for the states and actions separately, which are then concatenated together.\nA.3 VISUAL NAVIGATION\nThe parameters for TRPO are shown in Table 3. For this task, we use a neural network to form the joint embedding. We rescale the images to have width 40 and height 30 with RGB channels preserved, and we recenter the RGB values to lie within range [\u22121, 1]. Then, this preprocessed\nimage is passed through 2 convolution layers, each with 16 filters of size 5 \u00d7 5 and stride 2. The action is first embedded into a 256-dimensional vector where the embedding is learned, and then concatenated with the flattened output of the final convolution layer. The joint vector is then fed to a fully connected layer with 256 hidden units.\nUnlike previous experiments, we let the policy and the baseline share the same neural network. We found this to improve the stability of training baselines and also the end performance of the policy, possibly due to regularization effects and better learned features imposed by weight sharing. Similar weight-sharing techniques have also been explored in Mnih et al. (2016)."}, {"heading": "B HYPERPARAMETERS FOR BASELINE ALGORITHMS", "text": "B.1 MULTI-ARMED BANDITS\nThere are 3 algorithms with hyperparameters: UCB1, Optimistic Thompson Sampling (OTS), and -Greedy. We perform a coarse grid search to find the best hyperparameter for each of them. More specifically:\n\u2022 UCB1: We test c \u2208 {0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The best found parameter for each setting is given in Table 4.\nB.2 TABULAR MDPS\nThere are 4 algorithms with hyperparameters: Optimistic PSRL (OPSRL), BEB, -Greedy, UCRL2. Details are given below.\n\u2022 Optimistic PSRL (OPSRL): The hyperparameter is the number of posterior samples. We use up to 20 samples. The best found parameter for each setting is given in Table 7."}, {"heading": "C FURTHER ANALYSIS ON MULTI-ARMED BANDITS", "text": "In this section, we provide further analysis of the behavior of RL2 agent in comparison with the baseline algorithms, on the multi-armed bandit task. Certain algorithms such as UCB1 are designed not in the Bayesian context; instead they are tailored to be robust in adversarial cases. To highlight this aspect, we evaluate the algorithms on a different metric, namely the percentage of trials where the best arm is recovered. We treat the best arm chosen by the policy to be the arm that has been pulled most often, and the ground truth best arm is the arm with the highest mean parameter. In addition, we split the set of all possible bandit tasks into simpler and harder tasks, where the difficulty is measured by the -gap between the mean parameter of the best arm and the second best arm. We compare the percentage of recovering the best arm separately according to the gap, as shown in Table 11.\nNote that there are two columns associated with the UCB1 algorithm, where UCB1 (without \u201c\u2217\u201d) is evaluated with c = 0.2, the parameter that gives the best performance as evaluated by the average total reward, and UCB1\u2217 uses c = 1.0. Surprisingly, although using c = 1.0 performs the best in terms of recovering the best arm, its performance is significantly worse than using c = 0.2 when evaluated under the average total reward (369.2 \u00b1 2.2 vs. 405.8 \u00b1 2.2). This also explains that although RL2 does not perform the best according to this metric (which is totally expected, since it is not optimized under this metric), it achieves comparable average total reward as other bestperforming methods."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["REFERENCES Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Tim Salimans", "Diederik P Kingma"], "venue": "arXiv preprint arXiv:1602.07868,", "citeRegEx": "Salimans and Kingma.,? \\Q2016\\E", "shortCiteRegEx": "Salimans and Kingma.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": ", 2014; Mnih et al., 2015; Schulman et al., 2015), and acquiring advanced manipulation and locomotion skills (Levine et al., 2016; Lillicrap et al., 2015; Watter et al., 2015; Heess et al., 2015b; Schulman et al., 2015; 2016). However, many of the successes come at the expense of high sample complexity. For example, the state-of-the-art Atari results require tens of thousands of episodes of experience (Mnih et al., 2015) per game. To master a game, one would need to spend nearly 40 days playing it with no rest. In contrast, humans and animals are capable of learning a new task in a very small number of trials. Continuing the previous example, the human player in Mnih et al. (2015) only needed 2 hours of experience before mastering a game.", "startOffset": 8, "endOffset": 690}, {"referenceID": 3, "context": "Recent examples include the Labyrinth experiment in the A3C paper (Mnih et al., 2016), and the water maze experiment in the Recurrent DDPG paper (Heess et al.", "startOffset": 66, "endOffset": 85}], "year": 2016, "abstractText": "Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a \u201cfast\u201d reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (\u201cslow\u201d) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the \u201cfast\u201d RL algorithm on the current (previously unseen) MDP. We evaluate RL experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-armed bandit problems and finite MDPs. After RL is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the largescale side, we test RL on a vision-based navigation task and show that it scales up to high-dimensional problems.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_395"}