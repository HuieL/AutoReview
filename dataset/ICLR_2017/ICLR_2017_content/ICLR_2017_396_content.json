{"name": "ICLR_2017_396.pdf", "metadata": {"source": "CRF", "title": "DEEP ERROR-CORRECTING OUTPUT CODES", "authors": ["Guoqiang Zhong", "Yuchen Zheng", "Mengqi Li", "Junyu Dong"], "emails": ["gqzhong@ouc.edu.cn", "ouczyc@outlook.com", "sdrzbruce@163.com", "enri9615@outlook.com", "dongjunyu@ouc.edu.cn"], "sections": [{"heading": "1 INTRODUCTION", "text": "Error correcting output codes (ECOC) are an ensemble learning framework to address multi-class classification problems (Dietterich & Bakiri, 1995). The work by (Zhong & Liu, 2013) shows that the ECOC methods can also be used for feature learning, in either a linear or a nonlinear manner. However, although sophisticated coding and decoding strategies are applied (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its singlelayer structure. Therefore, to exploit the advantages of the ECOC framework, such as supervised ensemble learning and effective coding design, it\u2019s necessary to combine its ideas with that of deep learning.\nIn recent years, many deep learning models have been proposed to handle various challenging problems. Meantime, desirable performances in many domains have been achieved, such as image classification and detection, document analysis and recognition, natural language processing, and video analysis (Hinton & Salakhutdinov, 2006; Krizhevsky et al., 2012; Szegedy et al., 2014; Simonyan & Zisserman, 2014; Zhang et al., 2015; Wang & Ji, 2015; Hong et al., 2015). Among others, (Hinton &\nSalakhutdinov, 2006) presents the ground-breaking deep autoencoder that learns the weight matrices by pre-training the stacked restricted Boltzmann machines (RBMs) and fine-tuning the weights using gradient descent. It delivers much better representations of data than shallow feature learning algorithms, such as principal components analysis (PCA) (Jolliffe, 1986) and latent semantic analysis (LSA) (Deerwester et al., 1990). In order to boost the traditional autoencoder and prevent the \u201coverfitting\u201d problem, (Vincent et al., 2008) introduces the denosing autoencoder that corrupted the data with a random noise. Recently, most of the research focuses on deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which greatly improves the state-of-the-art in the areas of object recognition, unsegmented handwriting recognition and speech recognition (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014). However, existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. In the case of random initialization, to obtain good results, many training data and a long training time are generally used; while in the case of greedy layerwise pre-training, as the whole training data set needs to be used, the pre-training process is very time-consuming and difficult to find a stable solution.\nTo overcome the limitations of both traditional ECOC methods and deep learning models, and meanwhile, take advantages of both of them, in this paper, we propose a novel deep learning model called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple stacked ECOC modules, each of which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the probabilistic outputs of the combined binary classifiers as the outputs of a hidden layer or new representations of data. On the one hand, the ECOC modules can be learned layer by layer using the given supervisory information, and on the other hand, based on the ternary coding design, some classes of data are automatically neglected when training the binary classifiers, such that the weights are learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have compared DeepECOC with traditional ECOC, feature learning and deep learning algorithms to demonstrate the effectiveness and superiority of DeepECOC. The results are reported in Section 4.\nThe rest of this paper is organized as follows: In Section 2, we give a brief overview to related work. In Section 3, we present the proposed model, DeepECOC, in detail. The experimental results are reported in Section 4, while Section 5 concludes this paper with remarks and future work."}, {"heading": "2 RELATED WORK", "text": "Traditional ECOC framework has two steps: coding and decoding. In the coding step, an ECOC matrix is defined or learned from data, and the binary classifiers are trained based on the ECOC coding; in the decoding step, the class label is given to a test sample based on a similarity measure between codewords and outputs of the binary classifiers. The widely used coding strategies include one-versus-all (OneVsAll) (Nilsson, 1965), one-versus-one (OneVsOne) (Hastie et al., 1998), discriminant ECOC (DECOC) (Pujol et al., 2006), ECOC optimizing node embedding (ECOCONE) (Escalera et al., 2006), dense and sparse coding (Escalera et al., 2009; Allwein et al., 2001), and so on. Among them, the OneVsAll, OneVsOne, dense and sparse coding strategies are problem-independent, whilst the DECOC and ECOCONE are problem-dependent. Generally, the length of the codeword by the OneVsAll, OneVsOne, DECOC and ECOCONE coding designs is related to the number of classes, but that by the dense and sparse coding design is relatively flexible. In this work, we design the structure of DeepECOC based on the properties of each coding strategy. The commonly used binary ECOC decoding strategies are the Hamming decoding (Nilsson, 1965) and Euclidean decoding (Hastie et al., 1998). For ternary ECOC decoding strategies, the attenuated Euclidean decoding (Pujol et al., 2008), loss-based decoding (Allwein et al., 2001), and probabilistic-based decoding (Passerini et al., 2004) are widely used. Currently, the state-of-the-art ternary ECOC decoding strategies are the discrete pessimistic beta density distribution decoding and loss-weighted decoding (Escalera et al., 2010). In this work, for the simplicity of back propagation, we directly add a Softmax layer at the top of DeepECOC for the decoding. Note that, although many sophisticated coding and decoding strategies have been proposed in recent years (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its single-layer structure. To further exploit the advantages of ECOC, such as supervised ensemble learning and effective coding design, it\u2019s necessary to combine its ideas with that of deep learning.\nIn the literature of deep learning, there is some work that attempts to construct a deep architecture with multiple feature learning methods (Hinton & Salakhutdinov, 2006; Trigeorgis et al., 2014; Yuan et al., 2015; Zheng et al., 2015; 2014). For instance, deep autoencoder is built up by RBMs (Hinton & Salakhutdinov, 2006), and deep semi-NMF combines multiple steps of matrix factorization (Trigeorgis et al., 2014). Similarly, deep CNNs and RNNs can also be considered as deep models that learn the new representations of data layer by layer (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014). The success of these existing models demonstrate that deep networks are beneficial to the representation learning tasks, especially for the large scale applications. However, as discussed in the previous section, existing deep learning models are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training, which result in a long training time of the deep models. In this work, we propose the DeepECOC model, which is based on the stacked ECOC modules. When pre-training DeepECOC, the ECOC modules can be learned with the available supervisory information. Intuitively, as this manner of supervised pre-training has deterministic objective, the learned value of the parameters will be very close to the best local minimum on the solution manifold. Experimental results shown in Section 4 also demonstrate this fact."}, {"heading": "3 DEEP ERROR-CORRECTING OUTPUT CODES (DEEPECOC)", "text": "In this section, we first introduce the traditional ECOC framework, which is the important building block of DeepECOC. Then we present the learning procedures of DeepECOC in detail."}, {"heading": "3.1 THE ECOC FRAMEWORK", "text": "Error correcting output codes (ECOC), which combine multiple binary classifiers to solve multiclass classification problems, are an ensemble learning framework. The ECOC methods in general consist of two steps: coding and decoding. In the coding step, the ECOC coding matrix M \u2208 {\u22121, 1}C\u00d7L (binary case) or M \u2208 {\u22121, 0, 1}C\u00d7L (ternary case) is first defined or learned from the training data, where each row of M is the codeword of a class, each column corresponds to a dichotomizer (binary classifier), L is the length of the codewords (the number of binary classifiers), C is the number of classes, symbol \u20181\u2019 indicates positive class, \u2018-1\u2019 indicates negative class, and \u20180\u2019 indicates that a particular class is not considered by a given classifier. Then, the binary classifiers (dichotomizers) are trained according to the partition of the classes in the columns of M. Fig. 1 shows two coding matrices encoded with the one-versus-all (binary case) and one-versus-one (ternary case) coding strategies. The matrix is coded using several dichotomizers for a 4-class problem with respective codewords {y1, . . . , y4}. The white girds are coded by 1 (considered as positive class by the respective dichotomizer hj), the dark girds by -1 (considered as the negative class), and the gray girds by 0 (classes that are not considered by the respective dichotomizer hj). In the decoding step, the test data are predicted based on an adopted decoding strategy and the outputs of the binary classifiers.\nIn order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers (dichotomizers), which solve a quadratic programming problem\nmin w,b,\u03bei\nJ(w) = 1\n2 \u2016w\u20162 + C N\u2211 i=1 \u03bei\ns.t. yif(xi) \u2265 1\u2212 \u03bei, \u03bei \u2265 0, i = 1, . . . , N (1)\nwhere w and b are the coefficients and bias of the binary classifier, yi \u2208 {+1,\u22121}, \u03bei\u2019s are the slack variables, and N is the number of the training data. The discriminant function can be expressed as\nf(x) = wTx+ b. (2)\nThis problem can be solved as,\nw = N\u2211 i=1 \u03b1iyixi, (3)\nb = 1\nNSV NSV\u2211 xi\u2208SV,i=1 (yi \u2212wTxi), (4)\nwhere \u03b1i\u2019s are the non-negative Lagrange multipliers, Nsv is the number of support vectors and SV is the set of support vectors. The dual form of Problem (1) can be written as\nmax \u03b1 N\u2211 i=1 \u03b1i \u2212 1 2 N\u2211 i,j=1 \u03b1i\u03b1jyiyjx T i xj\n=\nN\u2211 i=1 \u03b1i \u2212 1 2 N\u2211 i,j=1 \u03b1i\u03b1jyiyjk(xi,xj)\ns.t. 0 \u2264 \u03b1i \u2264 \u03bb, i = 1, . . . , N, N\u2211 i=1 \u03b1iyi = 0, (5)\nwhere k(xi,xj) = xTi xj is the linear kernel function, \u03bb is a constant number, and \u03b1 = {\u03b11, . . . , \u03b1N} is the vector of Lagrange multipliers. Replacing the linear kernel function with a nonlinear kernel, such as the Gaussian kernel\nk(xi,xj) = exp(\u2212\u03c3\u22121\u2016xi \u2212 xj\u20162), (6)\nwe can learn a nonlinear SVM, where \u03c3 is the parameter for the Gaussian kernel function. The discriminant function of SVMs with a nonlinear kernel can be written as\nf(x) = N\u2211 i=1 \u03b1iyik(xi,x) + b. (7)\nApplying a decoding strategy on the outputs of the binary classifiers, the ECOC framework can be used for multi-class learning, while applying the sigmoid function on the values of the discriminant function, ECOC can be used for feature learning (Zhong & Liu, 2013). This is also the foundation of the DeepECOC model."}, {"heading": "3.2 DEEPECOC", "text": "To combine the advantages of ECOC and deep learning algorithms, we build the DeepECOC architecture as follows\nx qD\u2212\u2212\u2192 x\u0303 W1\u2212\u2212\u2192\nb1 h1 W2\u2212\u2212\u2192 b2 \u00b7 \u00b7 \u00b7 Wn\u22121\u2212\u2212\u2212\u2212\u2192 bn\u22121 hn\u22121 softmax\u2212\u2212\u2212\u2212\u2212\u2192 y, (8)\nwhere the first step makes the clean input x \u2208 [0, 1]d partially destroyed by means of a stochastic mapping x\u0303 \u223c qD(x\u0303 | x). In the corrupting process, we set a parameter called denoising rate \u03bd. For each input x, a fixed number \u03bdd of components are chosen at random, and their value is forced to 0, while the others are left untouched. This operation makes the model more robust and prevent the overfitting problem in most cases (Vincent et al., 2008). Subsequently, the \u201ccorrupted\u201d data are taken as inputs for the DeepECOC model. W1 and b1 are the weight matrix and bias vector learned from the first ECOC module. The output of the first hidden layer is denoted as\nh1 = s(W T 1 x+ b1), (9)\nwhere s(\u00b7) is the sigmoid activation function s(x) = 11+e\u2212x . From the second layer to the (n\u2212 1)- th layer, we use the stacked ECOC modules to learn the weight matrices and biases, which can be considered as weights between two successive layers of a deep network. Similarly, we use the output of the (k \u2212 1)-th layer as the input of the k-th layer,\nhk = s(W T k hk\u22121 + bk). (10)\nHere, hk can be viewed as an activation output and a new representation of the input datum x.\nFor example, if we adopt the OneVsAll coding strategy for one layer of the ECOC module, we first define the coding matrix MC\u00d7C , where C is the number of classes. Then, we can train C SVM classifiers to obtain the weight matrix W = {w1, . . . ,wi, . . . ,wC} and the bias b = {b1, . . . , bi, . . . , bC}. Next, we calculate the output of the first layer by using Eq. (9). Subsequently, we repeat this process layer by layer to build the DeepECOC model. It\u2019s obvious that, if we adopt different coding strategies, we can get different kinds of DeepECOC architectures.\nFor the last layer of DeepECOC, we employ the softmax regression for the multi-class learning. Its cost function is defined as\nJ(w) = \u2212 1 N ( N\u2211 i=1 K\u2211 j=1 I(yi = j) log exp(wTj h n\u22121 i )\u2211K l=1 exp(w T l h n\u22121 i ) ), (11)\nwhere I(x) is the indicator function, I(x) = 1 if x is true, else I(x) = 0. yi is the label corresponding to xi. It\u2019s easy to compute the probability that xi is classified to class j,\np(yi = j|xi,w) = exp(wTj h n\u22121 i )\u2211K\nl=1 exp(w T l h n\u22121 i )\n. (12)\nTaking derivatives, one can show that the gradient of J(w) with respect to w is,\n\u2207J(w) = \u2212 1 N N\u2211 i=1 [xi(I(yi = j)\u2212 p(yi = j|xi,w))]. (13)\nAfter the pre-training step, we use back propagation (Rumelhart et al., 1988) to fine tune the whole architecture. Moreover, we also employ a technique called \u201cdropout\u201d for regularization (Hinton et al., 2012). When a large feedforward neural network is trained on a small training set, dropout generally performs well on the test set. The basic idea of dropout is that each hidden node is randomly omitted from the network with a probability of \u03b2. In another view, dropout is a very efficient way to perform model averaging with neural networks. Through these processes, we finally obtain the DeepECOC model, which is robust and easy to be applied to multi-class classification tasks.\nNote that, compared to existing deep learning algorithms, DeepECOC have some important advantages. Firstly, unlike previous deep learning algorithms, DeepECOC are built with the ECOC modules and pre-trained in a supervised learning fashion. Secondly, if we adopt ternary coding strategies, due to the natural merit of ECOC, the weights can be learned using only part of the training data. Thirdly, in contrast to the learning of the weight matrices in previous deep learning models, the binary classifiers in each ECOC module can be learned in parallel, which may greatly speed up the learning of DeepECOC."}, {"heading": "4 EXPERIMENTS", "text": "To evaluate the effectiveness of the proposed method, DeepECOC, we conducted 4 parts of experiments. In the first part, we compared DeepECOC with some deep learning models and single-layer ECOC approaches on 16 data sets from the UCI machine learning repository 1. In the second part, we compared DeepECOC with traditional feature learning models, some deep learning models and single-layer ECOC approaches on the USPS handwritten digits 2, and tested DeepECOC with different number of hidden layers. In the third part, we used the MNIST handwritten digits 3 to further\n1http://archive.ics.uci.edu/ml/ 2http://www-i6.informatik.rwth-aachen.de/\u223ckeysers/usps.html 3http://yann.lecun.com/exdb/mnist/\ndemonstrate the effectiveness of DeepECOC for handwritten digits recognition. Finally, the CIFAR10 data set 4 was used to demonstrate the effectiveness of DeepECOC on image classification tasks. For all the data sets, the features were normalized within [0, 1]. In the following, we report the experimental results in detail."}, {"heading": "4.1 CLASSIFICATION ON 16 UCI MACHINE LEARNING REPOSITORY DATA SETS", "text": "The detail of the UCI data sets are shown in Table 1. In these experiments, we compared DeepECOC with autoencoder (AE) (Hinton & Salakhutdinov, 2006), denoising autoencoder (DAE) (Vincent et al., 2008) and single-layer ECOC approaches (Single) (Escalera et al., 2010). We built DeepECOC with the ECOC optimizing node embedding (ECOCONE) coding method (Escalera et al., 2006). Here, since we initialized ECOCONE with 3 different coding methods, i.e. one-versus-one, oneversus-all and DECOC, DeepECOC had 3 variants. In addition, the state-of-the-art linear loss-\n4http://www.cs.toronto.edu/\u223ckriz/cifar.html\nweighted (LLW) decoding strategy was used for ECOCONE. Finally, a structure with 3 hidden layers was adopted for DeepECOC, which had 0.1 denoising rate and 0.1 dropout rate:\nx qD\u2212\u2212\u2192 x\u0303 W1\u2212\u2212\u2192\nb1 h1 W2\u2212\u2212\u2192 b2 h2 W3\u2212\u2212\u2192 b3 h3 softmax\u2212\u2212\u2212\u2212\u2212\u2192 y. (14)\nFor the fine-tuning process, we used the stochastic gradient descent algorithm. The learning rate and epoches from different data sets are described in Table 3. The autoencoder and denoising autoencoder\u2019s architectures are as same as DeepECOC with ECOCONE initialized by one-versus-one. For single-layer ECOC approaches, we chose the best results shown in (Escalera et al., 2010) as our compared results. For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function as base classifiers. The parameters of SVMs were set to default (Chang & Lin, 2011).\nTable 2 shows the average classification accuracy and standard deviation on 16 UCI data sets. Except on the OptDigits data set, DeepECOC achieved the best results compared with autoencoder, denoising autoencoder and single-layer ECOC approaches. In fact, on the OptDigits data set, DeepECOC achieved comparative result with single-layer ECOC approaches. Among others, DeepECOC with ECOCONE (initialized by one-versus-one) coding strategy obtained the best results on 9 data sets, while DeepECOC with ECOCONE (initialized by DECOC) coding strategy obtained the best results on 5 data sets. From the mean rank values, we can see that DeepECOC with ECOCONE (initialized by one-versus-one and DECOC) strategy far surpass other compared methods."}, {"heading": "4.2 CLASSIFICATION ON THE USPS DATA SET", "text": "The USPS handwritten digits data set includes 7291 training samples and 2007 test samples from 10 classes. The size of the images is 16\u00d716 = 256. Our experiments on this data set were divided into 2 parts. Firstly, we compared DeepECOC with two traditional feature learning models (principal components analysis (PCA) (Jolliffe, 2002) and marginal Fisher analysis (MFA) (Yan et al., 2007)), autoencoder (AE), denoising autoencoder (DAE), LeNet (LeCun et al., 1998), PCANet (Chan et al., 2015) and single-layer ECOC approaches. Here, PCA is an unsupervised method, MFA is a supervised method. For MFA, the number of nearest neighbors for constructing the intrinsic graph was set to 5, while that for constructing the penalty graph was set to 15. For DeepECOC, we also used 3 coding design methods in this experiment. We used batch gradient descent for the fine-tuning process, the batch size was set to 100, the learning rate was set to 1, the number of epoch was set to 40000, the denoising rate, and dropout rate were set to 0.1. We also used SVMs with RBF kernel and default parameters as base classifiers. For single-layer ECOC approaches, we adopted ECOCONE (initialized by one-versus-one) as coding design method and linear loss-weighted (LLW) decoding strategy. For the LeNet model, we used 2 convolutional layers, two pooling layers and two fully connected layers. The kernel size of the convolutional layers and pooling layers was set to 2\u00d72, the stride was set to 1, the number of nodes of the first layer was set to 200, the epoch was set to 8000, the initial learning rate was set to 0.001, learning rate policy was set to \u201cinv\u201d, and the momentum was set to 0.9. For the PCANet model, we used two PCA-filter stages, one binary hashing stage and one blockwise histograms. The filter size, the number of filters, and the block size were set to k1 = k2 = 3, L1 = L2 = 4, and 7\u00d77, respectively. The experimental results are shown in Fig. 2(a). From Fig. 2(a), we can see that DeepECOC with ECOCONE (initialized by one-versus-one) coding strategy achieved the best result than other methods include traditional feature learning models, existing deep learning methods and single-layer ECOC approaches.\nIn the second part, we evaluated DeepECOC with different number of hidden layers. We used 2 to 6 hidden layers in our experiments. The parameter settings were as same as the first part. Fig. 2(b)\nshows the experimental results. We can see that DeepECOC obtained the best result when using 3 hidden layers. When the number of hidden layers is less than 3, the effectiveness of DeepECOC increases with the increasing of the number of hidden layers. Along with the number of hidden layers continues to grow, the effectiveness of DeepECOC decreases."}, {"heading": "4.3 CLASSIFICATION ON THE MNIST DATA SET", "text": "MNIST handwritten digits data set has a training set of 60,000 examples, and a test set of 10,000 examples with 784 dimensional features. We designed 2 architectures for autoencoder, denoising autoencoder and DeepECOC. The first architecture was 784 \u2212 Z1 \u2212 Z2 \u2212 Z3 \u2212 10, where Zi was the number of hidden neurons designed based on some ECOC coding strategies. We designed this architecture because we wanted to make autoencoder and denoising autoencoder had the same structure with DeepECOC. The second architecture is 784\u2212500\u2212500\u22122000\u221210. This architecture was used in (Hinton & Salakhutdinov, 2006). In order to make DeepECOC adapt to this structure, we used the dense and sparse coding design methods that can control the codeword length. Note that, the dense and sparse coding design methods are totally random and data-independent. The denoising rate and dropout rate were set to 0.1, the batch size was set to 100, the learning rate was set to 0.01, and the number of epoch was set to 80000. For LeNet model, we adopted the parameters as same as (LeCun et al., 1998). For PCANet model, we used two PCA-filter stages, one binary hashing stage and one blockwise histograms. In the PCANet, the filter size, the number of filters, and the block size were set to k1 = k2 = 8, L1 = L2 = 7, and 7\u00d7 7, respectively. Fig. 3(a) and Fig. 3(b) show the experimental results on 2 architectures. We can see that DeepECOC are comparative with existing deep learning methods on the second architecture and outperform\nthem on the first architecture. In addition, DeepECOC with both two architectures outperform the single-layer ECOC approaches."}, {"heading": "4.4 CLASSIFICATION ON THE LBP-CIFAR10 DATA SET", "text": "The CIFAR-10 dataset is a relative large scale data set which consists of 60000 32\u00d732 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. For the purpose of reducing computational cost, we attempted to extract features of the data using an efficient local binary patterns algorithm. As a result, the representations with dimensionality 36 and 256 were adopted and the data were normalized to [0, 1] as well, called LBP-CIFAR10 (36) and LBP-CIFAR10 (256). We also used 3 hidden layers for all deep learning methods. The learning rate was set to 0.1, and the epoch was set to 4000. For the LeNet model, we used 2 convolutional layers and two fully connected layers without pooling layers. The kernel size was set to 2 \u00d7 2, the stride was set to 1, the number of node of the first fully connected layer was set to 64, the epoch was set to 4000, the initial learning rate was set to 0.01, learning rate policy was set to \u201cinv\u201d, and the momentum was set to 0.9. For the PCANet model, we used two PCA-filter stages, one binary hashing stage and one blockwise histograms. In the PCANet, the filter size, the number of filters, and the block size were set to k1 = k2 = 3, L1 = L2 = 4, and 7\u00d77, respectively. The classification accuracy are reported in Table 4.\nFrom Table 4, we can easy to see that DeepECOC achieved the best results. Moreover, DeepECOC with ECOCONE (initialized by one-versus-one) coding strategy achieved the better results than autoencoder and denoising autoencoder, LeNet and PCANet. Hence, we can conclude that, DeepECOC are a general model to handle different real world applications and achieves desirable results in most cases."}, {"heading": "5 CONCLUSION", "text": "In this paper, we propose a novel deep learning model, called deep error correcting output codes (DeepECOC). DeepECOC extend traditional ECOC algorithms to a deep architecture fashion, and meanwhile, brings new elements to the deep learning area, such as supervised initialization, and automatic neglecting of part of the data during network training. Extensive experiments on 16 data sets from the UCI machine learning repository, the USPS and MNIST handwritten digits and the CIFAR-10 data set demonstrate the superiority of DeepECOC over traditional ECOC, feature learning and deep learning methods. In future work, we will further exploit the learnability of DeepECOC on large scale applications."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "PCANet: A Simple Deep Learning Baseline for Image Classification", "author": ["T.-H. Chan", "K. Jia", "S. Gao", "J. Lu", "Z. Zeng", "Y. Ma"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Ecoc-one: A novel coding and decoding strategy", "author": ["S. Escalera", "O. Pujol", "P. Radeva"], "venue": "In ICPR,", "citeRegEx": "Escalera et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Escalera et al\\.", "year": 2006}, {"title": "Separability of ternary codes for sparse designs of errorcorrecting output codes", "author": ["S. Escalera", "O. Pujol", "P. Radeva"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Escalera et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Escalera et al\\.", "year": 2009}, {"title": "On the decoding process in ternary error-correcting output codes", "author": ["S. Escalera", "O. Pujol", "P. Radeva"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Escalera et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Escalera et al\\.", "year": 2010}, {"title": "A Novel Connectionist System for Unconstrained Handwriting Recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "The annals of statistics,", "citeRegEx": "Hastie and Tibshirani,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1998}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Online tracking by learning discriminative saliency map with convolutional neural network", "author": ["S. Hong", "T. You", "S. Kwak", "B. Han"], "venue": "In ICML,", "citeRegEx": "Hong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2015}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": "New York: Springer-Verlag,", "citeRegEx": "Jolliffe.,? \\Q1986\\E", "shortCiteRegEx": "Jolliffe.", "year": 1986}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "New results on error correcting output codes of kernel machines", "author": ["A. Passerini", "M. Pontil", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Passerini et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Passerini et al\\.", "year": 2004}, {"title": "Discriminant ECOC: a heuristic method for application dependent design of error correcting output codes", "author": ["O. Pujol", "P. Radeva", "J. Vitria"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Pujol et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pujol et al\\.", "year": 2006}, {"title": "An incremental node embedding technique for error correcting output codes", "author": ["O. Pujol", "S. Escalera", "P. Radeva"], "venue": "Pattern Recognition,", "citeRegEx": "Pujol et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pujol et al\\.", "year": 2008}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A deep semi-nmf model for learning hidden representations", "author": ["G. Trigeorgis", "K. Bousmalis", "S. Zafeiriou", "B. Schuller"], "venue": "In ICML,", "citeRegEx": "Trigeorgis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Trigeorgis et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A.Manzagol"], "venue": "In ICML, pp", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Video Event Recognition with Deep Hierarchical Context Model", "author": ["X. Wang", "Q. Ji"], "venue": "In CVPR, pp", "citeRegEx": "Wang and Ji.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Ji.", "year": 2015}, {"title": "Graph embedding and extensions: a general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.-J. Zhang", "Q. Yang", "S. Lin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Yan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2007}, {"title": "Scene Recognition by Manifold Regularized Deep Learning Architecture", "author": ["Y. Yuan", "L. Mou", "X. Lu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Yuan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "In NIPS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Visual texture perception with feature learning models and deep architectures", "author": ["Y. Zheng", "G. Zhong", "J. Liu", "X. Cai", "J. Dong"], "venue": "In Pattern Recognition,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Stretching deep architectures for text recognition", "author": ["Y. Zheng", "Y. Cai", "G. Zhong", "Y. Chherawala", "Y. Shi", "J. Dong"], "venue": "In ICDAR,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "Adaptive error-correcting output codes", "author": ["G. Zhong", "M. Cheriet"], "venue": "In IJCAI,", "citeRegEx": "Zhong and Cheriet.,? \\Q2013\\E", "shortCiteRegEx": "Zhong and Cheriet.", "year": 2013}, {"title": "Error-correcting output codes based ensemble feature extraction", "author": ["G. Zhong", "C.-L. Liu"], "venue": "Pattern Recognition,", "citeRegEx": "Zhong and Liu.,? \\Q2013\\E", "shortCiteRegEx": "Zhong and Liu.", "year": 2013}, {"title": "Joint learning of error-correcting output codes and dichotomizers from data", "author": ["G. Zhong", "K. Huang", "C.-L. Liu"], "venue": "Neural Computing and Applications,", "citeRegEx": "Zhong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "However, although sophisticated coding and decoding strategies are applied (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its singlelayer structure.", "startOffset": 75, "endOffset": 141}, {"referenceID": 32, "context": "However, although sophisticated coding and decoding strategies are applied (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its singlelayer structure.", "startOffset": 75, "endOffset": 141}, {"referenceID": 14, "context": "Meantime, desirable performances in many domains have been achieved, such as image classification and detection, document analysis and recognition, natural language processing, and video analysis (Hinton & Salakhutdinov, 2006; Krizhevsky et al., 2012; Szegedy et al., 2014; Simonyan & Zisserman, 2014; Zhang et al., 2015; Wang & Ji, 2015; Hong et al., 2015).", "startOffset": 196, "endOffset": 357}, {"referenceID": 27, "context": "Meantime, desirable performances in many domains have been achieved, such as image classification and detection, document analysis and recognition, natural language processing, and video analysis (Hinton & Salakhutdinov, 2006; Krizhevsky et al., 2012; Szegedy et al., 2014; Simonyan & Zisserman, 2014; Zhang et al., 2015; Wang & Ji, 2015; Hong et al., 2015).", "startOffset": 196, "endOffset": 357}, {"referenceID": 11, "context": "Meantime, desirable performances in many domains have been achieved, such as image classification and detection, document analysis and recognition, natural language processing, and video analysis (Hinton & Salakhutdinov, 2006; Krizhevsky et al., 2012; Szegedy et al., 2014; Simonyan & Zisserman, 2014; Zhang et al., 2015; Wang & Ji, 2015; Hong et al., 2015).", "startOffset": 196, "endOffset": 357}, {"referenceID": 12, "context": "It delivers much better representations of data than shallow feature learning algorithms, such as principal components analysis (PCA) (Jolliffe, 1986) and latent semantic analysis (LSA) (Deerwester et al.", "startOffset": 134, "endOffset": 150}, {"referenceID": 23, "context": "In order to boost the traditional autoencoder and prevent the \u201coverfitting\u201d problem, (Vincent et al., 2008) introduces the denosing autoencoder that corrupted the data with a random noise.", "startOffset": 85, "endOffset": 107}, {"referenceID": 14, "context": "Recently, most of the research focuses on deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which greatly improves the state-of-the-art in the areas of object recognition, unsegmented handwriting recognition and speech recognition (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 261, "endOffset": 325}, {"referenceID": 7, "context": "Recently, most of the research focuses on deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which greatly improves the state-of-the-art in the areas of object recognition, unsegmented handwriting recognition and speech recognition (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 261, "endOffset": 325}, {"referenceID": 20, "context": "Recently, most of the research focuses on deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which greatly improves the state-of-the-art in the areas of object recognition, unsegmented handwriting recognition and speech recognition (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 261, "endOffset": 325}, {"referenceID": 17, "context": ", 1998), discriminant ECOC (DECOC) (Pujol et al., 2006), ECOC optimizing node embedding (ECOCONE) (Escalera et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 4, "context": ", 2006), ECOC optimizing node embedding (ECOCONE) (Escalera et al., 2006), dense and sparse coding (Escalera et al.", "startOffset": 50, "endOffset": 73}, {"referenceID": 5, "context": ", 2006), dense and sparse coding (Escalera et al., 2009; Allwein et al., 2001), and so on.", "startOffset": 33, "endOffset": 78}, {"referenceID": 0, "context": ", 2006), dense and sparse coding (Escalera et al., 2009; Allwein et al., 2001), and so on.", "startOffset": 33, "endOffset": 78}, {"referenceID": 18, "context": "For ternary ECOC decoding strategies, the attenuated Euclidean decoding (Pujol et al., 2008), loss-based decoding (Allwein et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 0, "context": ", 2008), loss-based decoding (Allwein et al., 2001), and probabilistic-based decoding (Passerini et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 16, "context": ", 2001), and probabilistic-based decoding (Passerini et al., 2004) are widely used.", "startOffset": 42, "endOffset": 66}, {"referenceID": 6, "context": "Currently, the state-of-the-art ternary ECOC decoding strategies are the discrete pessimistic beta density distribution decoding and loss-weighted decoding (Escalera et al., 2010).", "startOffset": 156, "endOffset": 179}, {"referenceID": 6, "context": "Note that, although many sophisticated coding and decoding strategies have been proposed in recent years (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its single-layer structure.", "startOffset": 105, "endOffset": 171}, {"referenceID": 32, "context": "Note that, although many sophisticated coding and decoding strategies have been proposed in recent years (Escalera et al., 2010; Zhong et al., 2012; Zhong & Cheriet, 2013), the learnability of ECOC is limited by its single-layer structure.", "startOffset": 105, "endOffset": 171}, {"referenceID": 22, "context": "In the literature of deep learning, there is some work that attempts to construct a deep architecture with multiple feature learning methods (Hinton & Salakhutdinov, 2006; Trigeorgis et al., 2014; Yuan et al., 2015; Zheng et al., 2015; 2014).", "startOffset": 141, "endOffset": 241}, {"referenceID": 26, "context": "In the literature of deep learning, there is some work that attempts to construct a deep architecture with multiple feature learning methods (Hinton & Salakhutdinov, 2006; Trigeorgis et al., 2014; Yuan et al., 2015; Zheng et al., 2015; 2014).", "startOffset": 141, "endOffset": 241}, {"referenceID": 29, "context": "In the literature of deep learning, there is some work that attempts to construct a deep architecture with multiple feature learning methods (Hinton & Salakhutdinov, 2006; Trigeorgis et al., 2014; Yuan et al., 2015; Zheng et al., 2015; 2014).", "startOffset": 141, "endOffset": 241}, {"referenceID": 22, "context": "For instance, deep autoencoder is built up by RBMs (Hinton & Salakhutdinov, 2006), and deep semi-NMF combines multiple steps of matrix factorization (Trigeorgis et al., 2014).", "startOffset": 149, "endOffset": 174}, {"referenceID": 14, "context": "Similarly, deep CNNs and RNNs can also be considered as deep models that learn the new representations of data layer by layer (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 126, "endOffset": 190}, {"referenceID": 7, "context": "Similarly, deep CNNs and RNNs can also be considered as deep models that learn the new representations of data layer by layer (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 126, "endOffset": 190}, {"referenceID": 20, "context": "Similarly, deep CNNs and RNNs can also be considered as deep models that learn the new representations of data layer by layer (Krizhevsky et al., 2012; Graves et al., 2009; Sak et al., 2014).", "startOffset": 126, "endOffset": 190}, {"referenceID": 23, "context": "This operation makes the model more robust and prevent the overfitting problem in most cases (Vincent et al., 2008).", "startOffset": 93, "endOffset": 115}, {"referenceID": 19, "context": "After the pre-training step, we use back propagation (Rumelhart et al., 1988) to fine tune the whole architecture.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "Moreover, we also employ a technique called \u201cdropout\u201d for regularization (Hinton et al., 2012).", "startOffset": 73, "endOffset": 94}, {"referenceID": 23, "context": "In these experiments, we compared DeepECOC with autoencoder (AE) (Hinton & Salakhutdinov, 2006), denoising autoencoder (DAE) (Vincent et al., 2008) and single-layer ECOC approaches (Single) (Escalera et al.", "startOffset": 125, "endOffset": 147}, {"referenceID": 6, "context": ", 2008) and single-layer ECOC approaches (Single) (Escalera et al., 2010).", "startOffset": 50, "endOffset": 73}, {"referenceID": 4, "context": "We built DeepECOC with the ECOC optimizing node embedding (ECOCONE) coding method (Escalera et al., 2006).", "startOffset": 82, "endOffset": 105}, {"referenceID": 6, "context": "For single-layer ECOC approaches, we chose the best results shown in (Escalera et al., 2010) as our compared results.", "startOffset": 69, "endOffset": 92}, {"referenceID": 13, "context": "Firstly, we compared DeepECOC with two traditional feature learning models (principal components analysis (PCA) (Jolliffe, 2002) and marginal Fisher analysis (MFA) (Yan et al.", "startOffset": 112, "endOffset": 128}, {"referenceID": 25, "context": "Firstly, we compared DeepECOC with two traditional feature learning models (principal components analysis (PCA) (Jolliffe, 2002) and marginal Fisher analysis (MFA) (Yan et al., 2007)), autoencoder (AE), denoising autoencoder (DAE), LeNet (LeCun et al.", "startOffset": 164, "endOffset": 182}, {"referenceID": 15, "context": ", 2007)), autoencoder (AE), denoising autoencoder (DAE), LeNet (LeCun et al., 1998), PCANet (Chan et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 1, "context": ", 1998), PCANet (Chan et al., 2015) and single-layer ECOC approaches.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "For LeNet model, we adopted the parameters as same as (LeCun et al., 1998).", "startOffset": 54, "endOffset": 74}], "year": 2016, "abstractText": "Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very timeconsuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-ofthe-art deep learning models in most cases.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_396"}