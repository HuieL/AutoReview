{"name": "ICLR_2017_401.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["WORD PROBLEMS", "Megan Leszczynski"], "emails": ["mel255@cornell.edu,", "jmoreira@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "We present a complete system architecture for a machine solver that automatically solves a class of physics word problems, namely classical mechanics of a point particle in free fall. This domain allows us to formulate one dynamical system to which all the physics problems in this domain can be mapped. The dynamical system describes how the state of the particle, defined by its location and velocity, changes over time. Correspondingly, the initial conditions for the dynamical system include the location and velocity of the particle at the time origin.\nGiven the word problem as input, the solver must first learn to extract the parameters needed to produce the dynamical system and also learn to identify the type of question. Two independently trained recurrent neural networks are used to complete these tasks. The first neural network, referred to as the labeler, learns to find the dynamical system parameters and locate the question within the problem statement. The second neural network, referred to as the classifier, identifies the type of question. Finally, the solver uses a numerical integrator to solve the dynamical system and produce the solution. We use a problem generator in order to produce disjoint datasets as input to the system for training and testing. The generator produces short-answer high school-level physics word problems with mixed units.\nAfter a brief related work section, we provide a more detailed description of the class of physics problems we address. We proceed to describe how the machine solver works and present experimental results. We conclude with a summary of our work and proposals for future works. The appendices contain additional details that did not fit in the body of the paper."}, {"heading": "2 RELATED WORK", "text": "Automatically solving word problems has been a research interest of the natural language processing community for some time, particularly with math word problems. The main challenge is to develop a semantic representation of the word problem. Kushman et al. (2014) learned to represent mathematical word problem with a system of equations, by aligning words in the word problem to templates. While their technique learns to induce multiple templates and assumes knowledge of numbers and nouns, we assume no knowledge of the words in the text but only map to one template.\nAnother study to solve math word problems was done by Hosseini et al. (2014). This study also assumes the ability to identify numbers and nouns in the text and uses a dependency parser to determine relationships between words in the text. Like the other study, this approach generalizes to math word problems that require different equations. Shi et al. (2015) similarly used a parser to solve math word problems. However, their parser maps the word problems to a carefully defined language they created called DOL, from which equations can be derived. Rather than use a parser to break down the word problems, we use neural networks to learn to identify key pieces of information. Our study is the first of our knowledge to apply recurrent neural networks to the task of solving word problems.\nWe chose to use recurrent neural networks (RNN) for the labeler and the classifier as both of their inputs consist of sequences of words. Recurrent neural networks are commonly used to process sequences, and as a result have found application in natural language processing tasks such as machine translation (Cho et al., 2014b) and speech recognition (Graves et al., 2013). After experimenting with different models, we obtained the most success with Long Short-Term Memory (LSTM) variants of RNNs. For additional discussion on RNNs in general, and LSTMs in particular, we refer the reader to Appendix A."}, {"heading": "3 PROBLEM SPECIFICATION", "text": "We consider the following class of physical systems (see Figure 1(a)): In a two-dimensional space, with gravity producing a downward constant acceleration g, there is one particle in free fall. That is, no forces other than gravity are acting on the particle. Movement of the particle starts at time t = 0, with an initial position defined by displacements d1 and d2 and initial velocity with components v1 and v2.\nThe time behavior of the particle can be represented by the dynamical system shown in Figure 1(b). The state vector ~x(t) = [x1(t), x2(t), x\u03071(t), x\u03072(t)]T consists of two positions and two velocities and its derivative depends only on itself and the acceleration of gravity, as shown in the figure. Combined with the initial condition ~x(0) = [d1, d2, v1, v2]T , the differential equation produces a unique solution.\nOur machine solver computes answers to word problems in the domain just described. The word problem must specify, sometimes indirectly, the five parameters of the dynamical system (d1, d2, v1, v2, and g). It must also include a question that can be answered by computing the time behavior of the system. We discuss how our machine solver works in the next section."}, {"heading": "4 MACHINE SOLVER", "text": "In this section we describe the machine solver, which is composed of two recurrent neural networks and the numerical integrator. The top-level system block diagram is shown in Figure 2."}, {"heading": "4.1 NEURAL NETWORK ARCHITECTURES", "text": "The data flow through the labeler and classifier neural networks is shown in Figure 3. We used TensorFlowTM1 to develop the neural network models for both labeler and the classifier. TensorFlow is an open source library from Google that allowed us to easily explore different models and training settings with already implemented RNN cells and optimizers (Abadi et al., 2015). We quickly experiment with the provided optimizers to find the optimal optimizer for each network.\nThe labeler is an LSTM network with one hidden layer of ten units. Figure 4 shows an example of the data flow through the labeler. The input to the labeler is the full problem statement and the output is a label for each word. The words are input into the labeler via an embedding that is randomly initialized and trained simultaneously with the weights and biases. The weights are also randomly initialized and the biases are initialized to zero. To limit the exploration of the parameter space, we set the dimension of the embedding to equal the number of hidden units.\nThe chosen RNN model is one that produces an output at each time step and has recurrent connection between hidden units, as described by Goodfellow et al. (2016) in Chapter 10, Figure 10.3. At each step of the input sequence, the RNN receives a word embedding and outputs a label for the word. The label that is outputted at each time step can fall into one of the ten categories shown in Table 1. In addition to tagging words for their relevancy to the dynamical system formulation, we tag the question part of the word problem to pass to the classifier.\nWe use three measures to assess the performance of the labeler: label accuracy, question accuracy, and overall accuracy. Label accuracy is measured as having matching labels in the predicted and expected (generated) labels, not including the question part of the word problem. Question accuracy is measured as having both the first word of the question and the last word of the question labeled correctly, as label-based post processing to extract the question relies only on these indices. Overall accuracy is measured as meeting both of the label and question accuracy criteria.\n1TensorFlow is a trademark of Google Inc.\nWe train the labeler with TensorFlow\u2019s Adam Optimizer, an initial learning rate of 0.1, and a minibatch size of 100 word problems. The Adam Optimizer uses adaptive learning rates and is particularly effective with sparse gradients (Kingma & Ba, 2014). We use early stopping based on a validation accuracy or when the training accuracy stops improving. We chose the network architecture and training settings after performing a limited grid search across the number of layers, number of units per a layer, and learning rate. (See Appendix B.)\nAfter the labeler assigns a label to each word, a post processing step maps the labels to the dynamical system parameters, converting the initial conditions and value of gravity to SI units if necessary.\nThe classifier is an LSTM network with one hidden layer of 1,000 units. An example of the data flow through the classifier is shown in Figure 5. For the problems in our dataset, the formulation part of the word problem does not provide information necessary to classify the type of question. Moreover, as sequences become longer, the performance of RNNs tend to decrease (Pascanu et al., 2013). Armed with these two observations, we chose to only have the question part of the word problem as the sequence to input into the classifier.\nAs with the labeler, we encode the words of the sequence into word embeddings, matching the dimension of the word embedding to the number of hidden units, and training them with the weights and biases. In this case, a sequence would be one question. Unlike the labeler, there is only one output for each sequence, occurring on the last step of the sequence. For more information see Chapter 10, figure 10.5 of Goodfellow et al. (2016) for an illustration. The singular output is the type of question, which can fall into one of the nine types shown in Table 2.\nThe classifier is trained with TensorFlow\u2019s Gradient Descent Optimizer, an initial learning rate of 0.5, and a mini-batch size of 100 questions. As with the labeler, we performed a grid search to choose these hyperparameters. (See Appendix B.)"}, {"heading": "4.2 NUMERICAL INTEGRATOR", "text": "The numerical integrator computes the evolution over time of the dynamical system shown in Figure 1(b). As input it receives the initial conditions, the value of g, and the type of question extracted from the labeler and the classifier. Using SciPy\u2019s ordinary differential equation integrator, a table of values representing the system\u2019s state to the point that the object hits the ground is iteratively constructed. The numerical solution is refined to a precision of 0.001 (one part in a thousand), based on the type of the question. For example, if the question is about the maximum height, we produce\na first instance of the table, find the maximum height in that table, and then search for the maximum around that value with increased precision, repeating until we reach the desired precision. Finally, the question type is used to determine which value from the table to output from the solver. This data flow is shown in Figure 6."}, {"heading": "4.3 TRAINING, VALIDATION, AND TEST SETS", "text": "We define the word problems with a grammar that is provided in the APPENDIX. The word problems in the training, validation, and test sets are exclusively made up of problems that follow the specifications laid out by the grammar. The grammar allows for mixed units, meaning that within the same problem, the height may have a metric unit, while the velocity may have a U.S. customary unit. The grammar also permits the initial conditions to be exposed in multiple ways. For instance, a theta value and speed will be provided in some problems, from which the solver would need to calculate the initial vertical velocity using the theta, whereas in other problems no theta value may be provided. Using mixed units and varying numbers of values to provide information about each initial condition allows us to increase the complexity of the problems within the scope of the dynamical system.\nThe grammar also ensures that the training set is disjoint from the validation and test sets, particularly in structure. Examples of generated problems are shown below in Figure 7. This is vital in assessing the ability of the trained networks to generalize.\nWe implement the grammar in Python. When a new problem is instantiated, the grammar rules are descended to build up the problem, making random choices when choices are available. Labels for each problem are also automatically generated. The complete generative model is shown in Figure 8. By using a problem generator to build our datasets, we are also free to choose the size of the dataset. Our problem generator is capable of generating \u223c26,000 different training problems and \u223c22,000 different test and validation problems."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "5.1 LEARNING PROGRESS", "text": "The datasets consisted of 7,000 word problems for training, 2,000 word problems for validation, and 1,000 word problems for test. The progress of training over time is shown in Figure 9. As can be seen in the left graph, the labeler learns to identify the beginning and end of the question faster than it learns to correctly predict the labels. The overall accuracy of the labeler is both limited by and equivalent to that of the label accuracy. With this particular model of the labeler, there is no problem for which the labeler correctly predicts the non-question labels, but incorrectly locates the question.\nThe training accuracy for the label, question, and overall reach 100% for all by the end of the first epoch. The classifier also reaches 100% accuracy on the training set by the end of the first epoch. The epoch is broken down into fractions as the training accuracy is evaluated every seven minibatches of 100 problems.\nThe accuracy on the test set after the labeler and classifier have been independently trained are shown in Table 3. The accuracy of the combined RNN system amounts to an overall accuracy of 99.8%. The labeler achieves 100% accuracy on predicting the non-question labels and incurs a small error on predicting the beginning and end of the question. As a result, the question that is extracted based on the labeler\u2019s predictions does not always match the true question. However, based on the classifier\u2019s accuracy of 99.8%, the classifier is often resilient to the errors that labeler makes in extracting the\nquestion. While the labeler incorrectly extracts ninety-one questions, the classifier only incorrectly classifies two questions from a test set of 1,000 word problems. Figure 12 in Appendix C shows examples of the labeler\u2019s errors and how the classifier handles them.\nWe note that for the two wrongly classified cases, both shown in Figure 12, the classification error is the same. That is, a question that should be about the speed of the object when it hits the ground is classified as a question about the maximum speed the object reaches. The numerical answer to the problem is the same for both classes of question. Therefore, even in the case of wrongly classified questions, the system produces the right answer.\nThe high accuracy of the labeler and classifier are not a total surprise. LSTMs have been shown to be very effective in learning context-free and even context-sensitive languages (Gers & Schmidhuber, 2001; Cleeremans et al., 1989; Rodriguez, 2001), including the ability to generalize and recognize structures not seen before. Our training, validation and test sets are from a regular language, as described in Appendix E, so an LSTM should do well in learning them. In fact, we have seen situations (with the test, validation and test sets all with distinct structures) where the labeler and classifier both achieve perfect accuracy on all test problems. We decided to include the data on the \u201cnot so perfect\u201d case because it illustrates some important points (Figure 12)."}, {"heading": "5.2 ANALYSIS OF TRAINED VARIABLES", "text": "The trained variables for both models consist of word embeddings for input to the RNN, and weights and biases within the RNN and from the RNN to the final output. We focus our evaluation on the RNN weights, as we believe these are more specific to the our physics problem solver. For an evaluation of the word embeddings, please see Appendix D.\nThe distributions of weights for the labeler and classifier are shown in figures 10. As the labeler was an LSTM network, there are weights from the input and the previous hidden values to input, forget, and an output gates, as well as to the memory cells. While there appears to be a high concentration of negative weights to the output gate and positive weights to the input gate, this is likely a result of random initialization of the weights as this pattern was not consistently found with other random initializations. The output weights, which go from the output of the LSTM cell\u2019s hidden units to the target labels, have a slightly wider range. The few number of zero weights indicates that the majority outputs from the hidden units of the LSTM cell contribute to making the final prediction of the label.\nThe LSTM weight distribution for the classifier is more uniform and compressed than that of the labeler. We believe this is due to the great increase in parameters since the classifier has 1,000- dimensional embeddings and 1,000 hidden units, leading to 8 million weights (Karpathy et al., 2015). We predict that each piece of information captured by the trained embeddings and hidden units makes a less significant contribution to the final prediction than with the labeler, as indicated by the classifier\u2019s smaller weight values. The range of the output values for the output weights similarly contributes to this prediction, with a very small range of weights which are mostly concentrated around zero.\nAfter examining the general distribution of weights, we also wanted to explore potential patterns of specific weights. We chose to explore the heat map of the weights for labeler since there are a magnitude fewer connections, allowing the patterns to be more readily examined. We include the heat map of the weight matrices for the connections between the hidden units of the labeler to the output predictions in Figure 11. Looking at the heat map, hidden units 3 and 8 seem to have a similar weight distribution across the output categories. We also see seemingly logical pairs forming, such\nas the strong positive weights associated with D UNIT and HEIGHT for hidden unit 6 and for V and THETA for hidden unit 0. However, there are also features that are challenging to explain, such as the strong positive contribution hidden unit 4 makes to predicting THETA while making an equally strong negative contribution to predicting STORY."}, {"heading": "6 CONCLUSIONS", "text": "We have developed a machine solver for a word problems on the physics of a free falling object in two-dimensional space with constant acceleration of gravity. The solver has three main components. The labeler labels each word of the problem to identify the parameters of a canonical dynamical system that describes the time evolution of the object, and the part of the problem that corresponds to the question being asked. The classifier classifies the question part. Finally, an integrator is used to solve the dynamical system, producing a numerical answer to the problem.\nA grammar-based generator is used to produce the training, validation and test set of problems for the neural networks. The grammar is specified so that the validation and test problems are structurally different from the training problems. We use a total of 10,000 generated problems, partitioned into 7,000 for training, 2,000 for validation and 1,000 for testing.\nWhen measured against the test set of 1,000 problems, the dynamical system parameters are correctly identified in all of them. The question part is precisely identified in 909 cases, but because\nthe classifier can work with partial questions, in the end all but 2 questions are classified correctly. Therefore, the combined accuracy of the two neural networks, for the purpose of solving the physics problems, is 99.8%.\nThere are several opportunities for future work. First, we would like to investigate more deeply how our neural networks work. In particular, what features of the word problem they are identifying and how specific units are responsible for that identification. Second, we could extend our solver by considering more complex physical situations, including additional forces, three-dimensional motion, multiple objects, and so on. We would have to extend our canonical dynamical system to represent those situations and/or use a collection of dynamical systems. We expect that the complexity of the neural networks and the training/validation/test sets will grow accordingly. Finally, the more ambitious goal would be to remove the canonical dynamical system(s) and train the networks to build their own. We believe this would be closer to the way humans solve these physics problems."}, {"heading": "A RECURRENT NEURAL NETWORKS", "text": "The labeler and classifier are both recurrent neural networks (RNNs). We provide background information on RNNs in this section, followed by an overview of Long Short-Term Memory (LSTM) networks, which are an advanced type of RNNs and were used to build our networks. A recurrent neural network receives the previous values of the hidden layer as input in addition to the current input values into the network. Thus each hidden unit retains information about the history of the sequence. As explained in Goodfellow et al. (2016), the fundamental behavior of recurrent neural networks can be captured in the following equation:\nh(t) = f(h(t\u22121), x(t); \u03b8),\nwhere h(t) represents the state of the RNN unit at time t, x(t) represents the current input, and \u03b8 represents the weights and biases. The function f is usually hyperbolic tangent (Karpathy et al., 2015). It is important to note that the weights and biases are reused across time. Thus, while an RNN with one hidden layer can be unfolded in time to having many layers, the weights and biases between each of the unfolded layers are shared.\nA limitation of the basic recurrent neural network described above is that it cannot retain information over long sequences. If a key piece of information for predicting an output at the end of a long sequence occurs at the very beginning of the sequence, the basic recurrent neural network will likely fail as a result of training difficulties. A popular solution for this limitation is the Long Short-Term Memory (LSTM) - essentially a highly capable, more complex type of recurrent neural network (Hochreiter & Schmidhuber, 1997). An LSTM is composed of a memory cell, and input, output, and forget gates that determine how to modify and reveal the contents of memory cell. Each of these gates has its own set of weights and biases that are connected to the inputs. Therefore the number of weights within a layer of an LSTM is quadrupled from that of a basic recurrent neural network to 2n\u00d7 4n, where n is the number of hidden units in the layer and assumes each layer has the same number of units. 2n is from the input being a concatenation of the output from the previous hidden layer (in time) with the current input, as occurs for all RNNs, and the 4n is for the connections to each of the three gates as well as to the memory cell input. More specifically, the equations for the LSTM are as follows (Graves, 2013); (Zaremba et al., 2014): ifo\nj\n = sigmsigmsigm\ntanh\nT2n,4n(hl\u22121thlt\u22121 )\nclt = f clt\u22121 + i j\nhlt = o tanh(clt)\nAs both of our neural network models have only one hidden layer, hl\u22121t merely refers to the current input. T2n,4n refers to the weight and bias transformationWx+b applied to the concatenated hidden layer inputs. The hyperbolic tangent and sigmoid functions are applied element-wise. The variables i, f , o, and j refer to the input gate, forget gate, output gate, and cell input, respectively.\nAnother potential solution to the inability of the basic recurrent neural network to capture long-term dependencies is the Gated Recurrent Unit (GRU) (Cho et al., 2014a), however, we had the most success with the LSTM for our specific labeler and classifier tasks."}, {"heading": "B CHOOSING THE RIGHT RNN CONFIGURATION", "text": "We selected the models for our RNNs by performing a grid search over the learning rate, the number of units, and the number of layers. The results of the grid search for the the labeler recurrent network are shown in Table 4 and the results for the classifier network are shown in Table 5. For each RNN, we chose the most efficient model, in that it requires the least space and obtains the greatest accuracy with the lowest training time.\nInterestingly, for the classifier, we see that models with two or three layers and lower learning rates achieve an equivalent accuracy as the one-layer model. However, they are inferior to the one layer model in that the multi-layer models require more space and usually require longer to train."}, {"heading": "C ERROR HANDLING", "text": "This section is included to illustrate examples of the the labeler network incorrectly extracting the question. In each of these cases, the classifier receives as input the labeler\u2019s incorrect output. The classifier\u2019s handling of these errors is shown in Figure 12."}, {"heading": "D WORD EMBEDDINGS", "text": "To input the words into both RNNs, the words were first encoded as word embeddings. Word embeddings map words to a multi-dimensional space, providing the words with numerical representations which expose relationships between words. The final embeddings for the labeler network are 10- dimensional, and the embeddings for the classifier network are 1,000-dimensional. Rather than use Word2Vec, we chose to train the embeddings simultaneously with the weights and biases. We were interested in seeing if embeddings trained for a particular task could capture intuitive word features, as can often be seen with embeddings trained with Word2Vec (Mikolov et al., 2013).\nIn order to explore the results of the trained embeddings, we used scikit-learn\u2019s implementation of tSNE to map the high-dimensional embeddings down to two dimensions (van der Maaten & Hinton, 2008). The results from t-SNE are shown in Figure 13. Words appear exactly as they appear in the word problem, and no stemmers are used.\nThe embeddings from the labeler network seem more intuitive, as numbers and similar units, such as \u201cm/s\u201d, \u201cmph\u201d, and \u201cft/s\u201d, are mapped to similar regions. We had hypothesized that the embedding may capture some word function related to the task the embeddings were being trained to perform. However, the objects seem to be distributed throughout the space and have no easily distinguishable pattern, despite playing a similar functional role in each word problem. It is even more difficult to discern any patterns from the embeddings from the classifier network. We do see that words such as \u201ctraveling\u201d, \u201ctraveled\u201d, and \u201ctravels\u201d map near each other, as well as question words \u201cWhat\u201d and \u201cHow\u201d. We predict that the limited vocabulary in the question space of only forty words may con-\ntribute to these more perplexing results by reducing the effectiveness of which t-SNE can determine the similarity between words."}, {"heading": "E WORD PROBLEM GRAMMAR", "text": "We used the grammar below to generate our word problems with a problem generator.\nNotation: \u201cobject\u201d is used as a parameter in order to enforce consistency between parts of the problem. Within a word problem, the same object must appear wherever an object symbol occurs. As used in the question part of the grammar, \u201cx1\u201d indicates horizontal displacement and \u201cx2\u201d indicates vertical displacement. When used with numbers, \u201c...\u201d indicates the sequence of numbers continues in between the bars.\n\u3008word problem(object)\u3009 ::= \u3008formulation(object)\u3009 \u3008question(object)\u3009\n\u3008object\u3009 ::= \u3008training object\u3009 | \u3008val test object\u3009\n\u3008training object\u3009 ::= golf ball | stone | chair | feather | soccer ball | rock | cannonball\n\u3008val test object\u3009 ::= pebble | ping pong ball | vacuum | tennis ball | basketball | hat\n\u3008formulation(object)\u3009 ::= \u3008training formulation(object)\u3009 | \u3008val test formulation(object)\u3009\n\u3008training formulation(object)\u3009 ::= A object is \u3008action\u3009. \u3008Assumption\u3009.\n\u3008val test formulation(object)\u3009 ::= \u3008Assumption\u3009. A object is \u3008action\u3009.\n\u3008assumption\u3009 ::= Let the acceleration due to gravity on Planet Watson be \u3008acceleration\u3009.\n| Assume the acceleration due to gravity is \u3008acceleration\u3009.\n\u3008acceleration\u3009 ::= \u3008accel value\u3009 \u3008accel unit\u3009\n\u3008accel value\u3009 ::= 1 | 2 | 3 | ... | 100\n\u3008accel unit\u3009 ::= m/s2 | ft/s2\n\u3008action\u3009 ::= \u3008moving\u3009 | \u3008stationary\u3009\n\u3008moving\u3009 ::= \u3008descent\u3009 | \u3008projectile\u3009\n\u3008descent\u3009 ::= descending at a speed of \u3008speed\u3009 | moving downwards at a speed of \u3008speed\u3009\n\u3008projectile\u3009 ::= \u3008proj verb\u3009 at a speed of \u3008speed\u3009 and an \u3008angle word\u3009 of \u3008angle\u3009 degrees\n\u3008proj verb\u3009 ::= thrown | fired | launched\n\u3008speed\u3009 ::= \u3008speed value\u3009 \u3008speed unit\u3009\n\u3008speed value\u3009 ::= 0 | 1 | 2 | ... | 99\n\u3008speed unit\u3009 ::= m/s | ft/s | mph\n\u3008angle word\u3009 ::= elevation | angle from the horizontal\n\u3008angle\u3009 ::= 1 | 2 | 3 | ... | 89\n\u3008stationary\u3009 ::= \u3008stat verb\u3009 from \u3008location\u3009\n\u3008stat verb\u3009 ::= released | dropped | let go\n\u3008location\u3009 ::= \u3008height\u3009 | the top of a \u3008num stories\u3009 story building, where each story is \u3008height\u3009\n\u3008height\u3009 ::= \u3008height value\u3009 \u3008height unit\u3009\n\u3008height value\u3009 ::= \u3008training height\u3009 | \u3008validation height\u3009 | \u3008test height\u3009\n\u3008training height\u3009 ::= 1 | 2 | 3 | ... | 50\n\u3008validation height\u3009 ::= 51 | 52 | 53 | ... | 76\n\u3008test height\u3009 ::= 77 | 78 | 79 | ... | 99\n\u3008height unit\u3009 ::= m | ft\n\u3008num stories\u3009 ::= 1 | 2 | 3 | ... | 10\n\u3008question(object)\u3009 ::= \u3008max question(object)\u3009 | \u3008prop cond question(object)\u3009\n\u3008max question(object)\u3009 ::= \u3008max x1(object)\u3009 | \u3008max x2(object)\u3009 | \u3008max speed(object)\u3009\n\u3008max x1(object)\u3009 ::= What is the maximum distance the object travels | What is the greatest distance the object travels\n\u3008max x2(object)\u3009 ::= \u3008training max x2(object)\u3009 | \u3008val test max x2(object)\u3009\n\u3008training max x2(object)\u3009 ::= What is the maximum height the object reaches?\n\u3008val test max x2(object)\u3009 ::= What is the greatest height the object reaches?\n\u3008max speed(object)\u3009 ::= What is the maximum speed the object obtains? | What is the greatest speed the object obtains?\n\u3008prop cond question(object)\u3009 ::= \u3008proposition(object)\u3009 \u3008condition\u3009\n\u3008proposition(object)\u3009 ::= \u3008unk time\u3009 | \u3008unk x1(object)\u3009 | \u3008unk speed(object)\u3009\n\u3008unk time\u3009 ::= How much time has elapsed | How much time has passed\n\u3008unk x1(object)\u3009 ::= How far has the object traveled | What distance has the object traveled\n\u3008unk speed(object)\u3009 ::= How fast is the object traveling | At what speed is the object traveling | What is the magnitude of the velocity of the object\n\u3008condition\u3009 ::= \u3008distance condition\u3009 | when it reaches its maximum height?\n\u3008distance condition\u3009 ::= \u3008training distance condition\u3009 | \u3008val test distance condition\u3009\n\u3008training distance condition\u3009 ::= when it reaches the ground? | when it strikes the ground?\n\u3008val test distance condition\u3009 ::= just before it touches the ground? | as it hits the ground?\nWhenever the grammar dictates a choice of construct (for example, when selecting the object of a word problem), a uniform random number generator is used to select one of the valid constructs. Therefore, the frequency of a particular form in the training, validation and test sets ultimately\ndepend on how many random choices are necessary to produce that form and how many variations there are in each choice.\nTable 6 illustrates the simple case of occurrence counts of the different objects in our word problems. The training set uses seven different objects, while the validation and test sets use six objects. Not surprisingly, each object in the training set appears in approximately 1/7 of the total number of problems in that set. Meanwhile, each object in the validation and test sets appears in approximately 1/5 of the total number of problems in those sets.\nA more interesting situation is illuatrated in Table 7 for the occurrence counts of question types. As shown in Table 2, there are nine different question types. However, the grammar works by first choosing one of two groups of questions: either max-type questions (the first three in Table 2) or conditional-type questions (the last six in Table 2). Within each group, there is equal probability for each question type. Consequently, as Table 7 shows, each of the max-type questions is approximately twice as common as each of the conditional-type questions."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi"], "venue": null, "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finite state automata and simple recurrent networks", "author": ["Axel Cleeremans", "David Servan-Schreiber", "James L. McClelland"], "venue": "Neural Comput.,", "citeRegEx": "Cleeremans et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Cleeremans et al\\.", "year": 1989}, {"title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2001\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Deep learning. Book in preparation for MIT Press. Book available from http://www.deeplearningbook.org, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman"], "venue": "In EMNLP,", "citeRegEx": "Hosseini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning to automatically solve algebra word problems", "author": ["Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay"], "venue": null, "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Efficient estimation of word", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 12, "context": "Kushman et al. (2014) learned to represent mathematical word problem with a system of equations, by aligning words in the word problem to templates.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "Another study to solve math word problems was done by Hosseini et al. (2014). This study also assumes the ability to identify numbers and nouns in the text and uses a dependency parser to determine relationships between words in the text.", "startOffset": 54, "endOffset": 77}, {"referenceID": 5, "context": "Another study to solve math word problems was done by Hosseini et al. (2014). This study also assumes the ability to identify numbers and nouns in the text and uses a dependency parser to determine relationships between words in the text. Like the other study, this approach generalizes to math word problems that require different equations. Shi et al. (2015) similarly used a parser to solve math word problems.", "startOffset": 54, "endOffset": 361}, {"referenceID": 5, "context": "The chosen RNN model is one that produces an output at each time step and has recurrent connection between hidden units, as described by Goodfellow et al. (2016) in Chapter 10, Figure 10.", "startOffset": 137, "endOffset": 162}, {"referenceID": 3, "context": "LSTMs have been shown to be very effective in learning context-free and even context-sensitive languages (Gers & Schmidhuber, 2001; Cleeremans et al., 1989; Rodriguez, 2001), including the ability to generalize and recognize structures not seen before.", "startOffset": 105, "endOffset": 173}, {"referenceID": 10, "context": "We believe this is due to the great increase in parameters since the classifier has 1,000dimensional embeddings and 1,000 hidden units, leading to 8 million weights (Karpathy et al., 2015).", "startOffset": 165, "endOffset": 188}], "year": 2016, "abstractText": "We build a machine solver for word problems on the physics of a free falling object under constant acceleration of gravity. Each problem consists of a formulation part, describing the setting, and a question part asking for the value of an unknown. Our solver consists of two long short-term memory recurrent neural networks and a numerical integrator. The first neural network (the labeler) labels each word of the problem, identifying the physical parameters and the question part of the problem. The second neural network (the classifier) identifies what is being asked in the question. Using the information extracted by both networks, the numerical integrator computes the solution. We observe that the classifier is resilient to errors made by the labeler, which does a better job of identifying the physics parameters than the question. Training, validation and test sets of problems are generated from a grammar, with validation and test problems structurally different from the training problems. The overall accuracy of the solver on the test cases is 99.8%.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_401"}