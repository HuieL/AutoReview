{"name": "ICLR_2017_449.pdf", "metadata": {"source": "CRF", "title": "DEEP UNSUPERVISED CLUSTERING WITH GAUSSIAN MIXTURE VARIATIONAL AUTOENCODERS", "authors": ["Nat Dilokthanakul", "Pedro A. M. Mediano", "Marta Garnelo", "Matthew C. H. Lee", "Hugh Salimbeni", "Kai Arulkumaran", "Murray Shanahan"], "emails": ["n.dilokthanakul14@imperial.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Unsupervised clustering remains a fundamental challenge in machine learning research. While longestablished methods such as k-means and Gaussian mixture models (GMMs) (Bishop, 2006) still lie at the core of numerous applications (Aggarwal & Reddy, 2013), their similarity measures are limited to local relations in the data space and are thus unable to capture hidden, hierarchical dependencies in latent spaces. Alternatively, deep generative models can encode rich latent structures. While they are not often applied directly to unsupervised clustering problems, they can be used for dimensionality reduction, with classical clustering techniques applied to the resulting low-dimensional space (Xie et al., 2015). This is an unsatisfactory approach as the assumptions underlying the dimensionality reduction techniques are generally independent of the assumptions of the clustering techniques.\nDeep generative models try to estimate the density of observed data under some assumptions about its latent structure, i.e., its hidden causes. They allow us to reason about data in more complex ways than in models trained purely through supervised learning. However, inference in models with complicated latent structures can be difficult. Recent breakthroughs in approximate inference have provided tools for constructing tractable inference algorithms. As a result of combining differentiable models with variational inference, it is possible to scale up inference to datasets of sizes that would not have been possible with earlier inference methods (Rezende et al., 2014). One popular algorithm under this framework is the variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014).\nIn this paper, we propose an algorithm to perform unsupervised clustering within the VAE framework. To do so, we postulate that generative models can be tuned for unsupervised clustering by making the assumption that the observed data is generated from a multimodal prior distribution, and, correspondingly, construct an inference model that can be directly optimised using the reparameterization trick. We also show that the problem of over-regularisation in VAEs can severely effect the performance of clustering, and that it can be mitigated with the minimum information constraint introduced by Kingma et al. (2016)."}, {"heading": "1.1 RELATED WORK", "text": "Unsupervised clustering can be considered a subset of the problem of disentangling latent variables, which aims to find structure in the latent space in an unsupervised manner. Recent efforts have moved towards training models with disentangled latent variables corresponding to different factors of variation in the data. Inspired by the learning pressure in the ventral visual stream, Higgins et al. (2016) were able to extract disentangled features from images by adding a regularisation coefficient to the lower bound of the VAE. As with VAEs, there is also effort going into obtaining disentangled features from generative adversarial networks (GANs) (Goodfellow et al., 2014). This has been recently achieved with InfoGANs (Chen et al., 2016a), where structured latent variables are included as part of the noise vector, and the mutual information between these latent variables and the generator distribution is then maximised as a mini-max game between the two networks. Similarly, Tagger (Greff et al., 2016), which combines iterative amortized grouping and ladder networks, aims to perceptually group objects in images by iteratively denoising its inputs and assigning parts of the reconstruction to different groups. Johnson et al. (2016) introduced a way to combine amortized inference with stochastic variational inference in an algorithm called structured VAEs. Structured VAEs are capable of training deep models with GMM as prior distribution. Shu et al. (2016) introduced a VAE with a multimodal prior where they optimize the variational approximation to the standard variational objective showing its performance in video prediction task.\nThe work that is most closely related to ours is the stacked generative semi-supervised model (M1+M2) by Kingma et al. (2014). One of the main differences is the fact that their prior distribution is a neural network transformation of both continuous and discrete variables, with Gaussian and categorical priors respectively. The prior for our model, on the other hand, is a neural network transformation of Gaussian variables, which parametrise the means and variances of a mixture of Gaussians, with categorical variables for the mixture components. Crucially, Kingma et al. (2014) apply their model to semi-supervised classification tasks, whereas we focus on unsupervised clustering. Therefore, our inference algorithm is more specific to the latter.\nWe compare our results against several orthogonal state-of-the-art techniques in unsupervised clustering with deep generative models: deep embedded clustering (DEC) (Xie et al., 2015), adversarial autoencoders (AAEs) (Makhzani et al., 2015) and categorial GANs (CatGANs) (Springenberg, 2015)."}, {"heading": "2 VARIATIONAL AUTOENCODERS", "text": "VAEs are the result of combining variational Bayesian methods with the flexibility and scalability provided by neural networks (Kingma & Welling, 2013; Rezende et al., 2014). Using variational inference it is possible to turn intractable inference problems into optimisation problems (Wainwright & Jordan, 2008), and thus expand the set of available tools for inference to include optimisation techniques as well. Despite this, a key limitation of classical variational inference is the need for the likelihood and the prior to be conjugate in order for most problems to be tractably optimised, which in turn can limit the applicability of such algorithms. Variational autoencoders introduce the use of neural networks to output the conditional posterior (Kingma & Welling, 2013) and thus allow the variational inference objective to be tractably optimised via stochastic gradient descent and standard backpropagation. This technique, known as the reparametrisation trick, was proposed to enable backpropagation through continuous stochastic variables. While under normal circumstances backpropagation through stochastic variables would not be possible without Monte Carlo methods, this is bypassed by constructing the latent variables through the combination of a deterministic function and a separate source of noise. We refer the reader to Kingma & Welling (2013) for more details."}, {"heading": "3 GAUSSIAN MIXTURE VARIATIONAL AUTOENCODERS", "text": "In regular VAEs, the prior over the latent variables is commonly an isotropic Gaussian. This choice of prior causes each dimension of the multivariate Gaussian to be pushed towards learning a separate continuous factor of variation from the data, which can result in learned representations that are structured and disentangled. While this allows for more interpretable latent variables (Higgins et al., 2016), the Gaussian prior is limited because the learnt representation can only be unimodal and does\nnot allow for more complex representations. As a result, numerous extensions to the VAE have been developed, where more complicated latent representations can be learned by specifying increasingly complex priors (Chung et al., 2015; Gregor et al., 2015; Eslami et al., 2016).\nIn this paper we choose a mixture of Gaussians as our prior, as it is an intuitive extension of the unimodal Gaussian prior. If we assume that the observed data is generated from a mixture of Gaussians, inferring the class of a data point is equivalent to inferring which mode of the latent distribution the data point was generated from. While this gives us the possibility to segregate our latent space into distinct classes, inference in this model is non-trivial. It is well known that the reparametrisation trick which is generally used for VAEs cannot be directly applied to discrete variables. Several possibilities for estimating the gradient of discrete variables have been proposed (Glynn, 1990; Titsias & La\u0301zaro-Gredilla, 2015). Graves (2016) also suggested an algorithm for backpropagation through GMMs. Instead, we show that by adjusting the architecture of the standard VAE, our estimator of the variational lower bound of our Gaussian mixture variational autoencoder (GMVAE) can be optimised with standard backpropagation through the reparametrisation trick, thus keeping the inference model simple."}, {"heading": "3.1 GENERATIVE AND RECOGNITION MODELS", "text": "Consider the generative model p\u03b2,\u03b8(y,x,w,z) = p(w)p(z)p\u03b2(x|w,z)p\u03b8(y|x), where an observed sample y is generated from a set of latent variables x,w and z under the following process:\nw \u223c N (0, I ) (1a) z \u223cMult(\u03c0) (1b)\nx|z,w \u223c K\u220f k=1 N ( \u00b5zk(w;\u03b2), diag ( \u03c32zk(w;\u03b2) ))zk (1c) y|x \u223c N ( \u00b5(x; \u03b8), diag ( \u03c32(x; \u03b8) )) or B(\u00b5(x; \u03b8)) . (1d)\nwhere K is a predefined number of components in the mixture, and \u00b5zk(\u00b7;\u03b2),\u03c32zk(\u00b7;\u03b2),\u00b5(\u00b7; \u03b8), and \u03c32(\u00b7; \u03b8) are given by neural networks with parameters \u03b2 and \u03b8, respectively. That is, the observed sample y is generated from a neural network observation model parametrised by \u03b8 and the continuous latent variable x. Furthermore, the distribution of x|w is a Gaussian mixture with means and variances specified by another neural network model parametrised by \u03b2 and with inputw.\nMore specifically, the neural network parameterised by \u03b2 outputs a set of K means \u00b5zk and K variances \u03c32zk , given w as input. A one-hot vector z is sampled from the mixing probability \u03c0 , which chooses one component from the Gaussian mixture. We set the parameter \u03c0k = K\u22121 to make z uniformly distributed. The generative and variational views of this model are depicted in Fig. 1.\n1\n3"}, {"heading": "3.2 INFERENCE WITH THE RECOGNITION MODEL", "text": "The generative model is trained with the variational inference objective, i.e. the log-evidence lower bound (ELBO), which can be written as\nLELBO = Eq [ p\u03b2,\u03b8(y,x,w,z)\nq(x,w,z |y)\n] . (2)\nWe assume the mean-field variational family q(x,w,z |y) as a proxy to the posterior which factorises as q(x,w,z |y) = \u220f i q\u03c6x(xi|yi)q\u03c6w(wi|yi)p\u03b2(z i|xi,wi), where i indexes over data points. To simplify further notation, we will drop i and consider one data point at a time. We parametrise each variational factor with the recognition networks \u03c6x and \u03c6w that output the parameters of the variational distributions and specify their form to be Gaussian posteriors. We derived the z-posterior, p\u03b2(z |x,w), as:\np\u03b2(zj = 1|x,w) = p(zj = 1)p(x|zj = 1,w)\u2211K k=1 p(zk = 1)p(x|zj = 1,w)\n= \u03c0jN (x|\u00b5j(w;\u03b2), \u03c3j(w;\u03b2))\u2211K k=1 \u03c0kN (x|\u00b5k(w;\u03b2), \u03c3k(w;\u03b2)) . (3)\nThe lower bound can then be written as, LELBO = Eq(x|y) [ log p\u03b8(y|x) ] \u2212 Eq(w|y)p(z|x,w) [ KL(q\u03c6x(x|y)||p\u03b2(x|w,z)) ] \u2212KL(q\u03c6w(w|y)||p(w))\u2212 Eq(x|y)q(w|y) [ KL(p\u03b2(z |x,w)||p(z)) ] .\n(4)\nWe refer to the terms in the lower bound as the reconstruction term, conditional prior term, w-prior term and z-prior term respectively."}, {"heading": "3.2.1 THE CONDITIONAL PRIOR TERM", "text": "The reconstruction term can be estimated by drawing Monte Carlo samples from q(x|y), where the gradient can be backpropagated with the standard reparameterisation trick (Kingma & Welling, 2013). The w-prior term can be calculated analytically.\nImportantly, by constructing the model this way, the conditional prior term can be estimated using Eqn. 5 without the need to sample from the discrete distribution p(z |x,w).\nEq(w|y)p(z|x,w) [ KL ( q\u03c6x(x|y)||p\u03b2(x|w,z) )] \u2248\n1\nM M\u2211 j=1 K\u2211 k=1 p\u03b2(zk = 1|x(j),w(j))KL ( q\u03c6x(x|y)||p\u03b2(x|w(j), zk = 1) ) (5) Since p\u03b2(z |x,w) can be computed for all z with one forward pass, the expectation over it can be calculated in a straightforward manner and backpropagated as usual. The expectation over q\u03c6w(w|y) can be estimated with M Monte Carlo samples and the gradients can be backpropagated via the reparameterisation trick. This method of calculating the expectation is similar to the marginalisation approach of Kingma et al. (2014), with a subtle difference. Kingma et al. (2014) need multiple forward passes to obtain each component of the z-posterior. Our method requires wider output layers of the neural network parameterised by \u03b2, but only need one forward pass. Both methods scale up linearly with the number of clusters."}, {"heading": "3.3 THE KL COST OF THE DISCRETE LATENT VARIABLE", "text": "The most unusual term in our ELBO is the z-prior term. The z-posterior calculates the clustering assignment probability directly from the value of x and w, by asking how far x is from each of the cluster positions generated by w. Therefore, the z-prior term can reduce the KL divergence between the z-posterior and the uniform prior by concurrently manipulating the position of the clusters and the encoded point x. Intuitively, it would try to merge the clusters by maximising the overlap between them, and moving the means closer together. This term, similar to other KLregularisation terms, is in tension with the reconstruction term, and is expected to be over-powered as the amount of training data increases."}, {"heading": "3.4 THE OVER-REGULARISATION PROBLEM", "text": "The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (Bowman et al., 2015; S\u00f8nderby et al., 2016; Kingma et al., 2016; Chen et al., 2016b). As a result of the strong influence of the prior, the obtained latent representations are often overly simplified and poorly represent the underlying structure of the data. So far there have been two main approaches to overcome this effect: one solution is to anneal the KL term during training by allowing the reconstruction term to train the autoencoder network before slowly incorporating the regularization from the KL term (S\u00f8nderby et al., 2016). The other main approach involves modifying the objective function by setting a cut-off value that removes the effect of the KL term when it is below a certain threshold (Kingma et al., 2016). As we show in the experimental section below, this problem of over-regularisation is also prevalent in the assignment of the GMVAE clusters and manifests itself in large degenerate clusters. While we show that the second approach suggested by Kingma et al. (2016) does indeed alleviate this merging phenomenon, finding solutions to the over-regularization problem remains a challenging open problem."}, {"heading": "4 EXPERIMENTS", "text": "The main objective of our experiments is not only to evaluate the accuracy of our proposed model, but also to understand the optimisation dynamics involved in the construction of meaningful, differentiated latent representations of the data. This section is divided in three parts:\n1. We first study the inference process in a low-dimensional synthetic dataset, and focus in particular on how the over-regularisation problem affects the clustering performance of the GMVAE and how to alleviate the problem;\n2. We then evaluate our model on an MNIST unsupervised clustering task; and 3. We finally show generated images from our model, conditioned on different values of the\nlatent variables, which illustrate that the GMVAE can learn disentangled, interpretable latent representations.\nThroughout this section we make use of the following datasets:\n\u2022 Synthetic data: We create a synthetic dataset mimicking the presentation of Johnson et al. (2016), which is a 2D dataset with 10,000 data points created from the arcs of 5 circles.\n\u2022 MNIST: The standard handwritten digits dataset, composed of 28x28 grayscale images and consisting of 60,000 training samples and 10,000 testing samples (LeCun et al., 1998).\n\u2022 SVHN: A collection of 32x32 images of house numbers (Netzer et al., 2011). We use the cropped version of the standard and the extra training sets, adding up to a total of approximately 600,000 images."}, {"heading": "4.1 SYNTHETIC DATA", "text": "We quantify clustering performance by plotting the magnitude of the z-prior term described in Eqn. 6 during training. This quantity can be thought of as a measure of how much different clusters overlap. Since our goal is to achieve meaningful clustering in the latent space, we would expect this quantity to go down as the model learns the separate clusters.\nLz = \u2212Eq(x|y)q(w|y) [ KL(p\u03b2(z |x,w)||p(z)) ] (6)\nEmpirically, however, we have found this not to be the case. The latent representations that our model converges to merges all classes into the same large cluster instead of representing information about the different clusters, as can be seen in Figs. 2d and 3a. As a result, each data point is equally likely to belong to any of clusters, rendering our latent representations completely uninformative with respect to the class structure.\nWe argue that this phenomenon can be interpreted as the result of over-regularisation by the z-prior term. Given that this quantity is driven up by the optimisation of KL term in the lower bound,\nit reaches its maximum possible value of zero, as opposed to decreasing with training to ensure encoding of information about the classes. We suspect that the prior has too strong of an influence in the initial training phase and drives the model parameters into a poor local optimum that is hard to be driven out off by the reconstruction term later on.\nThis observation is conceptually very similar to the over-regularisation problem encountered in regular VAEs and we thus hypothesize that applying similar heuristics should help alleviate the problem. We show in Fig. 2f that by using the previously mentioned modification to the lower-bound proposed by Kingma et al. (2016), we can avoid the over-regularisation caused by the z-prior. This is achieved by maintaining the cost from the z-prior at a constant value \u03bb until it exceeds that threshold. Formally, the modified z-prior term is written as:\nL\u2032z = \u2212max(\u03bb,Eq(x|y)q(w|y) [ KL(p\u03b2(z |x,w)||p(z)) ] ) (7)\nThis modification suppresses the initial effect of the z-prior to merge all clusters thus allowing them to spread out until the cost from the z-prior cost is high enough. At that point its effect is significantly reduced and is mostly limited to merging individual clusters that are overlapping sufficiently. This can be seen clearly in Figs. 2e and 2f. The former shows the clusters before the z-prior cost is taken into consideration, and as such the clusters have been able to spread out. Once the z-prior is activated, clusters that are very close together will be merged as seen in Fig. 2f.\nFinally, in order to illustrate the benefits of using neural networks for the transformation of the distributions, we compare the density observed by our model (Fig. 2c) with a regular GMM (Fig. 2c) in data space. As illustrated by the figures, the GMVAE allows for a much richer, and thus more accurate representations than regular GMMs, and is therefore more successful at modelling nonGaussian data."}, {"heading": "4.2 UNSUPERVISED IMAGE CLUSTERING", "text": "We now assess the model\u2019s ability to represent discrete information present in the data on an image clustering task. We train a GMVAE on the MNIST training dataset and evaluate its clustering performance on the test dataset. To compare the cluster assignments given by the GMVAE with the true image labels we follow the evaluation protocol of Makhzani et al. (2015), which we summarise here for clarity. In this method, we find the element of the test set with the highest probability of belonging to cluster i and assign that label to all other test samples belonging to i. This is then repeated for all clusters i = 1, ...,K, and the assigned labels are compared with the true labels to obtain an unsupervised classification error rate.\nWhile we observe the cluster degeneracy problem when training the GMVAE on the synthetic dataset, the problem does not arise with the MNIST dataset. We thus optimise the GMVAE using the ELBO directly, without the need for any modifications. A summary of the results obtained on the MNIST benchmark with the GMVAE as well as other recent methods is shown in Table 1. We achieve classification scores that are competitive with the state-of-the-art techniques1, except for adversarial autoencoders (AAE). We suspect the reason for this is, again, related to the KL terms in the VAE\u2019s objective. As indicated by Hoffman et al., the key difference in the adversarial autoencoders objective is the replacement of the KL term in the ELBO by an adversarial loss that allows the latent space to be manipulated more carefully (Hoffman & Johnson, 2016). Details of the network architecture used in these experiments can be found in Appendix A.\nEmpirically, we observe that increasing the number of Monte Carlo samples and the number of clusters makes the GMVAE more robust to initialisation and more stable as shown in Fig. 4. If fewer samples or clusters are used then the GMVAE can occasionally converge faster to poor local minima, missing some of the modes of the data distribution.\n1It is worth noting that shortly after our initial submission, Rui Shu published a blog post (http://ruishu.io/2016/12/25/gmvae/) with an analysis on Gaussian mixture VAEs. In addition to providing insightful comparisons to the aforementioned M2 algorithm, he implements a version that achieves competitive clustering scores using a comparably simple network architecture. Crucially, he shows that model M2 does not use discrete latent variables when trained without labels. The reason this problem is not as severe in the GMVAE might possibly be the more restrictive assumptions in the generative process, which helps the optimisation, as argued in his blog."}, {"heading": "4.2.1 IMAGE GENERATION", "text": "So far we have argued that the GMVAE picks up natural clusters in the dataset, and that these clusters share some structure with the actual classes of the images. Now we train the GMVAE with K = 10 on MNIST to show that the learnt components in the distribution of the latent space actually represent meaningful properties of the data. First, we note that there are two sources of stochasticity in play when sampling from the GMVAE, namely\n1. Sampling w from its prior, which will generate the means and variances of x through a neural network \u03b2; and\n2. Sampling x from the Gaussian mixture determined by w and z , which will generate the image through a neural network \u03b8.\nIn Fig. 5a we explore the latter option by settingw = 0 and sampling multiple times from the resulting Gaussian mixture. Each row in Fig. 5a corresponds to samples from a different component of the Gaussian mixture, and it can be clearly seen that samples from the same component consistently result in images from the same class of digit. This confirms that the learned latent representation contains well differentiated clusters, and exactly one per digit. Additionally, in Fig. 5b we explore the sensitivity of the generated image to the Gaussian mixture components by smoothly varying\nw and sampling from the same component. We see that while z reliably controls the class of the generated image,w sets the \u201cstyle\u201d of the digit.\nFinally, in Fig. 6 we show images sampled from a GMVAE trained on SVHN, showing that the GMVAE clusters visually similar images together."}, {"heading": "5 CONCLUSION", "text": "We have introduced a class of variational autoencoders in which one level of the latent encoding space has the form of a Gaussian mixture model, and specified a generative process that allows\nus to formulate a variational Bayes optimisation objective. We then discuss the problem of overregularisation in VAEs. In the context of our model, we show that this problem manifests itself in the form of cluster degeneracy. Crucially, we show that this specific manifestation of the problem can be solved with standard heuristics.\nWe evaluate our model on unsupervised clustering tasks using popular datasets and achieving competitive results compared to the current state of the art. Finally, we show via sampling from the generative model that the learned clusters in the latent representation correspond to meaningful features of the visible data. Images generated from the same cluster in latent space share relevant high-level features (e.g. correspond to the same MNIST digit) while being trained in an entirely unsupervised manner.\nIt is worth noting that GMVAEs can be stacked by allowing the prior on w to be a Gaussian mixture distribution as well. A deep GMVAE could scale much better with number of clusters given that it would be combinatorial with regards to both number of layers and number of clusters per layer. As such, while future research on deep GMVAEs for hierarchical clustering is a possibility, it is crucial to also address the enduring optimisation challenges associated with VAEs in order to do so."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to acknowledge the NVIDIA Corporation for the donation of a GeForce GTX Titan Z used in our experiments. We would like to thank Jason Rolfe, Rui Shu and the reviewers for useful comments. Importantly, we would also like to acknowledge that the variational family which we used throughout this version of the paper was suggested by an anonymous reviewer."}, {"heading": "A NETWORK PARAMETERS", "text": "For optimisation, we use Adam (Kingma & Ba, 2014) with a learning rate of 10\u22124 and standard hyperparameter values \u03b21 = 0.9, \u03b22 = 0.999 and = 10\u22128. The model architectures used in our experiments are shown in Tables A.1, A.2 and A.3."}], "references": [{"title": "Data clustering: algorithms and applications", "author": ["Charu C Aggarwal", "Chandan K Reddy"], "venue": "CRC Press,", "citeRegEx": "Aggarwal and Reddy.,? \\Q2013\\E", "shortCiteRegEx": "Aggarwal and Reddy.", "year": 2013}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1606.03657,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Variational lossy autoencoder", "author": ["Xi Chen", "Diederik P Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A Recurrent Latent Variable Model for Sequential Data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["SM Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1603.08575,", "citeRegEx": "Eslami et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eslami et al\\.", "year": 2016}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["PW Glynn"], "venue": "Communications of the ACM,", "citeRegEx": "Glynn.,? \\Q1990\\E", "shortCiteRegEx": "Glynn.", "year": 1990}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Stochastic backpropagation through mixture density distributions", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1607.05690,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Tagger: Deep unsupervised perceptual grouping", "author": ["Klaus Greff", "Antti Rasmus", "Mathias Berglund", "Tele Hotloo Hao", "J\u00fcrgen Schmidhuber", "Harri Valpola"], "venue": "arXiv preprint arXiv:1606.06724,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Early Visual Concept Learning with Unsupervised Deep Learning", "author": ["I. Higgins", "L. Matthey", "X. Glorot", "A. Pal", "B. Uria", "C. Blundell", "S. Mohamed", "A. Lerchner"], "venue": null, "citeRegEx": "Higgins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2016}, {"title": "Elbo surgery: yet another way to carve up the variational evidence lower bound", "author": ["Matthew D. Hoffman", "Matthew J. Johnson"], "venue": "Workshop in Advances in Approximate Bayesian Inference,", "citeRegEx": "Hoffman and Johnson.,? \\Q2016\\E", "shortCiteRegEx": "Hoffman and Johnson.", "year": 2016}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew J Johnson", "David Duvenaud", "Alexander B Wiltschko", "Sandeep R Datta", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1603.06277,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Stochastic video prediction with conditional density estimation", "author": ["R. Shu", "J. Brofos", "F. Zhang", "M. Ghavamzadeh", "H. Bui", "M. Kochenderfer"], "venue": "In European Conference on Computer Vision (ECCV) Workshop on Action and Anticipation for Visual Learning,", "citeRegEx": "Shu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shu et al\\.", "year": 2016}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "arXiv preprint arXiv:1602.02282,", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": "arXiv preprint arXiv:1511.06390,", "citeRegEx": "Springenberg.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg.", "year": 2015}, {"title": "Local expectation gradients for black box variational inference", "author": ["Michalis Titsias", "Miguel L\u00e1zaro-Gredilla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Titsias and L\u00e1zaro.Gredilla.,? \\Q2015\\E", "shortCiteRegEx": "Titsias and L\u00e1zaro.Gredilla.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "While longestablished methods such as k-means and Gaussian mixture models (GMMs) (Bishop, 2006) still lie at the core of numerous applications (Aggarwal & Reddy, 2013), their similarity measures are limited to local relations in the data space and are thus unable to capture hidden, hierarchical dependencies in latent spaces.", "startOffset": 81, "endOffset": 95}, {"referenceID": 21, "context": "As a result of combining differentiable models with variational inference, it is possible to scale up inference to datasets of sizes that would not have been possible with earlier inference methods (Rezende et al., 2014).", "startOffset": 198, "endOffset": 220}, {"referenceID": 21, "context": "One popular algorithm under this framework is the variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 80, "endOffset": 126}, {"referenceID": 8, "context": "As with VAEs, there is also effort going into obtaining disentangled features from generative adversarial networks (GANs) (Goodfellow et al., 2014).", "startOffset": 122, "endOffset": 147}, {"referenceID": 10, "context": "Similarly, Tagger (Greff et al., 2016), which combines iterative amortized grouping and ladder networks, aims to perceptually group objects in images by iteratively denoising its inputs and assigning parts of the reconstruction to different groups.", "startOffset": 18, "endOffset": 38}, {"referenceID": 24, "context": ", 2015) and categorial GANs (CatGANs) (Springenberg, 2015).", "startOffset": 38, "endOffset": 58}, {"referenceID": 21, "context": "VAEs are the result of combining variational Bayesian methods with the flexibility and scalability provided by neural networks (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 127, "endOffset": 173}, {"referenceID": 12, "context": "While this allows for more interpretable latent variables (Higgins et al., 2016), the Gaussian prior is limited because the learnt representation can only be unimodal and does", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "As a result, numerous extensions to the VAE have been developed, where more complicated latent representations can be learned by specifying increasingly complex priors (Chung et al., 2015; Gregor et al., 2015; Eslami et al., 2016).", "startOffset": 168, "endOffset": 230}, {"referenceID": 11, "context": "As a result, numerous extensions to the VAE have been developed, where more complicated latent representations can be learned by specifying increasingly complex priors (Chung et al., 2015; Gregor et al., 2015; Eslami et al., 2016).", "startOffset": 168, "endOffset": 230}, {"referenceID": 6, "context": "As a result, numerous extensions to the VAE have been developed, where more complicated latent representations can be learned by specifying increasingly complex priors (Chung et al., 2015; Gregor et al., 2015; Eslami et al., 2016).", "startOffset": 168, "endOffset": 230}, {"referenceID": 7, "context": "Several possibilities for estimating the gradient of discrete variables have been proposed (Glynn, 1990; Titsias & L\u00e1zaro-Gredilla, 2015).", "startOffset": 91, "endOffset": 137}, {"referenceID": 2, "context": "The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (Bowman et al., 2015; S\u00f8nderby et al., 2016; Kingma et al., 2016; Chen et al., 2016b).", "startOffset": 132, "endOffset": 217}, {"referenceID": 23, "context": "The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (Bowman et al., 2015; S\u00f8nderby et al., 2016; Kingma et al., 2016; Chen et al., 2016b).", "startOffset": 132, "endOffset": 217}, {"referenceID": 18, "context": "The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (Bowman et al., 2015; S\u00f8nderby et al., 2016; Kingma et al., 2016; Chen et al., 2016b).", "startOffset": 132, "endOffset": 217}, {"referenceID": 23, "context": "So far there have been two main approaches to overcome this effect: one solution is to anneal the KL term during training by allowing the reconstruction term to train the autoencoder network before slowly incorporating the regularization from the KL term (S\u00f8nderby et al., 2016).", "startOffset": 255, "endOffset": 278}, {"referenceID": 18, "context": "The other main approach involves modifying the objective function by setting a cut-off value that removes the effect of the KL term when it is below a certain threshold (Kingma et al., 2016).", "startOffset": 169, "endOffset": 190}, {"referenceID": 19, "context": "\u2022 MNIST: The standard handwritten digits dataset, composed of 28x28 grayscale images and consisting of 60,000 training samples and 10,000 testing samples (LeCun et al., 1998).", "startOffset": 154, "endOffset": 174}, {"referenceID": 20, "context": "\u2022 SVHN: A collection of 32x32 images of house numbers (Netzer et al., 2011).", "startOffset": 54, "endOffset": 75}, {"referenceID": 18, "context": "(e) Using the modification to the ELBO (Kingma et al., 2016) allows the clusters to spread out.", "startOffset": 39, "endOffset": 60}], "year": 2017, "abstractText": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_449"}