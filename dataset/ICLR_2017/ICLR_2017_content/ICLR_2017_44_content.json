{"name": "ICLR_2017_44.pdf", "metadata": {"source": "CRF", "title": "NEURAL NETWORK COMPRESSION", "authors": ["SOFT WEIGHT-SHARING", "Karen Ullrich", "Edward Meeds"], "emails": ["karen.ullrich@uva.nl", "tmeeds@gmail.com", "welling.max@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "\u201dBigger is better\u201d is the ruling maxim in deep learning land. Deep neural nets with billions of parameters are no longer an exception. Networks of such size are unfortunately not practical for mobile, on-device applications which face strong limitations with respect to memory and energy consumption. Compressing neural networks could not only improve memory and energy consumption, but also lead to less network bandwidth, faster processing and better privacy. It has been shown that large networks are heavily over-parametrized and can be compressed by approximately two orders of magnitude without significant loss of accuracy. Apparently, over-parametrization is beneficial for optimization, but not necessary for accurate prediction. This observation has opened the door for a number of highly successful compression algorithms, which either train the network from scratch (Hinton et al., 2015; Iandola et al., 2016; Courbariaux & Bengio, 2016; Courbariaux et al., 2016) or apply compression post-optimization (Han et al., 2015b;a; Guo et al., 2016; Chen et al., 2015; Wen et al., 2016).\nIt has been long known that compression is directly related to (variational) Bayesian inference and the minimum description principle (Hinton & Van Camp, 1993). One can show that good compression can be achieved by encoding the parameters of a model using a good prior and specifying the parameters up to an uncertainty given, optimally, by the posterior distribution. An ingenious bitsback argument can then be used to get a refund for using these noisy weights. A number of papers have appeared that encode the weights of a neural network with limited precision (say 8 bits per weight), effectively cashing in on this \u201dbits-back\u201d argument (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016). Some authors go so far of arguing that even a single bit per weight can be used without much loss of accuracy (Courbariaux et al., 2015; Courbariaux & Bengio, 2016).\nIn this work we follow a different but related direction, namely to learn the prior that we use to encode the parameters. In Bayesian statistics this is known as empirical Bayes. To encourage compression of the weights toK clusters, we fit a mixture of Gaussians prior model over the weights. This idea originates from the nineties, known as soft weight-sharing (Nowlan & Hinton, 1992) where it was used to regularize a neural network. Here our primary goal is network compression, but as was\nshown in Hinton & Van Camp (1993) these two objectives are almost perfectly aligned. By fitting the mixture components alongside the weights, the weights tend to concentrate very tightly around a number of cluster components, while the cluster centers optimize themselves to give the network high predictive accuracy. Compression is achieved because we only need to encodeK cluster means (in full precision) in addition to the assignment of each weight to one of these J values (using log(J) bits per weight). We find that competitive compression rates can be achieved by this simple idea."}, {"heading": "2 MDL VIEW ON VARIATIONAL LEARNING", "text": "Model compression was first discussed in the context of information theory. The minimum description length (MDL) principle identifies the best hypothesis to be the one that best compresses the data. More specifically, it minimizes the cost to describe the model (complexity cost LC) and the misfit between model and data (error cost LE) (Rissanen, 1978; 1986). It has been shown that variational learning can be reinterpreted as an MDL problem (Wallace, 1990; Hinton & Van Camp, 1993; Honkela & Valpola, 2004; Graves, 2011). In particular, given data D = { X = {xn}Nn=1,T = {tn}Nn=1 } , a set of parameters w = {wi}Ii=1 that describes the model and an approximation q(w) of the posterior p(w|D), the variational lower bound, also known as negative variational free energy, L(q(w),w) can be decomposed in terms of error and complexity losses\nL(q(w),w) = \u2212Eq(w) [ log ( p(D|w)p(w)\nq(w)\n)] = Eq(w) [\u2212 log p(D|w)]\ufe38 \ufe37\ufe37 \ufe38\nLE\n+ KL(q(w)||p(w))\ufe38 \ufe37\ufe37 \ufe38 LC (1)\nwhere p(w) is the prior over w and p(D|w) is the model likelihood. According to Shannon\u2019s source coding theorem, LE lower bounds the expected amount of information needed to communicate the targets T, given the receiver knows the inputs X and the model w. The functional form of the likelihood term is conditioned by the target distribution. For example, in case of regression the predictions of the model are assumed be normally distributed around the targets T.\np(D|w) = p(T|X,w) = N\u220f n=1 N (tn|xn,w) (2)\nwhere N (tn, xn,w) is a normal distribution. Another typical example is classification where the conditional distribution of targets given data is assumed to be Bernoulli distributed1. These assumptions eventually lead to the well known error functions, namely cross-entropy error and squared error for classification and regression, respectively. Before however we can communicate the data we first seek to communicate the model. Similarly to LE , LC is a lower bound for transmitting the model. More specifically, if sender and receiver agree on a prior, LC is the expected cost of communicating the parameters w. This cost is again twofold,\nKL(q(w)||p(w)) = Eq(w) [\u2212 log p(w)]\u2212H(q(w)) (3)\nwhere H(\u00b7) denotes the entropy. In Wallace (1990) and Hinton & Van Camp (1993) it was shown that noisy encoding of the weights can be beneficial due to the bits-back argument if the uncertainty does not harm the error loss too much. The number of bits to get refunded by an uncertain weight distribution q(w) is given by its entropy. Further, it can be shown that the optimal distribution for q(w) is the Bayesian posterior distribution. While bits-back is proven to be an optimal coding scheme (Honkela & Valpola, 2004), it is often not practical in real world settings. A practical way to cash in on noisy weights (or bits-back) is to only encode a weight value up to a limited number of bits. To see this, assume a factorized variational posteriors q(w) = \u220f q(wi). Each posterior q(wi) is associated with a Dirac distribution up to machine precision, for example, a Gaussian distribution with variance \u03c3, for small values of \u03c3. This implies that we formally incur a very small refund per weight,\nH(q(w)) = \u2212 \u222b \u2126 q(w) log q(w) dw = \u2212 \u222b RI N (w|0, \u03c3I) logN (w|0, \u03c3I) = [log(2\u03c0e\u03c32)]I . (4)\n1For more detailed discussion see Bishop (2006).\nNote that the more coarse the quantization of weights the more compressible the model. The bitsback scheme makes three assumptions: (i) weights are being transmitted independently, (ii) weights are independent of each other (no mutual information), and (iii) the receiver knows the prior. Han et al. (2015a) show that one can successfully exploit (i) and (ii) by using a form of arithmetic coding (Witten et al., 1987). In particular, they employ range coding schemes such as the Sparse Matrix Format (discussed in Appendix A). This is beneficial because the weight distribution has low entropy. Note that the cost of transmitting the prior should be negligible. Thus a factorized prior with different parameters for each factor is not desirable.\nThe main objective of this work is to find a suitable prior for optimizing the cross-entropy between a delta posterior q(w) and the prior p(w) while at the same time keeping a practical coding scheme in mind. Recall that the cross entropy is a lower bound on the average number of bits required to encode the weights of the neural network (given infinite precision). Following Nowlan & Hinton (1992) we will model the prior p(w) as a mixture of Gaussians,\np(w) = I\u220f i=1 J\u2211 j=0 \u03c0jN (wi|\u00b5j , \u03c32j ). (5)\nWe learn the mixture parameters \u00b5j , \u03c3j , \u03c0j via maximum likelihood simultaneously with the network weights. This is equivalent to an empirical Bayes approach in Bayesian statistics. For stateof-the-art compression schemes pruning plays a major role. By enforcing an arbitrary \u201czero\u201d component to have fixed \u00b50 = 0 location and \u03c00 to be close to 1, a desired weight pruning rate can be enforced. In this scenario \u03c00 may be fixed or trainable. In the latter case a Beta distribution as hyperprior might be helpful. The approach naturally encourages quantization because in order to optimize the cross-entropy the weights will cluster tightly around the cluster means, while the cluster means themselves move to some optimal location driven by LE . The effect might even be so strong that it is beneficial to have a Gamma hyper-prior on the variances of the mixture components to prevent the components from collapsing. Furthermore, note that, mixture components merge when there is not enough pressure from the error loss to keep them separated because weights are attracted by means and means are attracted by weights hence means also attract each other. In that way the network learns how many quantization intervals are necessary. We demonstrate that behaviour in Figure 3."}, {"heading": "3 RELATED WORK", "text": "There has been a recent surge in interest in compression in the deep neural network community. Denil et al. (2013) showed that by predicting parameters of neural networks there is great redundancy in the amount of parameters being used. This suggests that pruning, originally introduced to reduce structure in neural networks and hence improve generalization, can be applied to the problem of compression and speed-up (LeCun et al., 1989). In fact, (Han et al., 2015b; Guo et al., 2016) show that neural network survive severe weight pruning (up to 99%) without significant loss of accuracy. A variational version is is proposed by Molchanov et al. (2017), the authors learn the dropout rate for each weight in the network separately. Some parameters will effectively be pruned when the dropout rate is very high. In an approach slightly orthogonal to weight pruning, (Wen et al., 2016) applied structural regularization to prune entire sets of weights from the neural network. Such extreme weight pruning can lead to entire structures being obsolete, which for the case of convolutional filters, can greatly speed up prediction. Most importantly for compression, however, is that in conjunction with Compressed Sparse Column (CSC) format, weight pruning is a highly effective way to store and transfer weights. In Appendix A we discuss CSC format in more detail.\nReducing the bit size per stored weight is another approach to model compression. For example, reducing 32 bit floats to 1 bit leads to a 32\u00d7 storage improvement. Gong et al. (2014) proposed and experimented with a number of quantization approaches: binary quantization, k-means quantization, product quantization and residual quantization. Other work finds optimal fixed points (Lin et al., 2015), applies hashing (Chen et al., 2015) or minimizes the estimation error (Wu et al., 2015). Merolla et al. (2016) demonstrates that neural networks are robust against certain amounts of low precision; indeed several groups have exploited this and showed that decreasing the weight encoding precision has little to no effect on the accuracy loss (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016). Pushing the idea of extreme quantization, (Courbariaux et al., 2015) and Courbariaux & Bengio (2016) trained networks from scratch that use only 1bit weights with floating point gradients; to achieve competitive results, however, they require many more of these weights.\nHan et al. (2015a) elaborate on combining these ideas. They introduce an multi-step algorithm that compresses CNNS up to 49\u00d7. First, weights are pruned (giving 9\u2212 13\u00d7 compression); second they quantize the weights (increasing compression to 27\u2212 31\u00d7); and last, they apply Huffman Encoding (giving a final compression of 35\u221249\u00d7). The quantization step is trainable in that after each weight is assigned to a cluster centroid, the centroids get trained with respect to the original loss function. Note that this approach has several restrictions: the number of weights set to zero is fixed after the pruning step, as is the assignment of a weight to a given cluster in the second step. Our approach overcomes all these restrictions.\nA final approach to compressing information is to apply low rank matrix decomposition. First introduced by (Denton et al., 2014) and Jaderberg et al. (2014), and elaborated on by using low rank filters (Ioannou et al., 2015), low rank regularization (Tai et al., 2015) or combining low rank decomposition with sparsity (Liu et al., 2015)."}, {"heading": "4 METHOD", "text": "This section presents the procedure of network compression as applied in the experiment section. A summary can be found in Algorithm 1."}, {"heading": "4.1 GENERAL SET-UP", "text": "We retrain pre-trained neural networks with soft weight-sharing and factorized Dirac posteriors. Hence we optimize\nL(w, {\u00b5j , \u03c3j , \u03c0j}Jj=0) = LE + \u03c4LC (6) = \u2212 log p(T|X,w)\u2212 \u03c4 log p(w, {\u00b5j , \u03c3j , \u03c0j}Jj=0), (7)\nvia gradient descent, specifically using Adam (Kingma & Ba, 2014). The KL divergence reduces to the prior because the entropy term does not depend on any trainable parameters. Note that, similar to (Nowlan & Hinton, 1992) we weigh the log-prior contribution to the gradient by a factor of \u03c4 = 0.005. In the process of retraining the weights, the variances, means, and mixing proportions of all but one component are learned. For one component, we fix \u00b5j=0 = 0 and \u03c0j=0 = 0.999. Alternatively we can train \u03c0j=0 as well but restrict it by a Beta distribution hyper-prior. Our Gaussian MM prior is initialized with 24 + 1 = 17 components. We initialize the learning rate for the weights and means, log-variances and log-mixing proportions separately. The weights should be trained with approximately the same learning rate used for pre-training. The remaining learning rates are set to 5 \u00b7 10\u22124. Note that this is a very sensitive parameter. The Gaussian mixtures will collapse very fast as long as the error loss does not object. However if it collapses too fast weights might be left behind, thus it is important to set the learning rate such that the mixture does collapse too soon. If the learning rate is too small the mixture will converge too slowly. Another option to keep the mixture components from collapsing is to apply an Inverse-Gamma hyperprior on the mixture variances."}, {"heading": "4.2 INITIALIZATION OF MIXTURE MODEL COMPONENTS", "text": "In principle, we follow the method proposed by Nowlan & Hinton (1992). We distribute the means of the 16 non-fixed components evenly over the range of the pre-trained weights. The variances will be initialized such that each Gaussian has significant probability mass in its region. A good orientation for setting a good initial variance is weight decay rate the original network has been trained on. The trainable mixing proportions are initialized evenly \u03c0j = (1 \u2212 \u03c0j=0)/J . We also experimented with other approaches such as distributing the means such that each component assumes an equal amount of probability. We did not observe any significant improvement over the simpler initialization procedure."}, {"heading": "4.3 POST-PROCESSING", "text": "After re-training we set each weight to the mean of the component that takes most responsibility for it i.e. we quantize the weights. Before quantizing, however, there might be redundant components\nas explained in section 2. To eliminate those we follow Adhikari & Hollme\u0301n (2012) by computing the KL divergence between all components. For a KL divergence smaller than a threshold, we merge two components as follows\n\u03c0new = \u03c0i + \u03c0j , \u00b5new = \u03c0i\u00b5i + \u03c0j\u00b5j \u03c0i + \u03c0j , \u03c32new = \u03c0i\u03c3\n2 i + \u03c0j\u03c3 2 j\n\u03c0i + \u03c0j (8)\nfor two components with indices i and j. Finally, for practical compression we use the storage format used in Han et al. (2015a) (see Appendix A).\nAlgorithm 1 Soft weight-sharing for compression, our proposed algorithm for neural network model compression. It is divided into two main steps: network re-training and post-processing. Require: \u03c4 \u2190 set the trade-off between error and complexity loss Require: \u0398\u2190 set parameters for gradient decent scheme such as learning rate or momentum Require: \u03b1, \u03b2 \u2190 set gamma hyper-prior parameter (optional)\nw\u2190 initialize network weights with pre-trained network weights \u03b8 = {\u00b5j , \u03c3j , \u03c0j}Jj=1\u2190 initialize mixture parameters (see Sec. 4.2) while w, \u03b8 not converged do\nw, \u03b8 \u2190 \u2207w,\u03b8LE + \u03c4LC update w and \u03b8 with the gradient decent scheme of choice end while w \u2190 argmax\n\u00b5k \u03c0kN (w|\u00b5k, \u03c3k)\u2211 \u03c0jN (w|\u00b5j , \u03c3j) compute final weight by setting it to the mean that takes most\nresponsibility (for details see Sec. 4.3)"}, {"heading": "5 MODELS", "text": "We test our compression procedure on two neural network models used in previous work we compare against in our experiments:\n(a) LeNet-300-100 an MNIST model described in LeCun et al. (1998). As no pre-trained model is available, we train our own, resulting in an error rate of 1.89%.\n(b) LeNet-5-Caffe a modified version of the LeNet-5 MNIST model in LeCun et al. (1998). The model specification can be downloaded from the Caffe MNIST tutorial page 2. As no pre-trained model is available, we train our own, resulting in an error rate of 0.88%.\n(c) ResNets have been invented by He et al. (2015) and further developed by He et al. (2016) and Zagoruyko & Komodakis (2016). We choose a model version of the latter authors. In accordance with their notation, we choose a network with depth 16, width k = 4 and no dropout. This model has 2.7M parameters. In our experiments, we follow the authors by using only light augmentation, i.e., horizontal flips and random shifts by up to 4 pixels. Furthermore the data is normalized. The authors report error rates of 5.02% and 24.03% for CIFAR-10 and CIFAR-100 respectively. By reimplementing their model we trained models that achieve errors 6.48% and 28.23%."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 INITIAL EXPERIMENT", "text": "First, we run our algorithm without any hyper-priors, an experiment on LeNet-300-100. In Figure 1 we visualise the original distribution over weights, the final distribution over weight and how each weight changed its position in the training process. After retraining, the distribution is sharply peaked around zero. Note that with our procedure the optimization process automatically determines how many weights per layer are pruned. Specifically in this experiment, 96% of the first layer (235K\n2https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_ test.prototxt\nparameter), 90% of the second (30K) and only 18% of the final layer (10K) are pruned. From observations of this and other experiments, we conclude that the amount of pruned weights depends mainly on the number of parameters in the layer rather than its position or type (convolutional or fully connected).\nEvaluating the model reveals a compression rate of 64.2. The accuracy of the model does not drop significantly from 0.9811 to 0.9806. However, we do observe that the mixture components eventually collapse, i.e., the variances go to zero. This makes the prior inflexible and the optimization can easily get stuck because the prior is accumulating probability mass around the mixture means. For a weight, escaping from those high probability plateaus is impossible. This motivates the use hyper-priors such as an Inverse-Gamma prior on the variances to essentially lower bound them."}, {"heading": "6.2 HYPER-PARAMETER TUNING USING BAYESIAN OPTIMIZATION", "text": "The proposed procedure offers various freedoms: there are many hyper-parameters to optimize, one may use hyper-priors as motivated in the previous section or even go as far as using other distributions as mixture components.\nTo cope with the variety of choices, we optimize 13 hyper-parameters using the Bayesian optimization tool Spearmint Snoek et al. (2012). These include the learning rates of the weight and mixing components, the number of components, and \u03c4 . Furthermore, we assume an Inverse-Gamma prior over the variances separately for the zero component and the other components and a Beta prior over the zero mixing components.\nIn these experiments, we optimize re-training hyperparameters for LeNet-300-100 and LeNet-5Caffe. Due to computational restrictions, we set the number of training epochs to 40 (previously 100), knowing that this may lead to solutions that have not fully converged. Spearmint acts on an objective that balances accuracy loss vs compression rate. The accuracy loss in this case is measured over the training data. The results are shown in Figure 2. In the illustration we use the accuracy loss as given by the test data. The best results predicted by our spearmint objective are colored in dark blue. Note that we achieve competitive results in this experiment despite the restricted optimization time of 40 epochs, i.e. 18K updates.\nThe conclusions from this experiment are a bit unclear, on the one hand we do achieve state-ofthe-art results for LeNet-5-Caffe, on the other hand there seems to be little connection between the parameter settings of best results. One wonders if a 13 dimensional parameter space can be searched efficiently with the amount of runs we were conducting. It may be more reasonable to get more inside in the optimization process and tune parameters according to those."}, {"heading": "6.3 COMPRESSION RESULTS", "text": "We compare our compression scheme with Han et al. (2015a) and Guo et al. (2016) in Table 1. The results on MNIST networks are very promising. We achieve state-of-the-art compression rates in both examples. We can furthermore show results for a light version of ResNet with 2.7M parameters to illustrate that our method does scale to modern architectures. We used more components (64)\nhere to cover the large regime of weights. However, for large networks such as VGG with 138M parameters the algorithm is too slow to get usable results. We propose a solution for this problem in Appendix C; however, we do not have any experimental results yet."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "In this work we revived a simple and principled regularization method based on soft weight-sharing and applied it directly to the problem of model compression. On the one hand we showed that we can optimize the MDL complexity lower bound, while on the other hand we showed that our method works well in practice when being applied to different models. A short-coming of the method at the moment is its computational cost and the ease of implementation. For the first, we provide a proposal that will be tested in future work. The latter is an open question at the moment. Note that our method\u2014since it is optimizing the lower bound directly\u2014will most likely also work when applied to other storage formats, such as those proposed originally by Hinton & Van Camp (1993). In the future we would like to extend beyond Dirac posteriors as done in Graves (2011) by extending the weight sharing prior to more general priors. For example, from a compression point of view, we could learn to prune entire structures from the network by placing Bernoulli priors over structures such as convolutional filters or ResNet units. Furthermore, it could be interesting to train models from scratch or in a student-teacher setting."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Louis Smit, Christos Louizos, Thomas Kipf, Rianne van den Berg and Peter O\u2019Connor for helpful discussions on the paper and the public code3.\nThis research has been supported by Google."}, {"heading": "A REVIEW OF STATE-OF-THE-ART NEURAL NETWORK COMPRESSION", "text": "We apply the compression scheme proposed by Han et al. (2015b;a) that highly optimizes the storage utilized by the weights. First of all, the authors store the weights in regular compressed sparse-row (CSR) format. Instead of storing |W (l)| parameters with a bit length of (commonly) porig = 32 bit, CSR format stores three vectors (A, IR, IC).\n\u2022 A stores all non-zero entries. It is thus of size |W (l)| 6=0 \u00d7 porig, where |W (l)|6=0 is the number of non-zero entries in W (l).\n\u2022 IR Is defined recursively: IR0 = 0, IRk =IRk\u22121+ (number of non-zero entries in the (k \u2212 1)-th row of W (l)). It got K + 1 entries each of size porig.\n\u2022 IC contains the column index in W (l) of each element of A. The size is hence, |W (l)|6=0 \u00d7 porig.\nAn example shall illustrate the format, let\nW (l) =  0 0 0 1 0 2 0 0 0 0 0 0 2 5 0 0 0 0 0 1  than\nA = [1, 2, 2, 5, 1] IR = [0, 1, 2, 2, 4, 5]\nIC = [3, 1, 0, 1, 3]\nThe compression rate achieved by applying the CSC format naively is\nrp = |W (l)|\n2|W (l)|6=0 + (K + 1) (9)\nHowever, this result can be significantly improved by optimizing each of the three arrays.\nA.1 STORING THE INDEX ARRAY IR\nTo optimize IR, note that the biggest number in IR is |W (l)|6=0. This number will be much smaller than 2porig . Thus one could try to find p \u2208 Z+ such that |W (l)|6=0 < 2pprun . A codebook would not be necessary. Thus instead of storing (K + 1) values with porig, we store them with pprun depth.\nA.2 STORING THE INDEX ARRAY IC\nInstead of storing the indexes, we store the differences between indexes. Thus there is a smaller range of values being used. We further shrink the range of utilized values by filling A with zeros whenever the distance between two non-zero weights extends the span of 2pprun. Han et al. (2015a) propose p = 5 for fully connected layers and p = 8 for convolutional layers. An illustration of the process can is shown in Fig. 4. Furthermore, the indexes will be compressed Hoffman encoding.\nA.3 STORING THE WEIGHT ARRAY A\nIn order to minimize the storage occupied by A. We quantize the values of A. Storing indexes in A and a consecutive codebook. Indexing can be improved further by again applying Huffman encoding."}, {"heading": "B CONFIGURING THE HYPER-PRIORS", "text": "B.1 GAMMA DISTRIBUTION\nThe Gamma distribution is the conjugate prior for the precision of a univariate Gaussian distribution. It is defined for positive random variables \u03bb > 0.\n\u0393(\u03bb|\u03b1, \u03b2) = \u03b2 \u03b1\n\u0393(\u03b1) \u03bb\u03b1\u22121e\u2212\u03b2\u03bb (10)\nFor our purposes it is best characterised by its mode \u03bb\u2217 = \u03b1\u2212 1 \u03b2 and its variance var\u03b3 = \u03b1 \u03b22 . In our experiments we set the desired variance of the mixture components to 0.05. This corresponds to \u03bb\u2217 = 1/(0.05)2 = 400. We show the effect of different choices for the variance of the Gamma distribution in Figure 5.\nB.2 BETA DISTRIBUTION\nThe Beta distribution is the conjugate prior for the Bernoulli distribution, thus is often used to represent the probability for a binary event. It is defined for some random variable \u03c0j=0 \u2208 [0, 1]\nB(\u03c0j=0|\u03b1, \u03b2) = \u0393(\u03b1+ \u03b2)\n\u0393(\u03b1)\u0393(\u03b2) (\u03c0j=0)\n\u03b1\u22121(1\u2212 \u03c0j=0)\u03b2\u22121 (11)\nwith \u03b1, \u03b2 > 0. \u03b1 and \u03b2 can be interpreted as the effective number of observations prior to an experiment, of \u03c0j=0 = 1 and \u03c0j=0 = 0, respectively. In the literature, \u03b1 + \u03b2 is defined as the pseudo-count. The higher the pseudo-count the stronger the prior. In Figure 6, we show the Beta distribution at constant mode \u03c0\u2217j=0 = \u03b1\u2212 1\n\u03b1+ \u03b2 \u2212 2 = 0.9. Note, that, the beta distribution is a special\ncase of the Dirichlet distribution in a different problem setting it might be better to rely on this distribution to control all \u03c0j ."}, {"heading": "C SCALABILITY", "text": "Neural Networks are usually trained with a form of batch gradient decent (GD) algorithm. These methods fall into the umbrella of stochastic optimization (Robbins & Monro, 1951). Here the model parameters W are updated iteratively. At each iteration t, a set ofB data instances is used to compute a noisy approximation of the posterior derivative with respect to W given all data instances N .\n\u2207W log p(W|D) = N\nB B\u2211 n=1 \u2207W log p(tn|xn,w) + I\u2211 i=1 \u2207W log p(wi) (12)\nThis gradient approximation can subsequently be used in various update schemes such as simple GD.\nFor large models estimating the prior gradient can be an expensive operation. This is why we propose to apply similar measures for the gradient estimation of the prior as we did for the likelihood term. To do so, we sample K weights randomly. The noisy approximation of the posterior derivative is\nnow:\n\u2207W log p(W|D) = N\nB B\u2211 n=1 \u2207W log p(tn|xn,w) + I K K\u2211 i=1 \u2207W log p(wi) (13)"}, {"heading": "D FILTER VISUALISATION", "text": "In Figure D we show the pre-trained and compressed filters for the first and second layers of LeNet5-Caffe. For some of the feature maps from layer 2 seem to be redundant hence the almost empty columns. In Figure D we show the pre-trained and compressed filters for the first and second layers of LeNet-300-100."}], "references": [{"title": "Multiresolution mixture modeling using merging of mixture components", "author": ["Prem Raj Adhikari", "Jaakko Hollm\u00e9n"], "venue": null, "citeRegEx": "Adhikari and Hollm\u00e9n.,? \\Q2012\\E", "shortCiteRegEx": "Adhikari and Hollm\u00e9n.", "year": 2012}, {"title": "Pattern recognition", "author": ["Christopher M Bishop"], "venue": "Machine Learning,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Compressing convolutional neural networks", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1506.04449,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or \u22121", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux and Bengio.", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Jean-Pierre David", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized neural networks: Training neural networks with weights and activations constrained to +1 or", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Geoffrey E Hinton", "Drew Van Camp"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton and Camp.,? \\Q1993\\E", "shortCiteRegEx": "Hinton and Camp.", "year": 1993}, {"title": "Variational learning and bits-back coding: an informationtheoretic view to bayesian learning", "author": ["Antti Honkela", "Harri Valpola"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Honkela and Valpola.,? \\Q2004\\E", "shortCiteRegEx": "Honkela and Valpola.", "year": 2004}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 1mb model size", "author": ["Forrest N Iandola", "Matthew W Moskewicz", "Khalid Ashraf", "Song Han", "William J Dally", "Kurt Keutzer"], "venue": "arXiv preprint arXiv:1602.07360,", "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Training cnns with low-rank filters for efficient image classification", "author": ["Yani Ioannou", "Duncan Robertson", "Jamie Shotton", "Roberto Cipolla", "Antonio Criminisi"], "venue": "arXiv preprint arXiv:1511.06744,", "citeRegEx": "Ioannou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPs,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl D Lin", "Sachin S Talathi", "V Sreekanth Annapureddy"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K Esser", "Dharmendra Modha"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Variational dropout sparsifies deep neural networks", "author": ["Dmitry Molchanov", "Arsenii Ashukha", "Dmitry Vetrov"], "venue": "arXiv preprint arXiv:1701.05369,", "citeRegEx": "Molchanov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Molchanov et al\\.", "year": 2017}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Steven J Nowlan", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Nowlan and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1992}, {"title": "Modeling by shortest data", "author": ["Jorma Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Stochastic complexity and modeling", "author": ["Jorma Rissanen"], "venue": "The annals of statistics, pp", "citeRegEx": "Rissanen.,? \\Q1986\\E", "shortCiteRegEx": "Rissanen.", "year": 1986}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["Ganesh Venkatesh", "Eriko Nurvitadhi", "Debbie Marr"], "venue": "arXiv preprint arXiv:1610.00324,", "citeRegEx": "Venkatesh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venkatesh et al\\.", "year": 2016}, {"title": "Classification by minimum-message-length inference", "author": ["Chris S Wallace"], "venue": "In International Conference on Computing and Information,", "citeRegEx": "Wallace.,? \\Q1990\\E", "shortCiteRegEx": "Wallace.", "year": 1990}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Arithmetic coding for data compression", "author": ["Ian H Witten", "Radford M Neal", "John G Cleary"], "venue": "Communications of the ACM,", "citeRegEx": "Witten et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1987}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["Jiaxiang Wu", "Cong Leng", "Yuhang Wang", "Qinghao Hu", "Jian Cheng"], "venue": "arXiv preprint arXiv:1512.06473,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}, {"title": "2015a) propose p = 5 for fully connected layers and p = 8 for convolutional layers. An illustration of the process can is shown in Fig. 4. Furthermore, the indexes will be compressed Hoffman encoding. A.3 STORING THE WEIGHT ARRAY A In order to minimize the storage occupied by A. We quantize the values of A. Storing indexes", "author": ["2prun. Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}, {"title": "B CONFIGURING THE HYPER-PRIORS B.1 GAMMA DISTRIBUTION The Gamma distribution is the conjugate prior for the precision of a univariate Gaussian distribution. It is defined for positive random variables \u03bb", "author": ["Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "This observation has opened the door for a number of highly successful compression algorithms, which either train the network from scratch (Hinton et al., 2015; Iandola et al., 2016; Courbariaux & Bengio, 2016; Courbariaux et al., 2016) or apply compression post-optimization (Han et al.", "startOffset": 139, "endOffset": 236}, {"referenceID": 20, "context": "This observation has opened the door for a number of highly successful compression algorithms, which either train the network from scratch (Hinton et al., 2015; Iandola et al., 2016; Courbariaux & Bengio, 2016; Courbariaux et al., 2016) or apply compression post-optimization (Han et al.", "startOffset": 139, "endOffset": 236}, {"referenceID": 6, "context": "This observation has opened the door for a number of highly successful compression algorithms, which either train the network from scratch (Hinton et al., 2015; Iandola et al., 2016; Courbariaux & Bengio, 2016; Courbariaux et al., 2016) or apply compression post-optimization (Han et al.", "startOffset": 139, "endOffset": 236}, {"referenceID": 11, "context": ", 2016) or apply compression post-optimization (Han et al., 2015b;a; Guo et al., 2016; Chen et al., 2015; Wen et al., 2016).", "startOffset": 47, "endOffset": 123}, {"referenceID": 2, "context": ", 2016) or apply compression post-optimization (Han et al., 2015b;a; Guo et al., 2016; Chen et al., 2015; Wen et al., 2016).", "startOffset": 47, "endOffset": 123}, {"referenceID": 38, "context": ", 2016) or apply compression post-optimization (Han et al., 2015b;a; Guo et al., 2016; Chen et al., 2015; Wen et al., 2016).", "startOffset": 47, "endOffset": 123}, {"referenceID": 12, "context": "A number of papers have appeared that encode the weights of a neural network with limited precision (say 8 bits per weight), effectively cashing in on this \u201dbits-back\u201d argument (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 177, "endOffset": 247}, {"referenceID": 4, "context": "A number of papers have appeared that encode the weights of a neural network with limited precision (say 8 bits per weight), effectively cashing in on this \u201dbits-back\u201d argument (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 177, "endOffset": 247}, {"referenceID": 36, "context": "A number of papers have appeared that encode the weights of a neural network with limited precision (say 8 bits per weight), effectively cashing in on this \u201dbits-back\u201d argument (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 177, "endOffset": 247}, {"referenceID": 5, "context": "Some authors go so far of arguing that even a single bit per weight can be used without much loss of accuracy (Courbariaux et al., 2015; Courbariaux & Bengio, 2016).", "startOffset": 110, "endOffset": 164}, {"referenceID": 31, "context": "More specifically, it minimizes the cost to describe the model (complexity cost L) and the misfit between model and data (error cost L) (Rissanen, 1978; 1986).", "startOffset": 136, "endOffset": 158}, {"referenceID": 37, "context": "It has been shown that variational learning can be reinterpreted as an MDL problem (Wallace, 1990; Hinton & Van Camp, 1993; Honkela & Valpola, 2004; Graves, 2011).", "startOffset": 83, "endOffset": 162}, {"referenceID": 10, "context": "It has been shown that variational learning can be reinterpreted as an MDL problem (Wallace, 1990; Hinton & Van Camp, 1993; Honkela & Valpola, 2004; Graves, 2011).", "startOffset": 83, "endOffset": 162}, {"referenceID": 39, "context": "(2015a) show that one can successfully exploit (i) and (ii) by using a form of arithmetic coding (Witten et al., 1987).", "startOffset": 97, "endOffset": 118}, {"referenceID": 24, "context": "This suggests that pruning, originally introduced to reduce structure in neural networks and hence improve generalization, can be applied to the problem of compression and speed-up (LeCun et al., 1989).", "startOffset": 181, "endOffset": 201}, {"referenceID": 11, "context": "In fact, (Han et al., 2015b; Guo et al., 2016) show that neural network survive severe weight pruning (up to 99%) without significant loss of accuracy.", "startOffset": 9, "endOffset": 46}, {"referenceID": 38, "context": "In an approach slightly orthogonal to weight pruning, (Wen et al., 2016) applied structural regularization to prune entire sets of weights from the neural network.", "startOffset": 54, "endOffset": 72}, {"referenceID": 26, "context": "Other work finds optimal fixed points (Lin et al., 2015), applies hashing (Chen et al.", "startOffset": 38, "endOffset": 56}, {"referenceID": 2, "context": ", 2015), applies hashing (Chen et al., 2015) or minimizes the estimation error (Wu et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 40, "context": ", 2015) or minimizes the estimation error (Wu et al., 2015).", "startOffset": 42, "endOffset": 59}, {"referenceID": 12, "context": "(2016) demonstrates that neural networks are robust against certain amounts of low precision; indeed several groups have exploited this and showed that decreasing the weight encoding precision has little to no effect on the accuracy loss (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 238, "endOffset": 308}, {"referenceID": 4, "context": "(2016) demonstrates that neural networks are robust against certain amounts of low precision; indeed several groups have exploited this and showed that decreasing the weight encoding precision has little to no effect on the accuracy loss (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 238, "endOffset": 308}, {"referenceID": 36, "context": "(2016) demonstrates that neural networks are robust against certain amounts of low precision; indeed several groups have exploited this and showed that decreasing the weight encoding precision has little to no effect on the accuracy loss (Gupta et al., 2015; Courbariaux et al., 2014; Venkatesh et al., 2016).", "startOffset": 238, "endOffset": 308}, {"referenceID": 5, "context": "Pushing the idea of extreme quantization, (Courbariaux et al., 2015) and Courbariaux & Bengio (2016) trained networks from scratch that use only 1bit weights with floating point gradients; to achieve competitive results, however, they require many more of these weights.", "startOffset": 42, "endOffset": 68}, {"referenceID": 8, "context": "First introduced by (Denton et al., 2014) and Jaderberg et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 21, "context": "(2014), and elaborated on by using low rank filters (Ioannou et al., 2015), low rank regularization (Tai et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 35, "context": ", 2015), low rank regularization (Tai et al., 2015) or combining low rank decomposition with sparsity (Liu et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 27, "context": ", 2015) or combining low rank decomposition with sparsity (Liu et al., 2015).", "startOffset": 58, "endOffset": 76}], "year": 2017, "abstractText": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of \u201dsoft weight-sharing\u201d (Nowlan & Hinton, 1992). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_44"}