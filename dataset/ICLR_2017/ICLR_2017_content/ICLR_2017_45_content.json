{"name": "ICLR_2017_45.pdf", "metadata": {"source": "CRF", "title": "NEURAL PROGRAM LATTICES", "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "emails": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "A critical component of learning to act in a changing and varied world is learning higher-level abstractions of sequences of elementary tasks. Without such abstractions we would be forced to reason at the level of individual muscle contractions, making everyday tasks such as getting ready for work and making dinner almost impossible. Instead, as humans, we learn a hierarchy of skills starting with basic limb movements and eventually getting to the level of tasks such as get ready for work or drive to the airport. These abstractions have many different names. For example, in computer programming they are called functions or subroutines and in reinforcement learning they are called options or temporally extended actions. They facilitate learning in two important ways. First, they enable us to learn faster, i.e. with lower sample complexity. Second, they enable us to strongly generalize from our prior experience so that we can, for example, drive to a new location once we have learned how to drive to a few other locations.\nA primary mechanism used for learning is watching others perform a task. During such demonstrations, one typically observes the elementary operations performed, such as the movements of individual limbs or the mouse clicks in a computer interface. In some cases, the demonstrations can also provide supervision of the abstract operations (i.e., the abstraction hierarchy) that generated the elementary operations, either through a formal annotation process or through informal natural language descriptions. Recent work on Neural Programmer-Interpreters, NPI (Reed & de Freitas, 2016), has shown that when the training data includes both elementary and abstract operations, learning the abstractions results in strong generalization capabilities. This enables, for example, the ability to add very large numbers when trained only on the addition of relatively small numbers.\nProviding supervision of the abstract operations during a demonstration requires significant additional effort, however, and so in typical real-world scenarios we will observe only the elementary operations. For example, we can see a person\u2019s limbs move (elementary operations), but we cannot see the mental states that led to these movements (abstract operations). In the same vein, we\n\u2217Work done primarily while author was an intern at Microsoft Research.\ncan easily capture a user\u2019s clicks in an online application or their real-world movements using a skeletal tracking depth camera (Microsoft Corp. Redmond WA). NPI cannot directly be applied on data like this, however, because the data does not contain the abstraction hierarchy. This motivates the desire for a model which can learn an abstraction hierarchy from only sequences of elementary operations, but this is an ill-posed problem that requires either additional modeling assumptions or some strongly supervised data. In this work, we take a first step by assuming access to a small number of strongly supervised samples that provide the components of the abstraction hierarchy and disambiguate which of infinitely many abstraction hierarchies are preferred. While we currently only consider domains without noise, we believe our work provides a starting point for future research on adding additional modeling assumptions that could remove the need for strong supervision altogether.\nThere are several technical issues that arise in developing NPL, which are addressed in this paper. In section 2, we reformulate the NPI model to explicitly include a program call stack, which is necessary for the later modeling developments. Next we need to formulate a training objective for weakly supervised data instances. Ideally we could treat the abstract operations as latent quantities and optimize the marginalized log probability that arises from summing out the abstract operations. However, there are exponentially many such abstraction hierarchies, and so this is computationally intractable. To overcome this challenge, we compute an approximate dynamic program by building on two ideas from the literature. First, we draw inspiration from Connectionist Temporal Classification, CTC (Graves et al., 2006), observing that it provides a method for learning with latent alignments. In section 3.1 we reformulate the CTC objective into a feedforward process that executes a dynamic program. Applying this to our problem, however, requires handling the program call stack. In section 3.2 we do this through an approximation analogous to that of Stack-Augmented Recurrent Nets, StackRNNs (Joulin & Mikolov, 2015), resulting in a fully-differentiable feedforward process that executes a dynamic program to approximately compute the marginalized log probability that we desire. Finally, we observe in section 3.3 that there are alternative dynamic programs for approximating the desired marginalized log probability and present one that uses more computation to more closely resemble the exact (exponentially expensive) dynamic program while remaining tractable.\nOur key contributions can be summarized as follows:\n\u2022 We show how ideas from CTC and StackRNNs can be adapted and extended to enable the training of NPI-like models from only flat sequences of elementary operations and world states. \u2022 We introduce a method to compute a more accurate approximation of marginalized log proba-\nbilities in such models. \u2022 On the long-hand addition task from Reed & de Freitas (2016) and a new task involving arrang-\ning blocks in a grid-world, we demonstrate empirically that using NPL to train with elementary operation sequences combined with only a few training samples with full program traces can achieve similar performance to NPI but with weaker supervision."}, {"heading": "2 MODEL BACKGROUND", "text": "The NPI model is based on a Recurrent Neural Network (RNN) which, at each step, either calls an abstract program, performs an elementary operation, or returns from the current program. To make this decision, each step of the RNN takes as input: (1) a learnable embedding of program to execute, (2) embedded arguments for this program, and (3) an embedding of the current world state. Calling an abstract program resets the LSTM hidden state to zero and updates the program and arguments provided as input to the following steps. Returning from an abstract program inverts this process, restoring the hidden state and input program and arguments to those from before the program was called. Performing an elementary operation updates the world state, but leaves the current program and arguments in place, and performs the standard LSTM update of the hidden state.\nRather than present the details of the NPI model as in Reed & de Freitas (2016), we will cast it in the formulation that we will use throughout the paper. The main difference is that our presentation will explicitly maintain a call stack, which we will refer to as Stack-based NPI. Morally this does not change the model, but it will enable the extension to weaker supervision described in section 3.\nThe basic structure of the reformulated model can be seen in Figure 1. The model learns a library of programs, G, and arguments,R, to these programs, where each program g \u2208 Rn and each argument\nr \u2208 Rm is represented as an embedding, with n and m as the embedding dimensions. When a program is called with a list of arguments it performs a sequence of actions, where each action is one of: OP, PUSH, or POP. OP performs an elementary operation, e.g. move one step. PUSH calls to another program. POP returns from the current program back to the parent program.\nAn LSTM-based controller, shown in Figure 2, is used to generate the sequence of actions, deciding the action at timestep t based on the currently running program and arguments, gtin, the LSTM\u2019s internal state htin and an observation of the current world state, wt. To support calls to and returns from subprograms, the controller state contains two call stacks, one for the internal RNN state, which we denote as M (green in Figure 1), and one for the program and arguments, which we denote as S (red in Figure 1). M td and S t d refer to the elements at depth-d of the stacks at timestep t.\nThe training data for NPI requires full execution traces. We use \u03c0 to denote all the observations recorded in a single full exectution trace. Specifically, for timestep t in the execution we define \u03c0tw to be the input world state, and \u03c0 t a to be the decision of which of the following actions to take:\nOP: perform elementary operation \u03c0to\nPUSH: push the currently running program and LSTM internal state onto the call stack and call program \u03c0tg\nPOP: return to the parent program by popping the parent\u2019s program state off the top of the call stack\nNote that, as with the original NPI model, we also include arguments for both the operation and program calls, but for notational simplicity we subsume those into \u03c0to and \u03c0 t g respectively.\nThe stack updates are formally defined as:\nM t+1d =  J\u03c0ta = POPKM t1 + J\u03c0ta = OPKhtout + J\u03c0ta = PUSHK0, d = 0 J\u03c0ta = POPKM t2 + J\u03c0ta = OPKM t1 + J\u03c0ta = PUSHKhtout, d = 1 J\u03c0ta = POPKM td+1 + J\u03c0ta = OPKM td + J\u03c0ta = PUSHKM td\u22121, d > 1\nSt+1d = { J\u03c0ta = POPKSt1 + J\u03c0ta = OPKSt0 + J\u03c0ta = PUSHKgtout, d = 0 J\u03c0ta = POPKStd+1 + J\u03c0ta = OPKStd + J\u03c0ta = PUSHKStd\u22121, d > 0\n(2.1)\nThe conditions in the Iverson brackets choose which type of update should be performed based on the action type. POPing from the stack moves all items up one location in the stack. Performing an elementary OP, updates the top element of stack M to contain the new RNN hidden state but otherwise leaves the stacks unchanged. PUSHing onto the stack pushes the new program and arguments, gt\u22121out , onto stack S, pushes a default (zero) hidden state onto stack M , and moves all of the other elements in the stacks down one location.\nThe RNN cell inputs are:\nhtin =M t 0 = the current LSTM internal state,\ngtin = S t 0 = the current program and arguments,\nwt = \u03c0tw = the current world state.\nInside the RNN cell, as shown in Figure 2, gtin andw t are passed through a task specific encoder network, fenc to generate a combined embedding ut which is passed directly into an LSTM (Hochreiter & Schmidhuber, 1997). Formally,\nut = fenc(w t, gtin), h t out = flstm(u t, htin); (2.2)\nThe LSTM output is passed in parallel through four different decoder networks to generate the following probability distributions:\npta = the action p t r = the arguments for the program or operation ptg = the program to be called p t o = the elementary operation to be performed\nAt test time, we use a greedy decoder that makes the decision with the highest probability for each choice. Formally:\ngtout = \u03c6(p t g) = argmax\u03b3\u2208G p t g(\u03b3)\nAt training time our objective is to find neural network parameters \u03b8 which maximize the following (log) likelihood function:\np(\u03c0t) = J\u03c0ta = OPK pta(OP)pto(\u03c0to) + J\u03c0ta = PUSHK pta(PUSH)ptg(\u03c0tg) + J\u03c0ta = POPK pta(POP)\np(\u03c0) = T\u220f t=1 p(\u03c0t), L(\u03b8) = log p(\u03c0) (2.3)"}, {"heading": "3 NEURAL PROGRAM LATTICES", "text": "In this section we introduce our core contribution, a new framework for training NPI-like models when the training data contains only sequences of elementary actions instead of full program abstractions. The basis of our framework is the Neural Program Lattice, which approximately computes marginal probabilities using an end-to-end differentiable neural network.\nIn this section, the training data is an elementary operation trace \u03bb, which includes a sequence of elementary steps, \u03bbo, and a corresponding sequence of world states, \u03bbw. For each elementary step, \u03bbi, the elementary operation performed is \u03bbio and the input world state is \u03bb i w. We define O as a many-to-one map from a full execution trace \u03c0 to it\u2019s elementary operation trace \u03bb. With these\ndefinitions and p(\u03c0) as defined in equation 2.3, our desired (log) marginal likelihood for a single example becomes\nL(\u03b8) = log \u2211\n\u03c0\u2208O\u22121(\u03bb)\np(\u03c0). (3.1)\nComputing this quantity is intractable because the number of possible executions |O\u22121(\u03bb)| is exponential in the maximum length of \u03c0 and each execution may have unique stack states. In the following sections, we describe how to approximately compute this quantity so as to enable learning from weak supervision. To also learn from strong supervision, we simply add log p(\u03c0) terms to the objective for each strongly supervised example \u03c0."}, {"heading": "3.1 CTC AS A FEED-FORWARD NETWORK", "text": "In formulating a loss function which approximates the exponential sum in equation 3.1, the first challenge is aligning the elementary steps, \u03bbi, in the training data, to the timesteps, t, of the model. Specifically, when the model calls into a program or returns from a program in a given timestep, it does not perform any elementary operation in that timestep. As a result, the alignment between elementary steps in the data and the timesteps of the model depends crucially on the choice of highlevel abstraction. To overcome this challenge, we draw inspiration from CTC (Graves et al., 2006).\nCTC is an RNN-based neural network architecture used in speech recognition to handle the analogous problem of aligning audio sequence inputs to word sequence outputs. It can be seen as a combination of an RNN and a graphical model. The RNN computes a distribution over possible outputs for each timestep, while the graphical model consumes those distributions and uses a dynamic program to compute the marginal distribution over possible label sequences. A crucial assumption is that the RNN outputs at each timestep are conditionally independent, i.e. no feedback connections exist from the output layer back into the rest of the network. This assumption is incompatible with the NPI model because action decisions from timestep t determine the world state, hidden state, and program input for the next timestep. In section 3.2 we will adapt the CTC idea to work in the NPI setting. In this section we prepare by reformulating CTC into a feed forward neural network that can be trained with standard back propagation.\nThe main challenge solved by CTC is finding the alignment between the elementary steps, i, observed in the training data and the timesteps, t, of the model. To facilitate alignment discovery, the output layer in a CTC network is a softmax layer with a unit for each elementary operation in O, the set of elementary operations, as well as one additional unit for a BLANK output where no elementary operation is performed because (in our case) the model calls into a new program or returns from the current program. Define \u03b2 \u2208 O\u2032T as an output sequence over the alphabet O\u2032 = O \u222a BLANK. Additionally, define the many-to-one map B from an output sequence \u03b2 to \u03bbo the sequence of elementary operations created by removing all of the BLANK outputs from \u03b2. As discussed above, the CTC model assumes that the RNN inputs at time t are independent of the decisions made by the model, \u03c0. Thus for purposes of this subsection, we will assume both that htin = h t\u22121 out , and that w = (w1, . . . , wT ) and gin = (g1in, . . . , g T in) are provided as inputs and are thus independent of the output decisions. We can then formally define\npt(\u03b2t|w, gin) = { pta(POP|w, gin) + pta(PUSH|w, gin), \u03b2t = BLANK pta(OP|w, gin)pto(\u03b2t|w, gin), otherwise\np(\u03b2|w, gin) = |w|\u220f t=1 pt(\u03b2t|w, gin)\nL(\u03b8|\u03bbo, w, gin) = log p(\u03bbo|w, gin) = log \u2211\n\u03b2\u2208B\u22121(\u03bbo)\np(\u03b2|w, gin).\nThe dynamic program used by CTC to compute this likelihood is based on yti , the total probability that as of timestep t in the model we have generated \u03bb1:io , the first i elementary actions in \u03bbo. y t i is\ncalculated from w1:t and g1:tin , the first t elements in w and gin respectively. Formally, yti = \u2211\n\u03b2\u2208B\u22121(\u03bb1:io )\np(\u03b2|w1:t, g1:tin )\nL(\u03b8|\u03bbo, w, gin) = log y|w||\u03bbo|.\nWe can compute this value recursively as y00 = 1 and\nyti = p t(\u03bbio|w1:t, g1:tin )yt\u22121i\u22121 + p t(BLANK|w1:t, g1:tin )yt\u22121i . This formulation allows the likelihood to be computed in a feed-forward manner and the gradients of \u03b8 to be computed using standard back propagation through time. Note that if there were feedback connections in the model, then it would not be sufficient to only use yti as the dynamic programming state; we would need to keep track of all the different possible stack states after having produced the sequence prefix, which is what leads to the intractability."}, {"heading": "3.2 DIFFERENTIABLE CALL STACK", "text": "In the last section we assumed that the RNN inputs w, and gin were defined independently of the decisions \u03c0 made by the model and that htin = h t\u22121 out . In this section we show how to relax these assumptions to handle the full Stack-based NPI model described in section 2. The key idea is that rather than propagating forward all possible stack states, which leads to a combinatorial explosion, we will propagate forward a single stack state which is a weighted average of all possible stack states, where the weights are computed based on local probabilities of actions at each timestep. This operation is analogous to that used in StackRNNs (Joulin & Mikolov, 2015). The result is a tractable and differentiable forward execution process that no longer exactly computes the desired marginal likelihood. However, we will show experimentally that learning with this model for weakly supervised examples leads to the behavior that we would hope for if we were learning from the true marginal log likelihood. That is, we can share model parameters while training on strongly and weakly labeled examples, and adding the weakly labeled data improves generalization performance.\nIn more detail, we estimate all quantities specified in \u03c0 but not in \u03bb using a soft-argmax function that computes deterministic functions of the previously observed or estimated quantities. These estimated quantities are \u03c0a, \u03c0g , and implicitly \u03c0w. Both \u03c0w and \u03c0g can be directly replaced with a soft-argmax as follows:\nwt = \u2211 i\u2208I yt\u22121i \u03bb i w (3.2)\ngtout = \u03c6soft(p t g) = \u2211 \u03b3\u2208G ptg(\u03b3)\u03b3 (3.3)\nReplacing decision \u03c0ta with a soft-argmax changes the stack updates from equation 2.1 into differentiable stack updates similar to those used in Joulin & Mikolov (2015). Formally,\nyt = \u2211 i\u2208I yti\n\u03b1t(a) =\n{\u2211 i\u2208I(y t i/y t+1)pta(a)p t o(\u03bb i o), a = OP\n(yt/yt+1)pta(a), a 6= OP\nM t+1d =  \u03b1t(POP)M t1 + \u03b1 t(OP)htout + \u03b1 t(PUSH)0, d = 0 \u03b1t(POP)M t2 + \u03b1 t(OP)M t1 + \u03b1 t(PUSH)htout, d = 1\n\u03b1t(POP)M td+1 + \u03b1 t(OP)M td + \u03b1 t(PUSH)M td\u22121, d > 1\nSt+1d =\n{ \u03b1t(POP)St1 + \u03b1 t(OP)St0 + \u03b1 t(PUSH)gtout, d = 0\n\u03b1t(POP)Std+1 + \u03b1 t(OP)Std + \u03b1 t(PUSH)Std\u22121, d > 0\nwith \u03b1 introduced for notational simplicity. This change enables htin and g t in to now depend on the distribution over output decisions at time t\u2212 1 via the stack, as gtin = St0 and htin = M t0, where St0 (resp. M t0) are computed from S t\u22121 0 and S t\u22121 1 (resp. M t\u22121 0 and the LSTM cell\u2019s output at timestep t).\nThe last remaining complexity is that \u03bb does not indicate the necessary number of model timesteps. Thus the likelihood function must sum over all possible execution lengths up to some maximum T and ensure that the final action is a return, i.e. POP. If we define I = |\u03bbo| then formally,\nL(\u03b8) = log \u2211 t<T pta(POP)y t I\nThis gives a fully differentiable model for approximately maximizing the marginal probability of \u03bb."}, {"heading": "3.3 LATTICE OF PROGRAM STATES", "text": "Although the model we have defined so far is fully differentiable, the difficultly in training smoothed models of this form has been highlighted in the original Neural Turing Machine work (Graves et al., 2014) as well as much of the follow on work (Gaunt et al., 2016; Kurach et al., 2016; Graves et al., 2016; Neelakantan et al., 2016; Joulin & Mikolov, 2015). To help alleviate this difficulty, we introduce in this section the neural lattice structure after which Neural Program Lattices are named.\nTo motivate the need for this lattice, consider the set of possible program execution paths as a tree with a branch point for each timestep in the execution and a probability assigned to each path. Exact gradients could be computed by executing every path in the tree, calculating the gradient for each path, and then taking an average of the gradients weighted by the path probabilities. This solution is impractical however since it requires computation and memory that scales exponentially with the number of timesteps. To avoid this problem, the NTM and related techniques perform a single forward execution which is meant to approximately represent the simultaneous execution of all of the paths in the tree. To avoid the exponential explosion, the state at each timestep, i.e. tree depth, is approximated using a fixed-sized, representation. The approximation representation chosen by both NTM and Joulin & Mikolov (2015) is a soft-argmax of the states generated by performing each of the possible actions on the previous approximate state.\nWe observe that these two choices are really extreme points on what is a continuous spectrum of options. Instead of choosing to maintain a separate state representation for every path, or to group together all paths into a single representation, we can group together subsets of the paths and maintain an approximate state representation for each subset. This allows us to move along this spectrum, by trading higher memory and computational requirements for a hopefully closer approximation of the marginal probability.\nIn our implementation we group together execution paths at each timestep by call depth, l \u2208 L, and number of elementary operations performed so far, i \u2208 I , and maintain at each timestep a separate embedded state representation for each group of execution paths. Thus the unrolled linear architecture shown in Figure 1 becomes instead a lattice, as shown in Figure 3, with a grid of approximate program states at each timestep. Each node in this lattice represents the state of all paths that are at depth l and elementary operation i when they reach timestep t. Each node contains a soft-argmax of the stack states in M and S and an RNN cell identical to that in Figure 21. For each node we must also compute yt,li , the probability that at timestep t the execution is at depth l and at elementary operation i and has output the elementary operation sequence \u03bb1:i. As before we can compute this recursively as:\nyt+1,li = p t,l+1 a,i (POP)y t,l+1 i + p t,l a,i\u22121(OP)p t,l o,i\u22121(\u03bb i o)y t,l i\u22121 + p t,l\u22121 a,i (PUSH)y t,l\u22121 i .\nSimilarly, the averaged call stack values are computed recursively as follows:\n\u03b1t,l1,l2i1,i2 (a) = (y t,l1 i1 /yt+1,l2i2 )p t,l1 a,i1 (a)\nM t+1,ld,i =  \u03b1t,l+1,li,i (POP)M t,l+1 1,i + \u03b1 t,l,l i\u22121,i(OP)p t,l o,i\u22121(\u03bb i o)h t,l out,i\u22121 + \u03b1 t,l\u22121,l i,i (PUSH)0, d = 0 \u03b1t,l+1,li,i (POP)M t,l+1 2,i + \u03b1 t,l,l i\u22121,i(OP)p t,l o,i\u22121(\u03bb i o)M t,l 1,i\u22121 + \u03b1 t,l\u22121,l i,i (PUSH)h t,l\u22121 out,i\u22121, d = 1\n\u03b1t,l+1,li,i (POP)M t,l+1 d+1,i + \u03b1 t,l,l i\u22121,i(OP)p t,l o,i\u22121(\u03bb i o)M t,l d,i\u22121 + \u03b1 t,l\u22121,l i,i (PUSH)M t,l\u22121 d\u22121,i, d > 1\nSt+1,ld,i =\n{ \u03b1t,l+1,li,i (POP)S t,l+1 1,i + \u03b1 t,l,l i\u22121,i(OP)p t,l o,i\u22121(\u03bb i o)S t,l 0,i\u22121 + \u03b1 t,l\u22121,l i,i (PUSH)g t,l\u22121 out,i , d = 0\n\u03b1t,l+1,li,i (POP)S t,l+1 d+1,i + \u03b1 t,l,l i\u22121,i(OP)p t,l o,i\u22121(\u03bb i o)S t,l d,i\u22121 + \u03b1 t,l\u22121,l i,i (PUSH)S t,l\u22121 d\u22121,i, d > 0\nWe have left out the boundary conditions from the above updates for readability, the details of these are discussed in Appendix A.4.\nFinally, the likelihood function approximately maximizes the probability of paths which at any timestep have correctly generated all elementary operations in \u03bb, are currently at depth 0 and are returning from the current program. Formally,\nL(\u03b8) = log \u2211 t\u2208T pt,0a,I(POP)y t,0 I . (3.4)\nRemark: The specific choice to group by elementary operation index, and call depth was motivated by the representational advantages each provides. Specifically:\n\u2022 Grouping by elementary operation index: allows the model to represent the input world state exactly instead of resorting to the fuzzy world state representation from equation 3.2.\n\u2022 Grouping by call depth: allows the representation to place probability only on execution paths that return from all subprograms they execute, and return only once from the top level program as specified in equation 3.4.\nTable 1 summarizes these advantages and the computational trade-offs discussed earlier.\nFinally, in practice we find that values of the y\u2019s quickly underflow, and so we renormalize them at each timestep, as discussed in Appendix A.3."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we demonstrate the capability of NPL to learn on both the long-hand addition task (ADDITION) from Reed & de Freitas (2016) and a newly introduced task involving arranging blocks in a grid-world (NANOCRAFT). We show that using the NPL to train with mostly the weak supervision of elementary operation traces, and very few full program traces, our technique significantly outperforms traditional sequence-to-sequence models, and performs comparably to NPI models trained entirely with the strong supervision provided by full program traces. Details of the experimental settings are discussed in Appendix A.5.\nNANOCRAFT,PUSH MOVE_MANY(right),PUSH\nACT_MOVE(right),STAY .... <END>,POP\nBUILD_WALL(right),PUSH PLACE_AND_MOVE(right),PUSH\nACT_MOVE(right),STAY ACT_PLACE_BLOCK(wood, red),STAY <END>,POP\nPLACE_AND_MOVE(right),PUSH ACT_MOVE(right),STAY <END>,POP .... <END>,POP\nBUILD_WALL(down),PUSH .... <END>,POP ... <END>,POP\nMOVE_MANY(down),PUSH ACT_MOVE(right),STAY .... <END>,POP\n*\n*\n*\n*\n*\n*\nFigure 4: NANOCRAFT: An illustrative example program, where the agent (denoted as \u201c*\u201d) is required to build 3\u00d74 rectangular red wooden building at a certain location in a 6\u00d76 grid world. We can see that some of the blocks are already in place in the initial world-state. To build the building, the agent (program) first makes two calls to MOVE MANY to move into place in the X and Y dimensions, and then calls BUILD WALL four times to build the four walls of the building.\n0\n0.2\n0.4\n0.6\n0.8\n1\n16 32 64 128 256\n0- 1 AC\nCU RA\nCY\n# FULL\nNANOCRAFT WITH FULL WORLD\nNPL-Full NPL-64 NPL-128 NPL-256 Seq-64 Seq-128 Seq-256\nFigure 5: NANOCRAFT Sample Complexity: The x-axis varies the number of samples containing full program abstractions, while the y-axis shows the accuracy. NPL-{64,128,256} shows the accuracy of our model when trained with 64/128/256 training samples. NPI shows the accuracy of NPI, which can utilize only the samples containing full program abstractions. Finally, Seq-{64,128,256} shows the accuracy of a seq2seq baseline when trained on 64/128/256 samples. It\u2019s performance does not change as we vary the number of samples with full program abstractions since it cannot utilize the additional supervision they provide."}, {"heading": "4.1 SAMPLE COMPLEXITY", "text": "Task: We study the sample complexity using a task we call NANOCRAFT. In this task we consider an environment similar to those utilized in the reinforcement learning literature. The perceptual input comes from a 2-D grid world where each grid cell can be either empty or contain a block with both color and material attributes. The task is to move around the grid world and place blocks in the appropriate grid cells to form a rectangular building. The resulting building must have a set of provided attributes: (1) color, (2) material, (3) location, and sizes in the (4) X and (5) Y dimensions. As shown in the example in Figure 4, at each step the agent can take one of two primitive actions, place a block at the current grid cell with a specific color and material, or move in one of the four\n1with additional indexes for i and l on all of the inputs and outputs.\nADD,PUSH ADD1,PUSH\nACT_WRITE(3),STAY\n<END>,POP\nADD1,PUSH ACT_WRITE(7),STAY\n<END>,POP <END>,POP\nCARRY,PUSH ACT_PTR_MOVE(1, left),STAY ACT_WRITE(1),STAY ACT_PTR_MOVE(1, right),STAY <END>,POP LSHIFT,PUSH ACT_PTR_MOVE(0, left),STAY ACT_PTR_MOVE(1, left),STAY ACT_PTR_MOVE(2, left),STAY ACT_PTR_MOVE(3, left),STAY <END>,POP\nLSHIFT,PUSH ACT_PTR_MOVE(0, left),STAY ACT_PTR_MOVE(1, left),STAY ACT_PTR_MOVE(2, left),STAY ACT_PTR_MOVE(3, left),STAY <END>,POP\n2 4 5 8* * * *000 00 0 0 0\n2 4 5 8* *\n* *300\n10 0 0 0\n2 4 5 8*\n* * * 300 10\n0 0\n0\n2 4 5 8*\n* * * 370 10\n0 0\n0\nFigure 6: ADDITION: An illustrative example program of the addition of 25 to 48. We have four pointers (denoted as \u201c*\u201d), one for each row of the scratch pad. We repeatedly call ADD1 until we hit the left most entry in the scratch pad. Each call to ADD1 we call ACT WRITE to write the result, CARRY to write the carry digit (if necessary) and LSHIFT to shift all four pointers to the left to work on the next digit. The digit sequence on the fourth row of scratch pad is the result of the addition.\ncardinal directions. We explored both a fully observable setting, and a partially observable setting. In the fully observable setting, the world is presented as a stack of 3 grids, one indicating the material of the block at each location (or empty), a similar one for color and a final one-hot grid indicating the agent\u2019s location. In the partially observable setting, the agent is provided only two integers, indicating the color and material of the block (if any) at the current location. Finally, in both settings the world input state contains an auxiliary vector specifying the five attributes of the building to be built. In each sample, a random subset of the necessary blocks have already been placed in the world, and the agent must walk right over these locations without placing a block.\nExperiment Setup: We assume that data with full programmatic abstractions is much more difficult to obtain than data containing only flat operation sequences,2 so we study the sample complexity in terms of the number of such samples. All experiments were run with 10 different random seeds, and the best model was chosen using a separate validation set which is one-quarter the size of the training set.\nResults: Figure 5 shows the sample complexity for the NANOCRAFT task in the fully observable setting. We can see that NPL significantly outperforms the NPI baseline (NPI) when only a subset the total training samples have full abstractions. NPL similarly outperforms a sequence-to-sequence baseline (Seq-*) trained on all of the available data. We also performed preliminary experiments for the partially observable setting, and obtained similar results."}, {"heading": "4.2 GENERALIZATION ABILITY", "text": "Task: We study generalization ability using the ADDITION task from Reed & de Freitas (2016). The objective of this task is to read in two numbers represented as digit sequences and compute the digit sequence resulting from the summation of these two numbers. The goal is to let the model learn the basic procedure of long-hand addition: repeatedly add two one-digit numbers, write down the result (and the carry bit if necessary) and move to the left until the beginning of the numbers is reached. The whole procedure is represented using a four-row scratch pad, where the first and second rows are input digit sequences, the third row is the carry digit and the forth row the result. The model is provided a world-state observation which only provides a partial view into the full scratchpad state. Specifically, it is provided the integers at the location of four different pointers, each in one row of the scratchpad. The model has two possible elementary operations, either move a pointer left or right, or write a single digit into one of the four pointer locations. All four pointers start at the rightmost location (the least significant digit), and are gradually moved to the left by the\n2Operation sequences can be obtained by observing a human demonstrating a task, whereas full abstractions require additional effort to annotate such traces.\nprogram throughout the execution. Figure 6 gives an example of a full program trace as well as state of the scratch pad at a particular timestep.\nExperiment Setup: A primary advantage of learning programmatic abstractions over sequences is an increased generalization capability. To evaluate this, we train our model on samples ranging from 1 to 10 input digits . The training data contains an equal number of samples of each length (number of digits), and includes full program abstractions for only one randomly chosen sample for each length such that |FULL| = 10. We then test NPL using samples containing a much larger number of digits, ranging up to 1,000. On this task we found that both our model and the original NPI model were somewhat sensitive to the choice of initial seed, so we sample many different seeds and report both the mean and standard deviation, using a bootstrapping setup (Efron & Tibshirani (1994)) which is detailed in Appendix A.6.2.\nCompared Models: We originally compared to a standard flat LSTM sequence model. However, we found that even with 32 samples per digit such a model was not able to fit even the training data for samples with more than 4 or 5 digits, so we did not present these results.3. Instead, we compare to a model called S2S-Easy, which is the strongest baseline for this task from (Reed & de Freitas, 2016). This model is custom-designed for learning addition and so it represents a very strong baseline. We discuss the model details in Appendix A.6.1. For completeness we also compare to a reimplementation of NPI in two different training regimes.\nResults: Figure 7 shows the generalization capabilities of our model on the ADDITION task. Our model with \u201cone-shot\u201d strong supervision (NPL-16-1) significantly outperforms the S2S-Easy baseline even when the baseline is provided twice as many training samples (S2S-Easy-32). This is particularly notable given that the S2S-Easy model is specifically designed for the addition task. This result highlights the generalization capabilities our model brings by learning the latent structures which generate the observed sequences of elementary operations. Furthermore, we can see that\n3This is consistent with the findings of Reed & de Freitas (2016)\nthese latent structures are learned mostly from the unlabeled sequences, since the vanilla NPI model trained with only 1 sample per digit (NPI-1) cannot generalize beyond the 10-digit data on which it was trained. Finally, we can see that just a single fully supervised sample is sufficient since it enables our model to perform comparably with a vanilla NPI model trained with FULL supervision for all samples (NPI-16)."}, {"heading": "5 RELATED WORK", "text": "We have already discussed the most relevant past work upon which we directly build: CTC (Graves et al., 2006), StackRNNs (Joulin & Mikolov, 2015) and NPI (Reed & de Freitas, 2016).\nNeural Programs Training neural networks to perform algorithmic tasks has been the focus of much recent research. This work falls into two main categories: weakly supervised methods that learn from input-output examples, and strongly supervised methods that additionally have access to the sequence of elementary actions performed to generate the output.\nThe work on learning neural programs from input-output data was sparked by the surprising effectiveness of the Neural Turing Machine (NTM) (Graves et al., 2014). Similar to NTMs, many of the proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015). Some of this work has considered learning addition from input-output samples, a similar, but more challenging setup than our ADDITION domain. Zaremba & Sutskever (2014) makes use of a few training tricks to enable a standard LSTM to learn to add numbers up to length 9 when training on numbers of the same length. Kalchbrenner et al. (2015) proposes an architecture that is able to learn to add 15-digit numbers when trained on numbers of the same length. The Neural GPU model from (Kaiser & Sutskever, 2015) learns to add binary numbers 100 times longer than those seen during training, but requires tens of thousands of training samples and extensive hyperparameter searches. Additionally, using a decimal instead of binary representation with the Neural GPU model (as in our ADDITION task) is also reported to have a significant negative impact on performance.\nThe work on learning algorithms from sequence data has utilized both related techniques to ours as well as tackled related tasks. The most related techniques have augmented RNNs with various attention and memory architectures. In addition to those we have discussed earlier (Reed & de Freitas, 2016; Joulin & Mikolov, 2015), Grefenstette et al. (2015) proposes an alternative method for augmenting RNNs with a stack. From a task perspective, the most related work has considered variants of the scratchpad model for long-hand addition, similar or our ADDITION domain. This work has focused largely on more standard RNN architectures, starting with Cottrell & Tsung (1993), which showed that the standard RNN architectures at the time (Jordan, 1997; Elman, 1990) could successfully generalize to test samples approximately 5 times as long as those seen during training, if a few longer samples were included in the training set. More recently, Zaremba et al. (2015) showed that an RNN architecture using modern LSTM or GRU controllers can perfectly generalize to inputs 20 times as long as than those seen in the training data when trained in either a supervised or reinforcement learning setting. However this work was focused on trainability rather than data efficiency and so they utilized hundreds of thousands of samples for training.\nNPI (Reed & de Freitas, 2016) and NPL distinguish themselves from the above work with the explicit modeling of functional abstractions. These abstractions enable our model, with only 16 samples, to perfectly generalize to data sequences about 100 times as long as those in the training data. Furthermore, concurrent work (Cai, 2016) has shown that an unmodified NPI model can be trained to perform more complex algorithms such as BubbleSort, QuickSort and topological sorting by learning recursive procedures, and we expect that our method can be directly applied to reduce the amount of needed supervision for these tasks as well.\nReinforcement Learning In the reinforcement learning domain the most related work to ours is the options framework, for building abstractions over elementary actions (Sutton et al., 1999). This framework bears many similarities to both our model and to NPI. Specifically, at each time step the\nagent can choose either a one-step primitive action or a multi-step action policy called an option. As with our procedures, each option defines a policy over actions (either primitive or other options) and terminates according to some function. Much of the work on options has focused on the tabular setting where the set of possible states is small enough to consider them independently. More recent work has developed option discovery algorithms where the agent is encouraged to explore regions that were previously out of reach (Machado & Bowling, 2016) while other work has shown the benefits of manually chosen abstractions in large state spaces (Kulkarni et al., 2016). However, option discovery in large state spaces where non-linear state approximations are required is still an open problem, and our work can be viewed as a method for learning such options from expert trajectories.\nMuch work in reinforcement learning has also considered domains similar to ours. Specifically, grid-world domains similar to NANOCRAFT are quite standard environments in the reinforcement learning literature. One recent example is Sukhbaatar et al. (2015a), which showed that even the strongest technique they considered struggled to successfully perform many of the tasks. Their results highlight the difficultly of learning complex tasks in a pure reinforcement learning setup. In future work we would like to explore the use of our model in setups which mix supervised learning with reinforcement learning."}, {"heading": "6 CONCLUSION", "text": "In this paper, we proposed the Neural Program Lattice, a neural network framework that learns a hierarchical program structure based mostly on elementary operation sequences. On the NANOCRAFT and ADDITION tasks, we show that when training with mostly flat operation sequences, NPL is able to extract the latent programmatic structure in the sequences, and achieve state-of-the-art performance with much less supervision than existing models."}, {"heading": "A APPENDIX", "text": ""}, {"heading": "A.1 DATASET DETAILS", "text": "Table 2 lists the set of programs and elementary operations we used to generate the data for ADDITION and NANOCRAFT. The programs and elementary operations for ADDITION are identical to those in Reed & de Freitas (2016). Note that when training with weak supervision the training data contains only the elementary operations and does not contain the programs or arguments.\nA.2 IMPLEMENTATION DETAILS\nHere we describe the implementation details of the various component neural networks inside our implementation of the NPL. Note that the mappings are all the same for both ADDITION and NANOCRAFT except for fenc which is task dependent.\n\u2022 fenc for ADDITION: We represent the environment observation, (latent) programs and arguments as one-hot vectors of discrete states. We feed the concatenation of one-hot vectors for environment observation and argument through a linear decoder (with bias) to get a unified arg-env representation. We then embed the programs (via fembed) into an embedding space. Finally we feed the concatenation of arg-env vector and program vector through a 2-layer MLP with rectified linear (ReLU) hidden activation and linear decoder.\n\u2022 fenc for NANOCRAFT: We represent the environment observation as a grid of discrete states. Here we first embed each entry into an embedding space, and then feed this embedding through two convolutional layers and two MLP layers with ReLU hidden activation and linear decoder. We represent argument again as one-hot vectors and embed programs into an embedding space. Finally we feed the concatenation of argument vectors, convolutional vectors of environment observation and program vector through a 2-layer MLP with ReLU hidden activation and linear decoder.\n\u2022 flstm: We employ a two-layer LSTM cell for the mapping. The size of the hidden states is set to 128 for both ADDITION and NANOCRAFT.\n\u2022 fprog: This mapping will map the LSTM hidden state to a probability distribution over programs. The hidden state output of flstm is mapped through a linear projection to an 8- dimensional space, and then another linear projection (with bias) with softmax generates ptg .\n\u2022 faction and fop: Each of these encoders will output a probability distribution. We feed the top hidden states by flstm first through a linear projection (with bias) and then a softmax function to pta and p t o respectively."}, {"heading": "A.3 NORMALIZATION", "text": "When the operation sequence is too long, yt,li will become vanishingly small as t grows. To prevent our implementation from underflowing, we follow Graves et al. (2006) by renormalizing yt,li at each timestep and storing the normalized values and normalization constant separately. The new update rule becomes:\nyt+1,li = Jl < LKp t,l+1 a,i (POP)y\u0302 t,l+1 i + J0 < iKp t,l a,i\u22121(OP)p t o(\u03bb i o)y\u0302 t,l i\u22121 + J0 < lKp t,l\u22121 a,i (PUSH)y\u0302 t,l\u22121 i ,\nand we normalize the values and maintain a log-summation of the normalization constants\nY t = Y t\u22121 + log( \u2211 i,l yt,li ), y\u0302 t,l i = y t,l i / \u2211 i,l yt,li .\nThen the original update for yt+1 becomes\nlog(yt+1) = log sum exp(log(yt), log(pt,0I (POP)) + log(y\u0302 t,0 I ) + Y t),\nthe computation of which can be done robustly."}, {"heading": "A.4 UPDATE EQUATIONS WITH BOUNDARY CONDITIONS", "text": "In Section 3.3 we did not include the boundary conditions in our discussion to improve the readability. Our implementation, however, must account for the bounds on l, and i, as shown in Iverson brackets in the full update equations below:\nyt+1,li =Jl < LKp t,l+1 a,i (POP)y t,l+1 i + J0 < iKp t,l a,i\u22121(OP)p t,l o,i\u22121(\u03bb i o)y t,l i\u22121\n+ J0 < lKpt,l\u22121a,i (PUSH)y t,l\u22121 i\n\u03b1t,li (a) =(y t,l i /y t+1,l i )p t,l a,i(a)\nM t+1,ld,i =  Jl < LK\u03b1t,l+1i (POP)M t,l+1 1,i + J0 < iK\u03b1 t,l i\u22121(OP)p t,l o,i\u22121(\u03bb i o)h t,l out,i\u22121+ J0 < lK\u03b1t,l\u22121i (PUSH)0, d = 0 Jl < LK\u03b1t,l+1i (POP)M t,l+1 2,i + J0 < iK\u03b1 t,l i\u22121(OP)p t,l o,i\u22121(\u03bb i o)M t,l 1,i\u22121+ J0 < lK\u03b1t,l\u22121i (PUSH)h t,l\u22121 out,i\u22121, d = 1 Jl < LK\u03b1t,l+1i (POP)M t,l+1 d+1,i+ J0 < iK\u03b1 t,l i\u22121(OP)p t,l o,i\u22121(\u03bb i o)M t,l d,i\u22121+\nJ0 < lK\u03b1t,l\u22121i (PUSH)M t,l\u22121 d\u22121,i, d > 1\nSt,ld,i =  Jl < LK\u03b1t,l+1i (POP)S t,l+1 1,i + J0 < iK\u03b1 t,l i\u22121(OP)p t,l o,i\u22121(\u03bb i o)S t,l 0,i\u22121+ J0 < lK\u03b1t,l\u22121i (PUSH)g t,l\u22121 out,i , d = 0 Jl < LK\u03b1t,l+1i (POP)S t,l+1 d+1,i+ J0 < iK\u03b1 t,l i\u22121(OP)p t,l o,i\u22121(\u03bb i o)S t,l d,i\u22121+\nJ0 < lK\u03b1t,l\u22121i (PUSH)S t,l\u22121 d\u22121,i, d > 0"}, {"heading": "A.5 EXPERIMENTAL SETTINGS", "text": "As mentioned before, NPL can be trained jointly with full program abstractions (referred to as FULL) as well as elementary operation sequences (referred to as OP). When training with FULL samples, the training procedure is similar to that for NPI and we use this setting as one of our baselines. For each dataset on which we test NPL, we include mostly OP samples with only a small number of FULL samples. We pre-train the model solely on FULL samples for a few iterations to get a good initialization. After that, in each step we train with a batch of data purely from FULL or OP based on their proportions in the dataset and generate the parameter update in that step using the corresponding objective. For all tasks, we train the NPL using ADAM (Kingma & Ba, 2015) with base learning rate of 10\u22124 and batch size of 1. We decay the learning rate by a factor of 0.95 every 10,000 iterations. These settings were chosen using a manual search based on performance on the validation data."}, {"heading": "A.6 EXPERIMENTAL DETAILS FOR ADDITION", "text": "A.6.1 S2S-Easy BASELINE\nIn our initial seq2seq baseline tests for ADDITION we represented the data for 90 + 160 = 250 as the sequence: 90X160X250 However, we found that such a model was not able to fit the training data even when trained with 32 samples per number of digits. So we instead compared to the much stronger S2S-Easy baseline presented in Reed & de Freitas (2016). This baseline makes it much easier to learn addition through the following two modifications to the model: 1) reverse input digits, and 2) generate reversed output digits immediately at each time step, such that the data sequence looks like: output: 052 input 1: 090 input 2: 061 This model is quite specific to the ADDITION task (and would not work on the NANOCRAFT task for instance) and results in a very strong baseline. None-the-less, as we showed in Figure 7, our model still significantly outperforms this baseline."}, {"heading": "A.6.2 BOOTSTRAPPING", "text": "On the ADDITION task we found that both our model and the original NPI model were somewhat sensitive to the choice of initial seed. To test this sensitivity we ran our experiments for this task using a bootstrapping process (Efron & Tibshirani, 1994). We ran all models using 100 different seeds for each model. We then sampled 25 seed subsets, with replacement. For each subset, we choose the best seed using a validation set which was one-quarter the size of the original dataset, but consisted only of 10-digit samples. We performed this resampling procedure 100 times, and in Figure 7 we report the mean and standard deviation across the resampled seed sets."}], "references": [{"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Learning simple arithmetic procedures", "author": ["Garrison W Cottrell", "Fu-Sheng Tsung"], "venue": "Connection Science,", "citeRegEx": "Cottrell and Tsung.,? \\Q1993\\E", "shortCiteRegEx": "Cottrell and Tsung.", "year": 1993}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani"], "venue": "CRC press,", "citeRegEx": "Efron and Tibshirani.,? \\Q1994\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Neural functional programming", "author": ["John K Feser", "Marc Brockschmidt", "Alexander L Gaunt", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.01988,", "citeRegEx": "Feser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feser et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": "memory. Nature,", "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Jordan.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Joulin and Mikolov.,? \\Q2015\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Learning purposeful behaviour in the absence of rewards", "author": ["Marlos C Machado", "Michael Bowling"], "venue": "arXiv preprint arXiv:1605.07700,", "citeRegEx": "Machado and Bowling.,? \\Q2016\\E", "shortCiteRegEx": "Machado and Bowling.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Mazebase: A sandbox for learning from games", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Gabriel Synnaeve", "Soumith Chintala", "Rob Fergus"], "venue": "arXiv preprint arXiv:1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Reinforcement learning neural turing machines-revised", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "First, we draw inspiration from Connectionist Temporal Classification, CTC (Graves et al., 2006), observing that it provides a method for learning with latent alignments.", "startOffset": 75, "endOffset": 96}, {"referenceID": 6, "context": "To overcome this challenge, we draw inspiration from CTC (Graves et al., 2006).", "startOffset": 57, "endOffset": 78}, {"referenceID": 7, "context": "Although the model we have defined so far is fully differentiable, the difficultly in training smoothed models of this form has been highlighted in the original Neural Turing Machine work (Graves et al., 2014) as well as much of the follow on work (Gaunt et al.", "startOffset": 188, "endOffset": 209}, {"referenceID": 5, "context": ", 2014) as well as much of the follow on work (Gaunt et al., 2016; Kurach et al., 2016; Graves et al., 2016; Neelakantan et al., 2016; Joulin & Mikolov, 2015).", "startOffset": 46, "endOffset": 158}, {"referenceID": 8, "context": ", 2014) as well as much of the follow on work (Gaunt et al., 2016; Kurach et al., 2016; Graves et al., 2016; Neelakantan et al., 2016; Joulin & Mikolov, 2015).", "startOffset": 46, "endOffset": 158}, {"referenceID": 18, "context": ", 2014) as well as much of the follow on work (Gaunt et al., 2016; Kurach et al., 2016; Graves et al., 2016; Neelakantan et al., 2016; Joulin & Mikolov, 2015).", "startOffset": 46, "endOffset": 158}, {"referenceID": 6, "context": "We have already discussed the most relevant past work upon which we directly build: CTC (Graves et al., 2006), StackRNNs (Joulin & Mikolov, 2015) and NPI (Reed & de Freitas, 2016).", "startOffset": 88, "endOffset": 109}, {"referenceID": 7, "context": "The work on learning neural programs from input-output data was sparked by the surprising effectiveness of the Neural Turing Machine (NTM) (Graves et al., 2014).", "startOffset": 139, "endOffset": 160}, {"referenceID": 8, "context": "Similar to NTMs, many of the proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015).", "startOffset": 84, "endOffset": 239}, {"referenceID": 18, "context": "Similar to NTMs, many of the proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015).", "startOffset": 84, "endOffset": 239}, {"referenceID": 5, "context": "Similar to NTMs, many of the proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015).", "startOffset": 84, "endOffset": 239}, {"referenceID": 4, "context": "Similar to NTMs, many of the proposed architectures have used differentiable memory (Kurach et al., 2016; Graves et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015b; Neelakantan et al., 2016; Gaunt et al., 2016; Feser et al., 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015).", "startOffset": 84, "endOffset": 239}, {"referenceID": 22, "context": ", 2016), while others have used REINFORCE (Williams, 1992) to train neural networks that use sampling-based components to model memory access (Andrychowicz & Kurach, 2016; Zaremba & Sutskever, 2015).", "startOffset": 42, "endOffset": 58}, {"referenceID": 11, "context": "This work has focused largely on more standard RNN architectures, starting with Cottrell & Tsung (1993), which showed that the standard RNN architectures at the time (Jordan, 1997; Elman, 1990) could successfully generalize to test samples approximately 5 times as long as those seen during training, if a few longer samples were included in the training set.", "startOffset": 166, "endOffset": 193}, {"referenceID": 3, "context": "This work has focused largely on more standard RNN architectures, starting with Cottrell & Tsung (1993), which showed that the standard RNN architectures at the time (Jordan, 1997; Elman, 1990) could successfully generalize to test samples approximately 5 times as long as those seen during training, if a few longer samples were included in the training set.", "startOffset": 166, "endOffset": 193}, {"referenceID": 21, "context": "Reinforcement Learning In the reinforcement learning domain the most related work to ours is the options framework, for building abstractions over elementary actions (Sutton et al., 1999).", "startOffset": 166, "endOffset": 187}, {"referenceID": 16, "context": "More recent work has developed option discovery algorithms where the agent is encouraged to explore regions that were previously out of reach (Machado & Bowling, 2016) while other work has shown the benefits of manually chosen abstractions in large state spaces (Kulkarni et al., 2016).", "startOffset": 262, "endOffset": 285}], "year": 2017, "abstractText": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_45"}