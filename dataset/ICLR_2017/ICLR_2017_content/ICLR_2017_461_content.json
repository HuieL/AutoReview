{"name": "ICLR_2017_461.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["BLOOM FILTERS", "Joan Serr\u00e0", "Alexandros Karatzoglou"], "emails": ["firstname.lastname@telefonica.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of such inputs and outputs. This is the case, for instance, with recommender systems, where high-dimensional sparse vectors, typically in the order from tens of thousands to hundreds of millions, constitute both the input and the output of the model (e.g., Wu et al., 2016; Hidasi et al., 2016; Cheng et al., 2016; Strub et al., 2016). This results in large models that present a number of difficulties, both at training and prediction stages. Apart from training and prediction times, an obvious bottleneck of such models is space: their size (and even performance) is hampered by the physical memory of graphical processing units (GPUs), and they are difficult to deploy on mobile devices with limited hardware (cf. Han et al., 2016).\nOne option to reduce the size of sparse inputs and outputs is to embed them into a lower-dimensional space. Embedding sparse high-dimensional inputs is commonplace (e.g., Bengio et al., 2000; Turian et al., 2010; Mikolov et al., 2013). However, embedding sparse high-dimensional outputs, or even inputs and outputs at the same time, is much less common (cf. Weston et al., 2002; Bengio et al., 2010; Akata et al., 2015). Importantly, typical embeddings still require the storage and processing of large matrices with the same dimensionality as the input/output (like the original neural network model would do). Thus, the gains in terms of space are limited. As mentioned, the size of such models is dominated by the input/output dimensionality, with input and output layers representing about 99.94% of the total amount of weights of the model1.\nIn general, an ideal embedding procedure for sparse high-dimensional inputs/outputs should produce compact embeddings, of much lower dimensionality than the original input/output. In addition, it\n1An example can be found in the neural network model of Hidasi et al. (2016), which uses a gated recurrent unit to perform session-based recommendations with input/output layers of dimensionality 330,000 and internal layers of dimensionality 100.\nshould consume little space, both in terms of storage and memory space. Smaller sizes imply less parameters, thus training the model on embedded vectors would also be faster than with the original instances. The embedding of the output should also lead to a formulation for which the appropriate loss should be clear. Embeddings should not compromise the accuracy of the model nor the required number of training epochs to obtain that accuracy. In addition, no changes to the original core architecture of the model should be required to achieve good performance (obviously, input/output dimensions must change). The embedding should also be fast; if not to be done directly \u2018on-the-fly\u2019, at least fast enough so that speed improvements made during training are not lost in the embedding operation. Last, but not least, output embeddings should be easily reversible, so that the output of the model could be mapped to the original items at prediction time.\nIn this paper, we propose an unsupervised embedding technique that fulfills all the previous requirements. It can be applied to both input and output layers of neural network models that deal with binary (one-hot encoded) inputs and/or outputs. In addition, it produces lower-dimensionality binary embeddings that can be easily mapped to the original instances. Provided that the embedding dimension is not too low, the accuracy is not compromised. Furthermore, in some cases, we show that training with embedded vectors can even increase prediction accuracy. The embedding requires no changes to the core network structure nor to the model configuration, and works with a softmax output, the most common output activation for binary-coded instances. As it is unsupervised, the embedding does not require any preliminary training. Moreover, it is a constant-time operation that can be either performed on-the-fly, requiring no disk or memory space, or can be cached in memory, occupying orders of magnitude less space than a typical embedding matrix. Lower dimensionality of input/output vectors result in faster training, and the mapping from the embedded space to the original one does not add an overwhelming amount of time to the prediction stage. The proposed embedding is based on the idea of Bloom filters (Bloom, 1970), and therefore it inherits part of the theory developed around that idea (Blustein & El-Maazawi, 2002; Dillinger & Manolios, 2004; Mitzenmacher & Upfal, 2005; Bonomi et al., 2006)."}, {"heading": "2 RELATED WORK", "text": "A common approach to embed high-dimensional inputs is the hashing trick (Langford et al., 2007; Shi et al., 2009; Weinberger et al., 2009). However, the hashing trick approach does not deal with outputs, as it offers no explicit way to map back from the (dense) embedding space to the original space. A more elementary version of the hashing trick (Ganchev & Dredze, 2008) can be used at the outputs by considering it as a special case of the Bloom-based methodology proposed here. A framework providing both encoding and decoding strategies is the error-correcting output codes (ECOC) framework (Dietterich & Bakiri, 1995). Originally designed for single-class outputs, it can be also applied to class sets (Armano et al., 2012). The compressed sensing approach of Hsu et al. (2009) builds on top of ECOC to reduce multi-label regression to binary regression problems. Similarly, Cisse\u0301 et al. (2013) use Bloom filters to reduce multi-label classification to binary classification problems and improve the robustness of individual binary classifiers\u2019 errors. Another example of a framework offering recovery capabilities is kernel dependency estimation (Weston et al., 2002).\nData-dependent embeddings that require some form of learning also exist. A typical approach is to rely on variants of latent semantic analysis or singular value decomposition (SVD), exploiting similarities or correlations that may be present in the data. Again, the issue of mapping from the embedding space to the original space is left unresolved. Nonetheless, recently, Chollet (2016) has successfully applied a K-nearest neighbors (KNN) algorithm to perform such a mapping and to derive a ranking of the elements in the original space. An SVD decomposition of the pairwise mutual information matrix (PMI) is used to perform the embedding, and cosine similarity is used as loss function and to retrieve neighbors. Using the KNN trick offers the possibility to exploit different types of factorization of similarity-based matrices. Canonical correlation analysis is an example that considers both inputs and outputs at the same time (Hotelling, 1936). Other examples considering output embeddings are nuclear norm regularized learning (Amit et al., 2007), label embedding trees (Bengio et al., 2010), or the WSABIE algorithm (Weston et al., 2010). In the presence of side information, like text descriptions, element or class taxonomies, or manually-collected data, a range of approaches are applicable. Akata et al. (2015) provide a comprehensive list. In our study, we assume no side information is available and focus on input/output-based embeddings.\nFrom a more general perspective, reducing the space of (or compressing) neural network models is an active research topic, driven by the need to deploy such models in systems with limited hardware resources. A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers (Courbariaux et al., 2015; Han et al., 2016; Kim et al., 2016). A less frequently used approach is to reduce the model size before training (Chen et al., 2015). These methods typically do not focus on input layers and, to the best of our knowledge, none of them deals with high-dimensional outputs. It is also worth noting that a number of techniques have been proposed to efficiently deal with high-dimensional outputs, specially in the natural language processing domain. The hierarchical softmax approach (Morin & Bengio, 2005) or the more recent adaptive softmax (Grave et al., 2016) are two examples of those. Yet, as mentioned, the focus of these works is on speed, not on space. The work of Vincent et al. (2015) focuses on both aspects of very large sparse outputs but, to the best of our knowledge, cannot be applied to traditional softmax outputs."}, {"heading": "3 BLOOM EMBEDDINGS", "text": ""}, {"heading": "3.1 BLOOM FILTERS", "text": "Bloom filters (Bloom, 1970) are a compact probabilistic data structure that is used to represent sets of elements, and to efficiently check whether an element is a member of a set (Mitzenmacher & Upfal, 2005). Since the instances we deal with represent sets of one-hot encoded elements, Bloom filters are an interesting option to embed those in a compact space with good recovery (or checking) guarantees.\nIn essence, Bloom filters project every element of a set to k different positions of a binary array u of size m. Projections are done using a set of k independent hash functions H = {Hi}ki=1, each of which with a range from 1 to m, ideally distributing the projected elements uniformly at random (Mitzenmacher & Upfal, 2005). Proper independent hash functions can be derived using enhanced double hashing or triple hashing (Dillinger & Manolios, 2004). The number of hash functions k is usually a constant, k m, proportional to the expected number of elements to be projected.\nTo check if an element is in u, one feeds it to the k hash functionsH to get k array positions. If any of the bits at these positions is 0, then the element is definitely not in the set. Thus, element checks return no false negatives, meaning that the structure gives an answer with 100% recall (Mitzenmacher & Upfal, 2005). However, if all k bits at the projected positions are 1, then either the element is in the set, or the bits have by chance been set to 1 during the insertion of other set elements. This implies that false positives are possible, due to collisions between projections of different elements (Blustein & El-Maazawi, 2002). The values of m and k can be adjusted to control the probability of such collisions. However, in practice, m is usually constrained by space requirements, and k \u2264 10 is employed, independent of the number of elements to be projected, and giving less than 1% false positive probability (Bonomi et al., 2006)."}, {"heading": "3.2 EMBEDDING AND RECOVERY", "text": "In the following, we describe the use of Bloom filter techniques in embedding binary highdimensional instances, and the recovery or mapping to such instances from these embeddings. We denote our approach as Bloom embedding (BE). The idea we pursue is to embed both inputs and outputs and to perform training in the embedding space. To do so, only a probability-based output activation is required, together with a loss function that is appropriate for such activations.\nLet x be an input or output instance with dimensionality d, such that x = [x1, . . . xd], xi \u2208 {0, 1}. Instances x are assumed to be sparse, that is, \u2211d i=1 xi d. Because of that, we can more conveniently (and compactly) represent x as set z = {zi}ci=1, zi \u2208 N\u2264d, where c is the number of non-zero elements and zi is the position of such elements in x. For every set z, we generate an embedded instance u of dimensionality m < d, such that u = [u1, . . . um], ui \u2208 {0, 1}. To do so, we first set all m components of u to 0. Then, iteratively, for every element zi, i = 1, . . . c, and every projection Hj , j = 1, . . . k, we assign\nuHj(zi) = 1. (1)\nNotice that, since Hj has a range between 1 and m, k \u2265 1, and m < d, a number of zi elements may map to the same index of u. Bloom filters mitigate this by properly choosing k independent hash functions H (see above). Notice furthermore that the process has no space requirements, as H is computed on-the-fly. Finally, notice that the embedding of a set z is constant time: the process is O(ck), with c bounded by the maximum number of non-sparse elements in x, c d, and k being a constant that is set beforehand, k m < d. In practice, this constant time is dominated by the time spent on H to generate a hash. If we want to be faster than that, and at the same time ensure an optimal (uniform) distribution of the outputs of H , we can decide to compromise part of the available memory to pre-compute a hash matrix storing the projections or hash indices for allthe potential elements in z. We can do it by generating vectors h = [h1, . . . hk] for each zi, where hj is a uniformly randomly chosen integer between 1 and m (without replacement). This way, by pregenerating all projections for all d elements, we end up with a d\u00d7 k matrix H of integers between 1 and m, which we can easily store in random-access memory (RAM), not in the GPU memory.\nWe now explain how to recover a probability-based ranking of the d elements of x at the output of the model. Assuming a softmax activation is used, we have a probability vector v = [v1, . . . vm] that, at training time, is compared to the binary embedding u of some ground truth set z (or vector x). We can think of vi as the probability of being the projection of some element zl, that is, vi \u223c P (ui = 1) \u223c P (Hj(zl) = i) (see Eq. 1). To unravel the embedding v and map to the d original elements of x, we can understand v as a k-way factorization of every element xi. Following the idea of Bloom filters, if an element maps to ui and vi = 0, then the element is definitely not in the output of the model. Otherwise, if an element maps to ui and vi is relatively large, we want the likelihood of that element to reflect that. Specifically, given an element position zi from x, we can compute the likelihood of zi as\nL(zi) = k\u220f j=1 vHj(zi), (2)\nand assign outputs xi = L(zi). Alternatively, if a more numerically-stable output is desired, we can compute the negative log-likelihood\nL(zi) = \u2212 k\u2211\nj=1\nlog ( vHj(zi) ) . (3)\nBoth operations, when iterated for i = 1, . . . d, define a ranking over the elements in x, which is the most common way to define (and evaluate) sparse high-dimensional outputs. One could potentially also recover a probability distribution by re-normalization, but the problems we consider are information retrieval-type of problems (Manning et al., 2008), which are typically seen as ranking problems, such as ranking recommendations based on user preferences (Weimer et al., 2008).\nNote that BE, by construction, already offers a number of the aforementioned desired qualities for sparse high-dimensional embeddings (Sec. 1). Specifically, BE is designed for both inputs and outputs, offering a rank-based mapping between the original instances and the embedded vectors. BE yields a more compact representation of the original instance and requires no disk or memory space (at most some marginal RAM space, not GPU memory). In addition, BE can be performed on-the-fly, without training, and in constant time. In the following, we demonstrate the remaining desirable qualities using a comprehensive experimental setup: we show that the accuracy of the model is not compromised given a reasonable embedding dimension (sometimes it even improves), that no changes in the model architecture nor configuration are required, that training times are faster thanks to the reduction of the number of parameters of the model, that evaluation times do not carry much overhead, and that performance is generally better than a number of alternative approaches."}, {"heading": "4 EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1 GENERAL CONSIDERATIONS", "text": "We demonstrate that BE works under several settings and that it can be applied to multiple tasks. We consider a number of data sets, network architectures, configurations, and evaluation measures. In total, we define 7 different setups, which we summarize in Sec. 4.2 and detail in Appendix A. We\nalso demonstrate that BE is competitive with respect to the available alternatives. To this end, we consider 4 different state-of-the-art approaches, which we overview in Sec. 4.3.\nData sets are formed by inputs with n instances, corresponding to either individual instances (or onehot encoded user profiles) or to sequences of instances (or profile lists). Outputs, also of n instances, correspond to individual instances or to class labels. Instances have an original dimensionality d, corresponding to the cardinality of all possible profile items. Given the nature of the considered problems, instances are very sparse, with all but c elements being different from 0, c d, typically with c/d in the order of 10\u22125 (Table 1).\nFor each data set, and based on the literature, we select an appropriate baseline neural network architecture. We experiment with both feed-forward (autoencoder-like) and recurrent networks, carefully selecting their parameters and configuration to match (or even improve) the state-of-theart results. For the sake of comparison, we also choose appropriate and well-known evaluation measures. Depending on the data set, we work with mean average precision, reciprocal ranks, or accuracy (Manning et al., 2008).\nEach combination of data set, network architecture, configuration, and evaluation measure defines a task. For every task, we compute a baseline score S0, corresponding to running the plain neural network model without any embedding. We then report the performance of the i-th combination of training with a particular embedding on a particular task with respect to the baseline score using Si/S0. This way, we can compare the performance across different tasks using different evaluation measures, reporting relative improvement/loss with respect to the baseline. Similarly, to compare across different dimensionalities, we report the ratio of embedding dimensionality with respect to the original dimensionality, m/d, and to compare across different training and evaluation times, we report time ratios with respect to the baseline, Ti/T0."}, {"heading": "4.2 TASKS", "text": "We now give a brief summary of the 7 considered tasks (Tables 1 and 2). For a more detailed explanation related to data, network architecture, configuration, or evaluation methodology, we refer\nthe reader to Appendix A. Further references can be also found there. All data sets are publiclyavailable, and for all tasks we use categorical cross-entropy as loss function.\n1. Movielens (ML): movie recommendation with the Movielens data set (Harper & Konstan, 2015). We employ a 3-layer feed-forward neural network model and optimize its parameters with Adam. We evaluate the accuracy of the model with mean average precision.\n2. Penn treebank (PTB): next word prediction with the Penn treebank data set (Mikolov, 2012). We employ a long short-term memory (LSTM) network and optimize its parameters with stochastic gradient descent (SGD). We evaluate the accuracy of the model with the reciprocal rank of the correct prediction.\n3. CADE web directory (CADE): text categorization with the CADE web directory data set (Cardoso-Cachopo, 2007). We employ a 4-layer feed-forward neural network model and optimize its parameters with RMSprop. This is the only considered task where output embeddings are not required (classification into 12 text categories). We use accuracy as evaluation measure.\n4. Million song data set (MSD): song recommendation with the Million song data set (BertinMahieux et al., 2011). We employ a 3-layer feed-forward neural network model and optimize its parameters with Adam. We evaluate the accuracy of the model with mean average precision.\n5. Amazon book reviews (AMZ): book recommendation with the Amazon book reviews data set (McAuley et al., 2015). We employ a 4-layer feed-forward neural network and optimize its parameters with Adam. We evaluate the accuracy of the model with mean average precision.\n6. Book crossing (BC): book recommendation with the book crossing data set (Ziegler et al., 2005). We employ a 4-layer feed-forward neural network and optimize its parameters with Adam. We evaluate the accuracy of the model with mean average precision.\n7. YooChoose (YC): session-based recommendation with the YooChoose RecSys15 challenge data set2. We employ a gated recurrent unit (GRU) model and optimize its parameters with Adagrad. We evaluate the accuracy of the model with the reciprocal rank."}, {"heading": "4.3 ALTERNATIVE APPROACHES", "text": "To compare the performance of BE with the state-of-the-art, we consider 4 different embedding alternatives. We base our evaluation on performance, measured at a given input/output compression ratio. It is important to note that, in general, besides performance, alternative approaches do not present some of the other desired qualities (Sec. 1) that BE offers, such as on-the-fly operation, constant-time, no supervision, or no network/configuration changes.\n1. Hashing trick (HT). We first consider the popular hashing trick for classifier inputs (Langford et al., 2007; Weinberger et al., 2009). In general, these methodologies only focus on inputs and are not designed to deal with any type of output. Nonetheless, in the case of binary outputs, variants like the one used by Ganchev & Dredze (2008) can be adapted to map to the original items using Eqs. 2 or 3. In fact, considering this adaptation for recovery, the approach can be seen as a special case of BE with k = 1.\n2. Error-correcting output codes (ECOC). Originally designed for single-class targets (Dietterich & Bakiri, 1995), ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies (Armano et al., 2012). Yet, in the case of training neural networks, it is not clear which loss function should be used. The obvious choice would be to use the Hamming distance. However, in pre-analysis, a Hamming loss turned out to be significantly inferior than cross-entropy. Therefore, we use the latter in our experiments. We construct the ECOC matrix with the randomized hill-climbing method of Dietterich & Bakiri (1995).\n3. Pairwise mutual information (PMI). Recently, Chollet (2016) has proposed a PMI approach for embedding sets of image labels into a dense space of real-valued vectors. The approach\n2http://recsys.yoochoose.net\nis based on the SVD of a PMI matrix computed from counting pairwise co-occurrences. It uses cosine similarity as the loss function and, at prediction time, it performs KNN (again using cosine similarity) with the projection of individual labels to obtain a ranking.\n4. Canonical correlation analysis (CCA). CCA is a common way to learn a joint dense, realvalued embedding for both inputs and outputs at the same time (Hotelling, 1936). CCA can be computed using SVD on a correlation matrix (Hsu et al., 2012) and, similarly to PMI, we can use the KNN trick to rank elements or labels at prediction time. Correlation is now the metric of choice, both for the loss function and for determining the neighbors."}, {"heading": "5 RESULTS", "text": "We start by reporting on the performance of BE. First of all, we focus on performance as a function of the embedding dimension. As mentioned, to facilitate comparisons, we report in relative terms, using score ratios Si/S0 and dimensionality ratios m/d. When plotting the former as a function of the latter, we see several things that are worth noting (Fig. 1). Firstly, we observe that, for most of the tasks, score ratios approach 1 asm approaches d. This indicates that the introduction of BE does not degrade the original score of the Baseline when the embedding dimension m is comparable to the original dimension d. Secondly, we observe that the lower the dimensionality ratio, the lower the score ratio. This is to be expected, as one cannot embed sets of elements with their intrinsic dimensionality to an infinitesimally small m. Importantly, the reduction of Si/S0 should not be linear with m/d, but should maximize Si for low m (thus getting curves close to the top left corner of Fig. 1). We see that BE fulfills this requirement. In general, we can reduce the size of inputs and outputs 5 times (m/d = 0.2) and still maintain more than 92% of the value of the original score. The ML task is the only exception, which we think is due to the abnormally high density of the data (Table 1), inhibiting the embedding to low dimensions3. CADE is the task for which BE achieves the highest Si for low m. Presumably, the CADE task is the easiest one we consider, as only input embeddings are required.\nAn additional observation is worth noting (Fig. 1). Interestingly, we find that BE can improve the scores over the Baseline for a number of tasks. That is the case for 3 out of the 7 considered tasks: MSD with m/d \u2265 0.3, AMZ with m/d \u2265 0.2, and BC with 0.3 \u2264 m/d \u2264 0.6. The fact that an embedding performs better than the original Baseline has been also observed in some other methods\n3Note that the ML data is essentially collected through a survey-type method (Harper & Konstan, 2015).\nfor specific data sets (Weston et al., 2002; Langford et al., 2007; Chollet, 2016). For instance, Chollet (2016) has reported increases up to 7% using the PMI approach on the so-called JFT data set. Here, depending on the task and the embedding dimension, relative increases go from 1 to 12%. Given that the data sets where we observe these increases are some of the less dense ones (Table 1), we hypothesize that, in the case of BE, such increases come from having k times more active elements in the ground truth output (recall that one output element is projected k times using k independent hash functions, Sec. 3.2). With k more times elements set to 1 in the output, a better estimation of the gradient may be computed (larger errors that propagate back to the rest of the network).\nWe now focus on performance as a function of the number of projections k, reporting score ratios Si/S0 as above (Fig. 2). From repeating the plots for different values of m/d, we observe that Si/S0 is always low for k = 1 (Fig. 2, left), except when m approaches d, where we have an almost flat behavior (Fig. 2, right). In general, Si/S0 jumps up for k \u2265 2 and remains stable until k \u2248 10, where the decrease of Si/S0 becomes more apparent (Fig. 2, left). The best operating range typically corresponds to 2 \u2264 k \u2264 4. The ML task is again an exception, with a best operating range around 7 \u2264 k \u2264 10. Besides performance scores, it is interesting to assess whether the reduction of input and output dimensions has an effect to training and evaluation times. To this end, we plot the time ratios Ti/T0 as a function of the dimensionality ratio m/d (Fig. 3). Regarding training times, we basically observe a linear decrease with m/d (Fig. 3, left). ML is an exception to the trend, and CADE and AMZ experiment almost no decrease for very low dimensionality ratios m/d < 0.2. In general, we confirm faster training times thanks to the reduction of the number of parameters of the model, dominated by input/output matrices (output dimension also affecting the time to compute the loss function). We obtain a 2 times speedup for a 2 times input/output compression and, roughly, a little bit over 3 times speedup for a 5 times input/output compression. Regarding evaluation times, we also observe a linear trend (Fig. 3, right). However, this time, Ti/T0 is not as low, with values slightly above 1 but always below 1.5 (with the exception of CADE for m/d > 0.6). Overall, this indicates that, compared to the Baseline evaluation time, the mapping used by BE when reconstructing the output does not introduce an overwhelming amount of extra computation time. With the exception of ML, extra computation time is below 20% for m/d < 0.5.\nFinally, we compare the performance of BE to the one of the considered alternative methods. We do so by establishing a dimensionality ratio m/d and computing the corresponding score ratio Si/S0 for a given task (Table 3). We see that BE is better than the alternative methods in 5 out of the 7 tasks (10 out of the 14 considered test points). PMI is better in one of the tasks (CADE) and CCA is better also in one of the tasks (AMZ). It is relevant to note that, when BE wins, it always does so by a relatively large margin (see, for instance, the ML or YC tasks). Otherwise, when an alternative approach wins, generally it does so by a smaller margin (see, for instance, the AMZ task). These results become more relevant if we realize that PMI and CCA are both SVD-based\napproaches, introducing a separate degree of supervised learning to the task by exploiting pairwise element co-occurrences and correlations, respectively (Sec. 4.3). In contrast, BE does not require any learning. We formulate a co-occurrence-based version of BE in Appendix B, which achieves moderate performance increments over BE and more closely approaches the performance of PMI and CCA on the two tasks where BE was not already performing best. To conclude, a further interesting thing to note is that we confirm the small variation in the score ratios obtained for 2 \u2264 k \u2264 10 (Fig. 2). Here, score ratios for 3 \u2264 k \u2264 5 are often comparable in a statistical significance sense (Table 3)."}, {"heading": "6 CONCLUSION", "text": "We have proposed the use of Bloom embeddings to represent sparse high-dimensional binary-coded inputs and outputs. We have shown that a compact representation can be obtained without compromising the performance of the original neural network model or, in some cases, even increasing it by a substantial factor. Due to the compact representation, the loss function and the input and output layers deal with less parameters, which results in faster training times. The approach compares favorably with respect to the considered alternatives, and offers a number of further advantages such as on-the-fly operation or zero space requirements, all this without introducing changes to the core network architecture, task configuration, or loss function.\nIn the future, besides continuing to exploit co-occurrences (Appendix B), one could extend the proposed approach by considering further extensions of Bloom filters such as counting Bloom filters (Bonomi et al., 2006). In theory, those extensions could provide a more compact representation by breaking the binary nature of the embedding. However, they could require the modification of the loss function or the mapping process (Eqs. 2 and 3). A faster mapping process using the sorted probabilities of v could also be studied."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the curators of the data sets used in this study for making them publicly-available. We also thank Santi Pascual for his comments on a previous version of the paper."}, {"heading": "APPENDIX A TASKS DETAIL", "text": ""}, {"heading": "A.1 MOVIELENS (ML)", "text": "We first consider the task of movie recommendation with the Movielens 20M data set4 (Harper & Konstan, 2015). This data set comprises 20 million ratings applied to roughly 27,000 movies by over 138,000 users. To recommend movies that users would like, ratings, originally between 0.5 and 5 stars, were discretized with a threshold of 3.5. Then, movies with less than 5 ratings were removed, resulting in a total of 15,405 movies. User profiles were next built using a chronologically-ordered list of liked movies. We removed users with less than 2 movies and limited profiles to a maximum of 2,000 movies (less than 0.1% fulfilled this condition). Inputs and outputs were built by splitting user profiles uniformly at random, ensuring a minimum of one movie in both input and output. Finally, 10,000 random users were taken out for validation and another 10,000 for testing. The ML data set is the most dense data set we consider, with a median of 18 movies in input/output profiles (Table 1).\nTo perform recommendations with the ML data set, we build on top of Wu et al. (2016) and consider a 3-layer feed-forward neural network with a softmax output and 150 rectified linear units (Glorot et al., 2011) in the hidden layers. We initialize the weights with uniform random numbers, weighted by the input and output dimensionality of the layer (Glorot & Bengio, 2010). We optimize the weights of the network using cross-entropy and Adam (Kingma & Ba, 2015), with a learning rate of 0.001 and parameters \u03b21 = 0.9 and \u03b22 = 0.999. Training is performed for 15 epochs and with batches of 32 instances. If no improvement is seen on the validation set after one epoch, the learning rate is divided by 5. As done with all the other tasks, we make sure that the network architecture and the number of epochs is sufficient to achieve a state-of-the-art result. As the output probabilities define a ranking of movies that the user may like, the accuracy of the result is measured with mean average precision (Manning et al., 2008). The obtained baseline score S0 = 0.160 can be considered a state-of-the-art result (Wu et al., 2016). Performing movie rankings at random yields a score SR = 0.003."}, {"heading": "A.2 PENN TREEBANK (PTB)", "text": "Another task we consider is next-word prediction with the Penn treebank data set (Marcus et al., 1993). We employ the data made available by Mikolov (2012), which contains close to 1 million words and defines validation and test splits of roughly 74,000 and 82,000 words, respectively. The vocabulary is limited to 10,000 words, with all other words mapped to an \u2018unknown\u2019 token (Table 1). We consider the end of the sentence as an additional token and form input sequences of length 10.\nInspired by Graves (2013), we perform next word prediction with an LSTM network (Hochreiter & Schmidhuber, 1997). We set the inner dimensionality to 250 and train the network with SGD. We use a learning rate of 0.25, a momentum of 0.99, and clip gradients to have a maximum norm of 1 (Graves, 2013). We use batches of 128 instances and train the model for 10 epochs. As for the rest, we proceed as with the ML task. We evaluate the result using the reciprocal rank of the correct prediction (Manning et al., 2008). We achieve a performance of S0 = 0.342, which indicates that, on average, the correct word is ranked on the third position. Predicting words at random yields a score SR = 0.001.\n4http://grouplens.org/datasets/movielens/20m/"}, {"heading": "A.3 CADE WEB DIRECTORY (CADE)", "text": "We perform single-label text categorization using web pages classified by human experts from the CADE web directory of Brazilian web pages5 (Cardoso-Cachopo, 2007). The data set contains around 40,000 documents assigned to one of 12 categories such as services, education, health, or culture. We use the train and test splits provided by Cardoso-Cachopo (2007), further splitting the train set randomly to obtain a validation set from it. Validation and test splits comprise 5,000 and 13,661 documents, respectively. The size of the vocabulary is close to 200,000 words with a median number of 17 words per document (Table 1).\nTo perform classification we use a 4-layer feed-forward neural network with a softmax output. The number of units is, from input to output, 400, 200, 100, and 12, and we use rectified linear units as activations for the hidden layers. We train the network for 10 epochs, using batches of 32 instances and RMSprop (Tieleman & Hinton, 2012) with a learning rate of 0.0002 and exponential decay of 0.9. As for the rest, we proceed as with the ML task. We obtain a baseline accuracy of S0 = 58.0%, slightly superior than the best baseline reported by Cardoso-Cachopo (2007), and a random accuracy of SR = 8.5% (Table 2). Notice that this is the only data set that does not have a sparse instance or user profile as output."}, {"heading": "A.4 MILLION SONG DATA SET (MSD)", "text": "The next task we consider is song recommendation with the million song data set (Bertin-Mahieux et al., 2011). We take the Echo Nest taste profile subset6, which includes over 48 million play counts of around 384,000 songs for roughly 1 million users. We assume that a user likes a song when this has listened to it a minimum of 3 times. We then remove the songs that appear less than 20 times and build user profiles with a minimum of 5 songs. We split the data set as with the ML task, keeping 50,000 user profiles for validation and another 50,000 for testing. The MSD data set has a median of 5 songs in input/output profiles (Table 1).\nTo recommend future listens to the user we use a 3-layer feed-forward neural network with a softmax output and 300 rectified linear units in the hidden layers. We fit the model for 10 epochs with batches of 64 instances. As for the rest, we proceed as with the ML task. We obtain a baseline mean average precision of S0 = 0.066 and a random score of SR below 0.001."}, {"heading": "A.5 AMAZON BOOK REVIEWS (AMZ)", "text": "We also consider book recommendations with the Amazon book reviews data set7 (McAuley et al., 2015). The data set originally contains 22 million ratings of over 2 million books by approximately 3 million users. We proceed as with the ML data set, but this time setting the minimum number of ratings per book to 100 and splitting the data with 50,000 instances for validation and another 50,000 instances for testing.\nWe here use a 4-layer feed-forward neural network with a softmax output and 300 rectified linear units in the hidden layers. We fit the model for 10 epochs with batches of 64 instances and, as for the rest, we proceed as with the ML task. We obtain a baseline mean average precision of S0 = 0.049 and a random score of SR below 0.001."}, {"heading": "A.6 BOOK CROSSING (BC)", "text": "Continuing with book recommendations, we consider the book crossing data set8 (Ziegler et al., 2005). It contains 278,000 users providing over 1 million ratings about a little more than 271,000 books. We remove books with less than 2 ratings, discretize those by a threshold of 4, and proceed as with the ML data set, but keeping 2,500 users for validation and another 2,500 for testing. The BC data set is known to be a very sparse data set, specially after removing users with less than 2 book reviews (Table 1).\n5http://ana.cachopo.org/datasets-for-single-label-text-categorization 6http://labrosa.ee.columbia.edu/millionsong/tasteprofile 7http://jmcauley.ucsd.edu/data/amazon/ 8http://www2.informatik.uni-freiburg.de/\u02dccziegler/BX/\nTo perform recommendations we use the same architecture and configuration as with the MSD task, but this time we use 250 units in the hidden layers. We obtain a baseline mean average precision of S0 = 0.010 and a random score of SR below 0.001."}, {"heading": "A.7 YOOCHOOSE (YC)", "text": "We finally study session-based recommendations using the YooChoose RecSys15 challenge9 data. Here, the task is to predict the next click given a sequence of click events for a given session in an e-commerce site (Hidasi et al., 2016). We work with the training set of the challenge and keep only the click events. We take the first 2 million sessions of the data set which have a minimum of 2 clicks, and keep apart 50,000 for validation and another 50,000 for testing. We form sequences of, at most, 13 clicks to the 35,000 possible links (Table 1). Note that this is a sequential data set with one-hot encoded instances of only one event each.\nTo predict the next click we proceed as in Hidasi et al. (2016) and consider a GRU model (Cho et al., 2014). We set the inner dimensionality to 100 and train the network with Adagrad (Duchi et al., 2011), using a learning rate of 0.01. We use batches of 64 instances and train the model for 10 epochs. As for the rest, we proceed as with the ML task. As with PTB, we evaluate the result using the reciprocal rank of the correct prediction. We achieve a performance of S0 = 0.368, which can be assumed to be as good as state-of-the-art models on this data (Hidasi et al., 2016). Predicting clicks at random yields a score SR below 0.001."}, {"heading": "APPENDIX B GOING ONE STEP FURTHER WITH CO-OCCURRENCE-BASED", "text": "COLLISIONS"}, {"heading": "B.1 CO-OCCURRENCE-BASED BLOOM EMBEDDING (CBE)", "text": "In Bloom filters and BE, collisions are unavoidable due to the lower embedding dimensionality and the use of multiple projections (Sec. 3). In addition we have seen that alternative approaches produce embeddings by exploiting co-occurrence information (Secs. 2 and 4.3). Here, we study a variant of BE that takes advantage of co-occurrence information to adjust the collisions that will inevitably take place when performing the embedding. We denote this approach by co-occurrencebased Bloom embedding (CBE).\nWhat we propose is a quite straightforward approach to CBE, which does not add much extra precomputation time. Training and testing times remain the same, as CBE uses a pre-computed hashing matrix H (Sec. 3.2). The general idea of the proposed approach is to \u2018re-direct\u2019 the collisions of the co-occurring elements to the same bits or positions of u. Our implementation of this idea is detailed in Algorithm 1, and briefly explained below.\nAlgorithm 1 Pseudocode for CBE. Input: Input and/or output instances X (n\u00d7 d sparse binary matrix), embedding dimensionality m,\nnumber of projections k, and pre-computed hashing matrix H (d\u00d7 k integers matrix). Output: Co-occurrence-based hashing matrix H\u2032.\n1: C\u2190 XTX 2: C\u2190 C SGN(C\u2212AVGFREQ(X)) 3: cVAL, cROW, cCOL \u2190 COORD(LOWTRI(C)) 4: for i in ARGSORT(cVAL) 5: a, b\u2190 cROWi , cCOLi 6: r \u2190 URND(1,m, ha \u222a hb) 7: ja \u2190 URND(1, k, \u2205) 8: jb \u2190 URND(1, k, \u2205) 9: ha,ja , hb,jb \u2190 r\n10: return H\n9http://recsys.yoochoose.net/challenge.html\nFirst, we count pairwise co-occurrences and store them in a sparse matrix C (line 1). Next, we threshold C by the average element frequency in X using the Hadamard product and a componentwise sign function (line 2). We then get the lower triangular part of C and return it in coordinates format, that is, using a tuple of values, row indices, and column indices (line 3). We will use the order in cVAL to update the hash matrix H. To do so, we first loop over the indices of the sorted values of cVAL in increasing order (line 4). After selecting the corresponding elements a and b (line 5), we then draw integers from URND (lines 6\u20138). The function URND(x, y, z) is a uniform random integer generator between x and y (both included) such that the output integer is not included in the set z, that is, URND(x, y, z) 6\u2208 z. Rows a and b of H are transformed to sets ha and hb and its union is computed (line 6). Finally, we use the integers generated by URND to pick projections ja and jb from H, and assign them the same bit r (line 9). By updating the projections in H in increasing order of co-occurrence (line 4), we give priority to the pairs with largest co-occurrence, setting them to collide to the same bit r (line 9)."}, {"heading": "B.2 CBE RESULTS", "text": "Overall, the performance of CBE only provides moderate increments over the original BE approach (Fig. 4). With the exception of the BC task, the performance of CBE is always higher than the one of BE. However, with the exception of the AMZ task, we do not observe dramatic increases of CBE over BE. On average, such increases are between 0.4% and 8.4% (Table 4, right). One possible explanation for these moderate performance increases is the low co-occurrence in the considered data (Table 4, left). As it can be seen, typically less than 3% of all possible pairs show a cooccurrence. Moreover, the average co-occurrence count of such co-occurring pairs is very low, with ratios \u03c1 to the total number of instances n in the order of 10\u22125 or 10\u22126.\nDespite being moderate on average, we observed that the increments provided by CBE were more prominent for low dimensionality ratios m/d. By relating CBE with the best approaches resulting from the comparison of BE with the alternatives, we see that CBE is generally better than BE, sometimes with a statistically significant difference (Table 5). Furthermore, we see that CBE, being based on co-occurrences, more closely approaches PMI and CCA in the tasks where those were performing best, and even outperforms them in one test point (AMZ,m/d = 0.2; compare also with Table 3). Being closer to those co-occurrence-based approaches is an indication that CBE leverages co-occurrence information to some extent."}], "references": [{"title": "Label-embedding for image classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Akata et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Akata et al\\.", "year": 2015}, {"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML),", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "Error-correcting output codes for multi-label text categorization", "author": ["G. Armano", "C. Chira", "N. Hatami"], "venue": "In Proc. of the Italian Information Retrieval Conf. (IIR), pp", "citeRegEx": "Armano et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Armano et al\\.", "year": 2012}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2000}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P.W. Ellis", "B. Whitman", "P. Lamere"], "venue": "In Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR),", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["B.H. Bloom"], "venue": "Communications of the ACM,", "citeRegEx": "Bloom.,? \\Q1970\\E", "shortCiteRegEx": "Bloom.", "year": 1970}, {"title": "El-Maazawi. Bloom filters \u2013 a tutorial, analysis, and survey", "author": ["A.J. Blustein"], "venue": "Technical report,", "citeRegEx": "Blustein,? \\Q2002\\E", "shortCiteRegEx": "Blustein", "year": 2002}, {"title": "An improved construction for counting Bloom filters", "author": ["F. Bonomi", "M. Mitzenmacher", "R. Panigrahy", "S. Singh", "G. Varghese"], "venue": "European Symposium on Algorithms (ESA),", "citeRegEx": "Bonomi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bonomi et al\\.", "year": 2006}, {"title": "Improving methods for single-label text categorization", "author": ["A. Cardoso-Cachopo"], "venue": "PhD thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa,", "citeRegEx": "Cardoso.Cachopo.,? \\Q2007\\E", "shortCiteRegEx": "Cardoso.Cachopo.", "year": 2007}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J. Wilson", "S. Tyree", "K. Weinberger", "Y. Chen"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Wide & deep learning for recommender systems", "author": ["H.-T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir", "R. Anil", "Z. Haque", "L. Hong", "V. Jain", "X. Liu", "H. Shah"], "venue": "In Proc. of the Workshop on Deep Learning for Recommender Systems (DLRS),", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "In Proc. of the Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Information-theoretic label embeddings for large-scale image classification", "author": ["F. Chollet"], "venue": "ArXiv: 1607.05691,", "citeRegEx": "Chollet.,? \\Q2016\\E", "shortCiteRegEx": "Chollet.", "year": 2016}, {"title": "Robust Bloom filters for large multilabel classification tasks", "author": ["M. Ciss\u00e9", "N. Usunier", "T. Arti\u00e8res", "P. Gallinari"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ciss\u00e9 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ciss\u00e9 et al\\.", "year": 2013}, {"title": "BinaryConnect: training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Bloom filters in probabilistic verification", "author": ["P.C. Dillinger", "P. Manolios"], "venue": "In Proc. of the Int. Conf. on Formal Methods in Computer-Aided Design (FMCAD),", "citeRegEx": "Dillinger and Manolios.,? \\Q2004\\E", "shortCiteRegEx": "Dillinger and Manolios.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Small statistical models by random feature mixing", "author": ["K. Ganchev", "M. Dredze"], "venue": "In ACL Workshop on Mobile Language Processing (MLP), pp", "citeRegEx": "Ganchev and Dredze.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev and Dredze.", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Proc. of the Int. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of the Int. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Efficient softmax approximation for GPUs", "author": ["E. Grave", "A. Joulin", "M. Ciss\u00e9", "D. Grangier", "H. J\u00e9gou"], "venue": null, "citeRegEx": "Grave et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grave et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "ArXiv: 1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "The MovieLens datasets: history and context", "author": ["F.M. Harper", "J.K. Konstan"], "venue": "ACM Trans. on Interactive Intelligent Systems,", "citeRegEx": "Harper and Konstan.,? \\Q2015\\E", "shortCiteRegEx": "Harper and Konstan.", "year": 2015}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["B. Hidasi", "A. Karatzoglou", "L. Baltrunas", "D. Tikk"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Hidasi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hidasi et al\\.", "year": 2016}, {"title": "Long short-term memory networks", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3-4):321\u2013377,", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D.J. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "Technical report,", "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Building a large annotated corpus of English: the Penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewich"], "venue": "Computational Linguisitcs,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "In Proc. of the ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "McAuley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "ArXiv: 1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Probability and computing: randomized algorithms and probabilistic analysis", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": null, "citeRegEx": "Mitzenmacher and Upfal.,? \\Q2005\\E", "shortCiteRegEx": "Mitzenmacher and Upfal.", "year": 2005}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Proc. of the Int. Workshop on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Hybrid recommender system based on autoencoders", "author": ["F. Strub", "R. Gaudel", "J. Mary"], "venue": "In Proc. of the Workshop on Deep Learning for Recommender Systems (DLRS),", "citeRegEx": "Strub et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Strub et al\\.", "year": 2016}, {"title": "Lecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning 4,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent", "A. Br\u00e9bisson", "X. Bouthilier"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vincent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2015}, {"title": "COFI RANK - maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weimer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2008}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A. Smola"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Kernel dependency estimation", "author": ["J. Weston", "O. Chapelle", "A. Elisseeff", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weston et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2002}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine Learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Collaborative denoising auto-encoders for top-n recommender systems", "author": ["Y. Wu", "C. DuBois", "A.X. Zheng", "M. Ester"], "venue": "In Proc. of the ACM Int. Conf. on Web Search and Data Mining (WSDM),", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Improving recommendation lists through topic diversification", "author": ["C.-N. Ziegler", "S.M. McNee", "J.A. Konstan", "G. Lausen"], "venue": "In Proc. of the Int. World Wide Web Conf. (WWW), pp", "citeRegEx": "Ziegler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ziegler et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 26, "context": "This is the case, for instance, with recommender systems, where high-dimensional sparse vectors, typically in the order from tens of thousands to hundreds of millions, constitute both the input and the output of the model (e.g., Wu et al., 2016; Hidasi et al., 2016; Cheng et al., 2016; Strub et al., 2016).", "startOffset": 222, "endOffset": 306}, {"referenceID": 11, "context": "This is the case, for instance, with recommender systems, where high-dimensional sparse vectors, typically in the order from tens of thousands to hundreds of millions, constitute both the input and the output of the model (e.g., Wu et al., 2016; Hidasi et al., 2016; Cheng et al., 2016; Strub et al., 2016).", "startOffset": 222, "endOffset": 306}, {"referenceID": 42, "context": "This is the case, for instance, with recommender systems, where high-dimensional sparse vectors, typically in the order from tens of thousands to hundreds of millions, constitute both the input and the output of the model (e.g., Wu et al., 2016; Hidasi et al., 2016; Cheng et al., 2016; Strub et al., 2016).", "startOffset": 222, "endOffset": 306}, {"referenceID": 44, "context": "Embedding sparse high-dimensional inputs is commonplace (e.g., Bengio et al., 2000; Turian et al., 2010; Mikolov et al., 2013).", "startOffset": 56, "endOffset": 126}, {"referenceID": 38, "context": "Embedding sparse high-dimensional inputs is commonplace (e.g., Bengio et al., 2000; Turian et al., 2010; Mikolov et al., 2013).", "startOffset": 56, "endOffset": 126}, {"referenceID": 3, "context": "However, embedding sparse high-dimensional outputs, or even inputs and outputs at the same time, is much less common (cf. Weston et al., 2002; Bengio et al., 2010; Akata et al., 2015).", "startOffset": 117, "endOffset": 183}, {"referenceID": 0, "context": "However, embedding sparse high-dimensional outputs, or even inputs and outputs at the same time, is much less common (cf. Weston et al., 2002; Bengio et al., 2010; Akata et al., 2015).", "startOffset": 117, "endOffset": 183}, {"referenceID": 6, "context": "The proposed embedding is based on the idea of Bloom filters (Bloom, 1970), and therefore it inherits part of the theory developed around that idea (Blustein & El-Maazawi, 2002; Dillinger & Manolios, 2004; Mitzenmacher & Upfal, 2005; Bonomi et al.", "startOffset": 61, "endOffset": 74}, {"referenceID": 8, "context": "The proposed embedding is based on the idea of Bloom filters (Bloom, 1970), and therefore it inherits part of the theory developed around that idea (Blustein & El-Maazawi, 2002; Dillinger & Manolios, 2004; Mitzenmacher & Upfal, 2005; Bonomi et al., 2006).", "startOffset": 148, "endOffset": 254}, {"referenceID": 33, "context": "A common approach to embed high-dimensional inputs is the hashing trick (Langford et al., 2007; Shi et al., 2009; Weinberger et al., 2009).", "startOffset": 72, "endOffset": 138}, {"referenceID": 41, "context": "A common approach to embed high-dimensional inputs is the hashing trick (Langford et al., 2007; Shi et al., 2009; Weinberger et al., 2009).", "startOffset": 72, "endOffset": 138}, {"referenceID": 47, "context": "A common approach to embed high-dimensional inputs is the hashing trick (Langford et al., 2007; Shi et al., 2009; Weinberger et al., 2009).", "startOffset": 72, "endOffset": 138}, {"referenceID": 2, "context": "Originally designed for single-class outputs, it can be also applied to class sets (Armano et al., 2012).", "startOffset": 83, "endOffset": 104}, {"referenceID": 48, "context": "Another example of a framework offering recovery capabilities is kernel dependency estimation (Weston et al., 2002).", "startOffset": 94, "endOffset": 115}, {"referenceID": 28, "context": "Canonical correlation analysis is an example that considers both inputs and outputs at the same time (Hotelling, 1936).", "startOffset": 101, "endOffset": 118}, {"referenceID": 1, "context": "Other examples considering output embeddings are nuclear norm regularized learning (Amit et al., 2007), label embedding trees (Bengio et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 3, "context": ", 2007), label embedding trees (Bengio et al., 2010), or the WSABIE algorithm (Weston et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 15, "context": "A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers (Courbariaux et al., 2015; Han et al., 2016; Kim et al., 2016).", "startOffset": 137, "endOffset": 199}, {"referenceID": 24, "context": "A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers (Courbariaux et al., 2015; Han et al., 2016; Kim et al., 2016).", "startOffset": 137, "endOffset": 199}, {"referenceID": 31, "context": "A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers (Courbariaux et al., 2015; Han et al., 2016; Kim et al., 2016).", "startOffset": 137, "endOffset": 199}, {"referenceID": 10, "context": "A less frequently used approach is to reduce the model size before training (Chen et al., 2015).", "startOffset": 76, "endOffset": 95}, {"referenceID": 22, "context": "The hierarchical softmax approach (Morin & Bengio, 2005) or the more recent adaptive softmax (Grave et al., 2016) are two examples of those.", "startOffset": 93, "endOffset": 113}, {"referenceID": 6, "context": "Bloom filters (Bloom, 1970) are a compact probabilistic data structure that is used to represent sets of elements, and to efficiently check whether an element is a member of a set (Mitzenmacher & Upfal, 2005).", "startOffset": 14, "endOffset": 27}, {"referenceID": 8, "context": "However, in practice, m is usually constrained by space requirements, and k \u2264 10 is employed, independent of the number of elements to be projected, and giving less than 1% false positive probability (Bonomi et al., 2006).", "startOffset": 200, "endOffset": 221}, {"referenceID": 34, "context": "One could potentially also recover a probability distribution by re-normalization, but the problems we consider are information retrieval-type of problems (Manning et al., 2008), which are typically seen as ranking problems, such as ranking recommendations based on user preferences (Weimer et al.", "startOffset": 155, "endOffset": 177}, {"referenceID": 46, "context": ", 2008), which are typically seen as ranking problems, such as ranking recommendations based on user preferences (Weimer et al., 2008).", "startOffset": 113, "endOffset": 134}, {"referenceID": 34, "context": "Depending on the data set, we work with mean average precision, reciprocal ranks, or accuracy (Manning et al., 2008).", "startOffset": 94, "endOffset": 116}, {"referenceID": 37, "context": "Penn treebank (PTB): next word prediction with the Penn treebank data set (Mikolov, 2012).", "startOffset": 74, "endOffset": 89}, {"referenceID": 9, "context": "CADE web directory (CADE): text categorization with the CADE web directory data set (Cardoso-Cachopo, 2007).", "startOffset": 84, "endOffset": 107}, {"referenceID": 36, "context": "Amazon book reviews (AMZ): book recommendation with the Amazon book reviews data set (McAuley et al., 2015).", "startOffset": 85, "endOffset": 107}, {"referenceID": 51, "context": "Book crossing (BC): book recommendation with the book crossing data set (Ziegler et al., 2005).", "startOffset": 72, "endOffset": 94}, {"referenceID": 33, "context": "We first consider the popular hashing trick for classifier inputs (Langford et al., 2007; Weinberger et al., 2009).", "startOffset": 66, "endOffset": 114}, {"referenceID": 47, "context": "We first consider the popular hashing trick for classifier inputs (Langford et al., 2007; Weinberger et al., 2009).", "startOffset": 66, "endOffset": 114}, {"referenceID": 2, "context": "Originally designed for single-class targets (Dietterich & Bakiri, 1995), ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies (Armano et al., 2012).", "startOffset": 186, "endOffset": 207}, {"referenceID": 28, "context": "CCA is a common way to learn a joint dense, realvalued embedding for both inputs and outputs at the same time (Hotelling, 1936).", "startOffset": 110, "endOffset": 127}, {"referenceID": 29, "context": "CCA can be computed using SVD on a correlation matrix (Hsu et al., 2012) and, similarly to PMI, we can use the KNN trick to rank elements or labels at prediction time.", "startOffset": 54, "endOffset": 72}, {"referenceID": 8, "context": "In the future, besides continuing to exploit co-occurrences (Appendix B), one could extend the proposed approach by considering further extensions of Bloom filters such as counting Bloom filters (Bonomi et al., 2006).", "startOffset": 195, "endOffset": 216}], "year": 2017, "abstractText": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as \u2018on-the-fly\u2019 constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_461"}