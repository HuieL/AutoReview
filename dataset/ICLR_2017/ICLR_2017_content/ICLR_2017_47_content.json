{"name": "ICLR_2017_47.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["STEERABLE CNNS", "Taco S. Cohen", "Max Welling"], "emails": ["t.s.cohen@uva.nl", "m.welling@uva.nl"], "sections": [{"heading": null, "text": "It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively."}, {"heading": "1 INTRODUCTION", "text": "Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs. Human beings are able to learn new concepts with very few labels, and reproducing this ability is an important challenge for artificial intelligence research. From an applied perspective, improving the statistical efficiency of deep learning is vital because in many domains (e.g. medical image analysis), acquiring large amounts of labelled data is costly.\nTo improve the statistical efficiency of machine learning methods, many have sought to learn invariant representations. In deep learning, however, intermediate layers should not be invariant, because the relative pose of local features must be preserved for further layers (Cohen & Welling, 2016; Hinton et al., 2011). Thus, one is led to the idea of equivariance: a network is equivariant if the representations it produces transform in a predictable way under transformations of the input. In other words, equivariant networks produce representations that are steerable. Steerability makes it possible to apply filters not just in every position (as in a standard convolution layer), but in every pose, thus allowing for increased parameter sharing.\nPrevious work has shown that equivariant CNNs yield state of the art results on classification tasks (Cohen & Welling, 2016; Dieleman et al., 2016), even though they only enforce equivariance to small groups of transformations like rotations by multiples of 90 degrees. Learning representations that are equivariant to larger groups is likely to result in further gains, but the computational cost of current methods scales linearly with the size of the group, making this impractical. In this paper we present the general theory of steerable CNNs, which covers previous approaches but also shows how the computational cost can be decoupled from the size of the symmetry group, thus paving the way for future scaling.\nTo better understand the structure of steerable representations, we analyze them mathematically. We show that any steerable representation is a composition of low-dimensional elementary feature types. Each elementary feature can be steered independently of the others, and captures a distinct characteristic of the input that has an invariant or \u201cobjective\u201d meaning. This doctrine of \u201cobserverindependent quantities\u201d was put forward by (Weyl, 1939, ch. 1.4) and is used throughout physics. It has been applied to vision and representation learning by Kanatani (1990); Cohen (2013).\nThe mentioned type system puts constraints on the network weights and architecture. Specifically, since an equivariant filter bank is required to map given input feature types to given output feature types, the number of parameters required by such a filter bank is reduced. Furthermore, by the same logic that tells us not to add meters to seconds, steerability considerations prevent us from adding features of different types (e.g. for residual learning (He et al., 2016a)).\nThe rest of this paper is organized as follows. The theory of steerable CNNs is introduced in Section 2. Related work is discussed in Section 3, which is followed by classification experiments (Section 4) and a discussion and conclusion in Section 5."}, {"heading": "2 STEERABLE CNNS", "text": ""}, {"heading": "2.1 FEATURE MAPS AND FIBERS", "text": "Consider a 2D signal f : Z2 \u2192 RK with K channels. The signal may be an input to the network or a feature representation computed by a CNN. Since signals can be added and multiplied by scalars, the set of signals of this signature forms a linear space F . Each layer of the network has its own feature space Fl, but we will often suppress the layer index to reduce clutter. It is customary in deep learning to describe f \u2208 F as a stack of feature maps fk (for k = 1, . . . ,K). In this paper we also consider another decomposition of F into fibers. The fiber Fx at position x in the \u201cbase space\u201d Z2 is the K-dimensional vector space spanned by all channels at position x. Thus, f \u2208 F is comprised of feature vectors f(x) that live in the fibers Fx (see Figure 1(a)).\nGiven some group of transformations G that acts on points in Z2, we can transform signals f \u2208 F0:\n[\u03c00(g)f ] (x) = f(g \u22121x) (1)\nThis says that the pixel at g\u22121x gets moved to x by the transformation g \u2208 G. We note that \u03c00(g) is a linear operator.\nAn important property of \u03c00 is that \u03c00(gh) = \u03c00(g)\u03c00(h). Here, gh means composition of transformations in G, while \u03c00(g)\u03c00(h) denotes matrix multiplication. A vector space such as F0 equipped with a set of linear operators \u03c00 satisfying this condition is known as a group representation (or just representation, for short). A lot is known about group representations (Serre, 1977), and we will make extensive use of the theory, explaining the relevant concepts as needed."}, {"heading": "2.2 STEERABLE REPRESENTATIONS", "text": "Let (F , \u03c0) be a feature space with a group representation and \u03a6 : F \u2192 F \u2032 a convolutional network. The feature space F \u2032 is said to be (linearly) steerable with respect to G, if for all transformations g \u2208 G, the features \u03a6f and \u03a6\u03c0(g)f are related by a linear transformation \u03c0\u2032(g) that does not depend on f . So \u03c0\u2032(g) allows us to \u201csteer\u201d the features in F \u2032 without referring to the input in F from which they were computed.\nCombining the definition of steerability (i.e. \u03a6\u03c0(g) = \u03c0\u2032(g)\u03a6) with the fact that \u03c0 is a group representation, we find that \u03c0\u2032 must also be a group representation:\n\u03c0\u2032(gh)\u03a6f = \u03a6\u03c0(gh)f = \u03a6\u03c0(g)\u03c0(h)f = \u03c0\u2032(g)\u03a6\u03c0(h)f = \u03c0\u2032(g)\u03c0\u2032(h)\u03a6f (2)\nThat is, \u03c0\u2032(gh) = \u03c0\u2032(g)\u03c0\u2032(h) (at least in the span of the image of \u03a6). Figure 2 gives an illustration.\nFor simplicity, we will restrict our attention to discrete groups of transformations. The theory for continuous groups is almost completely analogous. Our running example will be the group p4m which consists of translations, rotations by 90 degrees around any point, and reflections. We further restrict our attention to groups that are constructed1 from the group of translations Z2 and a group H of transformations that fixes the origin 0 \u2208 Z2. For p4m, we have H = D4, the 8-element group of reflections and rotations about the origin.\nUsing this division, we can first construct a filter bank that generates H-steerable fibers, and then show that convolution with such a filter bank produces a feature space that is steerable with respect to the whole group G."}, {"heading": "2.3 EQUIVARIANT FILTER BANKS", "text": "A filter bank can be described as an array of dimension (K \u2032,K, s, s), where K,K \u2032 denote the number of input / output channels and s is the kernel size. For our purposes it is useful to think of a filter bank as a linear map \u03a8 : F \u2192 RK\u2032 that takes as input a signal f \u2208 F and produces a K \u2032-dimensional feature vector. The filter bank only looks at an s \u00d7 s patch in F , so the matrix representing \u03a8 has shape K \u2032 \u00d7K \u00b7 s2. To correlate a signal f using \u03a8, one would simply apply \u03a8 to translated copies of f , producing the output signal one fiber at a time.\nWe assume (by induction) that we have a representation \u03c0 that allows us to steer F . In order to make the output of the convolution steerable, we need the filter bank \u03a8 : F \u2192 RK\u2032 to be H-equivariant:\n\u03c1(h) \u03a8 = \u03a8\u03c0(h), \u2200h \u2208 H (3)\nfor some representation \u03c1 of H that acts on the output fibers (see Figure 3). Note that we only require equivariance with respect to H (which excludes translations) and not G, because translations can move patterns into and out of the receptive field of a fiber, making full translation equivariance impossible.\nThe space of maps satisfying the equivariance constraint is denoted HomH(\u03c0, \u03c1), because an equivariant map \u03a8 is a \u201chomomorphism of group representations\u201d, meaning it respects the structure of the representations. Equivariant maps are also sometimes called intertwiners (Serre, 1977).\nSince the equivariance constraint (eq. 3) is linear in \u03a8, the space HomH(\u03c0, \u03c1) of admissible filter banks is a vector space: any linear combination of maps \u03a8,\u03a8\u2032 \u2208 HomH(\u03c0, \u03c1) is again\nan intertwiner. Hence, given \u03c0 and \u03c1, we can compute a basis for HomH(\u03c0, \u03c1) by solving a linear system.\n1as a semi-direct product\nComputation of the intertwiner basis is done offline, before training. Once we have such a basis \u03c81, . . . , \u03c8n for HomH(\u03c0, \u03c1), we can express any equivariant filter bank \u03a8 as a linear combination \u03a8 = \u2211 i \u03b1i\u03c8i using parameters \u03b1i. As shown in Section 2.8, this can be done efficiently even in high dimensions."}, {"heading": "2.4 INDUCTION", "text": "We have shown how to parameterize filter banks that intertwine \u03c0 and \u03c1, making the output fibersHsteerable by \u03c1 if the input space F is H-steerable by \u03c0. In this section we show how H-steerability of fibers F \u2032x leads to G-steerability of the whole feature space F \u2032. This happens through a natural and important construction known as the induced representation (Mackey, 1952; 1953; 1968; Serre, 1977; Taylor, 1986; Folland, 1995; Kaniuth & Taylor, 2013).\nAs stated before, the correlation \u03a8 ? f could be computed by translating f before applying \u03a8: [\u03a8 ? f ] (x) = \u03a8 [ \u03c0(x)\u22121f ] . (4)\nWhere x \u2208 Z2 is interpreted as a translation when given as input to \u03c0. We can now calculate the transformation law of the output space. To do so, we apply a translation t and transformation r \u2208 H to f \u2208 F , yielding \u03c0(tr)f , and then perform the correlation with \u03a8. With a some algebra (Appendix A), we find:\n[\u03a8 ? [\u03c0(tr)f ]] (x) = \u03c1(r) [ [\u03a8 ? f ] ((tr)\u22121x) ] (5)\nNow if we define \u03c0\u2032 as [\u03c0\u2032(tr)f ] (x) = \u03c1(r) [ f((tr)\u22121x) ] (6)\nthen \u03a8 ? \u03c0(g)f = \u03c0\u2032(g)\u03a8 ? f (see Fig. 4). This representation \u03c0\u2032 is known as the representation of G induced by the representation \u03c1 of H , and is denoted \u03c0\u2032 = IndGH \u03c1.\nWhen parsing eq. 6, it is important to keep in mind that (as indicated by the square brackets) \u03c0\u2032 acts on the whole feature space F \u2032 while \u03c1 acts on individual fibers. If we compare the induced representation (eq. 6) to the representation \u03c00 defined in eq. 1, we see that the difference lies only in the presence of a factor \u03c1(r) applied to the fibers. This factor describes how the feature channels are mixed by the transformation. The color channels in the input space do not get mixed by geometrical transformations, so we say that \u03c00 is induced from the trivial representation \u03c10(h) = I .\nNow that we have a G-steerable feature space F \u2032, we can iterate the procedure by computing a basis for the space of intertwiners between \u03c0\u2032 (restricted to H) and some \u03c1\u2032 of our choosing."}, {"heading": "2.5 FEATURE TYPES AND CHARACTER THEORY", "text": "By now, the reader may be wondering how to choose \u03c1, or indeed what the space of representations that we can choose from looks like in the first place. We will answer these questions in this section by showing that each representation has a type (encoded as a short list of integers) that corresponds to a certain symmetry or invariance of the feature. We further show how the number of parameters of an equivariant filter bank depends on the types of the representations \u03c0 and \u03c1 that it intertwines. Our discussion will make use of a number of important elementary results from group representation theory which are stated but not proved. The reader wishing to go deeper may consult chapters 1 and 2 of the excellent book by Serre (1977).\nRecall that a group representation is a set of invertible linear maps \u03c1(g) : RK \u2192 RK satisfying \u03c1(gh) = \u03c1(g)\u03c1(h) for all elements g, h \u2208 H . It can be shown that any representation is a direct sum (i.e. block_diag plus change of basis) of a number of \u201celementary\u201d representations associated with G. These building blocks are called irreducible representations (or irreps), because they can\nthemselves not be block-diagonalized. In other words, if \u03d5i are the irreducible representations of H , any representation \u03c1 of H can be written in block-diagonal form:\n\u03c1(g) = A \u03d5i1(g) . . . \u03d5in A\u22121 (7) for some basis matrix A, and some ik that index the irreps (each irrep may occur 0 or more times).\nEach irreducible representation corresponds to a type of symmetry, as shown in table 1. For example, as can be seen in this table, the representations B1 and B2 represent the 90-degree rotation r as the matrix [\u22121], so the basis filters for these representations change sign when rotated by r. It should be noted that in the higher layers l > 0, elementary basis filters can look different because they depend on the representation \u03c0l that is being decomposed.\nThe fact that all representations can be decomposed into a direct sum of irreducibles implies that each representation has a basis-independent type: which irreducible representations appear in it, and with what multiplicity. For example, the input representation \u03c00 (table 1) has type (3, 0, 1, 1, 2). This means that, for instance, \u03c00(r) is block-diagonalized as:\nA\u22121\u03c00(r)A = block_diag([1] , [1] , [1] , [\u22121] , [\u22121] , [0 \u22121; 1 0] , [0 1;\u22121 0]). (8)\nWhere the block matrix contains (3, 0, 1, 1, 2) copies of the irreps (A1, A2, B1, B2, E), evaluated at r (see column r in table 1). The change of basis matrix A is constructed from the basis filters shown in table 1 (and the same A block-diagonalizes \u03c00(g) for all g).\nSo the most general way in which we can choose a representation \u03c1 is to choose multiplicities mi \u2265 0 and a basis matrixA. In Section 2.7 we will find that there is an important restriction on this freedom, which alleviates the need to choose a basis. The choice of multiplicities is then the only hyperparameter, analogous to the choice of the number of channels in an ordinary CNN. Indeed, the multiplicities determine the number of channels: K = \u2211 imi dim\u03d5i."}, {"heading": "2.6 DETERMINING THE TYPE OF THE INDUCED REPRESENTATION", "text": "By choosing the type of \u03c1, we also determine the type of \u03c0 = IndGH \u03c1 (restricted toH), but what is it? Explicit formulas exist (Reeder (2014); Serre (1977)) but are rather complicated, so we will present a simple computational procedure that can be used to determine the type of any representation. This procedure relies on the character \u03c7\u03c1(g) = Tr(\u03c1(g)) of the representation to be decomposed. The most important fact about characters is that the characters of irreps \u03d5i, \u03d5j are orthogonal:\n\u3008\u03c7\u03d5i , \u03c7\u03d5j \u3009 \u2261 1 |H| \u2211 h\u2208H \u03c7\u03d5i(h)\u03c7\u03d5j (h) = \u03b4ij . (9)\nFurthermore, since the trace of a direct sum equals the sum of the traces (i.e. \u03c7\u03c1\u2295\u03c1\u2032 = \u03c7\u03c1 + \u03c7\u03c1\u2032 ), and every representation \u03c1 is a direct sum of irreps, it follows that we can obtain the multiplicity of irrep \u03d5i in \u03c1 by computing the inner product with the i-th character:\n\u3008\u03c7\u03c1, \u03c7\u03d5i\u3009 = \u3008\u03c7\u2295jmj\u03d5j , \u03c7\u03d5i\u3009 = \u2329\u2211 j mj\u03c7\u03d5j , \u03c7\u03d5i \u232a = \u2211 j mj\u3008\u03c7\u03d5j , \u03c7\u03d5i\u3009 = mi (10)\nSo a simple dot product of characters is all we need to determine the type of a representation. As we will see next, the type of the input and output representation of a layer determines the parameter cost of that layer."}, {"heading": "2.6.1 THE PARAMETER COST OF EQUIVARIANT CONVOLUTION LAYERS", "text": "Steerable CNNs use parameters much more efficiently than ordinary CNNs. In this section we show how the number of parameters required by an equivariant layer is determined by the feature types of the input and output space, and how the efficiency of a choice of feature types may be evaluated.\nIn section 2.3, we found that a filter bank \u03a8 is equivariant if and only if it lies in the vector space called HomH(\u03c0, \u03c1). It follows that the number of parameters for such a filter bank is equal to the dimensionality of this space, n = dim HomH(\u03c0, \u03c1). This number is known as the intertwining number of \u03c0 and \u03c1 and plays an important role in the theory of group representations.\nAs with multiplicities, the intertwining number is easily computed using characters. It can be shown (Reeder, 2014) that the intertwining number equals:\ndim HomH(\u03c0, \u03c1) = \u3008\u03c7\u03c0, \u03c7\u03c1\u3009. (11) By linearity and the orthogonality of characters, we find that dim HomH(\u03c0, \u03c1) = \u2211 imim \u2032 i, for representations \u03c0, \u03c1 of type (m1, . . . ,mJ) and (m\u20321, . . . ,m \u2032 J), respectively. Thus, as far as the number of parameters of a steerable convolution layer is concerned, the only choice we have to make for \u03c1 is its type \u2013 a short list of integers mi.\nThe efficiency of a choice of type can be assessed using a quantity we call the parameter utilization:\n\u00b5 = dim\u03c0 \u00b7 dim \u03c1\ndim HomH(\u03c0, \u03c1) . (12)\nThe numerator equals s2K \u00b7 K \u2032: the number of parameters for a non-equivariant filter bank. The denominator equals the parameter cost of an equivariant filter bank with the same filter size and number of input/output channels. Typical values of \u00b5 in effective architectures are around |H|, e.g. \u00b5 = 8 for H = D4. Such a layer utilizes its parameters 8 times more intensively than an ordinary convolution layer."}, {"heading": "2.7 EQUIVARIANT NONLINEARITIES & CAPSULES", "text": "In the previous section we showed that only the basis-independent types of \u03c0 and \u03c1 play a role in determining the parameter cost of an equivariant filter bank. An equivalent representation \u03c1\u2032(g) = A\u03c1(g)A\u22121 will have the same type, and hence the same parameter cost as \u03c1. However, when it comes to nonlinearities, different bases behave differently.\nJust like a convolution layer (eq. 3), a layer of nonlinearities must commute with the group action. An elementwise nonlinearity \u03bd : R \u2192 R (or more generally, a fiber-wise nonlinearity \u03bd : RK \u2192 RK\u2032 ) is admissible for an input representation \u03c1 if there exists an output representation \u03c1\u2032 such that \u03bd applied after \u03c1 equals \u03c1\u2032 applied after \u03bd.\nSince commutation with nonlinearities depends on the basis, we need a more granular notion than the feature type. We define a \u03c1-capsule as a (typically low-dimensional) feature vector that transforms according to a representation \u03c1 (we may also refer to \u03c1 as the capsule). Thus, while a capsule has a type, not all representations of that type are equivalent as capsules. Given a catalogue of capsules \u03c1i (for i = 1, . . . , C) with multiplicities mi, we can construct a fiber as a stack of capsules that is steerable by a block-diagonal representation \u03c1 with mi copies of \u03c1i on the diagonal.\nLike the capsules of Hinton et al. (2011), our capsules encode the pose of a pattern in the input, and consist of a number of units (dimensions) that do not get mixed with the units of other capsules by symmetries. In this sense, a stack of capsules is disentangled (Cohen & Welling, 2014).\nWe have found a few simple types of capsules and corresponding admissible nonlinearities. It is easy to see that any nonlinearity is admissible for \u03c1 when the latter is realized by permutation matrices: permuting a list of coordinates and then applying a nonlinearity is the same as applying the nonlinearity and then permuting. If \u03c1 is realized by a signed permutation matrix, then CReLU(\u03b1) = (ReLU(\u03b1),ReLU(\u2212\u03b1)) introduced by Shang et al. (2016), or any concatenated nonlinearity \u03bd\u2032(\u03b1) = (\u03bd(\u03b1), \u03bd(\u2212\u03b1)), will be admissible. Any scale-free concatenated nonlinearity such as CReLU is admissible for a representation realized by monomial matrices (having the same nonzero pattern as a permutation matrix). Finally, we can always make a representation of a finite group orthogonal by a suitable choice of basis, which means that we can use any nonlinearity that acts only on the length of the vector.\nFor many groups, the irreps can be realized using signed permutation matrices, so we can use irreducible \u03d5i-capsules with concatenated nonlinearities such as CReLU. Another class of capsules, which we call quotient capsules, are naturally realized by permutation matrices, and are thus compatible with any nonlinearity. These are described in Appendix C."}, {"heading": "2.8 COMPUTATIONAL EFFICIENCY", "text": "Modern convolutional networks often use on the order of hundreds of channels K per layer Zagoruyko & Komodakis (2016). When using 3 \u00d7 3 filters, a filter bank can have on the order of 9K2 \u2248 106 dimensions. The number of parameters for an equivariant filter bank is about \u00b5 \u2248 10 times smaller, but a basis for the space of equivariant filter banks would still be about 106 \u00d7 105, which is too large to be practical.\nFortunately, the block-diagonal structure of \u03c0 and \u03c1 induces a block structure in \u03a8. Suppose \u03c0 = block_diag(\u03c01, . . . , \u03c0P ) and \u03c1 = block_diag(\u03c11, . . . , \u03c1Q). Then an intertwiner is a matrix of shape K \u2032 \u00d7Ks2, where K \u2032 = \u2211 i dim \u03c1 i and Ks2 = \u2211 i dim\u03c0\ni. This matrix has the following block structure:\n\u03a8 =  h11 \u2208 HomH(\u03c1 1, \u03c01) \u00b7 \u00b7 \u00b7 h1P \u2208 HomH(\u03c11, \u03c0P ) ... . . .\n... hR1 \u2208 HomH(\u03c1R, \u03c01) \u00b7 \u00b7 \u00b7 hRP \u2208 HomH(\u03c1R, \u03c0P )  (13) Each block hij corresponds to an input-output pair of capsules, and can be parameterized by a linear combination of basis matrices \u03c8ijk \u2208 HomH(\u03c1i, \u03c0j).\nIn practice, we typically use many copies of the same capsule (say ni copies of \u03c1i and mj copies of \u03c0j). Therefore, many of the blocks hij can be constructed using the same intertwiner basis. If we order equivalent capsules to be adjacent, the intertwiner consists of \u201cblocks of blocks\u201d. Each superblock Hij has shape ni dim \u03c1i \u00d7 mj dim\u03c0j , and consists of subblocks of shape dim \u03c1i \u00d7 dim\u03c0j .\nThe computation graph for an equivariant convolution layer is constructed as follows. Given a catalogue of capsules \u03c1i and corresponding post-activation capsules Act\u03bd \u03c1i, we compute the induced representations \u03c0i = IndGH Act\u03bd \u03c1\ni and the bases for HomH(\u03c1i, \u03c0j) in an offline step. The bases are stored as matrices \u03c8ij of shape dim \u03c1i \u00b7 dim\u03c0j \u00d7 dim HomH(\u03c1i, \u03c0j). Then, given a list of input / output multiplicities ni,mj for the capsules, a parameter matrix \u0398ij of shape dim HomH(\u03c1\ni, \u03c0j) \u00d7 nimj is instantiated. The superblocks Hij are obtained by a matrix multiplication \u03c8ij\u0398ij plus reshaping to shape dim \u03c1i \u00b7dim\u03c0j \u00d7nimj . Once all superblocks are filled in, the matrix \u03a8 is reshaped from K \u2032 \u00d7Ks2 to K \u2032 \u00d7K \u00d7 s\u00d7 s and convolved with the input."}, {"heading": "2.9 USING STEERABLE CNNS IN PRACTICE", "text": "A full understanding of the theory of steerable CNNs requires some knowledge of group representation theory, but using steerable CNN technology is not much harder than using ordinary CNNs. Instead of choosing a number of channels for a given layer, one chooses a list of multiplicities mi for each capsule in a library of capsules provided by the developer. To preserve equivariance, the activation function applied to a capsule must be chosen from a list of admissible nonlinearities for that capsule (which sometimes includes all nonlinearities). Finally, one must respect the type system and only add identical capsules (e.g. in ResNets). These constraints can all be checked automatically."}, {"heading": "3 RELATED WORK", "text": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability.\nIn machine learning, equivariant kernels were studied by Reisert (2008); Skibbe (2013). In the context of neural networks, various authors have studied equivariant representations. Capsules were introduced in Hinton et al. (2011), and significantly improved by Tieleman (2014). A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias.\nInvariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al. (2016). All of these models, as well as scattering networks, implicitly use the regular representation: feature maps are (often implicitly) conceived of as functions onG, and the action ofG on the space of functions onG is known as the regular representation (Serre (1977), Appendix B). Our work is the first to consider other kinds of equivariance in the context of CNNs.\nThe idea of adding a type system to neural networks has been explored by Olah (2015); Balduzzi & Ghifary (2016). We have shown that a type system emerges naturally from the decomposition of a linear representation of a mathematical structure (a group, in our case) associated with the representation learned by a neural network."}, {"heading": "4 EXPERIMENTS", "text": "We implemented steerable CNNs in Chainer (Tokui et al., 2015) and performed experiments on the CIFAR10 dataset (Krizhevsky, 2009) to determine if steerability is a useful inductive bias, and to determine the relative merits of the various types of capsules. In order to run experiments faster, and to see how steerable CNNs perform in the small-data regime, we used only 2000 training samples for our initial experiments.\nAs a baseline, we used the competitive wide residual networks (ResNets) architecture (He et al., 2016a;b; Zagoruyko & Komodakis, 2016). We tuned the capacity of this network for the reduced dataset size and settled on a 20 layer architecture (three residual blocks per stage, with two layers each, for three stages with feature maps of size 32 \u00d7 32, 16 \u00d7 16 and 8 \u00d7 8, various widths). We compared the baseline architecture to various kinds of steerable CNN, obtained by replacing the convolution layers by steerable convolution layers. To make sure that differences in performance were not simply due to underfitting or overfitting, we tuned the width (number of channels, K) using a validation set. The rest of the training procedure is identical to Cohen & Welling (2016), and is fixed for all of our experiments.\nWe first tested steerable CNNs that consist entirely of a single kind of capsule. We found that architectures with only one type do not perform very well (roughly 30-40% error, vs. 30% for plain ResNets trained on 2k samples from CIFAR10), except for those that use the regular representation capsule (Appendix C), which outperforms standard CNNs (26.75% error). This is not too surprising, because many capsules are quite restrictive in the spatial patterns they can express. The strong performance of regular capsules is consistent with the results of Cohen & Welling (2016), and can be explained by the fact that the regular representation contains all other (irreducible and quotient) representations as subrepresentations, and can therefore learn arbitrary spatial patterns.\nWe then created networks that use a mix of the more successful kinds of capsules. After a few preliminary experiments, we settled on a residual network that uses one mix of capsules for the input and output layer of a residual block, and another for the intermediate layer. The first representation\nconsists of quotient capsules: regular, qm, qmr2, qmr3 (see Appendix C) followed by ReLUs. The second consists of irreducible capsules: A1, A2, B1, B2, E(2x) followed by CReLUs. On CIFAR10 with 2k labels, this architecture works better than standard ResNets and regular capsules at 24.48% error.\nWhen tested on CIFAR10 with 4k labels (table 2), the method comes close to the state of the art in semi-supervised methods, that use additional unlabelled data (Rasmus et al., 2015), and better than transfer learning approaches such as DCGAN which achieves 26.2% error (Radford et al., 2015). When tested on the full CIFAR10 and CIFAR100 dataset, the steerable CNN substantially outperforms the ResNet (He et al., 2016b) baseline and achieves state of the art results (improving over wide and dense nets (Zagoruyko & Komodakis, 2016; Huang et al., 2016))."}, {"heading": "5 CONCLUSION & FUTURE WORK", "text": "We have presented a theoretical framework for understanding steerable representations in convolutional networks, and have shown that steerability is a useful inductive bias that can improve model accuracy, particularly when little data is available. Our experiments show that a simple steerable architecture achieves state of the art results on CIFAR10 and CIFAR100, outperforming recent architectures such as wide and dense residual networks.\nThe mathematical connection between representation learning and representation theory that we have established improves our understanding of the inner workings of (equivariant) convolutional networks, revealing the humble CNN as an elegant geometrical computation engine. We expect that this new tool (representation theory), developed over more than a century by mathematicians and physicists, will greatly benefit future investigations in this area.\nFor concreteness, we have used the group of flips and rotations by multiples of 90 degrees as a running example throughout this paper. This group already has some nontrivial characteristics (such as non-commutativity), but it is still small and discrete. The theory of steerable CNNs, however, readily extends to the continuous setting. Evaluating steerable CNNs for large, continuous and high-dimensional groups is an important piece of future work.\nAnother direction for future work is learning the feature types, which may be easier in the continuous setting because (for non-compact groups) the irreps live in a continuous space where optimization may be possible. Beyond classification, steerable CNNs are likely to be useful in geometrical tasks such as action recognition, pose and motion estimation, and continuous control tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We kindly thank Kenta Oono, Shuang Wu, Thomas Kipf and the anonymous reviewers for their feedback and suggestions. This research was supported by Facebook, Google and NWO (grant number NAI.14.108)."}, {"heading": "APPENDIX A: INDUCTION", "text": "In this section we will show that a stack of feature maps produced by convolution with an Hequivariant filter bank transforms according to the induced representation. That is, we will derive eq. 5, repeated here for convenience:\n[\u03a8 ? [\u03c0l(tr)f ]] (x) = \u03c1l+1(r) [ [\u03a8 ? f ] ((tr)\u22121x) ] (14)\nIn the main text, we mentioned that x \u2208 Z2 can be interpreted as a point or as a translation. Here we make this difference explicit, by writing x \u2208 Z2 for a point and x\u0304 \u2208 G for a translation. (The operation \u00b7\u0304 defines a section of the projection map G\u2192 Z2 that forgets the non-translational part of the transformation (Kaniuth & Taylor, 2013)).\nWith this notation, the convolution is defined as:\n[\u03a8 ? f ] (x) = \u03a8\u03c0(x\u0304\u22121)f (15)\nAlthough the induced representation can be described in a more general setting, we will use an explicit matrix representation of G to make it easier to check our computations. A general element of G is written as:\ng = tr = [ I T 0 1 ] [ R 0 0 1 ] = [ R T 0 1 ] (16)\nWhere R is the matrix representation of r (e.g. a 2 \u00d7 2 rotation / reflection matrix), and T is a translation vector. The section we use is:\nx = [ I x 0 1 ] (17)\nFinally, we will distinguish the action of G on itself, written gh for g, h \u2208 G (implemented as matrix-matrix multiplication) and its action on Z2, written g \u00b7x for g \u2208 G and x \u2208 Z2 (implemented as matrix-vector multiplication by adding a homogeneous coordinate to x).\nTo keep notation uncluttered, we will write \u03c0 = \u03c0l and \u03c1 = \u03c1l+1. In full detail, the derivation of the transformation law for the feature space induced by \u03c1 proceeds as follows:\n[\u03a8 ? [\u03c0(tr)f ]] (x) = \u03a8\u03c0(x\u0304\u22121)\u03c0(tr)f\n= \u03a8\u03c0(x\u0304\u22121tr)f\n= \u03a8\u03c0(rr\u22121x\u0304\u22121tr)f\n= \u03a8\u03c0(r)\u03c0(r\u22121x\u0304\u22121tr)f\n= \u03c1(r)\u03a8\u03c0(r\u22121x\u0304\u22121tr)f\n= \u03c1(r)\u03a8\u03c0((r\u22121t\u22121x\u0304r)\u22121)f = \u03c1(r)\u03a8\u03c0 ( (tr)\u22121 \u00b7 x) \u22121) f\n= \u03c1(r)[\u03a8 ? f ]((tr)\u22121 \u00b7 x)\n(18)\nThe last line is the result shown in the paper. The justification of each step is:\n1. Definition of ?\n2. \u03c0 is a homomorphism / group representation\n3. rr\u22121 is the identity, so can always multiply by it 4. \u03c0 is a homomorphism / group representation 5. \u03a8 \u2208 HomH(\u03c0, \u03c1) is equivariant to r \u2208 H . 6. Invert twice. 7. (tr)\u22121 \u00b7 x = r\u22121t\u22121xr can be checked by multiplying the matrices / vectors. 8. Definition of ?\nThe derivation above is somewhat involved and messy, so the reader may prefer to think geometrically (using the figures in the paper) instead of algebraically. This complexity is an artifact of the lack of abstraction in our presentation. The induced representation is really a very natural object to consider (abstractly, it is the \u201cadjoint functor\u201d to the restriction functor. A more abstract treatment of the induced representation can be found in Serre (1977); Mackey (1952); Reeder (2014). A treatment that is close to our own, but more general is the \u201calternate description\u201d found on page 49 of Kaniuth & Taylor (2013)."}, {"heading": "APPENDIX B: RELATION TO GROUP EQUIVARIANT CNNS", "text": "In this section we show that the recently introduced Group Equivariant Convolutional Networks (GCNNs, Cohen & Welling (2016)) are a special kind of steerable CNN. Specifically, a G-CNN is a steerable CNN with regular capsules.\nIn a G-CNN, the feature maps (except those of the input) are thought of as functions f : G \u2192 RK instead of functions on the plane f : Z2 \u2192 RK , as we do here. It is shown that the feature maps transform according to \u03c0(g)f(h) = f(g\u22121h). (19) This defines a linear representation of G known as the regular representation. It is easy to see that the regular representation is naturally realized by permutation matrices. Furthermore, it is known that the regular representation of G is induced by the regular representation of H . The latter is defined in Appendix C, and is what we refer to as \u201cregular capsules\u201d in the paper."}, {"heading": "APPENDIX C: REGULAR AND QUOTIENT FEATURES", "text": "Let H be a finite group. A subgroup of H is a subset that is also itself a group (i.e. closed under composition and inverses). The (left) coset of a subgroup K in H are the sets hK = {hk|k \u2208 K}. The cosets are disjoint and jointly cover the whole group H (i.e. they partition H). The set of all cosets of K in H is denoted H/K, and is also called the quotient of H by K.\nThe coset space caries a natural left action by H . Let a, b \u2208 H , then a \u00b7 bK = (ab)K. This action translates into an action on the space of functions on H/K. Let Q denote the space of functions f : H/K \u2192 R. Then we have the following representation of H:\n\u03c1(a)f(bK) = f(a\u22121 \u00b7 bK). (20)\nThe function f attaches a value to every coset. The H-action permutes these values, because it permutes the cosets. Hence, \u03c1 can be realized by permutation matrices. For small groups the explicit computations can easily be done by hand, while for large groups this task can be automated.\nIn this way, we get one permutation representation for each subgroup K of H . In particular, for the subgroup K = {e} (the trivial subgroup containing only the identity e), we have H/K \u223c= H . The representation in the space of functions on H is known as the \u201cregular representation\u201d. Using such regular representations in a steerable CNN is equivalent to using the group convolutions introduced in Cohen & Welling (2016), so steerable CNNs are a strict generalization of G-CNNs. At the other extreme, we take K = H , which gives the quotient H/K \u223c= {e}, the trivial group, which gives the trivial representation A1.\nFor the roto-reflection group H = D4, we have the following subgroups and associated quotient features\nSubgroup K quotient feature name dimensionality {e} regular 8 {e,m} qm 4 {e,mr} qmr 4 {e,mr2} qmr2 4 {e,mr3} qmr3 4 {e, r2} r2 4\n{e, r, r2, r3} r 2 e, r2,m,mr2 r2m 2 e, r2,mr,mr3 r2mr 2\nH A1 1"}], "references": [{"title": "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "Technical Report 001, MIT Center for Brains, Minds and Machines,", "citeRegEx": "Anselmi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2014}, {"title": "Strongly-Typed Recurrent Neural Networks", "author": ["D. Balduzzi", "M. Ghifary"], "venue": "Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Balduzzi and Ghifary.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and Ghifary.", "year": 2016}, {"title": "Learning Transformation Groups and their Invariants", "author": ["T. Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q2013\\E", "shortCiteRegEx": "Cohen.", "year": 2013}, {"title": "Learning the Irreducible Representations of Commutative Lie Groups", "author": ["T. Cohen", "M. Welling"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2014}, {"title": "Group equivariant convolutional networks", "author": ["T.S. Cohen", "M. Welling"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Welling.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2016}, {"title": "Rotation-invariant convolutional neural networks for galaxy morphology prediction", "author": ["S. Dieleman", "K.W. Willett", "J. Dambre"], "venue": "Monthly Notices of the Royal Astronomical Society,", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks", "author": ["S. Dieleman", "J. De Fauw", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Dieleman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2016}, {"title": "A Course in Abstract Harmonic Analysis", "author": ["G.B. Folland"], "venue": "CRC Press,", "citeRegEx": "Folland.,? \\Q1995\\E", "shortCiteRegEx": "Folland.", "year": 1995}, {"title": "The design and use of steerable filters", "author": ["W.T. Freeman", "E.H. Adelson"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Freeman and Adelson.,? \\Q1991\\E", "shortCiteRegEx": "Freeman and Adelson.", "year": 1991}, {"title": "Deep Symmetry Networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gens and Domingos.,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2014}, {"title": "Overcomplete Steerable Pyramid Filters and Rotation Invariance", "author": ["H. Greenspan", "S. Belongie", "R. Goodman", "P. Perona"], "venue": "Proceedings of the Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Greenspan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Greenspan et al\\.", "year": 1994}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "ICANN-11: International Conference on Artificial Neural Networks, Helsinki,", "citeRegEx": "Hinton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Densely Connected Convolutional Networks. 2016", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "URL http: //arxiv.org/abs/1608.06993", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Structured Receptive Fields in CNNs", "author": ["J.-H. Jacobsen", "J. van Gemert", "Z. Lou", "A.W. Smeulders"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Jacobsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jacobsen et al\\.", "year": 2016}, {"title": "Group-Theoretical Methods in Image Understanding", "author": ["K. Kanatani"], "venue": null, "citeRegEx": "Kanatani.,? \\Q1990\\E", "shortCiteRegEx": "Kanatani.", "year": 1990}, {"title": "Locally Scale-invariant Convolutional Neural Network", "author": ["A. Kanazawa", "A. Sharma", "D. Jacobs"], "venue": "Deep Learning and Representation Learning Workshop: NIPS, pp", "citeRegEx": "Kanazawa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kanazawa et al\\.", "year": 2014}, {"title": "Induced Representations of Locally Compact Groups", "author": ["E. Kaniuth", "K.F. Taylor"], "venue": null, "citeRegEx": "Kaniuth and Taylor.,? \\Q2013\\E", "shortCiteRegEx": "Kaniuth and Taylor.", "year": 2013}, {"title": "Receptive field families", "author": ["J.J. Koenderink", "a. J. Van Doorn"], "venue": "Biological Cybernetics,", "citeRegEx": "Koenderink and Doorn.,? \\Q1990\\E", "shortCiteRegEx": "Koenderink and Doorn.", "year": 1990}, {"title": "A Unified Theory for Steerable and Quadrature Filters. Communications in Computer and Information Science, 4 CCIS:201\u2013214", "author": ["K. Krajsek", "R. Mester"], "venue": null, "citeRegEx": "Krajsek and Mester.,? \\Q2007\\E", "shortCiteRegEx": "Krajsek and Mester.", "year": 2007}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lenc and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Lenc and Vedaldi.", "year": 2015}, {"title": "Group-theoretical model of feature extraction", "author": ["R. Lenz"], "venue": "Journal of the Optical Society of America A (Optics and Image Science),", "citeRegEx": "Lenz.,? \\Q1989\\E", "shortCiteRegEx": "Lenz.", "year": 1989}, {"title": "Induced Representations of Locally Compact Groups I", "author": ["G.W. Mackey"], "venue": "Annals of Mathematics,", "citeRegEx": "Mackey.,? \\Q1952\\E", "shortCiteRegEx": "Mackey.", "year": 1952}, {"title": "Induced Representations of Locally Compact Groups II. The Frobenius Reciprocity Theorem", "author": ["G.W. Mackey"], "venue": "Annals of Mathematics,", "citeRegEx": "Mackey.,? \\Q1953\\E", "shortCiteRegEx": "Mackey.", "year": 1953}, {"title": "Induced Representations of Groups and Quantum Mechanics", "author": ["G.W. Mackey"], "venue": "W.A. Benjamin Inc.,", "citeRegEx": "Mackey.,? \\Q1968\\E", "shortCiteRegEx": "Mackey.", "year": 1968}, {"title": "Group Invariant Scattering", "author": ["S. Mallat"], "venue": "Communications in Pure and Applied Mathematics,", "citeRegEx": "Mallat.,? \\Q2012\\E", "shortCiteRegEx": "Mallat.", "year": 2012}, {"title": "Learning rotation invariant convolutional filters for texture classification", "author": ["D. Marcos", "M. Volpi", "D. Tuia"], "venue": null, "citeRegEx": "Marcos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marcos et al\\.", "year": 2016}, {"title": "Neural Networks, Types, and Functional Programming", "author": ["C. Olah"], "venue": "URL https://colah.github", "citeRegEx": "Olah.,? \\Q2015\\E", "shortCiteRegEx": "Olah.", "year": 2015}, {"title": "Deep Roto-Translation Scattering for Object Classification", "author": ["E. Oyallon", "S. Mallat"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Oyallon and Mallat.,? \\Q2015\\E", "shortCiteRegEx": "Oyallon and Mallat.", "year": 2015}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv, pp", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Semi-supervised learning with Ladder Networks", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Notes on representations of finite groups, 2014", "author": ["M. Reeder"], "venue": "Group Integration Techniques in Pattern Analysis: A Kernel View. PhD thesis, Albert-LudwigsUniversity,", "citeRegEx": "Reeder.,? \\Q2008\\E", "shortCiteRegEx": "Reeder.", "year": 2008}, {"title": "Linear Representations of Finite Groups", "author": ["J.-P. Serre"], "venue": null, "citeRegEx": "Serre.,? \\Q1977\\E", "shortCiteRegEx": "Serre.", "year": 1977}, {"title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "author": ["W. Shang", "K. Sohn", "D. Almeida", "H. Lee"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination", "author": ["L. Sifre", "S. Mallat"], "venue": "IEEE conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sifre and Mallat.,? \\Q2013\\E", "shortCiteRegEx": "Sifre and Mallat.", "year": 2013}, {"title": "The steerable pyramid: a flexible architecture for multi-scale derivative computation", "author": ["E. Simoncelli", "W. Freeman"], "venue": "Proceedings of the International Conference on Image Processing,", "citeRegEx": "Simoncelli and Freeman.,? \\Q1995\\E", "shortCiteRegEx": "Simoncelli and Freeman.", "year": 1995}, {"title": "Spherical Tensor Algebra for Biomedical Image Analysis", "author": ["H. Skibbe"], "venue": "PhD thesis, Albert-Ludwigs-Universitat Freiburg im Breisgau,", "citeRegEx": "Skibbe.,? \\Q2013\\E", "shortCiteRegEx": "Skibbe.", "year": 2013}, {"title": "Noncommutative Harmonic Analysis", "author": ["M.E. Taylor"], "venue": "American Mathematical Society,", "citeRegEx": "Taylor.,? \\Q1986\\E", "shortCiteRegEx": "Taylor.", "year": 1986}, {"title": "Theory and Applications of Steerable Functions", "author": ["P.C.-S. Teo"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Teo.,? \\Q1998\\E", "shortCiteRegEx": "Teo.", "year": 1998}, {"title": "Optimizing Neural Networks that Generate Images", "author": ["T. Tieleman"], "venue": "PhD thesis,", "citeRegEx": "Tieleman.,? \\Q2014\\E", "shortCiteRegEx": "Tieleman.", "year": 2014}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "The classical groups: their invariants and representations", "author": ["H. Weyl"], "venue": null, "citeRegEx": "Weyl.,? \\Q1939\\E", "shortCiteRegEx": "Weyl.", "year": 1939}], "referenceMentions": [{"referenceID": 13, "context": "In deep learning, however, intermediate layers should not be invariant, because the relative pose of local features must be preserved for further layers (Cohen & Welling, 2016; Hinton et al., 2011).", "startOffset": 153, "endOffset": 197}, {"referenceID": 6, "context": "Previous work has shown that equivariant CNNs yield state of the art results on classification tasks (Cohen & Welling, 2016; Dieleman et al., 2016), even though they only enforce equivariance to small groups of transformations like rotations by multiples of 90 degrees.", "startOffset": 101, "endOffset": 147}, {"referenceID": 2, "context": "In deep learning, however, intermediate layers should not be invariant, because the relative pose of local features must be preserved for further layers (Cohen & Welling, 2016; Hinton et al., 2011). Thus, one is led to the idea of equivariance: a network is equivariant if the representations it produces transform in a predictable way under transformations of the input. In other words, equivariant networks produce representations that are steerable. Steerability makes it possible to apply filters not just in every position (as in a standard convolution layer), but in every pose, thus allowing for increased parameter sharing. Previous work has shown that equivariant CNNs yield state of the art results on classification tasks (Cohen & Welling, 2016; Dieleman et al., 2016), even though they only enforce equivariance to small groups of transformations like rotations by multiples of 90 degrees. Learning representations that are equivariant to larger groups is likely to result in further gains, but the computational cost of current methods scales linearly with the size of the group, making this impractical. In this paper we present the general theory of steerable CNNs, which covers previous approaches but also shows how the computational cost can be decoupled from the size of the symmetry group, thus paving the way for future scaling. To better understand the structure of steerable representations, we analyze them mathematically. We show that any steerable representation is a composition of low-dimensional elementary feature types. Each elementary feature can be steered independently of the others, and captures a distinct characteristic of the input that has an invariant or \u201cobjective\u201d meaning. This doctrine of \u201cobserverindependent quantities\u201d was put forward by (Weyl, 1939, ch. 1.4) and is used throughout physics. It has been applied to vision and representation learning by Kanatani (1990); Cohen (2013).", "startOffset": 154, "endOffset": 1917}, {"referenceID": 2, "context": "In deep learning, however, intermediate layers should not be invariant, because the relative pose of local features must be preserved for further layers (Cohen & Welling, 2016; Hinton et al., 2011). Thus, one is led to the idea of equivariance: a network is equivariant if the representations it produces transform in a predictable way under transformations of the input. In other words, equivariant networks produce representations that are steerable. Steerability makes it possible to apply filters not just in every position (as in a standard convolution layer), but in every pose, thus allowing for increased parameter sharing. Previous work has shown that equivariant CNNs yield state of the art results on classification tasks (Cohen & Welling, 2016; Dieleman et al., 2016), even though they only enforce equivariance to small groups of transformations like rotations by multiples of 90 degrees. Learning representations that are equivariant to larger groups is likely to result in further gains, but the computational cost of current methods scales linearly with the size of the group, making this impractical. In this paper we present the general theory of steerable CNNs, which covers previous approaches but also shows how the computational cost can be decoupled from the size of the symmetry group, thus paving the way for future scaling. To better understand the structure of steerable representations, we analyze them mathematically. We show that any steerable representation is a composition of low-dimensional elementary feature types. Each elementary feature can be steered independently of the others, and captures a distinct characteristic of the input that has an invariant or \u201cobjective\u201d meaning. This doctrine of \u201cobserverindependent quantities\u201d was put forward by (Weyl, 1939, ch. 1.4) and is used throughout physics. It has been applied to vision and representation learning by Kanatani (1990); Cohen (2013).", "startOffset": 154, "endOffset": 1931}, {"referenceID": 34, "context": "A lot is known about group representations (Serre, 1977), and we will make extensive use of the theory, explaining the relevant concepts as needed.", "startOffset": 43, "endOffset": 56}, {"referenceID": 34, "context": "Equivariant maps are also sometimes called intertwiners (Serre, 1977).", "startOffset": 56, "endOffset": 69}, {"referenceID": 24, "context": "This happens through a natural and important construction known as the induced representation (Mackey, 1952; 1953; 1968; Serre, 1977; Taylor, 1986; Folland, 1995; Kaniuth & Taylor, 2013).", "startOffset": 94, "endOffset": 186}, {"referenceID": 34, "context": "This happens through a natural and important construction known as the induced representation (Mackey, 1952; 1953; 1968; Serre, 1977; Taylor, 1986; Folland, 1995; Kaniuth & Taylor, 2013).", "startOffset": 94, "endOffset": 186}, {"referenceID": 39, "context": "This happens through a natural and important construction known as the induced representation (Mackey, 1952; 1953; 1968; Serre, 1977; Taylor, 1986; Folland, 1995; Kaniuth & Taylor, 2013).", "startOffset": 94, "endOffset": 186}, {"referenceID": 7, "context": "This happens through a natural and important construction known as the induced representation (Mackey, 1952; 1953; 1968; Serre, 1977; Taylor, 1986; Folland, 1995; Kaniuth & Taylor, 2013).", "startOffset": 94, "endOffset": 186}, {"referenceID": 34, "context": "The reader wishing to go deeper may consult chapters 1 and 2 of the excellent book by Serre (1977). Recall that a group representation is a set of invertible linear maps \u03c1(g) : R \u2192 R satisfying \u03c1(gh) = \u03c1(g)\u03c1(h) for all elements g, h \u2208 H .", "startOffset": 86, "endOffset": 99}, {"referenceID": 33, "context": "By choosing the type of \u03c1, we also determine the type of \u03c0 = IndH \u03c1 (restricted toH), but what is it? Explicit formulas exist (Reeder (2014); Serre (1977)) but are rather complicated, so we will present a simple computational procedure that can be used to determine the type of any representation.", "startOffset": 127, "endOffset": 141}, {"referenceID": 33, "context": "By choosing the type of \u03c1, we also determine the type of \u03c0 = IndH \u03c1 (restricted toH), but what is it? Explicit formulas exist (Reeder (2014); Serre (1977)) but are rather complicated, so we will present a simple computational procedure that can be used to determine the type of any representation.", "startOffset": 127, "endOffset": 155}, {"referenceID": 12, "context": "Like the capsules of Hinton et al. (2011), our capsules encode the pose of a pattern in the input, and consist of a number of units (dimensions) that do not get mixed with the units of other capsules by symmetries.", "startOffset": 21, "endOffset": 42}, {"referenceID": 35, "context": "If \u03c1 is realized by a signed permutation matrix, then CReLU(\u03b1) = (ReLU(\u03b1),ReLU(\u2212\u03b1)) introduced by Shang et al. (2016), or any concatenated nonlinearity \u03bd\u2032(\u03b1) = (\u03bd(\u03b1), \u03bd(\u2212\u03b1)), will be admissible.", "startOffset": 98, "endOffset": 118}, {"referenceID": 10, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995).", "startOffset": 96, "endOffset": 173}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007).", "startOffset": 122, "endOffset": 296}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007).", "startOffset": 122, "endOffset": 327}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007).", "startOffset": 122, "endOffset": 339}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability.", "startOffset": 122, "endOffset": 364}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability. In machine learning, equivariant kernels were studied by Reisert (2008); Skibbe (2013).", "startOffset": 122, "endOffset": 583}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability. In machine learning, equivariant kernels were studied by Reisert (2008); Skibbe (2013). In the context of neural networks, various authors have studied equivariant representations.", "startOffset": 122, "endOffset": 598}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability. In machine learning, equivariant kernels were studied by Reisert (2008); Skibbe (2013). In the context of neural networks, various authors have studied equivariant representations. Capsules were introduced in Hinton et al. (2011), and significantly improved by Tieleman (2014).", "startOffset": 122, "endOffset": 741}, {"referenceID": 6, "context": "Steerable filters were first studied for applications in signal processing and low-level vision (Freeman & Adelson, 1991; Greenspan et al., 1994; Simoncelli & Freeman, 1995). More or less explicit connections between steerability and group representation theory have been observed by Lenz (1989); Koenderink & Van Doorn (1990); Teo (1998); Krajsek & Mester (2007). As we have tried to demonstrate in this paper, representation theory is indeed the natural mathematical framework in which to study steerability. In machine learning, equivariant kernels were studied by Reisert (2008); Skibbe (2013). In the context of neural networks, various authors have studied equivariant representations. Capsules were introduced in Hinton et al. (2011), and significantly improved by Tieleman (2014). A theoretical account of equivariant representation learning in the brain is given by Anselmi et al.", "startOffset": 122, "endOffset": 788}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group.", "startOffset": 86, "endOffset": 108}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group.", "startOffset": 86, "endOffset": 189}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group.", "startOffset": 86, "endOffset": 238}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group.", "startOffset": 86, "endOffset": 263}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights.", "startOffset": 86, "endOffset": 318}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias.", "startOffset": 86, "endOffset": 441}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al.", "startOffset": 86, "endOffset": 671}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al.", "startOffset": 86, "endOffset": 695}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al.", "startOffset": 86, "endOffset": 749}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al. (2016). All of these models, as well as scattering networks, implicitly use the regular representation: feature maps are (often implicitly) conceived of as functions onG, and the action ofG on the space of functions onG is known as the regular representation (Serre (1977), Appendix B).", "startOffset": 86, "endOffset": 771}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al. (2016). All of these models, as well as scattering networks, implicitly use the regular representation: feature maps are (often implicitly) conceived of as functions onG, and the action ofG on the space of functions onG is known as the regular representation (Serre (1977), Appendix B).", "startOffset": 86, "endOffset": 1037}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al. (2016). All of these models, as well as scattering networks, implicitly use the regular representation: feature maps are (often implicitly) conceived of as functions onG, and the action ofG on the space of functions onG is known as the regular representation (Serre (1977), Appendix B). Our work is the first to consider other kinds of equivariance in the context of CNNs. The idea of adding a type system to neural networks has been explored by Olah (2015); Balduzzi & Ghifary (2016).", "startOffset": 86, "endOffset": 1222}, {"referenceID": 0, "context": "A theoretical account of equivariant representation learning in the brain is given by Anselmi et al. (2014). Group equivariant scattering networks were defined and studied by Mallat (2012) for compact groups, and by Sifre & Mallat (2013); Oyallon & Mallat (2015) for the roto-translation group. Jacobsen et al. (2016) describe a network that uses a fixed set of (possibly steerable) basis filters with learned weights. Lenc & Vedaldi (2015) showed empirically that convolutional networks tend to learn equivariant representations, which suggests that equivariance could be a good inductive bias. Invariant and equivariant CNNs have been studied by Gens & Domingos (2014); Kanazawa et al. (2014); Dieleman et al. (2015; 2016); Cohen & Welling (2016); Marcos et al. (2016). All of these models, as well as scattering networks, implicitly use the regular representation: feature maps are (often implicitly) conceived of as functions onG, and the action ofG on the space of functions onG is known as the regular representation (Serre (1977), Appendix B). Our work is the first to consider other kinds of equivariance in the context of CNNs. The idea of adding a type system to neural networks has been explored by Olah (2015); Balduzzi & Ghifary (2016). We have shown that a type system emerges naturally from the decomposition of a linear representation of a mathematical structure (a group, in our case) associated with the representation learned by a neural network.", "startOffset": 86, "endOffset": 1249}, {"referenceID": 42, "context": "We implemented steerable CNNs in Chainer (Tokui et al., 2015) and performed experiments on the CIFAR10 dataset (Krizhevsky, 2009) to determine if steerability is a useful inductive bias, and to determine the relative merits of the various types of capsules.", "startOffset": 41, "endOffset": 61}, {"referenceID": 21, "context": ", 2015) and performed experiments on the CIFAR10 dataset (Krizhevsky, 2009) to determine if steerability is a useful inductive bias, and to determine the relative merits of the various types of capsules.", "startOffset": 57, "endOffset": 75}, {"referenceID": 2, "context": "The rest of the training procedure is identical to Cohen & Welling (2016), and is fixed for all of our experiments.", "startOffset": 51, "endOffset": 74}, {"referenceID": 2, "context": "The rest of the training procedure is identical to Cohen & Welling (2016), and is fixed for all of our experiments. We first tested steerable CNNs that consist entirely of a single kind of capsule. We found that architectures with only one type do not perform very well (roughly 30-40% error, vs. 30% for plain ResNets trained on 2k samples from CIFAR10), except for those that use the regular representation capsule (Appendix C), which outperforms standard CNNs (26.75% error). This is not too surprising, because many capsules are quite restrictive in the spatial patterns they can express. The strong performance of regular capsules is consistent with the results of Cohen & Welling (2016), and can be explained by the fact that the regular representation contains all other (irreducible and quotient) representations as subrepresentations, and can therefore learn arbitrary spatial patterns.", "startOffset": 51, "endOffset": 693}, {"referenceID": 32, "context": "When tested on CIFAR10 with 4k labels (table 2), the method comes close to the state of the art in semi-supervised methods, that use additional unlabelled data (Rasmus et al., 2015), and better than transfer learning approaches such as DCGAN which achieves 26.", "startOffset": 160, "endOffset": 181}, {"referenceID": 14, "context": ", 2016b) baseline and achieves state of the art results (improving over wide and dense nets (Zagoruyko & Komodakis, 2016; Huang et al., 2016)).", "startOffset": 92, "endOffset": 141}], "year": 2017, "abstractText": "It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.", "creator": "TeX"}, "id": "ICLR_2017_47"}