{"name": "ICLR_2017_481.pdf", "metadata": {"source": "CRF", "title": "NEW LEARNING APPROACH BY GENETIC ALGORITHM IN A CONVOLUTIONAL NEURAL NETWORK FOR PATTERN RECOGNITION", "authors": ["Majid Mohammadi", "Mohammad Ali Mehrolhassani"], "emails": ["mohammadi@uk.ac.ir", "alimehrolhassani@yahoo.com", "6@2828", "6@1414", "16@1010", "16@55"], "sections": [{"heading": null, "text": "backpropagation algorithm and calculation of derivations of error, our innovative proposal refers to engaging TICA 2 filters and NSGA-II 3 genetic\nalgorithms to train the LeNet-5 CNN network. Consequently, genetic algorithm updates the weights of LeNet-5 CNN network similar to chromosome update. In our approach the weights of LeNet-5 are obtained in two stages. The first is pretraining and the second is fine-tuning. As a result, our approach impacts in learning task."}, {"heading": "1 Background", "text": "The CNN network presented by Fukushima (Fukushima, 1975; Fukushima, 1980; Fukushima, 1986; Fukushima, 1989; Imagawa, 1993) in 1975 to solve handwritten digit recognition problems, called it Neocognitron. Originally the CNN inspired by the works of Hubel and Wiesel (Wiese, 1962) on the neurons of the cat\u2019s visual cortex. The LeCun in (Y. LeCun, 1990) has implemented the first CNN, trained by online Backpropagation, with an extreme accurate recognition process (high accuracy percentage).\nBriefly, in the paper, using the TICA filters and NSGA-II algorithms caused to be capable of training a type of CNNs called LeNet-5 by NSGA-II algorithm in two stages: pre-training and fine-tuning on the tiny pack of handwritten digits 4 (a distinct pack encompasses 50 samples from MINST dataset).\n1 Convolutional Neural Network 2 Topographic independent component analysis 3 Non dominated Sorting Genetic Algorithm II 4 The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. It contents a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."}, {"heading": "1.1 The LeNet-5 Model", "text": "Figure 1 illustrates the principal architecture of LeNet-5 model which had been trained with LeCun`s Online BP 1 Algorithm.\nThe CNN extracts the features in hierarchy and creates a spectrum of input pattern upon the output layer; the obtain spectrums help to categorize the input facts. The learning method of the CNN concentrates upon extracting features of the pattern automatically.\nThe LeNet-5 consists of seven layers exclude the input layer (Duffner, 2007). The size of input dimension set to 32\u00d732 pixels. The first five layers, C1, S2, C3, S4, and C5 determine convolution and sub-sampling layers.\nThe small window that called receptive filed refers to a dimension of 5\u00d75 window for convolution layers and a sub-sampling ratio factor of 2. In figure 2 shows two preliminary layers associated with the input domain called retina. In general, at the end of each convolution layer the subsample layer is attached.\n1 Online back propagation"}, {"heading": "1.2 TICA Filters", "text": "Topography independent component analysis is a 2D map component that has been organized with adjacent component to extract similar features (figure 4). The component extracts visual features from tiny patches of natural images. In fact, the model has minimized the correlations of energies with a pertinent object function applied. The model can be a CNN (K. Kavukcuoglu, 2009) or can be an optimization algorithm (Koray, 2008). Figure 4 shows the TICA filters with the size of 16\u00d716 that after 5000 iterations obtains them."}, {"heading": "1.3 NSGA-II Algorithm", "text": "One way for obtaining the answers of equivalent to minimize or maximize exploits the evolutionary algorithms; thus, the evolutionary algorithms to be applied instead of derivative calculations. In the implemented algorithms, the answers instead of gene in chromosome and the chromosome called individual and finally the individual formed a population.\nThe population based on natural selection competes with others for searching. The algorithm performs on population some operation etc. crossover, mutation, selection; the population size, mutation rate, crossover rate and selection strategy conducts the outcome.\nThe times of running the algorithm called iteration. In the iteration evaluated the population by a function so-called fitness function. Several policies exist to stopping the algorithm: finding the appropriate answer or answers, specified iteration, converge the evaluating of individuals.\nIn fact, the population to be generated randomly in initializing section and evolved in iterations by randomly operators. Some individuals to be chose for mutation of genes and some of them to be chose to mixture with others. At the end of iteration selects to new population in next iteration with a specific policy from some of individuals. Researcher has implemented numerous evolutionary algorithms as the NSGA-II algorithm.\nSrinivas and Deb presented NSGA method in 1993 (Deb, 1995). The NSGA algorithm implements for MOP target purposes with a high performance. The regenerated version, NSGA-II presented in 2002 (Amrit Pratap, 2002) which use the number of domination for each individual by other individual. Constantly, for pertinent performance, the mentioned techniques help to acquire Pareto-Optimal 1 points with reduced complex computing by exerting non-dominated\n1 When in multi-objective optimization follows the set of optimum solutions which have no superiority with themselves, the set of answers called Pareto1-Optimal which is the name of economist nominated Vilferedo Pareto.\nindividuals and calculating of crowding point's distance individuals. Figure 5 illustrates the NSGA-II algorithm.\nTotally, the sorting functional NSGA-II algorithm has been improved by storing the frequency of each dominated individual with the others.\nThe NSGA-II has three crucial issues: quick sorting for non-dominated individuals; computation of crowded distance in entire Pareto-Front; method of individual selection.\nFor quick sorting, the entire individuals must be compared with themselves and pulled the nondominated out of them with assigning the rank 1. The task is repeated until the entire population allocates a rank from 1 to N to sort the individuals. Figure 6 illustrates the Pareto-Front issue in three parts. The part (a) shows the situation C member with the two lines` perpendicular paralleled with axes on C, as the adjacent of the specified individual C has posed over the district of top left, bottom left, top right and bottom right. The bottom left members dominated by C, If would be assumed the objective functions f1 and f2 maximized, the top right area dominated C member. The district of bottom right and top right have been ignored to determine for dominating. The matlab code declares in mathworks web site 1 .\n1 https://www.mathworks.com programmed by S. Mostapha Kalami Heris\nThe Pareto-Front refers to compute additional measure inside of population ranking called crowded distance. Therefore, the distance of chromosomes is computed and stored with respective adjacent in Pareto-Front by using Euclidean distance for entire members. Totally, the population rank and the crowded distance prepare the fitness value for entire members whilst the member with the low rank could be assigned to the low fitness value.\nThe method of individual selection determines the computation of ranking of non-dominated and crowded distance of individuals. Thus, two values acquired for ranking and computing crowded distance for comparing two members of population. The algorithm selects the member with lower priority. Competition between members of the population with identical rank caused to win the one which has the higher crowded distance. The later generation is induced by merging the winner parents and offspring in competition and after that the population`s size sets the primary length."}, {"heading": "2 The proposal", "text": "The proposal focused on genetic or heuristic algorithms to determine the weights of LeNet-5; without using the Backpropagation and the derivation. For better following our proposal, the steps listed below:\n1) The F6 layer has been eliminated in LeNet-5 model; consequently, the length of chromosome in GA reduces to 51046 variables. 2) The train and test validation function have merged and 50 samples in 10 classes (0, 1, 2, 3, 4, 5, 6, 7, 8, and 9) selected from MINST dataset randomly. The average of errors for\nthe entire 50 samples leads the LeNet-5 to learning.\n3) All the biases in LeNet-5 set to zero. The number of individual population or particles vector size sets to 100. 4) We have two stages with specified respective parameters (table 1) and techniques in the NSGA-II algorithm usage (figure 7). The first stage calls Pre-training and the second\nVariable Pre-training stage Fine-tuning Stage Iteration(s) 1000 1000 Number of population 100 27 Chromosome Lengths 51046 51046 Variable Minimum -0.9000000000000000 -4.0000000000000004 Variable Maximum 0.9000000000000000 6.0000000000000001 Population Crossover 0.2 0.1 Population Mutation 0.3 0.4 Ratio Mutation Genes 0.5 0.00025\n\u221a \u2211 ( )\nWhere denotes value of observation in model, denotes value of output desire in i location.\n(\n)\nWhere M denotes sum of number of templates become correct, N denotes number of all the templates.\n1 Root means square error 2 Misclassification rate 3 Multi-objective optimization"}, {"heading": "2.1 Pre-training stage", "text": "The parts of chromosome that assigns to the layer C1 and C3 in the LeNet-5 should not to be modified at the first stage; at the beginning of the NSGA-II algorithm, C1 and C3 initialized by 160 TICA filters with dimension of 16\u00d716 in section 1.2 randomly. The filters should be resized to dimension of 5\u00d75 to fit the LeNet-5`s weights matrix in respective layers (figure 9). Figure 10 illustrates the pre-training.\nalgorithm and weights of layers in LeNet-5 Neural network at Pre-training stage of initializing.\nThe pre-training algorithm declares in algorithm1 with pseudocode in matlab.\nAlgorithm 1 Pre-training 1: Define the parameters of algorithm matched with table 1 and the fitness function equal to x2 and y2 in Cost function 2: Generate initial population`s chromosomes randomly (POP) from the 160 TICA filter dimension of 5\u00d75 3: [TestData TargetData]=SelectSamples(); % Select 50 samples from MINST dataset 3: for i:=1:npop 4: chromosomtoCNN(pop(i)); % Replace the vector of the chromosome into the CNN neural network 5: [RMSE MCR]=PerformanceCNN(TestData,TargetData); 6: pop(i).Cost=Cost(RMSE MCR); % Calculate the cost function for chromosome 7: end % Initializing population 8: for iteration=1:nIteration 9: Create popC for Crossover population 10: for iCrossover=1: nCrossover 11: i1=BinaryTournamentSelection(pop); % Select an individual from population 12: i2=BinaryTournamentSelection(pop); % Select an individual from population 13:[popC(iCrossover,1).Position\npopC(iCrossover,2).Position]=Crossover(pop(i1).Position,pop(i2).Position,VarRange); 14: popC(iCrossover,1).Position(1:1684)=pop(i1).PrimaryPosition(1:1684); % Replace the Choromosome TICA filter 15: chromosomtoCNN(popC(iCrossover)); 16: [RMSE MCR]=PerformanceCNN(TestData,TargetData); 17: popC(iCrossover,1).Cost=Cost(RMSE MCR); % Calculate the cost function for chromosome 18: popC(iCrossover,2).Position(1:1684)=pop(i2).PrimaryPosition(1:1684); % Replace the Choromosome TICA filter 19: chromosomtoCNN(pop(iCrossover)); 20: [RMSE MCR]=PerformanceCNN(TestData,TargetData); 21: popC(iCrossover,2).Cost=Cost(RMSE MCR); % Calculate the cost function for chromosome 22:end % Crossover 23: Create popM for Mutation population 24: for iMutation=1: nMutation 25: i=BinaryTournamentSelection(pop); 26: popM(iMutation).Position=Mutate(pop(i).Position,mu,VarRange); 27: popM(iMutation).Position(1:1684)=pop(i).PrimaryPosition(1:1684); % Replace the Choromosome TICA filter 28: chromosomtoCNN(pop(iMutation)); 29: [RMSE MCR]=PerformanceCNN(TestData,TargetData); 30: popM(iMutation).Cost=Cost(RMSE MCR); % Calculate the cost function for chromosome 31: end % Mutation 33: popC=popC(:); % Turn to on dimension 34: pop=[pop popC popM] % Merge all population 35: [pop F]=NonDominatedSorting(pop); % Sorting the non-dominated individual based on rank of batch for population 36: pop=CalcCrowdingDistance(pop,F); % Computing crowding distance for population 37: pop=SortPopulation(pop); % Sorting the rank population with priority in non-dominated and the crowding distance 38: pop=pop(1:nPop); % Reducing population to the number initial individuals N 39:end % Iteration"}, {"heading": "2.2 Fine-training stage", "text": "The 27 pertinent individuals have been selected from the preceding stage for initializing preliminary population. The entire layers` weights have been authorized to be modify in the LeNet-5; caused the TICA filters to be improved to extract pertinent features in the layer C1 and C3 of the LeNet-5 scheme by the NSGA-II algorithm. In figure 11 and 12 present graphs for RMSE and MCR errors.\nThe code of the fine-tuning algorithm declares in Algorithm 1 with remove the lines of replaced the TICA filter in the chromosome (lines 14, 18, 27) and with initialize the parameters at beginning from table 1."}, {"heading": "2.3 The results", "text": "Our practices (methods in table 3 and 4 and respective figures) in the article are listed at table 2. (Note: each of the practices has been rendered five times and the average of best results (minimum in RMSE and MCR) has been chosen and rounded in the specified iterations at table 3 and 4 and respective figures). It should be noted that the RMSE and MCR not have related with together; in whole results some time they are opposed (Figure 13).\nThe \u2018*\u2019 sign marks our proposal`s results in table 2. The two stages` results (pre-training and finetuning) have been typed in column 1 and 2 of table 3 and 4 for RMSE and MCR; learning achieved with tracing the results."}, {"heading": "No Experiments DataSet Number of sample", "text": "In figure 12, circumstances of the results for the practices 3 up to 7 imply plenty of errors in the MCR error. Figure 13 depicted the outcomes in the proposal in second stage from original values.\nNote: Cost function or fitness function equivalent in MOP in the entire experimental algorithms has applied the ax 2 +by 2 equivalent (x denotes RMSE and y denotes MCR; a=b equal to one) a=b equal to one). In SOP 1 separately applied power equivalent (x 2 ).\nFinally, in the study, the table 5 shows the comparative results with the trained LeNet-5 and a derivation trained model has been practiced on the 50 samples comparing of the ability of generalization. Hence, for test phase, our assessment was carried out on 10,000 samples which\n1 Single-objective problem\nwere selected from the MINST dataset randomly. Our model tested with 35 percent errors and the other tested with 55 percent in MCR measure."}, {"heading": "No Experiments MCR", "text": "Finally, the novelties in the proposal were listed below:\n1) The TICA filters dimension of 16\u00d716 resized to dimension of 5\u00d75 without being required of learning them for LeNet-5 scheme. It means researcher can apply TICA filters in their\nconvolutional neural network with resizing them in desired condition directly; as dimension of 16\u00d716 to 5\u00d75. 2) Supervised learning has been employed at the beginning of our proposal with TICA filters. TICA filters must be learnt by neural network by unsupervised training from\nnatural patch images and afterward the original dataset must be taught to CNN model by supervised training. 3) The weights of layer C1 and C3 were not modified to the end of the first stage simultaneously and afterwards allowed them to be modified in the second stage.\n4) Appling the GA algorithm, as NSGA-II trains the paramount neurons in neural networks as the LeNet-5. Subsequently, the plethora of computation is not required in derivation\nand Backpropagation algorithm; therefore, the approach is advantageous to parallel processing. 5) The generalization test is well as the obtain models in the article trained with a few samples."}, {"heading": "3 Conclusion", "text": "In the study presented the procedure by applying the TICA filters and the NSGA-II genetic algorithms with RMSE and MCR objective functions for training of the LeNet-5 convolutional neural network in two stages. The first stage called pre-training and the second stage called finetuning based on optimization solution. In fact, the TICA filters and NSGA-II algorithm with simple and useful methods in computations helped us to learn the input patterns.\nFurthermore, the proposal is capable of rendering in parallel processing on GPU and cloud computing (future proposal)."}], "references": [{"title": "Meyarivan A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["Amrit Pratap Sameer Agarwal"], "venue": "Duffner Stefan Face Image Analysis With Convolutional Neural Networks . [s.l.] : Dissertation,", "citeRegEx": "Agarwal and ,? \\Q2002\\E", "shortCiteRegEx": "Agarwal and ", "year": 2002}, {"title": "A neural-network model for selective attention in visual pattern recognition", "author": ["K. Fukushima"], "venue": null, "citeRegEx": "2007", "shortCiteRegEx": "2007", "year": 1986}, {"title": "Recognition and segmentation of connected characters with selective attention", "author": ["T. Imagawa K. Fukushima"], "venue": null, "citeRegEx": "Fukushima,? \\Q1962\\E", "shortCiteRegEx": "Fukushima", "year": 1962}, {"title": "Koray Kavukcuoglu, Marc\u2019Aurelio, Ranzato, Yann LeCun Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition", "author": ["K. Kavukcuoglu M.A. Ranzato", "R. Fergus", "Y. LeCun Learning invariant features through topographic filter maps ."], "venue": "New York : [s.n.], 2008.", "citeRegEx": "Ranzato et al\\.,? 2009", "shortCiteRegEx": "Ranzato et al\\.", "year": 2009}], "referenceMentions": [], "year": 2017, "abstractText": "Almost all of the presented articles in the CNN 1 are based on the error backpropagation algorithm and calculation of derivations of error, our innovative proposal refers to engaging TICA 2 filters and NSGA-II 3 genetic algorithms to train the LeNet-5 CNN network. Consequently, genetic algorithm updates the weights of LeNet-5 CNN network similar to chromosome update. In our approach the weights of LeNet-5 are obtained in two stages. The first is pretraining and the second is fine-tuning. As a result, our approach impacts in learning task.", "creator": "Microsoft\u00ae Word 2010"}, "id": "ICLR_2017_481"}