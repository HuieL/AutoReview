{"name": "ICLR_2017_483.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["CLASSIFIER GANS", "Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "emails": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Characterizing the structure of natural images has been a rich research endeavor. Natural images obey intrinsic invariances and exhibit multi-scale statistical structures that have historically been difficult to quantify (Simoncelli & Olshausen, 2001). Recent advances in machine learning offer an opportunity to substantially improve the quality of image models. Improved image models advance the state-of-the-art in image denoising (Balle\u0301 et al., 2015), compression (Toderici et al., 2016), in-painting (van den Oord et al., 2016a), and super-resolution (Ledig et al., 2016). Better models of natural images also improve performance in semi-supervised learning tasks (Kingma et al., 2014; Springenberg, 2015; Odena, 2016; Salimans et al., 2016) and reinforcement learning problems (Blundell et al., 2016).\nOne method for understanding natural image statistics is to build a system that synthesizes images de novo. There are several promising approaches for building image synthesis models. Variational autoencoders (VAEs) maximize a variational lower bound on the log-likelihood of the training data (Kingma & Welling, 2013; Rezende et al., 2014). VAEs are straightforward to train but introduce potentially restrictive assumptions about the approximate posterior distribution (but see Rezende & Mohamed (2015); Kingma et al. (2016)). Autoregressive models dispense with latent variables and directly model the conditional distribution over pixels (van den Oord et al., 2016a;b). These models produce convincing samples but are costly to sample from and do not provide a latent representation. Invertible density estimators transform latent variables directly using a series of parameterized functions constrained to be invertible (Dinh et al., 2016). This technique allows for exact log-likelihood computation and exact inference, but the invertibility constraint is restrictive.\nGenerative adversarial networks (GANs) offer a distinct and promising approach that focuses on a game-theoretic formulation for training an image synthesis model (Goodfellow et al., 2014). Recent work has shown that GANs can produce convincing image samples on datasets with low variability and low resolution (Denton et al., 2015; Radford et al., 2015). However, GANs struggle to generate globally coherent, high resolution samples - particularly from datasets with high variability. Moreover, a theoretical understanding of GANs is an on-going research topic (Uehara et al., 2016; Mohamed & Lakshminarayanan, 2016).\n\u2217Work completed as a participant in the 2016-2017 Google Brain Residency program.\nar X\niv :1\n61 0.\n09 58\n5v 1\n[ st\nat .M\nL ]\n3 0\nO ct\n2 01\n6\nIn this work we demonstrate that that adding more structure to the GAN latent space along with a specialized cost function results in higher quality samples. We exhibit 128 \u00d7 128 pixel samples from all classes of the ImageNet dataset (Russakovsky et al., 2015) with increased global coherence (Figure 1). Importantly, we demonstrate quantitatively that our high resolution samples are not just naive resizings of low resolution samples. In particular, downsampling our 128 \u00d7 128 samples to 32 \u00d7 32 leads to a 50% decrease in visual discriminability. We also introduce a new metric for assessing the variability across image samples and employ this metric to demonstrate that our synthesized images exhibit diversity comparable to training data for a large fraction (84.7%) of ImageNet classes."}, {"heading": "2 BACKGROUND", "text": "A generative adversarial network (GAN) consists of two neural networks trained in opposition to one another. The generator G takes as input a random noise vector z and outputs an image Xfake = G(z). The discriminator D receives as input either a training image or a synthesized image from the generator and outputs a probability distribution P (S |X) = D(X) over possible image sources. The discriminator is trained to maximize the log-likelihood it assigns to the correct source:\nL = E[logP (S = real | Xreal)] + E[logP (S = fake | Xfake)]\nThe generator is trained to minimize that same quantity.\nThe basic GAN framework can be augmented using side information. One strategy is to supply both the generator and discriminator with class labels in order to produce class conditional samples (Mirza & Osindero, 2014). Class conditional synthesis can significantly improve the quality of generated samples (van den Oord et al., 2016b). Richer side information such as image captions and bounding box localizations may improve sample quality further (Reed et al., 2016a;b).\nInstead of feeding side information to the discriminator, one can task the discriminator with reconstructing side information. This is done by modifying the discriminator to contain an auxiliary decoder network1 that outputs the class label for the training data (Odena, 2016; Salimans et al., 2016) or a subset of the latent variables from which the samples are generated (Chen et al., 2016). Forcing a model to perform additional tasks is known to improve performance on the original task (e.g. Sutskever et al. (2014); Szegedy et al. (2014); Ramsundar et al. (2016)). In addition, an auxiliary decoder could leverage pre-trained discriminators (e.g. image classifiers) for further improving the synthesized images (Nguyen et al., 2016). Motivated by these considerations, we introduce a model that combines both strategies for leveraging side information. That is, the model proposed below is class conditional, but with an auxiliary decoder that is tasked with reconstructing class labels."}, {"heading": "3 AC-GANS", "text": "We propose a variant of the GAN architecture which we call an auxiliary classifier GAN (or ACGAN - see Figure 2). In the AC-GAN, every generated sample has a corresponding class label, c \u223c pc in addition to the noise z. G uses both to generate images Xfake = G(c, z). The discriminator gives both a probability distribution over sources and a probability distribution over the class labels, P (S | X), P (C | X) = D(X). The objective function has two parts: the log-likelihood of the correct source, LS , and the log-likelihood of the correct class, LC .\nLS = E[logP (S = real | Xreal)] + E[logP (S = fake | Xfake)] LC = E[logP (C = c | Xreal)] + E[logP (C = c | Xfake)]\nD is trained to maximize LS + LC while G is trained to maximize LC \u2212 LS . AC-GANs learn a representation for z that is independent of class label (e.g. Kingma et al. (2014)).\nEarly experiments demonstrated that increasing the number of classes trained on while holding the model fixed decreased the quality of the model outputs (Appendix B). The structure of the ACGAN model permits separating large datasets into subsets by class and training a generator and discriminator for each subset. We exploit this property in our experiments to train across the entire ImageNet data set."}, {"heading": "4 RESULTS", "text": "We train several AC-GAN models on the ImageNet data set (Russakovsky et al., 2015). Broadly speaking, the architecture of the generator G is a series of \u2018deconvolution\u2019 layers that transform the noise z and class c into an image (Odena et al., 2016). We train two variants of the model architecture for generating images at 128 \u00d7 128 and 64 \u00d7 64 spatial resolutions. The discriminator D is a deep convolutional neural network with a Leaky ReLU nonlinearity (Maas et al., 2013). See Appendix A for more details. As mentioned earlier, we find that reducing the variability introduced by all 1000 classes of ImageNet significantly improves the quality of training. We train 100 AC-GAN models \u2013 each on images from just 10 classes \u2013 for 50000 mini-batches of size 100.\nEvaluating the quality of image synthesis models is challenging due to the variety of probabilistic criteria (Theis et al., 2015) and the lack of a perceptually meaningful image similarity metric. Nonetheless, in subsequent sections we attempt to measure the quality of the AC-GAN by building several ad-hoc measures for image sample discriminability and diversity. Our hope is that this work might provide quantitative measures that may be used to aid training and subsequent development of image synthesis models.\n1 Alternatively, one can force the discriminator to work with the joint distribution (X, z) and train a separate inference network that computes q(z|X) (Dumoulin et al., 2016; Donahue et al., 2016)."}, {"heading": "4.1 GENERATING HIGH RESOLUTION IMAGES IMPROVES DISCRIMINABILITY", "text": "Building a class-conditional image synthesis model necessitates measuring the extent to which synthesized images appear to belong to the intended class. In particular, we would like to know that a high resolution sample is not just a naive resizing of a low resolution sample. Consider a simple experiment: pretend there exists a model that synthesizes 32\u00d732 images. One can trivially increase the resolution of synthesized images by performing bilinear interpolation. This would yield higher resolution images, but these images would just be blurry versions of the low resolution images that are not discriminable. Hence, the goal of an image synthesis model is not simply to produce high resolution images, but to produce high resolution images that are more discriminable than low resolution images.\nTo measure discriminability, we feed synthesized images to a pre-trained Inception network (Szegedy et al., 2015) and report the fraction of the samples for which the Inception network assigned the correct label2. We calculate this accuracy measure on a series of real and synthesized images which have had their spatial resolution artificially decreased by bilinear interpolation (Figure 3,\n2 One could also use the Inception score (Salimans et al., 2016), but our method has several advantages: accuracy figures are easier to interpret than exponentiated KL-divergences; accuracy may be assessed for individual classes; accuracy measures whether a class-conditional model generated samples from\ntop panels). Note that as the spatial resolution is decreased, the accuracy decreases - indicating that resulting images contain less class information (Figure 3, scores below top panels). We summarized this finding across all 1000 ImageNet classes for the ImageNet training data (black), a 128 \u00d7 128 resolution AC-GAN (red) and a 64 \u00d7 64 resolution AC-GAN (blue) in Figure 3 (bottom, left). The black curve (clipped) provides an upper-bound on the discriminability of real images.\nThe goal of this analysis is to show that synthesizing higher resolution images leads to increased discriminability. The 128\u00d7 128 model achieves an accuracy of 10.1% \u00b1 2.0% versus 7.0% \u00b1 2.0% with samples resized to 64\u00d7 64 and 5.0% \u00b1 2.0% with samples resized to 32\u00d7 32. In other words, downsizing the outputs of the AC-GAN to 32 \u00d7 32 and 64 \u00d7 64 decreases visual discriminability by 50% and 38% respectively. Furthermore, 84.4% of the ImageNet classes have higher accuracy at 128\u00d7 128 than at 32\u00d7 32 (Figure 3, bottom left). We performed the same analysis on an AC-GAN trained to 64 \u00d7 64 spatial resolution. This model achieved less discriminability than a 128\u00d7128 AC-GAN model. Accuracies from the 64\u00d764 model plateau at a 64\u00d764 spatial resolution consistent with previous results. Finally, the 64\u00d764 resolution model achieves less discriminability at 64 spatial resolution than the 128\u00d7 128 model."}, {"heading": "4.2 MEASURING THE DIVERSITY OF GENERATED IMAGES", "text": "An image synthesis model is not very interesting if it only outputs one image. Indeed, a well-known failure mode of GANs is that the generator will collapse and output a single prototype that maximally fools the discriminator (Goodfellow et al., 2014; Salimans et al., 2016). A class-conditional model of images is not very interesting if it only outputs one image per class. The Inception accuracy can not measure whether a model has collapsed. A model that simply memorized one example from each ImageNet class would do very well by this metric. Thus, we seek a complementary metric to explicitly evaluate the intra-class diversity of samples generated by the AC-GAN.\nSeveral methods exist for quantitatively evaluating image similarity by attempting to predict human perceptual similarity judgements. The most successful of these is multi-scale structural similarity (MS-SSIM) (Wang et al., 2004b; Ma et al., 2016). MS-SSIM is a multi-scale variant of a wellcharacterized perceptual similarity metric that attempts to discount aspects of an image that are not important for human perception (Wang et al., 2004a). MS-SSIM values range between 0.0 and 1.0; higher MS-SSIM values correspond to perceptually more similar images. As a proxy for image diversity, we measure the MS-SSIM scores between randomly chosen pairs of images within a given class. Samples from classes that have higher diversity result in lower mean MS-SSIM scores (Figure 4, left columns); samples from classes with lower diversity have higher mean MS-SSIM scores (Figure 4, right columns). Training images from the ImageNet training data contain a variety of mean MS-SSIM scores across the classes indicating the variability of image diversity in ImageNet classes (Figure 5, left panel, x-axis). Note that the highest mean MS-SSIM score (indicating the least variability) is 0.25 for the training data.\nWe calculate the mean MS-SSIM score for all 1000 ImageNet classes generated by the AC-GAN model. We track this value during training to identify whether the generator has collapsed (Figure 5, right panel, red curve). We also employ this metric to compare the diversity of the training images to the samples from the GAN model after training has completed. Figure 5 (left) plots the mean MS-SSIM values for image samples and training data broken up by class. The blue line is the line of equality. Out of the 1000 classes, we find that 847 have mean sample MS-SSIM scores below that of the maximum MS-SSIM for the training data. In other words, 84.7% of classes have sample variability that exceeds that of the least variable class from the ImageNet training data."}, {"heading": "4.3 GENERATED IMAGES ARE BOTH DIVERSE AND DISCRIMINABLE", "text": "We have presented quantitative metrics demonstrating that AC-GAN samples may be diverse and discriminable but we have yet to examine how these metrics interact. Figure 6 shows the joint distribution of Inception accuracies and MS-SSIM scores across all classes. Inception accuracy and MS-SSIM are anti-correlated (r2 = \u22120.16). In fact, 74% of the classes with low diversity (MSSSIM\u2265 0.25) contain Inception accuracies\u2264 1%. These results suggest that GANs that drop modes\nthe intended class. To compute the Inception accuracy, we modified a version of Inception-v3 supplied in https://github.com/openai/improved-gan/.\nhot dog artichokepromontory green apple MS-SSIM = 0.11 MS-SSIM = 0.29 MS-SSIM = 0.41 MS-SSIM = 0.90\nare most likely to produce low quality images. Conversely, 78% of classes with high diversity (MSSSIM < 0.25) have Inception accuracies that exceed 1%. In comparison, the Inception-v3 model achieves 78.8% accuracy on average across all 1000 classes (Szegedy et al., 2015). A fraction of the classes AC-GAN samples reach this level of accuracy. This indicates opportunity for future image synthesis models."}, {"heading": "4.4 COMPARISON TO PREVIOUS RESULTS", "text": "Previous quantitative results for image synthesis models trained on ImageNet are reported in terms of log-likelihood (van den Oord et al., 2016a;b). Log-likelihood is a coarse and potentially inaccurate measure of sample quality (Theis et al., 2015). Addditionally, log-likelihood is intractable to compute for GANs. Instead we compare with previous state-of-the-art results on CIFAR-10 using a lower spatial resolution (32 \u00d7 32). Following the procedure in Salimans et al. (2016), we compute\nthe Inception score3 for 50000 samples from an AC-GAN with resolution (32 \u00d7 32), split into 10 groups at random. We also compute the Inception score for 25000 extra samples, split into 5 groups at random. We select the best model based on the first score and report the second score. Performing a grid search across 27 hyperparameter configurations, we are able to achieve a score of 8.25\u00b1 0.07 compared to state of the art 8.09\u00b1 0.07 (Salimans et al., 2016). Moreover, we accomplish this without employing any of the new techniques introduced in that work (i.e. virtual batch normalization, minibatch discrimination, and label smoothing). This provides additional evidence that AC-GANs are effective even without the benefit of class splitting (Appendix B)."}, {"heading": "4.5 SEARCHING FOR SIGNATURES OF OVERFITTING", "text": "One possibility that must be investigated is that the AC-GAN has overfit on the training data. As a first check that the network does not memorize the training data, we identify the nearest neighbors of image samples in the training data measured by L1 distance in pixel space (Figure 7). The nearest neighbors from the training data do not resemble the corresponding samples. This provides evidence that the AC-GAN is not merely memorizing the training data.\nA more sophisticated method for understanding the degree of overfitting in a model is to explore that model\u2019s latent space by interpolation. In an overfit model one might observe discrete transitions in the interpolated images and regions in latent space that do not correspond to meaningful images (Bengio et al., 2012; Radford et al., 2015; Dinh et al., 2016). Figure 8 (left) highlights interpolations in the latent space between several image samples. Notably, the generator learned that certain combinations of dimensions correspond to semantically meaningful features (e.g. size of the arch, length of a bird\u2019s beak) and there are no discrete transitions or \u2018holes\u2019 in the latent space. A second method for exploring the latent space of the AC-GAN is to exploit the structure of the model. The AC-GAN factorizes its representation into class information and a class-independent latent representation z. Sampling the AC-GAN with z fixed but altering the class label corresponds to generating samples with the same \u2018style\u2019 across multiple classes (Kingma et al., 2014). Figure 8 (right) shows samples\n3 The Inception score is given by exp (Ex[DKL(p(y|x) || p(y))]) where x is a particular image, p(y|x) is the conditional output distribution over the classes in a pre-trained Inception network (Szegedy et al., 2014) given x, and p(y) is the marginal distribution over the classes.\nfrom 8 bird classes. Elements of the same row have the same z. Although the class changes for each column, elements of the global structure (e.g. position, layout, background) are preserved, indicating that AC-GAN can represent certain types of \u2018compositionality\u2019."}, {"heading": "5 DISCUSSION", "text": "This work introduced the AC-GAN architecture and demonstrated that AC-GANs can generate globally coherent ImageNet samples. We provided a new quantitative metric for image discriminability as a function of spatial resolution. Using this metric we demonstrated that our samples are more discriminable than those from a model that generates lower resolution images and performs a naive resize operation. We also analyzed the diversity of our samples with respect to the training data and provided some evidence that the image samples from the majority of classes are comparable in diversity to ImageNet training data. We hope that these metrics might provide quantitative measures of sample quality for evaluating and improving future image synthesis models.\nSeveral directions exist for building upon this work. Much work needs to be done to improve the visual discriminability of the 128\u00d7128 resolution model. Although some synthesized image classes exhibit high Inception accuracies, the average Inception accuracy of the model (10.1% \u00b1 2.0%) is still far below real training data at 81%. One immediate opportunity for addressing this is to augment the discriminator with a pre-trained model to perform additional supervised tasks (e.g. image segmentation, Ronneberger et al. (2015)). Such techniques might allow for the synthesis of even higher resolution images with global coherence and meaningful visual content.\nImproving the robustness and reliability of training a GAN is an ongoing research topic. Only 84.7% of the ImageNet classes avoided mode dropping and exhibited a diversity comparable to real training data. Training stability was vastly aided by dividing up 1000 ImageNet classes across 100 AC-GAN models. Building a single unified model that could generate diverse samples from all 1000 classes would be an important step forward.\nImage synthesis models provide a unique opportunity for performing semi-supervised learning. Namely, these models build a rich prior over natural image statistics that can be leveraged by classifiers to improve predictions on datasets for which few labels exist. The AC-GAN model can perform semi-supervised learning by simply ignoring the component of the loss arising from class labels when a label is unavailable for a given training image. Interestingly, prior work suggests that achieving good sample quality might be independent of success in semi-supervised learning (Salimans et al., 2016)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the developers of TensorFlow (Abadi et al., 2016). We thank Luke Metz and Vincent Dumoulin for extensive and helpful comments on drafts. We also thank Ben Poole, Sam Schoenholz, Barret Zoph, Mart\u0131\u0301n Abadi, Manjunath Kudlur and Jascha Sohl-Dickstein for helpful discussions."}, {"heading": "A HYPERPARAMETERS", "text": ""}, {"heading": "B MEASURING THE EFFECT OF CLASS SPLITS ON IMAGE SAMPLE QUALITY.", "text": "Class conditional image synthesis affords the opportunity to divide up a dataset based on image label. In our final model we divide 1000 ImageNet classes across 100 AC-GAN models. In this section we describe early experiments that highlight the benefit of cutting down the diversity of classes for training an AC-GAN. We employed an ordering of the labels and divided it into contiguous groups of 10. This ordering can be seen in the following section, where we display samples from all 1000 classes. Two aspects of the split merit discussion: the number of classes per split and the intra-split diversity.\nWe find that training a fixed model on more classes harms the model\u2019s ability to produce compelling samples (Figure 9). Performance on larger splits can be improved by giving the model more parameters. However, using a small split is not sufficient to achieve good performance. We were unable to train a GAN (Goodfellow et al., 2014) to converge reliably even for a split size of 1.\nThis raises the question of whether it is easier to train a model on a diverse set of classes than on a similar set of classes. We were unable to find conclusive evidence that the selection of classes in a split significantly affects sample quality."}, {"heading": "C SAMPLES FROM ALL 1000 IMAGENET CLASSES", "text": "The following is a link to 10 samples from each of the 1000 ImageNet classes: https://goo.gl/photos/8bgHBkCwDEVTXAPaA"}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Abadi"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "Abadi,? \\Q2016\\E", "shortCiteRegEx": "Abadi", "year": 2016}, {"title": "Density modeling of images using a generalized normalization transformation", "author": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "venue": "CoRR, abs/1511.06281,", "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2015}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "CoRR, abs/1207.4404,", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Model-Free Episodic Control", "author": ["C. Blundell", "B. Uria", "A. Pritzel", "Y. Li", "A. Ruderman", "J. Z Leibo", "J. Rae", "D. Wierstra", "D. Hassabis"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "ArXiv eprints,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Robert Fergus"], "venue": "CoRR, abs/1506.05751,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Density estimation using real NVP", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": "CoRR, abs/1605.08803,", "citeRegEx": "Dinh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2016}, {"title": "Adversarial Feature Learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "ArXiv e-prints,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Adversarially Learned Inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": null, "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Generative Adversarial Networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["D. P Kingma", "M. Welling"], "venue": "ArXiv e-prints,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Semisupervised learning with deep generative models", "author": ["Diederik P. Kingma", "Danilo Jimenez Rezende", "Shakir Mohamed", "Max Welling"], "venue": "CoRR, abs/1406.5298,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "venue": "CoRR, abs/1606.04934,", "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network", "author": ["C. Ledig", "L. Theis", "F. Huszar", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi"], "venue": null, "citeRegEx": "Ledig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ledig et al\\.", "year": 2016}, {"title": "Group mad competition - a new methodology to compare objective image quality models", "author": ["Kede Ma", "Qingbo Wu", "Zhou Wang", "Zhengfang Duanmu", "Hongwei Yong", "Hongliang Li", "Lei Zhang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew Maas", "Awni Hannun", "Andrew Ng"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "CoRR, abs/1411.1784,", "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Learning in implicit generative models", "author": ["Shakir Mohamed", "Balaji Lakshminarayanan"], "venue": "arXiv preprint arXiv:1610.03483,", "citeRegEx": "Mohamed and Lakshminarayanan.,? \\Q2016\\E", "shortCiteRegEx": "Mohamed and Lakshminarayanan.", "year": 2016}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator", "author": ["Anh Mai Nguyen", "Alexey Dosovitskiy", "Jason Yosinski", "Thomas Brox", "Jeff Clune"], "venue": "networks. CoRR,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Semi-Supervised Learning with Generative Adversarial Networks", "author": ["A. Odena"], "venue": "ArXiv e-prints,", "citeRegEx": "Odena.,? \\Q2016\\E", "shortCiteRegEx": "Odena.", "year": 2016}, {"title": "Deconvolution and checkerboard artifacts", "author": ["Augustus Odena", "Vincent Dumoulin", "Chris Olah"], "venue": null, "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Massively multitask networks for drug discovery", "author": ["Bharath Ramsundar", "Steven Kearnes", "Patrick Riley", "Dale Webster", "David Konerding", "Vijay Pande"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Ramsundar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ramsundar et al\\.", "year": 2016}, {"title": "Learning what and where to draw", "author": ["Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee"], "venue": "arXiv preprint arXiv:1610.02454,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Generative adversarial text-to-image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Variational Inference with Normalizing Flows", "author": ["D. Rezende", "S. Mohamed"], "venue": "ArXiv e-prints,", "citeRegEx": "Rezende and Mohamed.,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed.", "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ArXiv e-prints,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "venue": "CoRR, abs/1505.04597,", "citeRegEx": "Ronneberger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ronneberger et al\\.", "year": 2015}, {"title": "Improved Techniques for Training GANs", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Natural image statistics and neural representation", "author": ["Eero Simoncelli", "Bruno Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Simoncelli and Olshausen.,? \\Q2001\\E", "shortCiteRegEx": "Simoncelli and Olshausen.", "year": 2001}, {"title": "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks", "author": ["J.T. Springenberg"], "venue": "ArXiv e-prints,", "citeRegEx": "Springenberg.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Le Quoc V"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rethinking the inception architecture for computer", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "vision. CoRR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "ArXiv e-prints,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Full resolution image compression with recurrent neural networks", "author": ["George Toderici", "Damien Vincent", "Nick Johnston", "Sung Jin Hwang", "David Minnen", "Joel Shor", "Michele Covell"], "venue": "CoRR, abs/1608.05148,", "citeRegEx": "Toderici et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2016}, {"title": "Generative Adversarial Nets from a Density Ratio Estimation Perspective", "author": ["M. Uehara", "I. Sato", "M. Suzuki", "K. Nakayama", "Y. Matsuo"], "venue": null, "citeRegEx": "Uehara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Uehara et al\\.", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Zhou Wang", "Alan C Bovik", "Hamid R Sheikh", "Eero P Simoncelli"], "venue": "IEEE transactions on image processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Zhou Wang", "Eero P Simoncelli", "Alan C Bovik"], "venue": "In Signals, Systems and Computers,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Softmax activation to K of the units and a Sigmoid activation to the remaining unit. We also use activation noise in the discriminator", "author": ["Salimans"], "venue": null, "citeRegEx": "Salimans,? \\Q2016\\E", "shortCiteRegEx": "Salimans", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Improved image models advance the state-of-the-art in image denoising (Ball\u00e9 et al., 2015), compression (Toderici et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 34, "context": ", 2015), compression (Toderici et al., 2016), in-painting (van den Oord et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 11, "context": "Better models of natural images also improve performance in semi-supervised learning tasks (Kingma et al., 2014; Springenberg, 2015; Odena, 2016; Salimans et al., 2016) and reinforcement learning problems (Blundell et al.", "startOffset": 91, "endOffset": 168}, {"referenceID": 30, "context": "Better models of natural images also improve performance in semi-supervised learning tasks (Kingma et al., 2014; Springenberg, 2015; Odena, 2016; Salimans et al., 2016) and reinforcement learning problems (Blundell et al.", "startOffset": 91, "endOffset": 168}, {"referenceID": 19, "context": "Better models of natural images also improve performance in semi-supervised learning tasks (Kingma et al., 2014; Springenberg, 2015; Odena, 2016; Salimans et al., 2016) and reinforcement learning problems (Blundell et al.", "startOffset": 91, "endOffset": 168}, {"referenceID": 28, "context": "Better models of natural images also improve performance in semi-supervised learning tasks (Kingma et al., 2014; Springenberg, 2015; Odena, 2016; Salimans et al., 2016) and reinforcement learning problems (Blundell et al.", "startOffset": 91, "endOffset": 168}, {"referenceID": 3, "context": ", 2016) and reinforcement learning problems (Blundell et al., 2016).", "startOffset": 44, "endOffset": 67}, {"referenceID": 26, "context": "Variational autoencoders (VAEs) maximize a variational lower bound on the log-likelihood of the training data (Kingma & Welling, 2013; Rezende et al., 2014).", "startOffset": 110, "endOffset": 156}, {"referenceID": 6, "context": "Invertible density estimators transform latent variables directly using a series of parameterized functions constrained to be invertible (Dinh et al., 2016).", "startOffset": 137, "endOffset": 156}, {"referenceID": 9, "context": "Generative adversarial networks (GANs) offer a distinct and promising approach that focuses on a game-theoretic formulation for training an image synthesis model (Goodfellow et al., 2014).", "startOffset": 162, "endOffset": 187}, {"referenceID": 5, "context": "Recent work has shown that GANs can produce convincing image samples on datasets with low variability and low resolution (Denton et al., 2015; Radford et al., 2015).", "startOffset": 121, "endOffset": 164}, {"referenceID": 21, "context": "Recent work has shown that GANs can produce convincing image samples on datasets with low variability and low resolution (Denton et al., 2015; Radford et al., 2015).", "startOffset": 121, "endOffset": 164}, {"referenceID": 35, "context": "Moreover, a theoretical understanding of GANs is an on-going research topic (Uehara et al., 2016; Mohamed & Lakshminarayanan, 2016).", "startOffset": 76, "endOffset": 131}, {"referenceID": 19, "context": "This is done by modifying the discriminator to contain an auxiliary decoder network1 that outputs the class label for the training data (Odena, 2016; Salimans et al., 2016) or a subset of the latent variables from which the samples are generated (Chen et al.", "startOffset": 136, "endOffset": 172}, {"referenceID": 28, "context": "This is done by modifying the discriminator to contain an auxiliary decoder network1 that outputs the class label for the training data (Odena, 2016; Salimans et al., 2016) or a subset of the latent variables from which the samples are generated (Chen et al.", "startOffset": 136, "endOffset": 172}, {"referenceID": 4, "context": ", 2016) or a subset of the latent variables from which the samples are generated (Chen et al., 2016).", "startOffset": 81, "endOffset": 100}, {"referenceID": 18, "context": "image classifiers) for further improving the synthesized images (Nguyen et al., 2016).", "startOffset": 64, "endOffset": 85}, {"referenceID": 20, "context": "Broadly speaking, the architecture of the generator G is a series of \u2018deconvolution\u2019 layers that transform the noise z and class c into an image (Odena et al., 2016).", "startOffset": 145, "endOffset": 165}, {"referenceID": 15, "context": "The discriminator D is a deep convolutional neural network with a Leaky ReLU nonlinearity (Maas et al., 2013).", "startOffset": 90, "endOffset": 109}, {"referenceID": 33, "context": "Evaluating the quality of image synthesis models is challenging due to the variety of probabilistic criteria (Theis et al., 2015) and the lack of a perceptually meaningful image similarity metric.", "startOffset": 109, "endOffset": 129}, {"referenceID": 8, "context": "1 Alternatively, one can force the discriminator to work with the joint distribution (X, z) and train a separate inference network that computes q(z|X) (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 152, "endOffset": 197}, {"referenceID": 7, "context": "1 Alternatively, one can force the discriminator to work with the joint distribution (X, z) and train a separate inference network that computes q(z|X) (Dumoulin et al., 2016; Donahue et al., 2016).", "startOffset": 152, "endOffset": 197}, {"referenceID": 32, "context": "To measure discriminability, we feed synthesized images to a pre-trained Inception network (Szegedy et al., 2015) and report the fraction of the samples for which the Inception network assigned the correct label2.", "startOffset": 91, "endOffset": 113}, {"referenceID": 28, "context": "We calculate this accuracy measure on a series of real and synthesized images which have had their spatial resolution artificially decreased by bilinear interpolation (Figure 3, 2 One could also use the Inception score (Salimans et al., 2016), but our method has several advantages: accuracy figures are easier to interpret than exponentiated KL-divergences; accuracy may be assessed for individual classes; accuracy measures whether a class-conditional model generated samples from", "startOffset": 219, "endOffset": 242}, {"referenceID": 9, "context": "Indeed, a well-known failure mode of GANs is that the generator will collapse and output a single prototype that maximally fools the discriminator (Goodfellow et al., 2014; Salimans et al., 2016).", "startOffset": 147, "endOffset": 195}, {"referenceID": 28, "context": "Indeed, a well-known failure mode of GANs is that the generator will collapse and output a single prototype that maximally fools the discriminator (Goodfellow et al., 2014; Salimans et al., 2016).", "startOffset": 147, "endOffset": 195}, {"referenceID": 14, "context": "The most successful of these is multi-scale structural similarity (MS-SSIM) (Wang et al., 2004b; Ma et al., 2016).", "startOffset": 76, "endOffset": 113}, {"referenceID": 32, "context": "8% accuracy on average across all 1000 classes (Szegedy et al., 2015).", "startOffset": 47, "endOffset": 69}, {"referenceID": 33, "context": "Log-likelihood is a coarse and potentially inaccurate measure of sample quality (Theis et al., 2015).", "startOffset": 80, "endOffset": 100}, {"referenceID": 2, "context": "In an overfit model one might observe discrete transitions in the interpolated images and regions in latent space that do not correspond to meaningful images (Bengio et al., 2012; Radford et al., 2015; Dinh et al., 2016).", "startOffset": 158, "endOffset": 220}, {"referenceID": 21, "context": "In an overfit model one might observe discrete transitions in the interpolated images and regions in latent space that do not correspond to meaningful images (Bengio et al., 2012; Radford et al., 2015; Dinh et al., 2016).", "startOffset": 158, "endOffset": 220}, {"referenceID": 6, "context": "In an overfit model one might observe discrete transitions in the interpolated images and regions in latent space that do not correspond to meaningful images (Bengio et al., 2012; Radford et al., 2015; Dinh et al., 2016).", "startOffset": 158, "endOffset": 220}, {"referenceID": 11, "context": "Sampling the AC-GAN with z fixed but altering the class label corresponds to generating samples with the same \u2018style\u2019 across multiple classes (Kingma et al., 2014).", "startOffset": 142, "endOffset": 163}, {"referenceID": 28, "context": "Interestingly, prior work suggests that achieving good sample quality might be independent of success in semi-supervised learning (Salimans et al., 2016).", "startOffset": 130, "endOffset": 153}], "year": 2016, "abstractText": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 \u00d7 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128\u00d7 128 samples are more than twice as discriminable as artificially resized 32\u00d7 32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_483"}