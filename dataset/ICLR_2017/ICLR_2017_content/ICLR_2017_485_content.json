{"name": "ICLR_2017_485.pdf", "metadata": {"source": "CRF", "title": "SIGNIFICANCE OF SOFTMAX-BASED FEATURES OVER METRIC LEARNING-BASED FEATURES", "authors": ["Shota Horiguchi", "Daiki Ikami", "Kiyoharu Aizawa"], "emails": ["horiguchi@t.u-tokyo.ac.jp", "ikami@t.u-tokyo.ac.jp", "aizawa@t.u-tokyo.ac.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent developments in deep convolutional neural networks have made it possible to classify many classes of images with high accuracy. It has also been shown that such classification networks work well as feature extractors. Features extracted from classification networks show excellent performance in image classification (Donahue et al., 2014), detection, and retrieval (Razavian et al., 2014; Liu et al., 2015), even when they have been trained to classify 1000 classes of the ImageNet dataset (Russakovsky et al., 2015). It has also been shown that fine-tuning for target domains further improves the features\u2019 performance (Wan et al., 2014; Babenko et al., 2014).\nOn the other hand, distance metric learning (DML) approaches have recently attracted considerable attention. These obtain a feature space in which distance corresponds to class similarity; it is not a byproduct of the classification network. End-to-end distance metric learning is a typical approach to constructing a feature extractor using convolutional neural networks and has been the focus of numerous studies (Bell & Bala, 2015; Schroff et al., 2015). Some DML methods have been reported to show state-of-the-art performance in fine-grained classification (Rippel et al., 2016) and clustering and retrieval (Song et al., 2016) contexts.\nHowever, there have been few experiments comparing softmax-based feature extraction with DMLbased feature extraction under the same network architecture or with adequate fine-tuning. An analysis providing a true comparison of DML features and softmax-based features is long overdue. As we explain more fully in the following section, we contend that there is no reason that DML, which learns feature embedding explicitly, should outperform a softmax-based feature extractor.\nFig. 1 depicts the feature vectors extracted from a softmax-based classification network and a metric learning-based network. We used LeNet architecture for both networks, and trained on the MNIST dataset (LeCun et al., 1998). For DML, we used the contrastive loss function (Hadsell et al., 2006) to map images in two-dimensional space. For softmax-based classification, we added a two- or three-dimensional fully connected layer before the output layer for visualization. DML succeeds in learning feature embedding (Fig. 1a). Softmax-based classification networks can also achieve a result very similar to that obtained by DML: Images are located near one another if they belong to the same class and far apart otherwise (Fig. 1b, Fig. 1c).\nOur contributions in this paper are as follows:\n\u2022 We show methods to exploit the ability of deep features extracted from softmax-based networks, such as normalization and proper dimensionality reduction. This is not technically novel, but this must be useful for fair comparison between image representations.\n\u2022 We demonstrate that deep features extracted from softmax-based classification networks show markedly better results on fine-grained classification, attribute estimation, clustering, and retrieval tasks than those from DML-based networks in almost all datasets.\n\u2022 We show that DML-based methods offer performance competitive to softmax-based methods only when the training dataset consists of a very small number of samples per class."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 PREVIOUS WORK", "text": ""}, {"heading": "2.1.1 SOFTMAX-BASED CLASSIFICATION AND REPURPOSING OF THE CLASSIFIER AS A FEATURE EXTRACTOR", "text": "Convolutional neural networks have demonstrated great potential for highly accurate image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016). It has been shown that features extracted from classification networks can be repurposed as a good feature representation for novel tasks (Donahue et al., 2014; Razavian et al., 2014; Qian et al., 2015) even if the network was trained on ImageNet (Russakovsky et al., 2015). For obtaining better feature representations, fine-tuning is also effective (Babenko et al., 2014)."}, {"heading": "2.1.2 DEEP DISTANCE METRIC LEARNING", "text": "Distance metric learning (DML), which learns a distance metric, has been widely studied (Bromley et al., 1994; Chopra et al., 2005; Chechik et al., 2010; Qian et al., 2015). Recent studies have focused on end-to-end deep distance metric learning (Bell & Bala, 2015; Schroff et al., 2015; Li et al., 2015; Rippel et al., 2016; Song et al., 2016). However, in most studies comparisons of end-to-end DML with features extracted from classification networks have not been performed using architectures and conditions suited to enable a true comparison of performance. Bell & Bala (2015) compared classification networks and siamese networks, but they used coarse class labels for classification networks and fine labels for siamese networks; thus, it was left unclear whether siamese networks are better for feature-embedding learning than classification networks. Schroff et al. (2015) used triplet loss for deep metric learning in their FaceNet, which showed performance that was state of the art at the time, but their network was deeper than that of the previous method (Taigman et al., 2014); thus, triplet loss might not have been the only reason for the performance improvement, and the contribution from adopting triplet loss remains uncertain. Rippel et al. (2016) used the Magnet\nLoss function for their DML. They tried softmax-based features as a comparison, but their results are unfairly low from our results as shown in Section 4.2 and 4.3. Song et al. (2016) used lifted structured feature embedding, another state-of-the-art DML method; however, they only compared their method with a softmax-based classification network pretrained on ImageNet (Russakovsky et al., 2015) and did not compare it with a fine-tuned network."}, {"heading": "2.2 DIFFERENCES BETWEEN SOFTMAX-BASED CLASSIFICATION AND METRIC LEARNING", "text": "For classification, the softmax function (Eq. 1) is typically used:\npc = exp(uc)\u2211C i=1 exp(ui) , (1)\nwhere pc denotes the probability that the vector u belongs to the class c. The loss of the softmax function is defined by the cross-entropy\nE = \u2212 C\u2211\nc=1\nqc log pc, (2)\nwhere q is a one-hot encoding of the correct class of u. To minimize the cross-entropy loss, networks are trained to make the output vector u close to its corresponding one-hot vector. It is important to note that the target vectors (the correct outputs of the network) are fixed during the entire training (Fig. 2).\nOn the other hand, DML methods use distance between samples. They do not use the values of the labels; rather, they ascertain whether the labels are the same between target samples. For example, contrastive loss Hadsell et al. (2006) considers the distance d between a pair of samples:\nE = 1\n2 qd2 + (1\u2212 q)max(\u03b1\u2212 d, 0), (3)\nwhere \u03b1 represents the margin and q \u2208 {0, 1} indicates whether the images in a pair are in the same class (1) or not (0). Recent studies (Schroff et al., 2015; Rippel et al., 2016; Song et al., 2016) use pairwise distances between three or more images at the same time for fast convergence and efficient calculation. However, these methods have some drawbacks. DML methods sometimes require complicated operations such as hard negative sampling (Schroff et al., 2015; Rippel et al., 2016) and k-means clustering for every epoch (Rippel et al., 2016). For DML, in contrast to optimization of the softmax cross-entropy loss, the optimization targets are not always consistent during training even if all possible distances within the mini-batch are considered. Thus, the DML optimization converges very slowly and is not stable and unsteadily. An additional problem is that methods for sampling positive pairs and negative pairs have not been established."}, {"heading": "3 METHODS", "text": ""}, {"heading": "3.1 DIMENSIONALITY REDUCTION LAYER", "text": "One of DML\u2019s strength in using fine-tuning is the flexibility of its output dimensionality. When using features of a mid-layer of a softmax classification network, on the other hand, the dimensionality of the features is fixed. Some existing methods (Babenko et al., 2014) use PCA or discriminative dimensionality reduction to reduce the number of feature dimensions. In our experiment, we evaluated three methods for changing the feature dimensionality. Following conventional PCA approaches, we extracted features from a 1024-dimensional pool5 layer of GoogLeNet (Szegedy et al., 2015; Ioffe & Szegedy, 2015) (Fig. 3a) and applied PCA to reduce the dimensionality. In a contrasting approach, we made use of a fully connected layer: We added a fully connected layer having the required number of neurons just before the output layer (FCR 1, Fig. 3b). We also investigated a third approach in which a fully connected layer is added followed by a dropout layer (FCR 2, Fig. 3c). We intend to show that the features extracted from the pool5 layer of FCR 2 provide better performance than those from FCR 1 even though they differ only in the positions of their dropout layers."}, {"heading": "3.2 NORMALIZATION", "text": "In this study, all the features extracted from the classification networks were from the last layer before the last output layer. The outputs were normalized by the softmax function and then evaluated by the cross-entropy loss function in the networks. Assume that the output vector is p = {pi| \u2211 i pi = 1}. For arbitrary positive constant \u03b1, y = {log\u03b1pi} returns the same vector p after the softmax function is applied. The features x we extract from the networks are given as x = W\u22121y, where W denotes the linear projection matrix from the layer before the output layer to the output layer. The vector y has an ambiguity in its scale, thus vector x, a linear transform of y, also has an ambiguity in the scale; therefore x should be normalized. As Fig. 1b clearly indicates, the distance between features extracted from a softmax-based classifier should be evaluated by cosine similarity, not by the Euclidean distance.\nSome studies used L2 normalization for deep features extracted from softmax-based classification networks (Taigman et al., 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al., 2012; Rippel et al., 2016; Song et al., 2016; Wei et al., 2016). In this study, we also planned to validate the efficiency of normalizing deep features."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we compare the deep features extracted from classification networks to those reported from state-of-the-art deep metric learning methods (Rippel et al., 2016; Song et al., 2016) in their performance on several tasks."}, {"heading": "4.1 PROCEDURE", "text": "All our networks were fine-tuned from the weights that were pretrained on ImageNet (Russakovsky et al., 2015). To evaluate fine-grained classification and attribute estimation performances, we used GoogLeNet with batch normalization (Ioffe & Szegedy, 2015) and did not use any dimentionality reduction layers described in Section 3.1. To evaluate clustering and retrieval performances we used GoogLeNet without batch normalization (Szegedy et al., 2015) and dimentionality reduction layers. We used the Caffe (Jia et al., 2014) framework for our experiments."}, {"heading": "4.2 FINE-GRAINED CLASSIFICATION", "text": "For the evaluation of deep features in fine-grained classification tasks, we used three image datasets: Stanford Dogs (Khosla et al., 2011), Oxford 102 Flowers (Nilsback & Zisserman, 2008), and Oxford-IIIT Pet (Parkhi et al., 2012). For the softmax-base method we fine-tuned the classifier from weights that were pretrained on ImageNet. We defined the learning rate using validation data, setting the learning rate to 0.0001 for the Stanford Dogs dataset and the Oxford-IIIT Pet dataset and to 0.001 for the Oxford 102 Flowers dataset. Learning rates were not changed during the training.\nWe rescaled all the input up by 30% and randomly cropped 224 \u00d7 224. These strategies are exactly the same as those of the previous method (Rippel et al., 2016).\nWe show the mean error rates for the three datasets in Table 1. All our results were evaluated using a 1-nearest neighbor search of the 1024-dimensional vectors extracted from the pool5 layer of GoogLeNet with batch normalization (Ioffe & Szegedy, 2015). In all the experiments, the features extracted from the fine-tuned classification network show the best fine-grained classification performance. Our results of softmax-based classification are better than the results in Rippel et al. (2016). The experiments of softmax-based classification in Rippel et al. (2016) were not the best."}, {"heading": "4.3 ATTRIBUTE ESTIMATION", "text": "Rippel et al. (2016) evaluated features\u2019 expressiveness using mean attribute precision and showed that the features generated by their proposed method contain intra-class diversity. In this section, we investigate the intra-class diversity of softmax features. We use the ImageNet Attribute dataset (Rippel et al., 2016), which consists of overlap between the ImageNet training set (Russakovsky et al., 2015) and the Object Attribute dataset (Russakovsky & Fei-Fei, 2010). We used only the images and their class labels during our training of the softmax classifier and did not use attributes.\nTable 2 shows the error rates of 90-way classification under different training methods. Our finetuned softmax classifier outperformed those of Rippel et al. (2016) by a considerable margin. Fig. 4 shows the mean attribute precision for the ImageNet Attribute dataset. Our fine-tuned softmax features markedly outperformed those from Rippel et al. (2016). These results implicitly indicate that the features extracted from the pool5 layer contain intra-class diversity that is better than those from DML networks designed to keep intra-class diversity.\nTable 2: Classification error rates for the ImageNet Attribute dataset.\nApproach Error\nRippel et al. (2016) (Softmax prob) 14.1% Rippel et al. (2016) (Triplet) 26.8% Rippel et al. (2016) (Magnet) 15.9%\nOurs (Softmax prob) 7.68% Ours (Softmax pool5) 11.9% Ours (Softmax pool5 + L2) 10.7%\n20 22 24 26 28\nNeighbourhood size\n0.2\n0.4\n0.6\n0.8\n1\nM ea\nn at\ntr ib\nut e\npr ec\nis io\nn Rippel (Softmax prob) Rippel (Triplet) Rippel (Magnet) Ours (Softmax pool5 + L2)\nFigure 4: Mean attribute precision for the ImageNet Attribute dataset."}, {"heading": "4.4 CLUSTERING AND RETRIEVAL", "text": "Here, we give our evaluation of clustering and retrieval scores for the state-of-the-art DML method (Song et al., 2016) and for the softmax classification networks. We used the Caltech UCSD Birds 200-2011 (CUB) dataset (Wah et al., 2011), the Stanford Cars 196 (CAR) dataset (Krause et al., 2013), and the Stanford Online Products (OP) dataset (Song et al., 2016). For CUB and CAR, we used the first half of the dataset classes for training and the rest for testing. For OP, we used the training\u2013testing class split provided. The dataset properties are shown in Table 3. We emphasize that the class sets used for training and testing are completely different. We multiplied the learning rates of the changed layers (output layers for all models and the fully connected layer added for FCR 1 and FCR 2) by 10. The batch size was set to 128, and the maximum number of iterations for our training was set to 20,000. These training strategies are exactly the same as those used in the earlier study (Song et al., 2016).\nFor clustering evaluation, we applied k-means clustering 100 times and calculated the average standard F1 and NMI (Manning et al., 2008); the value for k was set to the number of classes in the test set. For retrieval evaluation, we used the Recall@K metric (Jegou et al., 2011).\nWe show the results for the CUB dataset in Fig. 5 and for the CAR dataset in Fig. 6. We notice that we have been able to reproduce nearly exactly the scores of lifted structured feature embedding (Song et al., 2016). However, the deep features extracted from the softmax-based classification networks outperformed the lifted structured feature embedding in all the evaluation metrics.\nFor F1 and NMI, all of the softmax models, including PCA, FCR 1, and FCR 2, show markedly better scores than does lifted structured feature embedding. It is clear that L2 normalization improves the scores of all the softmax-based models. The scores of PCA and FCR 1 drop slightly as the feature dimensionality decreases from 1024 for both the CUB dataset and the CAR dataset. On the other hand, FCR 2, which has a fully connected layer followed by a dropout layer, improves the scores in spite of the reduction in dimensionality, as shown in Fig. 6. It may be that 1024 dimensions is too large to describe the image classes. This result may imply that to obtain the best features we\nneed to first determine the optimum dimensionality of the feature space for the dataset and then apply PCA.\nFor the Recall@K metric, we used 1024-dimensional features for the CUB dataset and 256- dimensional features for the CAR dataset. The softmax-based features outperformed the DMLbased features. The differences between PCA, FCR 1, and FCR 2 are very minor. Regarding feature normalization, features without normalization show worse scores than do L2-normalized features.\nFig. 7 shows the standard F1, NMI, and Recall@K for the Online Products dataset. We used 1024- dimensional features for the Recall@K metric. As shown in Table 3, the OP dataset is very different from the CUB and CAR datasets in terms of the number of classes and the number of samples per class; the number of samples per class in the OP dataset is limited to 5.3 on average. In contrast to CUB and CAR, in the OP dataset the scores for softmax and for lifted structured feature embedding are nearly the same.\nFrom the results for these three datasets, we conjecture that the number of images contained in the dataset has a considerable effect on softmax-based classification. In other words, it is difficult for DML to make use of the rich information from a large number of samples because of the randomness described in the previous section. Hence, we changed the size of datasets by subsampling the images of CUB and CAR datasets for each class and ran the experiments again. We constructed seven datasets of different sizes, containing 5, 10, 20, 40, 60, 80, and 100 %, respectively, of the whole dataset. As shown in Fig. 8 and Fig. 9, the difference between the scores for softmax and DML is small or close to zero if the size of the training dataset is small. The gap between softmax and DML becomes larger as the dataset size increases. It is surprising that the scores of lifted structured feature embedding on the CUB dataset did not increase even though we used more images for the training\n(Fig. 8). It can be said that DML cannot exploit large training datasets, whereas the softmax-based classifier can obtain features of high expressiveness."}, {"heading": "5 CONCLUSION", "text": "Because there was no equitable comparison in previous studies, we conducted comparisons of the softmax-based classifier and DML methods using a design that would enable the methods to objectively demonstrate their true performance capabilities. Our results show that the features extracted from softmax-based classifiers perform better than those from state-of-the-art DML methods (Rippel et al., 2016; Song et al., 2016) on fine-grained classification, clustering, and retrieval tasks, especially when the size of the training dataset is large. The experimental results also show that softmax-based features exhibit rich intra-class diversity even though the softmax classifier is not explicitly designed to do so, unlike to the previous method (Rippel et al., 2016). It is obvious that the softmax-based features are still strong baselines. We hope that softmax-based features are taken into account when evaluating the performance of deep features.\nLimitations. When the number of classes are huge, it is hard to train classification networks due to GPU memory constraints. DML-based methods are suitable for such cases because they do not need the output layer which is proportional to the number of classes. For cross-domain tasks, such as sketches to photos (Yu et al., 2016; Sangkloy et al., 2016) or aerial views to ground views (Lin et al., 2015), DML is also effective. Classification-based learning needs complicated learning strategies like in Castrejon et al. (2016). DML-based methods can learn cross-domain representation only by using a pair of networks."}], "references": [{"title": "Benchmarking large-scale fine-grained categorization", "author": ["Anelia Angelova", "Philip M. Long"], "venue": "In WACV, pp", "citeRegEx": "Angelova and Long.,? \\Q2014\\E", "shortCiteRegEx": "Angelova and Long.", "year": 2014}, {"title": "Efficient object detection and segmentation for fine-grained recognition", "author": ["Anelia Angelova", "Shenghuo Zhu"], "venue": "In CVPR, pp", "citeRegEx": "Angelova and Zhu.,? \\Q2013\\E", "shortCiteRegEx": "Angelova and Zhu.", "year": 2013}, {"title": "Neural codes for image retrieval", "author": ["Artem Babenko", "Anton Slesarev", "Alexandr Chigorin", "Victor Lempitsky"], "venue": "In ECCV, pp", "citeRegEx": "Babenko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2014}, {"title": "Learning visual similarity for product design with convolutional neural networks", "author": ["Sean Bell", "Kavita Bala"], "venue": null, "citeRegEx": "Bell and Bala.,? \\Q2015\\E", "shortCiteRegEx": "Bell and Bala.", "year": 2015}, {"title": "Signature verification using a \u201dsiamese\u201d time delay neural network", "author": ["Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "In NIPS,", "citeRegEx": "Bromley et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1994}, {"title": "Learning aligned cross-modal representations from weakly aligned data", "author": ["Lluis Castrejon", "Yusuf Aytar", "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "In CVPR,", "citeRegEx": "Castrejon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Castrejon et al\\.", "year": 2016}, {"title": "Large scale online learning of image similarity through ranking", "author": ["Gal Chechik", "Varun Sharma", "Uri Shalit", "Samy Bengio"], "venue": "JMLR, 11:1109\u20131135,", "citeRegEx": "Chechik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2010}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In CVPR, pp", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML, pp", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Fine-grained categorization by alignments", "author": ["E. Gavves", "B. Fernando", "C.G.M. Snoek", "A.W.M. Smeulders", "T. Tuytelaars"], "venue": "In ICCV, pp", "citeRegEx": "Gavves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gavves et al\\.", "year": 2013}, {"title": "Local alignments for fine-grained categorization", "author": ["Efstratios Gavves", "Basura Fernando", "Cees G.M. Snoek", "Arnold W.M. Smeulders", "Tinne Tuytelaars"], "venue": null, "citeRegEx": "Gavves et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gavves et al\\.", "year": 2015}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In CVPR, pp", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: accelerating deep network training by reducint internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Product quantization for nearest neighbor", "author": ["Herve Jegou", "Matthijs Douze", "Cordelia Schmid"], "venue": "search. TPAMI,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Novel dataset for finegrained image categorization", "author": ["Aditya Khosla", "Nityananda Jayadevaprakash", "Bangpeng Yao", "Li Fei-Fei"], "venue": "In First Workshop on Fine-Grained Visual Categorization,", "citeRegEx": "Khosla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khosla et al\\.", "year": 2011}, {"title": "3d object representations for fine-grained categorization", "author": ["Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei"], "venue": "In 4th International IEEE Workshop on 3D Representation and Recognition,", "citeRegEx": "Krause et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Joint embeddings of shapes and images via CNN image purification", "author": ["Yangyan Li", "Hao Su", "Charles Ruizhongtai Qi", "Noa Fish", "Daniel Cohen-Or", "Leonidas J. Guibas"], "venue": "ACM TOG,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning deep representations for groundto-aerial geolocalization", "author": ["Tsung-Yi Lin", "Yin Cui", "Serge Belongie", "James Hays"], "venue": "In CVPR, pp", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "DeepIndex for accurate and efficient image retrieval", "author": ["Yu Liu", "Yanming Guo", "Song Wu", "Michael S. Lew"], "venue": "In ICMR,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Generalized max pooling", "author": ["Naila Murray", "Florent Perronnin"], "venue": "In CVPR, pp", "citeRegEx": "Murray and Perronnin.,? \\Q2014\\E", "shortCiteRegEx": "Murray and Perronnin.", "year": 2014}, {"title": "Automated flower classification over a large number of classes", "author": ["M.E. Nilsback", "A. Zisserman"], "venue": "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,", "citeRegEx": "Nilsback and Zisserman.,? \\Q2008\\E", "shortCiteRegEx": "Nilsback and Zisserman.", "year": 2008}, {"title": "Cats and dogs", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman", "C.V. Jawahar"], "venue": "In CVPR, pp", "citeRegEx": "Parkhi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parkhi et al\\.", "year": 2012}, {"title": "Fine-grained visual categorization via multistage metric learning", "author": ["Qi Qian", "Rong Jing", "Shenghuo Zhu", "Yuanqing Lin"], "venue": "In CVPR, pp", "citeRegEx": "Qian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2015}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In CVPR Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Metric learning with adaptive density discrimination", "author": ["Oren Rippel", "Manohar Paluri", "Piotr Dollar", "Lubomir Bourdev"], "venue": "In ICLR,", "citeRegEx": "Rippel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2016}, {"title": "Attribute learning in large-scale datasets", "author": ["Olga Russakovsky", "Li Fei-Fei"], "venue": "In ECCV, International Workshop on Parts and Attributes,", "citeRegEx": "Russakovsky and Fei.Fei.,? \\Q2010\\E", "shortCiteRegEx": "Russakovsky and Fei.Fei.", "year": 2010}, {"title": "The sketchy database: Learning to retrieve badly drawn bunnies", "author": ["Patsorn Sangkloy", "Nathan Burnell", "Cusuh Ham", "James Hays"], "venue": "ACM TOG,", "citeRegEx": "Sangkloy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sangkloy et al\\.", "year": 2016}, {"title": "FaceNet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In CVPR, pp", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Deep metric learning via lifted structured feature embedding", "author": ["Hyun Oh Song", "Yu Xiang", "Stefanie Jegelka", "Silvio Savarese"], "venue": "In CVPR,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR, pp", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "DeepFace: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In CVPR,", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,", "citeRegEx": "Wah et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wah et al\\.", "year": 2011}, {"title": "Deep learning for content-based image retrieval: A comprehensive study", "author": ["Ji Wan", "Dayong Wang", "Steven Chu Hong Hoi", "Pengcheng Wu", "Jianke Zhu", "Yongdong Zhang", "Jintao Li"], "venue": "In ACMMM,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Dense human body correspondences using convolutional networks", "author": ["Lingyu Wei", "Qixing Huang", "Duygu Ceylan", "Etienne Vouga", "Hao Li"], "venue": "In CVPR,", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}, {"title": "Hyper-class augmented and regularized deep learning for fine-grained image classification", "author": ["Saining Xie", "Tinbao Yang", "Xiaoyu Wang", "Yuanqing Lin"], "venue": "In CVPR,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Sketch me that shoe", "author": ["Qian Yu", "Feng Liu", "Yi-Zhe Song", "Tao Xiang", "Timothy M. Hospedales", "Chen-Change Loy"], "venue": "In CVPR,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Features extracted from classification networks show excellent performance in image classification (Donahue et al., 2014), detection, and retrieval (Razavian et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 28, "context": ", 2014), detection, and retrieval (Razavian et al., 2014; Liu et al., 2015), even when they have been trained to classify 1000 classes of the ImageNet dataset (Russakovsky et al.", "startOffset": 34, "endOffset": 75}, {"referenceID": 22, "context": ", 2014), detection, and retrieval (Razavian et al., 2014; Liu et al., 2015), even when they have been trained to classify 1000 classes of the ImageNet dataset (Russakovsky et al.", "startOffset": 34, "endOffset": 75}, {"referenceID": 38, "context": "It has also been shown that fine-tuning for target domains further improves the features\u2019 performance (Wan et al., 2014; Babenko et al., 2014).", "startOffset": 102, "endOffset": 142}, {"referenceID": 2, "context": "It has also been shown that fine-tuning for target domains further improves the features\u2019 performance (Wan et al., 2014; Babenko et al., 2014).", "startOffset": 102, "endOffset": 142}, {"referenceID": 32, "context": "End-to-end distance metric learning is a typical approach to constructing a feature extractor using convolutional neural networks and has been the focus of numerous studies (Bell & Bala, 2015; Schroff et al., 2015).", "startOffset": 173, "endOffset": 214}, {"referenceID": 29, "context": "Some DML methods have been reported to show state-of-the-art performance in fine-grained classification (Rippel et al., 2016) and clustering and retrieval (Song et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 34, "context": ", 2016) and clustering and retrieval (Song et al., 2016) contexts.", "startOffset": 37, "endOffset": 56}, {"referenceID": 19, "context": "We used LeNet architecture for both networks, and trained on the MNIST dataset (LeCun et al., 1998).", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": "For DML, we used the contrastive loss function (Hadsell et al., 2006) to map images in two-dimensional space.", "startOffset": 47, "endOffset": 69}, {"referenceID": 18, "context": "Convolutional neural networks have demonstrated great potential for highly accurate image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 102, "endOffset": 194}, {"referenceID": 35, "context": "Convolutional neural networks have demonstrated great potential for highly accurate image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 102, "endOffset": 194}, {"referenceID": 12, "context": "Convolutional neural networks have demonstrated great potential for highly accurate image recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).", "startOffset": 102, "endOffset": 194}, {"referenceID": 8, "context": "It has been shown that features extracted from classification networks can be repurposed as a good feature representation for novel tasks (Donahue et al., 2014; Razavian et al., 2014; Qian et al., 2015) even if the network was trained on ImageNet (Russakovsky et al.", "startOffset": 138, "endOffset": 202}, {"referenceID": 28, "context": "It has been shown that features extracted from classification networks can be repurposed as a good feature representation for novel tasks (Donahue et al., 2014; Razavian et al., 2014; Qian et al., 2015) even if the network was trained on ImageNet (Russakovsky et al.", "startOffset": 138, "endOffset": 202}, {"referenceID": 27, "context": "It has been shown that features extracted from classification networks can be repurposed as a good feature representation for novel tasks (Donahue et al., 2014; Razavian et al., 2014; Qian et al., 2015) even if the network was trained on ImageNet (Russakovsky et al.", "startOffset": 138, "endOffset": 202}, {"referenceID": 2, "context": "For obtaining better feature representations, fine-tuning is also effective (Babenko et al., 2014).", "startOffset": 76, "endOffset": 98}, {"referenceID": 4, "context": "Distance metric learning (DML), which learns a distance metric, has been widely studied (Bromley et al., 1994; Chopra et al., 2005; Chechik et al., 2010; Qian et al., 2015).", "startOffset": 88, "endOffset": 172}, {"referenceID": 7, "context": "Distance metric learning (DML), which learns a distance metric, has been widely studied (Bromley et al., 1994; Chopra et al., 2005; Chechik et al., 2010; Qian et al., 2015).", "startOffset": 88, "endOffset": 172}, {"referenceID": 6, "context": "Distance metric learning (DML), which learns a distance metric, has been widely studied (Bromley et al., 1994; Chopra et al., 2005; Chechik et al., 2010; Qian et al., 2015).", "startOffset": 88, "endOffset": 172}, {"referenceID": 27, "context": "Distance metric learning (DML), which learns a distance metric, has been widely studied (Bromley et al., 1994; Chopra et al., 2005; Chechik et al., 2010; Qian et al., 2015).", "startOffset": 88, "endOffset": 172}, {"referenceID": 32, "context": "Recent studies have focused on end-to-end deep distance metric learning (Bell & Bala, 2015; Schroff et al., 2015; Li et al., 2015; Rippel et al., 2016; Song et al., 2016).", "startOffset": 72, "endOffset": 170}, {"referenceID": 20, "context": "Recent studies have focused on end-to-end deep distance metric learning (Bell & Bala, 2015; Schroff et al., 2015; Li et al., 2015; Rippel et al., 2016; Song et al., 2016).", "startOffset": 72, "endOffset": 170}, {"referenceID": 29, "context": "Recent studies have focused on end-to-end deep distance metric learning (Bell & Bala, 2015; Schroff et al., 2015; Li et al., 2015; Rippel et al., 2016; Song et al., 2016).", "startOffset": 72, "endOffset": 170}, {"referenceID": 34, "context": "Recent studies have focused on end-to-end deep distance metric learning (Bell & Bala, 2015; Schroff et al., 2015; Li et al., 2015; Rippel et al., 2016; Song et al., 2016).", "startOffset": 72, "endOffset": 170}, {"referenceID": 36, "context": "(2015) used triplet loss for deep metric learning in their FaceNet, which showed performance that was state of the art at the time, but their network was deeper than that of the previous method (Taigman et al., 2014); thus, triplet loss might not have been the only reason for the performance improvement, and the contribution from adopting triplet loss remains uncertain.", "startOffset": 194, "endOffset": 216}, {"referenceID": 32, "context": "Recent studies (Schroff et al., 2015; Rippel et al., 2016; Song et al., 2016) use pairwise distances between three or more images at the same time for fast convergence and efficient calculation.", "startOffset": 15, "endOffset": 77}, {"referenceID": 29, "context": "Recent studies (Schroff et al., 2015; Rippel et al., 2016; Song et al., 2016) use pairwise distances between three or more images at the same time for fast convergence and efficient calculation.", "startOffset": 15, "endOffset": 77}, {"referenceID": 34, "context": "Recent studies (Schroff et al., 2015; Rippel et al., 2016; Song et al., 2016) use pairwise distances between three or more images at the same time for fast convergence and efficient calculation.", "startOffset": 15, "endOffset": 77}, {"referenceID": 32, "context": "DML methods sometimes require complicated operations such as hard negative sampling (Schroff et al., 2015; Rippel et al., 2016) and k-means clustering for every epoch (Rippel et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 29, "context": "DML methods sometimes require complicated operations such as hard negative sampling (Schroff et al., 2015; Rippel et al., 2016) and k-means clustering for every epoch (Rippel et al.", "startOffset": 84, "endOffset": 127}, {"referenceID": 29, "context": ", 2016) and k-means clustering for every epoch (Rippel et al., 2016).", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "Some existing methods (Babenko et al., 2014) use PCA or discriminative dimensionality reduction to reduce the number of feature dimensions.", "startOffset": 22, "endOffset": 44}, {"referenceID": 35, "context": "Following conventional PCA approaches, we extracted features from a 1024-dimensional pool5 layer of GoogLeNet (Szegedy et al., 2015; Ioffe & Szegedy, 2015) (Fig.", "startOffset": 110, "endOffset": 155}, {"referenceID": 36, "context": "Some studies used L2 normalization for deep features extracted from softmax-based classification networks (Taigman et al., 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al.", "startOffset": 106, "endOffset": 128}, {"referenceID": 18, "context": ", 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al., 2012; Rippel et al., 2016; Song et al., 2016; Wei et al., 2016).", "startOffset": 86, "endOffset": 169}, {"referenceID": 29, "context": ", 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al., 2012; Rippel et al., 2016; Song et al., 2016; Wei et al., 2016).", "startOffset": 86, "endOffset": 169}, {"referenceID": 34, "context": ", 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al., 2012; Rippel et al., 2016; Song et al., 2016; Wei et al., 2016).", "startOffset": 86, "endOffset": 169}, {"referenceID": 39, "context": ", 2014), whereas many recent studies have used the features without any normalization (Krizhevsky et al., 2012; Rippel et al., 2016; Song et al., 2016; Wei et al., 2016).", "startOffset": 86, "endOffset": 169}, {"referenceID": 29, "context": "In this section, we compare the deep features extracted from classification networks to those reported from state-of-the-art deep metric learning methods (Rippel et al., 2016; Song et al., 2016) in their performance on several tasks.", "startOffset": 154, "endOffset": 194}, {"referenceID": 34, "context": "In this section, we compare the deep features extracted from classification networks to those reported from state-of-the-art deep metric learning methods (Rippel et al., 2016; Song et al., 2016) in their performance on several tasks.", "startOffset": 154, "endOffset": 194}, {"referenceID": 35, "context": "To evaluate clustering and retrieval performances we used GoogLeNet without batch normalization (Szegedy et al., 2015) and dimentionality reduction layers.", "startOffset": 96, "endOffset": 118}, {"referenceID": 15, "context": "We used the Caffe (Jia et al., 2014) framework for our experiments.", "startOffset": 18, "endOffset": 36}, {"referenceID": 16, "context": "For the evaluation of deep features in fine-grained classification tasks, we used three image datasets: Stanford Dogs (Khosla et al., 2011), Oxford 102 Flowers (Nilsback & Zisserman, 2008), and Oxford-IIIT Pet (Parkhi et al.", "startOffset": 118, "endOffset": 139}, {"referenceID": 26, "context": ", 2011), Oxford 102 Flowers (Nilsback & Zisserman, 2008), and Oxford-IIIT Pet (Parkhi et al., 2012).", "startOffset": 78, "endOffset": 99}, {"referenceID": 29, "context": "These strategies are exactly the same as those of the previous method (Rippel et al., 2016).", "startOffset": 70, "endOffset": 91}, {"referenceID": 29, "context": "We use the ImageNet Attribute dataset (Rippel et al., 2016), which consists of overlap between the ImageNet training set (Russakovsky et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 34, "context": "Here, we give our evaluation of clustering and retrieval scores for the state-of-the-art DML method (Song et al., 2016) and for the softmax classification networks.", "startOffset": 100, "endOffset": 119}, {"referenceID": 37, "context": "We used the Caltech UCSD Birds 200-2011 (CUB) dataset (Wah et al., 2011), the Stanford Cars 196 (CAR) dataset (Krause et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 17, "context": ", 2011), the Stanford Cars 196 (CAR) dataset (Krause et al., 2013), and the Stanford Online Products (OP) dataset (Song et al.", "startOffset": 45, "endOffset": 66}, {"referenceID": 34, "context": ", 2013), and the Stanford Online Products (OP) dataset (Song et al., 2016).", "startOffset": 55, "endOffset": 74}, {"referenceID": 34, "context": "These training strategies are exactly the same as those used in the earlier study (Song et al., 2016).", "startOffset": 82, "endOffset": 101}, {"referenceID": 23, "context": "For clustering evaluation, we applied k-means clustering 100 times and calculated the average standard F1 and NMI (Manning et al., 2008); the value for k was set to the number of classes in the test set.", "startOffset": 114, "endOffset": 136}, {"referenceID": 14, "context": "For retrieval evaluation, we used the Recall@K metric (Jegou et al., 2011).", "startOffset": 54, "endOffset": 74}, {"referenceID": 34, "context": "We notice that we have been able to reproduce nearly exactly the scores of lifted structured feature embedding (Song et al., 2016).", "startOffset": 111, "endOffset": 130}], "year": 2017, "abstractText": "The extraction of useful deep features is important for many computer vision tasks. Deep features extracted from classification networks have proved to perform well in those tasks. To obtain features of greater usefulness, end-to-end distance metric learning (DML) has been applied to train the feature extractor directly. End-to-end DML approaches such as Magnet Loss and lifted structured feature embedding show state-of-the-art performance in several image recognition tasks. However, in these DML studies, there were no equitable comparisons between features extracted from a DML-based network and those from a softmaxbased network. In this paper, by presenting objective comparisons between these two approaches under the same network architecture, we show that the softmaxbased features are markedly better than the state-of-the-art DML features for tasks such as fine-grained recognition, attribute estimation, clustering, and retrieval.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_485"}