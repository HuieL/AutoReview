{"name": "ICLR_2017_51.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEPENDENCY PARSING", "Timothy Dozat", "Christopher D. Manning"], "emails": ["tdozat@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015). However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks.\nThe current state-of-the-art transition-based neural dependency parser (Kuncoro et al., 2016) substantially outperforms many much simpler neural graph-based parsers. We modify the neural graphbased approach first proposed by Kiperwasser & Goldberg (2016) in a few ways to achieve competitive performance: we build a network that\u2019s larger but uses more regularization; we replace the traditional MLP-based attention mechanism and affine label classifier with biaffine ones; and rather than using the top recurrent states of the LSTM in the biaffine transformations, we first put them through MLP operations that reduce their dimensionality. Furthermore, we compare models trained with different architectures and hyperparameters to motivate our approach empirically. The resulting parser maintains most of the simplicity of neural graph-based approaches while approaching the performance of the SOTA transition-based one."}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "Transition-based parsers\u2014such as shift-reduce parsers\u2014parse sentences from left to right, maintaining a \u201cbuffer\u201d of words that have not yet been parsed and a \u201cstack\u201d of words whose head has not been seen or whose dependents have not all been fully parsed. At each step, transition-based parsers can access and manipulate the stack and buffer and assign arcs from one word to another. One can then train any multi-class machine learning classifier on features extracted from the stack, buffer, and previous arc actions in order to predict the next action.\nChen & Manning (2014) make the first successful attempt at incorporating deep learning into a transition-based dependency parser. At each step, the (feedforward) network assigns a probability to each action the parser can take based on word, tag, and label embeddings from certain words on the\nstack and buffer. A number of other researchers have attempted to address some limitations of Chen & Manning\u2019s Chen & Manning parser by augmenting it with additional complexity: Weiss et al. (2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together.\nTransition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al. (2014) for machine translation. In Kiperwasser & Goldberg\u2019s 2016 model, the (bidirectional) LSTM\u2019s recurrent output vector for each word is concatenated with each possible head\u2019s recurrent vector, and the result is used as input to an MLP that scores each resulting arc. The predicted tree structure at training time is the one where each word depends on its highestscoring head. Labels are generated analogously, with each word\u2019s recurrent output vector and its gold or predicted head word\u2019s recurrent vector being used in a multi-class MLP.\nSimilarly, Hashimoto et al. (2016) include a graph-based dependency parser in their multi-task neural model. In addition to training the model with multiple distinct objectives, they replace the traditional MLP-based attention mechanism that Kiperwasser & Goldberg (2016) use with a bilinear one (but still using an MLP label classifier). This makes it analogous to Luong et al.\u2019s 2015 proposed attention mechanism for neural machine translation. Cheng et al. (2016) likewise propose a graph-based neural dependency parser, but in a way that attempts to circumvent the limitation of other neural graph-based parsers being unable to condition the scores of each possible arc on previous parsing decisions. In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word, they have additional, unidirectional recurrent networks (leftto-right and right-to-left) that keep track of the probabilities of each previous arc, and use these together to predict the scores for the next arc."}, {"heading": "3 PROPOSED DEPENDENCY PARSER", "text": ""}, {"heading": "3.1 DEEP BIAFFINE ATTENTION", "text": "We make a few modifications to the graph-based architectures of Kiperwasser & Goldberg (2016), Hashimoto et al. (2016), and Cheng et al. (2016), shown in Figure 2: we use biaffine attention instead of bilinear or traditional MLP-based attention; we use a biaffine dependency label classifier; and we apply dimension-reducing MLPs to each recurrent output vector ri before applying the biaffine transformation.1 The choice of biaffine rather than bilinear or MLP mechanisms makes the classifiers in our model analogous to traditional affine classifiers, which use an affine transformation over a single LSTM output state ri (or other vector input) to predict the vector of scores si for all classes (1). We can think of the proposed biaffine attention mechanism as being a traditional affine\n1In this paper we follow the convention of using lowercase italic letters for scalars and indices, lowercase bold letters for vectors, uppercase italic letters for matrices, uppercase bold letters for higher order tensors. We also maintain this notation when indexing; so row i of matrix R would be represented as ri.\nclassifier, but using a (d \u00d7 d) linear transformation of the stacked LSTM output RU (1) in place of the weight matrix W and a (d\u00d7 1) transformation Ru(2) for the bias term b (2).\nsi =Wri + b Fixed-class affine classifier (1)\ns (arc) i =\n( RU (1) ) ri + ( Ru(2) ) Variable-class biaffine classifier (2)\nIn addition to being arguably simpler than the MLP-based approach (involving one bilinear layer rather than two linear layers and a nonlinearity), this has the conceptual advantage of directly modeling both the prior probability of a word j receiving any dependents in the term r>j u\n(2) and the likelihood of j receiving a specific dependent i in the term r>j U\n(1)ri. Analogously, we also use a biaffine classifier to predict dependency labels given the gold or predicted head yi (3).\ns (label) i = r > yiU (1)ri + (ryi \u2295 ri)>U (2) + b Fixed-class biaffine classifier (3) This likewise directly models each of the prior probability of each class, the likelihood of a class given just word i (how probable a word is to take a particular label), the likelihood of a class given just the head word yi (how probable a word is to take dependents with a particular label), and the likelihood of a class given both word i and its head (how probable a word is to take a particular label given that word\u2019s head).\nApplying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision. That is, every top recurrent state ri will need to carry enough information to identify word i\u2019s head, find all its dependents, exclude all its non-dependents, assign itself the correct label, and assign all its dependents their correct labels, as well as transfer any relevant information to the recurrent states of words before and after it. Thus ri necessarily contains significantly more information than is needed to compute any individual score, and training on this superfluous information needlessly reduces parsing speed and increases the risk of overfitting. Reducing dimensionality and applying a nonlinearity (4 - 6) addresses both of these problems. We call this a deep bilinear attention mechanism, as opposed to shallow bilinear attention, which uses the recurrent states directly.\nh (arc-dep) i = MLP (arc-dep)(ri) (4)\nh (arc-head) j = MLP (arc-head)(rj) (5)\ns (arc) i = H (arc-head)U (1)h (arc-dep) i (6)\n+H(arc-head)u(2)\nWe apply MLPs to the recurrent states before using them in the label classifier as well. As with other graph-based models, the predicted tree at training time is the one where each word is a dependent of its highest scoring head (although at test time we ensure that the parse is a well-formed tree via the MST algorithm)."}, {"heading": "3.2 HYPERPARAMETER CONFIGURATION", "text": "Aside from architectural differences between ours and the other graph-based parsers, we make a number of hyperparameter choices that allow us to outperform theirs, laid out in Table 1. We use 100-dimensional uncased word vectors2 and POS tag vectors; three BiLSTM layers (400 dimensions in each direction); and 500- and 100-dimensional ReLU MLP layers. We also apply dropout at every stage of the model: we drop words and tags (independently); we drop nodes in the LSTM layers (input and recurrent connections), applying the same dropout mask at every recurrent timestep (cf. the Bayesian dropout of Gal & Ghahramani (2015)); and we drop nodes in the MLP layers and classifiers, likewise applying the same dropout mask at every timestep. We optimize the network with annealed Adam (Kingma & Ba, 2014) for about 50,000 steps, rounded up to the nearest epoch."}, {"heading": "4 EXPERIMENTS & RESULTS", "text": ""}, {"heading": "4.1 DATASETS", "text": "We show test results for the proposed model on the English Penn Treebank, converted into Stanford Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter (PTB-SD 3.3.0 and PTB-SD 3.5.0); the Chinese Penn Treebank; and the CoNLL 09 shared task dataset,3 following standard practices for each dataset. We omit punctuation from evaluation only for the PTB-SD and CTB. For the English PTB-SD datasets, we use POS tags generated from the Stanford POS tagger (Toutanova et al., 2003); for the Chinese PTB dataset we use gold tags; and for the CoNLL 09 dataset we use the provided predicted tags. Our hyperparameter search was done with the PTB-SD 3.5.0 validation dataset in order to minimize overfitting to the more popular PTB-SD 3.3.0 benchmark, and in our hyperparameter analysis in the following section we report performance on the PTB-SD 3.5.0 test set, shown in Tables 2 and 3."}, {"heading": "4.2 HYPERPARAMETER CHOICES", "text": ""}, {"heading": "4.2.1 ATTENTION MECHANISM", "text": "We examined the effect of different classifier architectures on accuracy and performance. What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy. The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings, but because the label classifier is much larger ((801\u00d7c\u00d7801) as opposed to (101\u00d7 c\u00d7 101)), it runs much slower and overfits. One way to decrease this overfitting is by increasing the MLP dropout, but that of course doesn\u2019t change parsing speed; another way is to decrease the recurrent size to 300, but this hinders unlabeled accuracy without increasing parsing speed up to the same levels as our deeper model. We also implemented the MLP-based approach to attention and classification used in Kiperwasser & Goldberg (2016).4 We found this version to\n2We compute a \u201ctrained\u201d embedding matrix composed of words that occur at least twice in the training dataset and add these embeddings to their corresponding pretrained embeddings. Any words that don\u2019t occur in either embedding matrix are replaced with a separate OOV token.\n3We exclude the Japanese dataset from our evaluation because we do not have access to it. 4In the version of TensorFlow we used, the model\u2019s memory requirements during training exceeded the available memory on a single GPU when default settings were used, so we reduced the MLP hidden size to 200\nlikewise be somewhat slower and significantly underperform the deep biaffine approach in both labeled and unlabeled accuracy."}, {"heading": "4.2.2 NETWORK SIZE", "text": "We also examine more closely how network size influences speed and accuracy. In Kiperwasser & Goldberg\u2019s 2016 model, the network uses 2 layers of 125-dimensional bidirectional LSTMs; in Hashimoto et al.\u2019s 2016 model, it has one layer of 100-dimensional bidirectional LSTMs dedicated to parsing (two lower layers are also trained on other objectives); and Cheng et al.\u2019s 2016 model has one layer of 368-dimensional GRU cells. We find that using three or four layers gets significantly better performance than two layers, and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance.5"}, {"heading": "4.2.3 RECURRENT CELL", "text": "GRU cells have been promoted as a faster and simpler alternative to LSTM cells, and are used in the approach of Cheng et al. (2016); however, in our model they drastically underperformed LSTM cells. We also implemented the coupled input-forget gate LSTM cells (Cif-LSTM) suggested by Greff et al. (2015),6 finding that while the resulting model still slightly underperforms the more popular LSTM cells, the difference between the two is much smaller. Additionally, because the gate and candidate cell activations can be computed simultaneously with one matrix multiplication, the Cif-LSTM model is faster than the GRU version even though they have the same number of parameters. We hypothesize that the output gate in the Cif-LSTM model allows it to maintain a sparse recurrent output state, which helps it adapt to the high levels of dropout needed to prevent overfitting in a way that GRU cells are unable to do.\n5The model with 400-dimensional recurrent states significantly outperforms the 300-dimensional one on the validation set, but not on the test set\n6In addition to using a coupled input-forget gate, we remove the first tanh nonlinearity, which is no longer needed when using a coupled gate"}, {"heading": "4.2.4 EMBEDDING DROPOUT", "text": "Because we increase the parser\u2019s power, we also have to increase its regularization. In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in Table 1, we also regularize the input layer. We drop 33% of words and 33% of tags during training: when one is dropped the other is scaled by a factor of two to compensate, and when both are dropped together, the model simply gets an input of zeros. Models trained with only word or tag dropout but not both wind up signficantly overfitting, hindering label accuracy and\u2014in the latter case\u2014attachment accuracy. Interestingly, not using any tags at all actually results in better performance than using tags without dropout."}, {"heading": "4.2.5 OPTIMIZER", "text": "We choose to optimize with Adam (Kingma & Ba, 2014), which (among other things) keeps a moving average of the L2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average, ensuring that the magnitude of the gradients will on average be close to one. However, we find that the value for \u03b22 recommended by Kingma & Ba\u2014which controls the decay rate for this moving average\u2014is too high for this task (and we suspect more generally). When this value is very large, the magnitude of the current update is heavily influenced by the larger magnitude of gradients very far in the past, with the effect that the optimizer can\u2019t adapt quickly to recent changes in the model. Thus we find that setting \u03b22 to .9 instead of .999 makes a large positive impact on final performance."}, {"heading": "4.3 RESULTS", "text": "Our model gets nearly the same UAS performance on PTB-SD 3.3.0 as the current SOTA model from Kuncoro et al. (2016) in spite of its substantially simpler architecture, and gets SOTA UAS performance on CTB 5.17 as well as SOTA performance on all CoNLL 09 languages. It is worth noting that the CoNLL 09 datasets contain many non-projective dependencies, which are difficult or impossible for transition-based\u2014but not graph-based\u2014parsers to predict. This may account for some of the large, consistent difference between our model and Andor et al.\u2019s 2016 transition-based model applied to these datasets.\n7We\u2019d like to thank Zhiyang Teng for finding a bug in the original code that affected the CTB 5.1 dataset\nWhere our model appears to lag behind the SOTA model is in LAS, indicating one of a few possibilities. Firstly, it may be the result of inefficiencies or errors in the GloVe embeddings or POS tagger, in which case using alternative pretrained embeddings or a more accurate tagger might improve label classification. Secondly, the SOTA model is specifically designed to capture phrasal compositionality; so another possibility is that ours doesn\u2019t capture this compositionality as effectively, and that this results in a worse label score. Similarly, it may be the result of a more general limitation of graph-based parsers, which have access to less explicit syntactic information than transition-based parsers when making decisions. Addressing these latter two limitations would require a more innovative architecture than the relatively simple one used in current neural graph-based parsers."}, {"heading": "5 CONCLUSION", "text": "In this paper we proposed using a modified version of bilinear attention in a neural dependency parser that increases parsing speed without hurting performance. We showed that our larger but more regularized network outperforms other neural graph-based parsers and gets comparable performance to the current SOTA transition-based parser. We also provided empirical motivation for the proposed architecture and configuration over similar ones in the existing literature. Future work will involve exploring ways of bridging the gap between labeled and unlabeled accuracy and augment the parser with a smarter way of handling out-of-vocabulary tokens for morphologically richer languages."}], "references": [{"title": "Globally normalized transition-based neural networks. In Association for Computational Linguistics, 2016", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "URL https://arxiv.org/abs/1603", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Leveraging linguistic structure for open domain information extraction", "author": ["Gabor Angeli", "Melvin Johnson Premkumar", "Christopher D Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Angeli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Gal and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "A joint many-task model: Growing a neural network for multiple nlp tasks", "author": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "venue": "arXiv preprint arXiv:1611.01587,", "citeRegEx": "Hashimoto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "CoRR, abs/1611.05774,", "citeRegEx": "Kuncoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Dependency-based word embeddings", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "ACL", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Grounded semantic parsing for complex knowledge extraction", "author": ["Ankur P Parikh", "Hoifung Poon", "Kristina Toutanova"], "venue": "In Proceedings of North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Parikh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2015}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Compositional learning of embeddings for relation paths in knowledge bases and text", "author": ["Kristina Toutanova", "Xi Victoria Lin", "Wen-tau Yih"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "2%\u2014and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.", "startOffset": 68, "endOffset": 90}, {"referenceID": 4, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015).", "startOffset": 262, "endOffset": 372}, {"referenceID": 1, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015).", "startOffset": 262, "endOffset": 372}, {"referenceID": 18, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015).", "startOffset": 262, "endOffset": 372}, {"referenceID": 16, "context": "Dependency parsers\u2014which annotate sentences in a way designed to be easy for humans and computers alike to understand\u2014have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015).", "startOffset": 262, "endOffset": 372}, {"referenceID": 13, "context": "The current state-of-the-art transition-based neural dependency parser (Kuncoro et al., 2016) substantially outperforms many much simpler neural graph-based parsers.", "startOffset": 71, "endOffset": 93}, {"referenceID": 1, "context": ", 2016; Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015). However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks. The current state-of-the-art transition-based neural dependency parser (Kuncoro et al., 2016) substantially outperforms many much simpler neural graph-based parsers. We modify the neural graphbased approach first proposed by Kiperwasser & Goldberg (2016) in a few ways to achieve competitive performance: we build a network that\u2019s larger but uses more regularization; we replace the traditional MLP-based attention mechanism and affine label classifier with biaffine ones; and rather than using the top recurrent states of the LSTM in the biaffine transformations, we first put them through MLP operations that reduce their dimensionality.", "startOffset": 8, "endOffset": 544}, {"referenceID": 13, "context": "(2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together.", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "A number of other researchers have attempted to address some limitations of Chen & Manning\u2019s Chen & Manning parser by augmenting it with additional complexity: Weiss et al. (2015) and Andor et al.", "startOffset": 160, "endOffset": 180}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together. Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al.", "startOffset": 11, "endOffset": 887}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together. Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al. (2014) for machine translation.", "startOffset": 11, "endOffset": 1036}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together. Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al. (2014) for machine translation. In Kiperwasser & Goldberg\u2019s 2016 model, the (bidirectional) LSTM\u2019s recurrent output vector for each word is concatenated with each possible head\u2019s recurrent vector, and the result is used as input to an MLP that scores each resulting arc. The predicted tree structure at training time is the one where each word depends on its highestscoring head. Labels are generated analogously, with each word\u2019s recurrent output vector and its gold or predicted head word\u2019s recurrent vector being used in a multi-class MLP. Similarly, Hashimoto et al. (2016) include a graph-based dependency parser in their multi-task neural model.", "startOffset": 11, "endOffset": 1607}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together. Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al. (2014) for machine translation. In Kiperwasser & Goldberg\u2019s 2016 model, the (bidirectional) LSTM\u2019s recurrent output vector for each word is concatenated with each possible head\u2019s recurrent vector, and the result is used as input to an MLP that scores each resulting arc. The predicted tree structure at training time is the one where each word depends on its highestscoring head. Labels are generated analogously, with each word\u2019s recurrent output vector and its gold or predicted head word\u2019s recurrent vector being used in a multi-class MLP. Similarly, Hashimoto et al. (2016) include a graph-based dependency parser in their multi-task neural model. In addition to training the model with multiple distinct objectives, they replace the traditional MLP-based attention mechanism that Kiperwasser & Goldberg (2016) use with a bilinear one (but still using an MLP label classifier).", "startOffset": 11, "endOffset": 1844}, {"referenceID": 0, "context": "(2015) and Andor et al. (2016) augment it with a beam search and a conditional random field loss objective to allow the parser to \u201cundo\u201d previous actions once it finds evidence that they may have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to represent the stack and buffer, getting state-of-the-art performance by building in a way of composing parsed phrases together. Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a time. Consequently, these parsers don\u2019t use machine learning for directly predicting edges; they use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast, use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention mechanism as Bahdanau et al. (2014) for machine translation. In Kiperwasser & Goldberg\u2019s 2016 model, the (bidirectional) LSTM\u2019s recurrent output vector for each word is concatenated with each possible head\u2019s recurrent vector, and the result is used as input to an MLP that scores each resulting arc. The predicted tree structure at training time is the one where each word depends on its highestscoring head. Labels are generated analogously, with each word\u2019s recurrent output vector and its gold or predicted head word\u2019s recurrent vector being used in a multi-class MLP. Similarly, Hashimoto et al. (2016) include a graph-based dependency parser in their multi-task neural model. In addition to training the model with multiple distinct objectives, they replace the traditional MLP-based attention mechanism that Kiperwasser & Goldberg (2016) use with a bilinear one (but still using an MLP label classifier). This makes it analogous to Luong et al.\u2019s 2015 proposed attention mechanism for neural machine translation. Cheng et al. (2016) likewise propose a graph-based neural dependency parser, but in a way that attempts to circumvent the limitation of other neural graph-based parsers being unable to condition the scores of each possible arc on previous parsing decisions.", "startOffset": 11, "endOffset": 2039}, {"referenceID": 9, "context": "We make a few modifications to the graph-based architectures of Kiperwasser & Goldberg (2016), Hashimoto et al. (2016), and Cheng et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 6, "context": "(2016), and Cheng et al. (2016), shown in Figure 2: we use biaffine attention instead of bilinear or traditional MLP-based attention; we use a biaffine dependency label classifier; and we apply dimension-reducing MLPs to each recurrent output vector ri before applying the biaffine transformation.", "startOffset": 12, "endOffset": 32}, {"referenceID": 17, "context": "For the English PTB-SD datasets, we use POS tags generated from the Stanford POS tagger (Toutanova et al., 2003); for the Chinese PTB dataset we use gold tags; and for the CoNLL 09 dataset we use the provided predicted tags.", "startOffset": 88, "endOffset": 112}, {"referenceID": 6, "context": "GRU cells have been promoted as a faster and simpler alternative to LSTM cells, and are used in the approach of Cheng et al. (2016); however, in our model they drastically underperformed LSTM cells.", "startOffset": 112, "endOffset": 132}, {"referenceID": 6, "context": "GRU cells have been promoted as a faster and simpler alternative to LSTM cells, and are used in the approach of Cheng et al. (2016); however, in our model they drastically underperformed LSTM cells. We also implemented the coupled input-forget gate LSTM cells (Cif-LSTM) suggested by Greff et al. (2015),6 finding that while the resulting model still slightly underperforms the more popular LSTM cells, the difference between the two is much smaller.", "startOffset": 112, "endOffset": 304}, {"referenceID": 12, "context": "0 as the current SOTA model from Kuncoro et al. (2016) in spite of its substantially simpler architecture, and gets SOTA UAS performance on CTB 5.", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark\u2014 outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%\u2014and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_51"}