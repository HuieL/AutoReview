{"name": "ICLR_2017_58.pdf", "metadata": {"source": "CRF", "title": "OPTIMIZED MAXIMUM MEAN DISCREPANCY", "authors": ["Dougal J. Sutherland", "Hsiao-Yu Tung", "Heiko Strathmann", "Soumyajit De", "Aaditya Ramdas", "Alex Smola", "Arthur Gretton"], "emails": ["dougal@gmail.com", "htung@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many problems in testing and learning require evaluating distribution similarity in high dimensions, and on structured data such as images or audio. When a complex generative model is learned, it is necessary to provide feedback on the quality of the samples produced. The generative adversarial network (Goodfellow et al., 2014; Gutmann et al., 2014) is a popular method for training generative models, where a rival discriminator attempts to distinguish model samples from reference data. Training of the generator and discriminator is interleaved, such that a saddle point is eventually reached in the joint loss.\nA useful insight into the behavior of GANs is to note that when the discriminator is properly trained, the generator is tasked with minimizing the Jensen-Shannon divergence measure between the model and data distributions. When the model is insufficiently powerful to perfectly simulate the test data, as in most nontrivial settings, the choice of divergence measure is especially crucial: it determines which compromises will be made. A range of adversarial divergences were proposed by Huszar (2015), using a weight to interpolate between KL, inverse KL, and Jensen-Shannon. This weight may be interpreted as a prior probability of observing samples from the model or the real world: when there is a greater probability of model samples, we approach reverse KL and the model seeks out modes of the data distribution. When there is a greater probability of drawing from the data distribution, the model approaches the KL divergence, and tries to cover the full support of the data, at the expense of producing some samples in low probability regions.\nThis insight was further developed by Nowozin et al. (2016), who showed that a much broader range of f -divergences can be learned for the discriminator in adversarial models, based on the variational formulation of f -divergences of Nguyen et al. (2008). For a given f -divergence, the model learns the composition of the density ratio (of data to model density) with the derivative of f , by comparing generator and data samples. This provides a lower bound on the \u201ctrue\u201d divergence that would be\nobtained if the density ratio were perfectly known. In the event that the model is in a smaller class than the true data distribution, this broader family of divergences implements a variety of different approximations: some focus on individual modes of the true sample density, others try to cover the support. It is straightforward to visualize these properties in one or two dimensions (Nowozin et al., 2016, Figure 5), but in higher dimensions it becomes difficult to anticipate or visualize the behavior of these various divergences.\nAn alternative family of divergences are the integral probability metrics (M\u00fcller, 1997), which find a witness function to distinguish samples from P and Q.1 A popular such class of witness functions in GANs is the maximum mean discrepancy (Gretton et al., 2012a), simultaneously proposed by Dziugaite et al. (2015) and Li et al. (2015). The architecture used in these two approaches is actually quite different: Dziugaite et al. use the MMD as a discriminator directly at the level of the generated and test images, whereas Li et al. apply the MMD on input features learned from an autoencoder, and share the decoding layers of the autoencoder with the generator network (see their Figure 1(b)). The generated samples have better visual quality in the latter method, but it becomes difficult to analyze and interpret the algorithm given the interplay between the generator and discriminator networks. In a related approach, Salimans et al. (2016) propose to use feature matching, where the generator is tasked with minimizing the squared distance between expected discriminator features under the model and data distributions, thus retaining the adversarial setting.\nIn light of these varied approaches to discriminator training, it is important to be able to evaluate quality of samples from a generator against reference data. An approach used in several studies is to obtain a Parzen window estimate of the density and compute the log-likelhiood (Goodfellow et al., 2014; Nowozin et al., 2016; Breuleux et al., 2011). Unfortunately, density estimates in such high dimensions are known to be very unreliable both in theory (Wasserman, 2006, Ch. 6) and in practice (Theis et al., 2016). We can instead ask humans to evaluate the generated images (Denton et al., 2015; Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images. Recall that different divergence measures result in different degrees of mode-seeking: if we rely on human evaluation, we may tend towards always using divergences with under-dispersed samples.\nWe propose to use the MMD to distinguish generator and reference data, with features and kernels chosen to maximize the test power of the quadratic-time MMD of Gretton et al. (2012a). Optimizing MMD test power requires a sophisticated treatment due to the different form of the null and alternative distributions (Section 2). We also develop an efficient approach to obtaining quantiles of the MMD distribution under the null (Section 3). We demonstrate on simple artificial data that simply maximizing the MMD (as in Sriperumbudur et al., 2009) provides a less powerful test than our approach of explicitly maximizing test power. Our procedure applies even when our definition of the MMD is computed on features of the inputs, since these can also be trained by power maximization.\nWhen designing an optimized MMD test, we should choose a kernel family that allows us to visualize where the probability mass of the two samples differs most. In our experiments on GAN performance evaluation, we use an automatic relevance determination (ARD) kernel over the output dimensions, and learn which coordinates differ meaningfully by finding which kernels retain significant bandwidth when the test power is optimized. We may further apply the method of Lloyd & Ghahramani (2015, Section 5) to visualize the witness function associated with this MMD, by finding those model and data samples occurring at the maxima and minima of the witness function (i.e., the samples from one distribution least likely to be in high probability regions of the other). The optimized witness function gives a test with greater power than a standard RBF kernel, suggesting that the associated witness function peaks are an improved representation of where the distributions differ. We also propose a novel generative model based on the feature matching idea of Salimans et al. (2016), using MMD rather than their \u201cminibatch discrimination\u201d heuristic, for a more principled and more stable enforcement of sample diversity, without requiring labeled data.\n1Only the total variation distance is both an f -divergence and an IPM (Sriperumbudur et al., 2012)."}, {"heading": "2 MAXIMIZING TEST POWER OF A QUADRATIC MMD TEST", "text": "Our methods rely on optimizing the power of a two-sample test over the choice of kernel. We first describe how to do this, then review alternative kernel selection approaches."}, {"heading": "2.1 MMD AND TEST POWER", "text": "We will begin by reviewing the maximum mean discrepancy and its use in two-sample tests. Let k be the kernel of a reproducing kernel Hilbert space (RKHS)Hk of functions on a set X . We assume that k is measurable and bounded, supx\u2208X k(x, x) <\u221e. Then the MMD inHk between two distributions P and Q over X is (Gretton et al., 2012a):\nMMD2k(P,Q) := Ex,x\u2032 [k(x, x\u2032)] + Ey,y\u2032 [k(y, y\u2032)]\u2212 2Ex,y [k(x, y)] (1)\nwhere x, x\u2032 iid\u223c P and y, y\u2032 iid\u223c Q. Many kernels, including the popular Gaussian RBF, are characteristic (Fukumizu et al., 2008; Sriperumbudur et al., 2010), which implies that the MMD is a metric, and in particular that MMDk(P,Q) = 0 if and only if P = Q, so that tests with any characteristic kernel are consistent. That said, different characteristic kernels will yield different test powers for finite sample sizes, and so we wish to choose a kernel k to maximize the test power. Below, we will usually suppress explicit dependence on k.\nGiven X = {X1, . . . , Xm} iid\u223c P and Y = {Y1, . . . , Ym} iid\u223c Q,2 one estimator of MMD(P,Q) is\nM\u0302MD 2 U(X,Y ) := 1( m 2 ) \u2211 i6=i\u2032 k(Xi, Xi\u2032) + 1( m 2 ) \u2211 j 6=j\u2032 k(Yj , Yj\u2032)\u2212 2( m 2 ) \u2211 i6=j k(Xi, Yj). (2)\nThis estimator is unbiased, and has nearly minimal variance among unbiased estimators (Gretton et al., 2012a, Lemma 6).\nFollowing Gretton et al. (2012a), we will conduct a hypothesis test with null hypothesis H0 : P = Q and alternative H1 : P 6= Q, using test statistic m M\u0302MD 2\nU(X,Y ). For a given allowable probability of false rejection \u03b1, we choose a test threshold c\u03b1 and reject H0 if m M\u0302MD 2 U(X,Y ) > c\u03b1.\nUnder H0 : P = Q, m M\u0302MD 2\nU(X,Y ) converges asymptotically to a distribution that depends on the unknown distribution P (Gretton et al., 2012a, Theorem 12); we thus cannot evaluate the test threshold c\u03b1 in closed form. We instead estimate a data-dependent threshold c\u0302\u03b1 via permutation: randomly partition the data X \u222a Y into X \u2032 and Y \u2032 many times, evaluate m M\u0302MD2U(X \u2032, Y \u2032) on each split, and estimate the (1 \u2212 \u03b1)th quantile c\u0302\u03b1 from these samples. Section 3 discusses efficient computation of this process.\nWe now describe a mechanism to choose the kernel k so as to maximize the power of its associated test. First, note that under the alternative H1 : P 6= Q, M\u0302MD 2 U is asymptotically normal,\nM\u0302MD 2 U(X,Y )\u2212 MMD2(P,Q)\u221a Vm(P,Q) D\u2192 N (0, 1), (3)\nwhere Vm(P,Q) denotes the asymptotic variance of the M\u0302MD 2\nU estimator for samples of size m from P and Q (Serfling, 1980). The power of our test is thus, using Pr1 to denote probability under H1,\nPr1\n( m M\u0302MD 2\nU(X,Y ) > c\u0302\u03b1 ) = Pr1\n( M\u0302MD 2\nU(X,Y )\u2212 MMD2(P,Q)\u221a Vm(P,Q) > c\u0302\u03b1/m\u2212 MMD2(P,Q)\u221a Vm(P,Q)\n)\n\u2192 \u03a6 (\nMMD2(P,Q)\u221a Vm(P,Q) \u2212 c\u03b1 m \u221a Vm(P,Q)\n) (4)\nwhere \u03a6 is the CDF of the standard normal distribution. The second step follows by (3) and the convergence of c\u0302\u03b1 \u2192 c\u03b1 (Alba Fern\u00e1ndez et al., 2008). Test power is therefore maximized by\n2We assume for simplicity that the number of samples from the two distributions is equal.\nmaximizing the argument of \u03a6: i.e. increasing the ratio of MMD2(P,Q) to \u221a Vm(P,Q), and\nreducing the ratio of c\u03b1 to m \u221a Vm(P,Q). For a given kernel k, Vm is O(m\u22121), while both c\u03b1 and MMD2 are constants. Thus the first term is O (\u221a m ) , and the second is O ( 1/ \u221a m ) . Two situations therefore arise: when m is small relative to the difference in P and Q (i.e., we are close to the null), both terms need to be taken into acccount to maximize test power. Here, we propose to maximize (4) using the efficient computation of c\u0302\u03b1 in Section 3. As m grows, however, we can asymptotically maximize the power of the test by\nchoosing a kernel k that maximizes the t-statistic tk(P,Q) := MMD2k(P,Q)/ \u221a V (k) m (P,Q). In\npractice, we maximize an estimator of tk(P,Q) given by t\u0302k(X,Y ) := M\u0302MD 2 U(X,Y )/ \u221a V\u0302m(X,Y ),\nwith V\u0302m(X,Y ) discussed shortly.\nTo maintain the validity of the hypothesis test, we will need to divide the observed data X and Y into a \u201ctraining sample,\u201d used to choose the kernel, and a \u201ctesting sample,\u201d used to perform the final hypothesis test with the learned kernel.\nWe next consider families of kernels over which to optimize. The most common kernels used for MMD tests are standard kernels from the literature, e.g. the Gaussian RBF, Mat\u00e9rn, or Laplacian kernels. It is the case, however, that for any function z : X1 \u2192 X2 and any kernel \u03ba : X2 \u00d7X2 \u2192 R, the composition \u03ba \u25e6 z is also a kernel on X1.3 We can thus choose a function z to extract meaningful features of the inputs, and use a standard kernel \u03ba to compare those features. We can select such a function z (as well as \u03ba) by performing kernel selection on the family of kernels \u03ba \u25e6 z. To do so, we merely need to maximize t\u0302\u03ba\u25e6z(X,Y ) through standard optimization techniques based on the gradient of t\u0302\u03ba\u25e6z with respect to the parameterizations of z and \u03ba.\nWe now give an expression for an empirical estimate V\u0302m of the variance Vm(P,Q) that appears in our test power. This estimate is similar to that given by Bounliphone et al. (2016, Appendix A.1), but incorporates second-order terms and corrects some small sources of bias. Though the expression is somewhat unwieldy, it is defined by various sums of the kernel matrices and is differentiable with respect to the kernel k.\nVm(P,Q) is given in terms of expectations of k under P and Q in Appendix A. We replace these expectations with finite-sample averages, giving us the required estimator. Define matrices KXY , K\u0303XX , and K\u0303Y Y by (KXY )i,j = k(Xi, Yj), (K\u0303XX)ii = 0, (K\u0303XX)ij = k(Xi, Xj) for i 6= j, and K\u0303Y Y similarly to K\u0303XX . Let e be an m-vector of ones, and use the falling factorial notation (m)r := m(m\u2212 1) \u00b7 \u00b7 \u00b7 (m\u2212 r + 1). Then an unbiased estimator for Vm(P,Q) is:\nV\u0302m := 4\n(m)4\n[\u2225\u2225\u2225K\u0303XXe\u2225\u2225\u22252 + \u2225\u2225\u2225K\u0303Y Y e\u2225\u2225\u22252]+ 4(m2 \u2212m\u2212 1) m3(m\u2212 1)2 [ \u2016KXY e\u20162 + \u2016KTXY e\u20162 ] \u2212 8 m2(m2 \u2212 3m+ 2) [ eTK\u0303XXKXY e+ e TK\u0303Y YK T XY e\n] + 8\nm2(m)3\n[( eTK\u0303XXe+ e TK\u0303Y Y e ) ( eTKXY e )] \u2212 2(2m\u2212 3)\n(m)2(m)4\n[( eTK\u0303XXe )2 + ( eTK\u0303Y Y e )2] \u2212 4(2m\u2212 3) m3(m\u2212 1)3 [( eTKXY e )2] \u2212 2 m(m3 \u2212 6m2 + 11m\u2212 6) [\u2225\u2225\u2225K\u0303XX\u2225\u2225\u22252 F + \u2225\u2225\u2225K\u0303Y Y \u2225\u2225\u22252 F ] + 4(m\u2212 2) m2(m\u2212 1)3 \u2016KXY \u2016 2 F . (5)"}, {"heading": "2.2 OTHER APPROACHES TO MMD KERNEL SELECTION", "text": "The most common practice in performing two-sample tests with MMD is to use a Gaussian RBF kernel, with bandwidth set to the median pairwise distance among the joint data. This heuristic often works well, but fails when the scale on which P and Q vary differs from the scale of their overall variation (as in the synthetic experiment of Section 4). Ramdas et al. (2015a;b) study the power of\n3If z is injective and \u03ba characteristic, then \u03ba \u25e6 z is characteristic. Whether any fixed \u03ba \u25e6 z is consistent, however, is less relevant than the power of the \u03ba \u25e6 z we choose \u2014 which is what we maximize.\nthe median heuristic in high-dimensional problems, and justify its use for the case where the means of P and Q differ.\nAn early heuristic for improving test power was to simply maximize M\u0302MD 2\nU. Sriperumbudur et al. (2009) proved that, for certain classes of kernels, this yields a consistent test. As further shown by Sriperumbudur et al., however, maximizing MMD amounts to minimizing training classification error under linear loss. Comparing with (4), this is plainly not an optimal approach for maximizing test power, since variance is ignored.4 One can also consider maximizing criteria based on cross validation (Sugiyama et al., 2011; Gretton et al., 2012b; Strathmann, 2012). This approach is not differentiable, and thus difficult to maximize among more than a fixed set of candidate kernels. Moreover, where this cross-validation is used to maximize the MMD on a validation set (as in Sugiyama et al., 2011), it again amounts to maximizing classification performance rather than test performance, and is suboptimal in the latter setting (Gretton et al., 2012b, Figure 1). Finally, Gretton et al. (2012b) previously studied direct optimization of the power of an MMD test for a streaming estimator of the MMD, for which optimizing the ratio of the empirical statistic to its variance also optimizes test power. This streaming estimator uses data very inefficiently, however, often requiring m2 samples to achieve power comparable to tests based on M\u0302MD 2\nU with m samples (Ramdas et al., 2015a).\n3 EFFICIENT IMPLEMENTATION OF PERMUTATION TESTS FOR M\u0302MD 2\nU\nPractical implementations of tests based on M\u0302MD 2\nU require efficient estimates of the test threshold c\u0302\u03b1. There are two known test threshold estimates that lead to a consistent test: the permutation test mentioned above, and a more sophisticated null distribution estimate based on approximating the eigenspectrum of the kernel, previously reported to be faster than the permutation test (Gretton et al., 2009). In fact, the relatively slow reported performance of the permutation approach was due to the naive Matlab implementation of the permutation test in the code accompanying Gretton et al. (2012a), which creates a new copy of the kernel matrix for every permutation. We show here that, by careful design, permutation thresholds can be computed substantially faster \u2013 even when compared to parallelized state-of-the-art spectral solvers (not used by Gretton et al.).\nFirst, we observe that we can avoid copying the kernel matrix simply by generating permutation indices for each null sample and accessing the precomputed kernel matrix in permuted order. In practice, however, this does not give much performance gain due to the random nature of memoryaccess which conflicts with how modern CPUs implement caching. Second, if we rather maintain an inverse map of the permutation indices, we can easily traverse the matrix in a sequential fashion. This approach exploits the hardware prefetchers and reduces the number of CPU cache misses from almost 100% to less than 10%. Furthermore, the sequential access pattern of the kernel matrix enables us to invoke multiple threads for computing the null samples, each traversing the matrix sequentially, without compromising the locality of reference in the CPU cache.\nWe consider an example problem of computing the test using 200 null distribution samples on m = 2000 two-dimensional samples, comparing a Gaussian to a Laplace distribution with matched moments. We compare our optimized permutation test against a spectral test using the highlyoptimized (and proprietary) state-of-the-art spectral solver of Intel\u2019s MKL library (Intel, 2003\u201317). All results are averaged over 30 runs; the variance across runs was negligible.\nFigure 1 (left) shows the obtained speedups as the number of computing threads grow for m = 2000. Our implementation is not only faster on a single thread, but also saturates more slowly as the number of threads increases. Figure 1 (right) shows timings for increasing problem sizes (i.e. m) when using all available system threads (here 24). For larger problems, our permutation implementation (scaling as O(m2)) is an order of magnitude faster than the spectral test (scaling as O(m3)). For smaller\n4With regards to classification vs testing: there has been initial work by Ramdas et al. (2016), who study the simplest setting of the two multivariate Gaussians with known covariance matrices. Here, one can use linear classifiers, and the two sample test boils down to testing for differences in means. In this setting, when the classifier is chosen to be Fisher\u2019s LDA, then using the classifier accuracy on held-out data as a test statistic turns out to be minimax optimal in \u201crate\u201d (dependence on dimensionality and sample size) but not in constants, meaning that there do exist tests which achieve the same power with fewer samples. The result has been proved only for this statistic and setting, however, and generalization to other statistics and settings is an open question.\nproblems (for which Gretton et al. suggested the spectral test), there is still a significant performance increase.\nFor further reference, we also report timings of available non-parallelized implementations for m = 2000, compared to our version\u2019s 12s in Figure 1 (left): 87s for an open-sourced spectral test in Shogun using eigen3 (Sonnenburg et al., 2016; Guennebaud et al., 2010), 381s for the reference Matlab spectral implementation (Gretton et al., 2012a), and 182s for a naive Python permutation test that partly avoids copying via masking. (All of these times also exclude kernel computation.)"}, {"heading": "4 EXPERIMENTS", "text": "Code for these experiments is available at github.com/dougalsutherland/opt-mmd.\nSynthetic data We consider the problem of bandwidth selection for Gaussian RBF kernels on the Blobs dataset of Gretton et al. (2012b). P here is a 5\u00d7 5 grid of two-dimensional standard normals, with spacing 10 between the centers. Q is laid out identically, but with covariance \u03b5\u22121\u03b5+1 between the coordinates (so that the ratio of eigenvalues in the variance is \u03b5.) Figure 2a shows two samples from X and Y with \u03b5 = 6. Note that when \u03b5 = 1, P = Q.\nFor \u03b5 \u2208 {1, 2, 4, 6, 8, 10}, we take m = 500 samples from each distribution and compute M\u0302MD 2\nU(X,Y ), V\u0302m(X,Y ), and c\u03020.1 using 1 000 permutations, for Gaussian RBF kernels with each of 30 bandwidths. We repeat this process 100 times. Figure 2b shows that the median heuristic always chooses too large a bandwidth. When maximizing MMD alone, we see a bimodal distribution of bandwidths, with a significant number of samples falling into the region with low test power. The variance of M\u0302MD 2\nU is much higher in this region, however, hence optimizing the ratio t\u0302 never returns these bandwidths. Figure 2c shows that maximizing t\u0302 outperforms maximizing the MMD across a variety of problem parameters, and performs near-optimally.\nModel criticism As an example of a real-world two-sample testing problem, we will consider distinguishing the output of a generative model from the reference distribution it attempts to reproduce. We will use the semi-supervised GAN model of Salimans et al. (2016), trained on the MNIST dataset of handwritten images.5 True samples from the dataset are shown in Figure 3a; samples from the learned model are in Figure 3b. Salimans et al. (2016) called their results \u201ccompletely indistinguishable from dataset images,\u201d and reported that annotators on Mechanical Turk were able to distinguish samples\n5We used their code for a minibatch discrimination GAN, with 1000 labels, and chose the best of several runs.\nonly in 52.4% of cases. Comparing the results, however, there are several pixel-level artifacts that make distinguishing the datasets trivial; our methods can pick up on these quickly.\nTo make the problem more interesting, we discretized the sampled pixels into black or white (which barely changes the images visually). The samples are then in {0, 1}28\u00d728. We trained an automatic relevance determination (ARD)-type kernel: in the notation of Section 2.1, z scales each pixel by some learned value, and k is a Gaussian RBF kernel with a learned global bandwidth. We optimized t\u0302 on 2 000 samples in batches of size 500 using the Adam optimizer (Kingma & Ba, 2015), where the learned weights are visualized in Figure 3c. This network has essentially perfect discriminative power: testing it on 100 different samples with 1000 permutations for each test, in 98 cases we obtained p-values of 0.000 and twice got 0.001. By contrast, using an RBF kernel with a bandwidth optimized by maximizing the t statistic gave a less powerful test: the worst p-value in 100 repetitions was 0.135, with power 57% at the \u03b1 = 0.01 threshold. An RBF kernel based on the median heuristic, which here found a bandwidth five times the size of the t-statistic-optimized bandwidth, performed worse still: three out of 100 repetitions found a p-value of exactly 1.000, and power at the .01 threshold was 42%. The learned weights show that the model differs from the true dataset along the outsides of images, as well as along a vertical line in the center.\nWe can investigate these results in further detail using the approach of Lloyd & Ghahramani (2015), considering the witness function associated with the MMD, which has largest amplitude where the probability mass of the two samples is most different. Thus, samples falling at maxima and minima of the witness function best represent the difference in the distributions. The value of the witness function on each sample is plotted in Figure 3d, along with some images with different values of the witness function. Apparently, the GAN is slightly overproducing images resembling the /-like digits on the left, while underproducing vertical 1s. It is not the case that the GAN is simply underproducing 1s in general: the p-values of a \u03c72 contingency test between the outputs of digit classifiers on the two distributions are uniform. This subtle difference in proportions among types of digits would be quite difficult for human observers to detect. Our testing framework allows the model developer to find such differences and decide whether to act on them. One could use a more complex representation function z to detect even more subtle differences between distributions.\nGAN criterion We now demonstrate the use of MMD as a training criterion in GANs. We consider two basic approaches, and train on MNIST.6 First, the generative moment matching network (GMMN; Figure 4a) approach (Li et al., 2015; Dziugaite et al., 2015) uses an MMD statistic computed with an\n6Implementation details: We used the architecture of Li et al. (2015): the generator consists of fully connected layers with sizes 10, 64, 256, 256, 1024, 784, each with ReLU activations except the last, which uses sigmoids. The kernel function for GMMNs is a sum of Gaussian RBF kernels with fixed bandwidths 2, 5, 10, 20, 40, 80. For the feature matching GAN, we use a discriminator with fully connected layers of size 512, 256, 256, 128, 64, each with sigmoid activation. We then concatenate the raw image and each layer of features as input to the same mixture of RBF kernels as for GMMNs. We optimize with SGD. Initialization for all parameters are Gaussian with standard deviation 0.1 for the GMMNs and 0.2 for feature matching. Learning rates\nRBF kernel directly on the images as the discriminator of a GAN model. The t-GMMN (Figure 4b) has the generator minimize the t\u0302k statistic for a fixed kernel.7 Compared to standard GMMNs, the t-GMMN more directly attempts to make the distributions indistinguishable under the kernel function; it avoids a situation like that of Figure 3d, where although the MMD value is quite small, the two distributions are perfectly distinguishable due to the small variance.\nNext, feature matching GANs (Figure 4c) train the discriminator as a classifier like a normal GAN, but train the generator to minimize the MMD between generator samples and reference samples with a kernel computed on intermediate features of the discriminator. Salimans et al. (2016) proposed feature matching using the mean features at the top of the discriminator (effectively using an MMD with a linear kernel); we instead use MMD with a mixture of RBF kernels, ensuring that the full feature distributions match, rather than just their means. This helps avoid the common failure mode of GANs where the generator collapses to outputting a small number of samples considered highly realistic by the discriminator. Using the MMD-based approach, however, no single point can approximate the feature distribution. The minibatch discrimination approach of Salimans et al. (2016) attempts to solve the same problem, by introducing features measuring the similarity of each sample to a selection of other samples, but we were unable to get it to work without labels to force the discriminator in a reasonable direction; Figure 4d demonstrates some of those failures, with each row showing six samples from each of six representative runs of the model."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Tim Salimans, Ian Goodfellow, and Wojciech Zaremba for providing their code and for gracious assistance in using it, as well as Jeff Schneider for helpful discussions.\nare 2, 0.02, 0.5, respectively. Learning rate for the feature matching discriminator is set to 0.01. All experiments are run for 50 000 iterations and use a momentum optimizer with with momentum 0.9.\n7One could additionally update the kernel adversarially, by maximizing the t\u0302k statistic based on generator samples, but we had difficulty in optimizing this model: the interplay between generator and discriminator adds some difficulty to this task."}], "references": [{"title": "A test for the two-sample problem based on empirical characteristic functions", "author": ["V. Alba Fern\u00e1ndez", "M. Jim\u00e9nez-Gamero", "J. Mu\u00f1oz Garcia"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Fern\u00e1ndez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2008}, {"title": "A test of relative similarity for model selection in generative models", "author": ["Wacha Bounliphone", "Eugene Belilovsky", "Matthew B. Blaschko", "Ioannis Antonoglou", "Arthur Gretton"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bounliphone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2016}, {"title": "Quickly generating representative samples from an rbm-derived process", "author": ["Olivier Breuleux", "Yoshua Bengio", "Pascal Vincent"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Gintare Karolina Dziugaite", "Daniel M. Roy", "Zoubin Ghahramani"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Kernel measures of conditional dependence", "author": ["Kenji Fukumizu", "Arthur Gretton", "Xiaohai Sun", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fukumizu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2008}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A fast, consistent kernel two-sample test", "author": ["Arthur Gretton", "Kenji Fukumizu", "Zaid Harchaoui", "Bharath K. Sriperumbudur"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alex J. Smola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["Arthur Gretton", "Bharath Sriperumbudur", "Dino Sejdinovic", "Heiko Strathmann", "Massimiliano Pontil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Statistical inference of intractable generative models via classification", "author": ["Michael U. Gutmann", "Ritabrata Dutta", "Samuel Kaski", "Jukka Corander"], "venue": null, "citeRegEx": "Gutmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2014}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood", "author": ["Ferenc Huszar"], "venue": null, "citeRegEx": "Huszar.,? \\Q2015\\E", "shortCiteRegEx": "Huszar.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Statistical model criticism using kernel two sample tests", "author": ["James Robert Lloyd", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lloyd and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Lloyd and Ghahramani.", "year": 2015}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["Alfred M\u00fcller"], "venue": "Advances in Applied Probability,", "citeRegEx": "M\u00fcller.,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller.", "year": 1997}, {"title": "On optimal quantization rules in some problems in sequential decentralized detection", "author": ["XuanLong Nguyen", "Martin J. Wainwright", "Michael I. Jordan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2008}, {"title": "Tomioka. f -GAN: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance based High Dimensional Two Sample Testing, 2015a", "author": ["Aaaditya Ramdas", "Sashank Reddi", "Barnab\u00e1s P\u00f3czos", "Aarti Singh", "Larry Wasserman"], "venue": null, "citeRegEx": "Ramdas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2015}, {"title": "On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions", "author": ["Aaditya Ramdas", "Sashank J. Reddi", "Barnab\u00e1s P\u00f3czos", "Aarti Singh", "Larry Wasserman"], "venue": "In AAAI Conference on Artificial Intelligence, 2015b. arXiv:1406.2083", "citeRegEx": "Ramdas et al\\.,? \\Q2083\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2083}, {"title": "Classification accuracy as a proxy for two sample testing, 2016", "author": ["Aaditya Ramdas", "Aarti Singh", "Larry Wasserman"], "venue": null, "citeRegEx": "Ramdas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2016}, {"title": "Improved techniques for training GANs", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Approximation Theorems of Mathematical Statistics", "author": ["Robert J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q1980\\E", "shortCiteRegEx": "Serfling.", "year": 1980}, {"title": "Kernel choice and classifiability for RKHS embeddings of probability distributions", "author": ["Bharath K. Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Gert R.G. Lanckriet", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2009}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Gert R.G. Lanckriet", "Bernhard Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "On the empirical estimation of integral probability metrics", "author": ["Bharath K. Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Bernhard Sch\u00f6lkopf", "Gert R.G. Lanckriet"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2012}, {"title": "Adaptive Large-Scale Kernel Two-Sample Testing", "author": ["Heiko Strathmann"], "venue": "M.Sc. thesis,", "citeRegEx": "Strathmann.,? \\Q2012\\E", "shortCiteRegEx": "Strathmann.", "year": 2012}, {"title": "Least-squares two-sample test", "author": ["Masashi Sugiyama", "Taiji Suzuki", "Yuta Itoh", "Takafumi Kanamori", "Manabu Kimura"], "venue": "Neural Networks,", "citeRegEx": "Sugiyama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2011}, {"title": "Unbiased estimators for the variance of MMD estimators, 2019", "author": ["Dougal J. Sutherland"], "venue": null, "citeRegEx": "Sutherland.,? \\Q1906\\E", "shortCiteRegEx": "Sutherland.", "year": 1906}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "The appendix deriving the estimator has been replaced by Sutherland (2019).", "startOffset": 57, "endOffset": 75}, {"referenceID": 6, "context": "The generative adversarial network (Goodfellow et al., 2014; Gutmann et al., 2014) is a popular method for training generative models, where a rival discriminator attempts to distinguish model samples from reference data.", "startOffset": 35, "endOffset": 82}, {"referenceID": 10, "context": "The generative adversarial network (Goodfellow et al., 2014; Gutmann et al., 2014) is a popular method for training generative models, where a rival discriminator attempts to distinguish model samples from reference data.", "startOffset": 35, "endOffset": 82}, {"referenceID": 11, "context": "A range of adversarial divergences were proposed by Huszar (2015), using a weight to interpolate between KL, inverse KL, and Jensen-Shannon.", "startOffset": 52, "endOffset": 66}, {"referenceID": 11, "context": "A range of adversarial divergences were proposed by Huszar (2015), using a weight to interpolate between KL, inverse KL, and Jensen-Shannon. This weight may be interpreted as a prior probability of observing samples from the model or the real world: when there is a greater probability of model samples, we approach reverse KL and the model seeks out modes of the data distribution. When there is a greater probability of drawing from the data distribution, the model approaches the KL divergence, and tries to cover the full support of the data, at the expense of producing some samples in low probability regions. This insight was further developed by Nowozin et al. (2016), who showed that a much broader range of f -divergences can be learned for the discriminator in adversarial models, based on the variational formulation of f -divergences of Nguyen et al.", "startOffset": 52, "endOffset": 676}, {"referenceID": 11, "context": "A range of adversarial divergences were proposed by Huszar (2015), using a weight to interpolate between KL, inverse KL, and Jensen-Shannon. This weight may be interpreted as a prior probability of observing samples from the model or the real world: when there is a greater probability of model samples, we approach reverse KL and the model seeks out modes of the data distribution. When there is a greater probability of drawing from the data distribution, the model approaches the KL divergence, and tries to cover the full support of the data, at the expense of producing some samples in low probability regions. This insight was further developed by Nowozin et al. (2016), who showed that a much broader range of f -divergences can be learned for the discriminator in adversarial models, based on the variational formulation of f -divergences of Nguyen et al. (2008). For a given f -divergence, the model learns the composition of the density ratio (of data to model density) with the derivative of f , by comparing generator and data samples.", "startOffset": 52, "endOffset": 871}, {"referenceID": 15, "context": "An alternative family of divergences are the integral probability metrics (M\u00fcller, 1997), which find a witness function to distinguish samples from P and Q.", "startOffset": 74, "endOffset": 88}, {"referenceID": 6, "context": "An approach used in several studies is to obtain a Parzen window estimate of the density and compute the log-likelhiood (Goodfellow et al., 2014; Nowozin et al., 2016; Breuleux et al., 2011).", "startOffset": 120, "endOffset": 190}, {"referenceID": 17, "context": "An approach used in several studies is to obtain a Parzen window estimate of the density and compute the log-likelhiood (Goodfellow et al., 2014; Nowozin et al., 2016; Breuleux et al., 2011).", "startOffset": 120, "endOffset": 190}, {"referenceID": 2, "context": "An approach used in several studies is to obtain a Parzen window estimate of the density and compute the log-likelhiood (Goodfellow et al., 2014; Nowozin et al., 2016; Breuleux et al., 2011).", "startOffset": 120, "endOffset": 190}, {"referenceID": 3, "context": "We can instead ask humans to evaluate the generated images (Denton et al., 2015; Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images.", "startOffset": 59, "endOffset": 103}, {"referenceID": 21, "context": "We can instead ask humans to evaluate the generated images (Denton et al., 2015; Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images.", "startOffset": 59, "endOffset": 103}, {"referenceID": 2, "context": ", 2012a), simultaneously proposed by Dziugaite et al. (2015) and Li et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 2, "context": ", 2012a), simultaneously proposed by Dziugaite et al. (2015) and Li et al. (2015). The architecture used in these two approaches is actually quite different: Dziugaite et al.", "startOffset": 37, "endOffset": 82}, {"referenceID": 2, "context": ", 2012a), simultaneously proposed by Dziugaite et al. (2015) and Li et al. (2015). The architecture used in these two approaches is actually quite different: Dziugaite et al. use the MMD as a discriminator directly at the level of the generated and test images, whereas Li et al. apply the MMD on input features learned from an autoencoder, and share the decoding layers of the autoencoder with the generator network (see their Figure 1(b)). The generated samples have better visual quality in the latter method, but it becomes difficult to analyze and interpret the algorithm given the interplay between the generator and discriminator networks. In a related approach, Salimans et al. (2016) propose to use feature matching, where the generator is tasked with minimizing the squared distance between expected discriminator features under the model and data distributions, thus retaining the adversarial setting.", "startOffset": 37, "endOffset": 693}, {"referenceID": 2, "context": ", 2016; Breuleux et al., 2011). Unfortunately, density estimates in such high dimensions are known to be very unreliable both in theory (Wasserman, 2006, Ch. 6) and in practice (Theis et al., 2016). We can instead ask humans to evaluate the generated images (Denton et al., 2015; Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images. Recall that different divergence measures result in different degrees of mode-seeking: if we rely on human evaluation, we may tend towards always using divergences with under-dispersed samples. We propose to use the MMD to distinguish generator and reference data, with features and kernels chosen to maximize the test power of the quadratic-time MMD of Gretton et al. (2012a). Optimizing MMD test power requires a sophisticated treatment due to the different form of the null and alternative distributions (Section 2).", "startOffset": 8, "endOffset": 1011}, {"referenceID": 2, "context": ", 2016; Breuleux et al., 2011). Unfortunately, density estimates in such high dimensions are known to be very unreliable both in theory (Wasserman, 2006, Ch. 6) and in practice (Theis et al., 2016). We can instead ask humans to evaluate the generated images (Denton et al., 2015; Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images. Recall that different divergence measures result in different degrees of mode-seeking: if we rely on human evaluation, we may tend towards always using divergences with under-dispersed samples. We propose to use the MMD to distinguish generator and reference data, with features and kernels chosen to maximize the test power of the quadratic-time MMD of Gretton et al. (2012a). Optimizing MMD test power requires a sophisticated treatment due to the different form of the null and alternative distributions (Section 2). We also develop an efficient approach to obtaining quantiles of the MMD distribution under the null (Section 3). We demonstrate on simple artificial data that simply maximizing the MMD (as in Sriperumbudur et al., 2009) provides a less powerful test than our approach of explicitly maximizing test power. Our procedure applies even when our definition of the MMD is computed on features of the inputs, since these can also be trained by power maximization. When designing an optimized MMD test, we should choose a kernel family that allows us to visualize where the probability mass of the two samples differs most. In our experiments on GAN performance evaluation, we use an automatic relevance determination (ARD) kernel over the output dimensions, and learn which coordinates differ meaningfully by finding which kernels retain significant bandwidth when the test power is optimized. We may further apply the method of Lloyd & Ghahramani (2015, Section 5) to visualize the witness function associated with this MMD, by finding those model and data samples occurring at the maxima and minima of the witness function (i.e., the samples from one distribution least likely to be in high probability regions of the other). The optimized witness function gives a test with greater power than a standard RBF kernel, suggesting that the associated witness function peaks are an improved representation of where the distributions differ. We also propose a novel generative model based on the feature matching idea of Salimans et al. (2016), using MMD rather than their \u201cminibatch discrimination\u201d heuristic, for a more principled and more stable enforcement of sample diversity, without requiring labeled data.", "startOffset": 8, "endOffset": 2688}, {"referenceID": 25, "context": "Only the total variation distance is both an f -divergence and an IPM (Sriperumbudur et al., 2012).", "startOffset": 70, "endOffset": 98}, {"referenceID": 5, "context": "Many kernels, including the popular Gaussian RBF, are characteristic (Fukumizu et al., 2008; Sriperumbudur et al., 2010), which implies that the MMD is a metric, and in particular that MMDk(P,Q) = 0 if and only if P = Q, so that tests with any characteristic kernel are consistent.", "startOffset": 69, "endOffset": 120}, {"referenceID": 24, "context": "Many kernels, including the popular Gaussian RBF, are characteristic (Fukumizu et al., 2008; Sriperumbudur et al., 2010), which implies that the MMD is a metric, and in particular that MMDk(P,Q) = 0 if and only if P = Q, so that tests with any characteristic kernel are consistent.", "startOffset": 69, "endOffset": 120}, {"referenceID": 7, "context": "This estimator is unbiased, and has nearly minimal variance among unbiased estimators (Gretton et al., 2012a, Lemma 6). Following Gretton et al. (2012a), we will conduct a hypothesis test with null hypothesis H0 : P = Q and alternative H1 : P 6= Q, using test statistic m M\u0302MD 2 U(X,Y ).", "startOffset": 87, "endOffset": 153}, {"referenceID": 22, "context": "where Vm(P,Q) denotes the asymptotic variance of the M\u0302MD 2 U estimator for samples of size m from P and Q (Serfling, 1980).", "startOffset": 107, "endOffset": 123}, {"referenceID": 27, "context": "4 One can also consider maximizing criteria based on cross validation (Sugiyama et al., 2011; Gretton et al., 2012b; Strathmann, 2012).", "startOffset": 70, "endOffset": 134}, {"referenceID": 26, "context": "4 One can also consider maximizing criteria based on cross validation (Sugiyama et al., 2011; Gretton et al., 2012b; Strathmann, 2012).", "startOffset": 70, "endOffset": 134}, {"referenceID": 17, "context": "Sriperumbudur et al. (2009) proved that, for certain classes of kernels, this yields a consistent test.", "startOffset": 0, "endOffset": 28}, {"referenceID": 7, "context": ", 2011; Gretton et al., 2012b; Strathmann, 2012). This approach is not differentiable, and thus difficult to maximize among more than a fixed set of candidate kernels. Moreover, where this cross-validation is used to maximize the MMD on a validation set (as in Sugiyama et al., 2011), it again amounts to maximizing classification performance rather than test performance, and is suboptimal in the latter setting (Gretton et al., 2012b, Figure 1). Finally, Gretton et al. (2012b) previously studied direct optimization of the power of an MMD test for a streaming estimator of the MMD, for which optimizing the ratio of the empirical statistic to its variance also optimizes test power.", "startOffset": 8, "endOffset": 480}, {"referenceID": 7, "context": "There are two known test threshold estimates that lead to a consistent test: the permutation test mentioned above, and a more sophisticated null distribution estimate based on approximating the eigenspectrum of the kernel, previously reported to be faster than the permutation test (Gretton et al., 2009).", "startOffset": 282, "endOffset": 304}, {"referenceID": 7, "context": "There are two known test threshold estimates that lead to a consistent test: the permutation test mentioned above, and a more sophisticated null distribution estimate based on approximating the eigenspectrum of the kernel, previously reported to be faster than the permutation test (Gretton et al., 2009). In fact, the relatively slow reported performance of the permutation approach was due to the naive Matlab implementation of the permutation test in the code accompanying Gretton et al. (2012a), which creates a new copy of the kernel matrix for every permutation.", "startOffset": 283, "endOffset": 499}, {"referenceID": 18, "context": "With regards to classification vs testing: there has been initial work by Ramdas et al. (2016), who study the simplest setting of the two multivariate Gaussians with known covariance matrices.", "startOffset": 74, "endOffset": 95}, {"referenceID": 7, "context": "Synthetic data We consider the problem of bandwidth selection for Gaussian RBF kernels on the Blobs dataset of Gretton et al. (2012b). P here is a 5\u00d7 5 grid of two-dimensional standard normals, with spacing 10 between the centers.", "startOffset": 111, "endOffset": 134}, {"referenceID": 21, "context": "We will use the semi-supervised GAN model of Salimans et al. (2016), trained on the MNIST dataset of handwritten images.", "startOffset": 45, "endOffset": 68}, {"referenceID": 21, "context": "We will use the semi-supervised GAN model of Salimans et al. (2016), trained on the MNIST dataset of handwritten images.5 True samples from the dataset are shown in Figure 3a; samples from the learned model are in Figure 3b. Salimans et al. (2016) called their results \u201ccompletely indistinguishable from dataset images,\u201d and reported that annotators on Mechanical Turk were able to distinguish samples", "startOffset": 45, "endOffset": 248}, {"referenceID": 13, "context": "6 First, the generative moment matching network (GMMN; Figure 4a) approach (Li et al., 2015; Dziugaite et al., 2015) uses an MMD statistic computed with an", "startOffset": 75, "endOffset": 116}, {"referenceID": 4, "context": "6 First, the generative moment matching network (GMMN; Figure 4a) approach (Li et al., 2015; Dziugaite et al., 2015) uses an MMD statistic computed with an", "startOffset": 75, "endOffset": 116}, {"referenceID": 13, "context": "Implementation details: We used the architecture of Li et al. (2015): the generator consists of fully connected layers with sizes 10, 64, 256, 256, 1024, 784, each with ReLU activations except the last, which uses sigmoids.", "startOffset": 52, "endOffset": 69}, {"referenceID": 21, "context": "Figure 3: Model criticism of Salimans et al. (2016)\u2019s semi-supervised GAN on MNIST.", "startOffset": 29, "endOffset": 52}, {"referenceID": 21, "context": "Salimans et al. (2016) proposed feature matching using the mean features at the top of the discriminator (effectively using an MMD with a linear kernel); we instead use MMD with a mixture of RBF kernels, ensuring that the full feature distributions match, rather than just their means.", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "Salimans et al. (2016) proposed feature matching using the mean features at the top of the discriminator (effectively using an MMD with a linear kernel); we instead use MMD with a mixture of RBF kernels, ensuring that the full feature distributions match, rather than just their means. This helps avoid the common failure mode of GANs where the generator collapses to outputting a small number of samples considered highly realistic by the discriminator. Using the MMD-based approach, however, no single point can approximate the feature distribution. The minibatch discrimination approach of Salimans et al. (2016) attempts to solve the same problem, by introducing features measuring the similarity of each sample to a selection of other samples, but we were unable to get it to work without labels to force the discriminator in a reasonable direction; Figure 4d demonstrates some of those failures, with each row showing six samples from each of six representative runs of the model.", "startOffset": 0, "endOffset": 616}, {"referenceID": 21, "context": "Part d shows six runs of the minibatch discrimination model of Salimans et al. (2016), trained without labels \u2014 the same model that, with labels, generated Figure 3b.", "startOffset": 63, "endOffset": 86}], "year": 2019, "abstractText": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model\u2019s samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier. This post-publication revision corrects some errors in constants of the estimator (5). The appendix deriving the estimator has been replaced by Sutherland (2019).", "creator": "LaTeX with hyperref"}, "id": "ICLR_2017_58"}