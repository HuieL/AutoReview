{"name": "ICLR_2017_5.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Martin Arjovsky", "L\u00e9on Bottou"], "emails": ["martinarjovsky@gmail.com", "leonb@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Generative adversarial networks (GANs)(Goodfellow et al., 2014a) have achieved great success at generating realistic and sharp looking images. However, they are widely general methods, now starting to be applied to several other important problems, such as semisupervised learning, stabilizing sequence learning methods for speech and language, and 3D modelling. (Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016)\nHowever, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures. (Radford et al., 2015; Salimans et al., 2016)\nDespite their success, there is little to no theory explaining the unstable behaviour of GAN training. Furthermore, approaches to attacking this problem still rely on heuristics that are extremely sensitive to modifications. This makes it extremely hard to experiment with new variants, or to use them in new domains, which limits their applicability drastically. This paper aims to change that, by providing a solid understanding of these issues, and creating principled research directions towards adressing them.\nIt is interesting to note that the architecture of the generator used by GANs doesn\u2019t differ significantly from other approaches like variational autoencoders (Kingma & Welling, 2013). After all, at the core of it we first sample from a simple prior z \u223c p(z), and then output our final sample g\u03b8(z), sometimes adding noise in the end. Always, g\u03b8 is a neural network parameterized by \u03b8, and the main difference is how g\u03b8 is trained.\nTraditional approaches to generative modeling relied on maximizing likelihood, or equivalently minimizing the Kullback-Leibler (KL) divergence between our unknown data distribution Pr and our generator\u2019s distribution Pg (that depends of course on \u03b8). If we assume that both distributions are continuous with densities Pr and Pg , then these methods try to minimize\nKL(Pr\u2016Pg) = \u222b X Pr(x) log Pr(x) Pg(x) dx\nThis cost function has the good property that it has a unique minimum at Pg = Pr, and it doesn\u2019t require knowledge of the unknown Pr(x) to optimize it (only samples). However, it is interesting to see how this divergence is not symetrical between Pr and Pg:\n\u2022 If Pr(x) > Pg(x), then x is a point with higher probability of coming from the data than being a generated sample. This is the core of the phenomenon commonly described as \u2018mode dropping\u2019: when there are large regions with high values of Pr, but small or zero values in Pg . It is important to note that when Pr(x) > 0 but Pg(x) \u2192 0, the integrand inside the KL grows quickly to infinity, meaning that this cost function assigns an extremely high cost to a generator\u2019s distribution not covering parts of the data.\n\u2022 If Pr(x) < Pg(x), then x has low probability of being a data point, but high probability of being generated by our model. This is the case when we see our generator outputting an image that doesn\u2019t look real. In this case, when Pr(x)\u2192 0 and Pg(x) > 0, we see that the value inside the KL goes to 0, meaning that this cost function will pay extremely low cost for generating fake looking samples.\nClearly, if we would minimize KL(Pg\u2016Pr) instead, the weighting of these errors would be reversed, meaning that this cost function would pay a high cost for generating not plausibly looking pictures. Generative adversarial networks have been shown to optimize (in its original formulation), the Jensen-shannon divergence, a symmetric middle ground to this two cost functions\nJSD(Pr\u2016Pg) = 1\n2 KL(Pr\u2016PA) +\n1 2 KL(Pg\u2016PA)\nwhere PA is the \u2018average\u2019 distribution, with density Pr+Pg2 . An impressive experimental analysis of the similarities, uses and differences of these divergences in practice can be seen at Theis et al. (2016). It is indeed conjectured that the reason of GANs success at producing reallistically looking images is due to the switch from the traditional maximum likelihood approaches. (Theis et al., 2016; Huszar, 2015). However, the problem is far from closed.\nGenerative adversarial networks are formulated in two steps. We first train a discriminator D to maximize\nL(D, g\u03b8) = Ex\u223cPr [logD(x)] + Ex\u223cPg [log(1\u2212D(x))] (1)\nOne can show easily that the optimal discriminator has the shape\nD\u2217(x) = Pr(x)\nPr(x) + Pg(x) (2)\nand that L(D\u2217, g\u03b8) = 2JSD(Pr\u2016Pg)\u2212 2 log 2, so minimizing equation (1) as a function of \u03b8 yields minimizing the Jensen-Shannon divergence when the discriminator is optimal. In theory, one would expect therefore that we would first train the discriminator as close as we can to optimality (so the cost function on \u03b8 better approximates the JSD), and then do gradient steps on \u03b8, alternating these two things. However, this doesn\u2019t work. In practice, as the discriminator gets better, the updates to the generator get consistently worse. The original GAN paper argued that this issue arose from saturation, and switched to another similar cost function that doesn\u2019t have this problem. However, even with this new cost function, updates tend to get worse and optimization gets massively unstable. Therefore, several questions arize:\n\u2022 Why do updates get worse as the discriminator gets better? Both in the original and the new cost function.\n\u2022 Why is GAN training massively unstable?\n\u2022 Is the new cost function following a similar divergence to the JSD? If so, what are its properties?\n\u2022 Is there a way to avoid some of these issues?\nThe fundamental contributions of this paper are the answer to all these questions, and perhaps more importantly, to introduce the tools to analyze them properly. We provide a new direction designed to avoid the instability issues in GANs, and examine in depth the theory behind it. Finally, we state a series of open questions and problems, that determine several new directions of research that begin with our methods."}, {"heading": "2 SOURCES OF INSTABILITY", "text": "The theory tells us that the trained discriminator will have cost at most 2 log 2 \u2212 2JSD(Pr\u2016Pg). However, in practice, if we just train D till convergence, its error will go to 0, as observed in Figure 1, pointing to the fact that the JSD between them is maxed out. The only way this can happen is if the distributions are not continuous1, or they have disjoint supports.\nOne possible cause for the distributions not to be continuous is if their supports lie on low dimensional manifolds. There is strong empirical and theoretical evidence to believe that Pr is indeed extremely concentrated on a low dimensional manifold (Narayanan & Mitter, 2010). As of Pg , we will prove soon that such is the case as well.\nIn the case of GANs, Pg is defined via sampling from a simple prior z \u223c p(z), and then applying a function g : Z \u2192 X , so the support of Pg has to be contained in g(Z). If the dimensionality of Z is less than the dimension of X (as is typically the case), then it\u2019s imposible for Pg to be continuous. This is because in most cases g(Z) will be contained in a union of low dimensional manifolds, and therefore have measure 0 in X . Note that while intuitive, this is highly nontrivial, since having an ndimensional parameterization does absolutely not imply that the image will lie on an n-dimensional manifold. In fact, there are many easy counterexamples, such as Peano curves, lemniscates, and many more. In order to show this for our case, we rely heavily on g being a neural network, since we are able to leverage that g is made by composing very well behaved functions. We now state this properly in the following Lemma: Lemma 1. Let g : Z \u2192 X be a function composed by affine transformations and pointwise nonlinearities, which can either be rectifiers, leaky rectifiers, or smooth strictly increasing functions (such as the sigmoid, tanh, softplus, etc). Then, g(Z) is contained in a countable union of manifolds of dimension at most dimZ . Therefore, if the dimension of Z is less than the one of X , g(Z) will be a set of measure 0 in X .\nProof. See Appendix A.\nDriven by this, this section shows that if the supports of Pr and Pg are disjoint or lie in low dimensional manifolds, there is always a perfect discriminator between them, and we explain exactly how and why this leads to an unreliable training of the generator."}, {"heading": "2.1 THE PERFECT DISCRIMINATION THEOREMS", "text": "For simplicity, and to introduce the methods, we will first explain the case where Pr and Pg have disjoint supports. We say that a discriminator D : X \u2192 [0, 1] has accuracy 1 if it takes the value 1 on a set that contains the support of Pr and value 0 on a set that contains the support of Pg . Namely, Pr[D(x) = 1] = 1 and Pg[D(x) = 0] = 1. Theorem 2.1. If two distributions Pr and Pg have support contained on two disjoint compact subsetsM and P respectively, then there is a smooth optimal discrimator D\u2217 : X \u2192 [0, 1] that has accuracy 1 and \u2207xD\u2217(x) = 0 for all x \u2208M\u222aP .\nProof. The discriminator is trained to maximize Ex\u223cPr [logD(x)] + Ex\u223cPg [log(1\u2212D(x))] SinceM and P are compact and disjoint, 0 < \u03b4 = d(P,M) the distance between both sets. We now define\nM\u0302 = {x : d(x,M) \u2264 \u03b4/3} P\u0302 = {x : d(x, P ) \u2264 \u03b4/3}\nBy definition of \u03b4 we have that P\u0302 and M\u0302 are clearly disjoint compact sets. Therefore, by Urysohn\u2019s smooth lemma there exists a smooth function D\u2217 : X \u2192 [0, 1] such that D\u2217|M\u0302 \u2261 1 and D\u2217|P\u0302 \u2261 0. Since logD\u2217(x) = 0 for all x in the support of Pr and log(1\u2212D\u2217(x)) = 0 for all x in the support of Pg , the discriminator is completely optimal and has accuracy 1. Furthermore, let x be inM\u222aP . If we assume that x \u2208 M, there is an open ball B = B(x, \u03b4/3) on which D\u2217|B is constant. This shows that\u2207xD\u2217(x) \u2261 0. Taking x \u2208 P and working analogously we finish the proof.\n1By continuous we will actually refer to an absolutely continuous random variable (i.e. one that has a density), as it typically done. For further clarification see Appendix B.\nIn the next theorem, we take away the disjoint assumption, to make it general to the case of two different manifolds. However, if the two manifolds match perfectly on a big part of the space, no discriminator could separate them. Intuitively, the chances of two low dimensional manifolds having this property is rather dim: for two curves to match in space in a specific segment, they couldn\u2019t be perturbed in any arbitrarilly small way and still satisfy this property. To do this, we will define the notion of two manifolds perfectly aligning, and show that this property never holds with probability 1 under any arbitrarilly small perturbations.\nDefinition 2.1. We first need to recall the definition of transversallity. LetM andP be two boundary free regular submanifolds of F , which in our cases will simply be F = Rd. Let x \u2208 M \u2229 P be an intersection point of the two manifolds. We say that M and P intersect transversally in x if TxM+ TxP = TxF , where TxM means the tangent space ofM around x.\nDefinition 2.2. We say that two manifolds without boundaryM and P perfectly align if there is an x \u2208M\u2229P such thatM and P don\u2019t intersect transversally in x. We shall note the boundary and interior of a manifoldM by \u2202M and Int M respectively. We say that two manifoldsM and P (with or without boundary) perfectly align if any of the boundary free manifold pairs (IntM, Int P), (IntM, \u2202P), (\u2202M, Int P) or (\u2202M, \u2202P) perfectly align.\nThe interesting thing is that we can safely assume in practice that any two manifolds never perfectly align. This can be done since an arbitrarilly small random perturbation on two manifolds will lead them to intersect transversally or don\u2019t intersect at all. This is precisely stated and proven in Lemma 2.\nAs stated by Lemma 3, if two manifolds don\u2019t perfectly align, their intersection L = M\u2229 P will be a finite union of manifolds with dimensions strictly lower than both the dimension ofM and the one of P . Lemma 2. LetM andP be two regular submanifolds of Rd that don\u2019t have full dimension. Let \u03b7, \u03b7\u2032 be arbitrary independent continuous random variables. We therefore define the perturbed manifolds as M\u0303 =M+ \u03b7 and P\u0303 = P + \u03b7\u2032. Then\nP\u03b7,\u03b7\u2032(M\u0303 does not perfectly align with P\u0303) = 1\nProof. See Appendix A.\nLemma 3. Let M and P be two regular submanifolds of Rd that don\u2019t perfectly align and don\u2019t have full dimension. Let L =M\u2229P . IfM and P don\u2019t have boundary, then L is also a manifold, and has strictly lower dimension than both the one ofM and the one of P . If they have boundary, L is a union of at most 4 strictly lower dimensional manifolds. In both cases, L has measure 0 in bothM and P .\nProof. See Appendix A.\nWe now state our perfect discrimination result for the case of two manifolds. Theorem 2.2. Let Pr and Pg be two distributions that have support contained in two closed manifoldsM and P that don\u2019t perfectly align and don\u2019t have full dimension. We further assume that Pr and Pg are continuous in their respective manifolds, meaning that if there is a set A with measure 0 in M, then Pr(A) = 0 (and analogously for Pg). Then, there exists an optimal discriminator D\u2217 : X \u2192 [0, 1] that has accuracy 1 and for almost any x inM or P , D\u2217 is smooth in a neighbourhood of x and \u2207xD\u2217(x) = 0.\nProof. By Lemma 3 we know that L = M\u2229P is strictly lower dimensional than bothM and P , and has measure 0 on both of them. By continuity, Pr(L) = 0 and Pg(L) = 0. Note that this implies the support of Pr is contained inM\\L and the support of Pg is contained in P \\ L. Let x \u2208 M \\ L. Therefore, x \u2208 Pc (the complement of P) which is an open set, so there exists a ball of radius x such that B(x, x) \u2229 P = \u2205. This way, we define\nM\u0302 = \u22c3\nx\u2208M\\L\nB(x, x/3)\nWe define P\u0302 analogously. Note that by construction these are both open sets on Rd. SinceM\\L \u2286 M\u0302, and P \\ L \u2286 P\u0302 , the support of Pr and Pg is contained in M\u0302 and P\u0302 respectively. As well by construction, M\u0302 \u2229 P\u0302 = \u2205. Let us defineD\u2217(x) = 1 for all x \u2208 M\u0302, and 0 elsewhere (clearly including P\u0302 . Since logD\u2217(x) = 0 for all x in the support of Pr and log(1\u2212D\u2217(x)) = 0 for all x in the support of Pg , the discriminator is completely optimal and has accuracy 1. Furthermore, let x \u2208 M\u0302. Since M\u0302 is an open set and D\u2217 is constant on M\u0302, then \u2207xD\u2217|M\u0302 \u2261 0. Analogously, \u2207xD\u2217|P\u0302 \u2261 0. Therefore, the set of points where D\u2217 is non-smooth or has non-zero gradient insideM\u222aP is contained in L, which has null-measure in both manifolds, therefore concluding the theorem.\nThese two theorems tell us that there are perfect discriminators which are smooth and constant almost everywhere inM and P . The fact that the discriminator is constant in both manifolds points to the fact that we won\u2019t really be able to learn anything by backproping through it, as we shall see in the next subsection. To conclude this general statement, we state the following theorem on the divergences of Pr and Pg , whose proof is trivial and left as an exercise to the reader. Theorem 2.3. Let Pr and Pg be two distributions whose support lies in two manifoldsM and P that don\u2019t have full dimension and don\u2019t perfectly align. We further assume that Pr and Pg are continuous in their respective manifolds. Then,\nJSD(Pr\u2016Pg) = log 2 KL(Pr\u2016Pg) = +\u221e KL(Pg\u2016Pr) = +\u221e\nNote that these divergences will be maxed out even if the two manifolds lie arbitrarilly close to each other. The samples of our generator might look impressively good, yet both KL divergences will be infinity. Therefore, Theorem 2.3 points us to the fact that attempting to use divergences out of the box to test similarities between the distributions we typically consider might be a terrible idea. Needless to say, if these divergencies are always maxed out attempting to minimize them by gradient descent isn\u2019t really possible. We would like to have a perhaps softer measure, that incorporates a notion of distance between the points in the manifolds. We will come back to this topic later in section 3, where we explain an alternative metric and provide bounds on it that we are able to analyze and optimize."}, {"heading": "2.2 THE CONSEQUENCES, AND THE PROBLEMS OF EACH COST FUNCTION", "text": "Theorems 2.1 and 2.2 showed one very important fact. If the two distributions we care about have supports that are disjoint or lie on low dimensional manifolds, the optimal discriminator will be perfect and its gradient will be zero almost everywhere."}, {"heading": "2.2.1 THE ORIGINAL COST FUNCTION", "text": "We will now explore what happens when we pass gradients to the generator through a discriminator. One crucial difference with the typical analysis done so far is that we will develop the theory for an approximation to the optimal discriminator, instead of working with the (unknown) true discriminator. We will prove that as the approximaton gets better, either we see vanishing gradients or the massively unstable behaviour we see in practice, depending on which cost function we use.\nIn what follows, we denote by \u2016D\u2016 the norm\n\u2016D\u2016 = sup x\u2208X |D(x)|+ \u2016\u2207xD(x)\u20162\nThe use of this norm is to make the proofs simpler, but could have been done in another Sobolev norm \u2016 \u00b7 \u20161,p for p < \u221e covered by the universal approximation theorem in the sense that we can guarantee a neural network approximation in this norm (Hornik, 1991).\nTheorem 2.4 (Vanishing gradients on the generator). Let g\u03b8 : Z \u2192 X be a differentiable function that induces a distribution Pg . Let Pr be the real data distribution. Let D be a differentiable discriminator. If the conditions of Theorems 2.1 or 2.2 are satisfied, \u2016D \u2212 D\u2217\u2016 < , and Ez\u223cp(z) [ \u2016J\u03b8g\u03b8(z)\u201622 ] \u2264M2, 2 then\n\u2016\u2207\u03b8Ez\u223cp(z)[log(1\u2212D(g\u03b8(z)))]\u20162 < M\n1\u2212\nProof. In both proofs of Theorems 2.1 and 2.2 we showed that D\u2217 is locally 0 on the support of Pg . Then, using Jensen\u2019s inequality and the chain rule on this support we have\n\u2016\u2207\u03b8Ez\u223cp(z)[log(1\u2212D(g\u03b8(z)))]\u201622 \u2264 Ez\u223cp(z) [ \u2016\u2207\u03b8D(g\u03b8(z))\u201622 |1\u2212D(g\u03b8(z))|2 ] \u2264 Ez\u223cp(z) [ \u2016\u2207xD(g\u03b8(z))\u201622\u2016J\u03b8g\u03b8(z)\u201622\n|1\u2212D(g\u03b8(z))|2 ] < Ez\u223cp(z) [ (\u2016\u2207xD\u2217(g\u03b8(z))\u20162 + )2 \u2016J\u03b8g\u03b8(z)\u201622\n(|1\u2212D\u2217(g\u03b8(z))| \u2212 )2\n]\n= Ez\u223cp(z)\n[ 2\u2016J\u03b8g\u03b8(z)\u201622\n(1\u2212 )2\n]\n\u2264M2 2\n(1\u2212 )2\nTaking square root of everything we get\n\u2016\u2207\u03b8Ez\u223cp(z)[log(1\u2212D(g\u03b8(z)))]\u20162 < M\n1\u2212 finishing the proof\nCorollary 2.1. Under the same assumptions of Theorem 2.4\nlim \u2016D\u2212D\u2217\u2016\u21920\n\u2207\u03b8Ez\u223cp(z)[log(1\u2212D(g\u03b8(z)))] = 0\n2Since M can depend on \u03b8, this condition is trivially verified for a uniform prior and a neural network. The case of a Gaussian prior requires more work because we need to bound the growth on z, but is also true for current architectures.\nThis shows that as our discriminator gets better, the gradient of the generator vanishes. For completeness, this was experimentally verified in Figure 2. The fact that this happens is terrible, since the fact that the generator\u2019s cost function being close to the Jensen Shannon divergence depends on the quality of this approximation. This points us to a fundamental: either our updates to the discriminator will be inacurate, or they will vanish. This makes it difficult to train using this cost function, or leave up to the user to decide the precise amount of training dedicated to the discriminator, which can make GAN training extremely hard.\n2.2.2 THE \u2212 logD ALTERNATIVE\nTo avoid gradients vanishing when the discriminator is very confident, people have chosen to use a different gradient step for the generator.\n\u2206\u03b8 = \u2207\u03b8Ez\u223cp(z) [\u2212 logD(g\u03b8(z))]\nWe now state and prove for the first time which cost function is being optimized by this gradient step. Later, we prove that while this gradient doesn\u2019t necessarily suffer from vanishing gradients, it does cause massively unstable updates (that have been widely experienced in practice) under the prescence of a noisy approximation to the optimal discriminator. Theorem 2.5. Let Pr and Pg\u03b8 be two continuous distributions, with densities Pr and Pg\u03b8 respectively. Let D\u2217 = PrPg\u03b80 +Pr be the optimal discriminator, fixed for a value \u03b803. Therefore,\nEz\u223cp(z) [\u2212\u2207\u03b8 logD\u2217(g\u03b8(z))|\u03b8=\u03b80 ] = \u2207\u03b8 [KL(Pg\u03b8\u2016Pr)\u2212 2JSD(Pg\u03b8\u2016Pr)] |\u03b8=\u03b80 (3)\nBefore diving into the proof, let\u2019s look at equation (3) for a second. This is the inverted KL minus two JSD. First of all, the JSDs are in the opposite sign, which means they are pushing for the distributions to be different, which seems like a fault in the update. Second, the KL appearing in the equation is KL(Pg\u2016Pr), not the one equivalent to maximum likelihood. As we know, this KL assigns an extremely high cost to generating fake looking samples, and an extremely low cost on mode dropping; and the JSD is symetrical so it shouldn\u2019t alter this behaviour. This explains what we see in practice, that GANs (when stabilized) create good looking samples, and justifies what is commonly conjectured, that GANs suffer from an extensive amount of mode dropping.\n3This is important since when backpropagating to the generator, the discriminator is assumed fixed\nProof. We already know by Goodfellow et al. (2014a) that\nEz\u223cp(z) [\u2207\u03b8 log(1\u2212D\u2217(g\u03b8(z)))|\u03b8=\u03b80 ] = \u2207\u03b82JSD(Pg\u03b8\u2016Pr)|\u03b8=\u03b80 Furthermore, as remarked by Huszar (2016),\nKL(Pg\u03b8\u2016Pr) = Ex\u223cPg\u03b8\n[ log Pg\u03b8 (x)\nPr(x) ] = Ex\u223cPg\u03b8 [ log Pg\u03b80 (x)\nPr(x)\n] \u2212 Ex\u223cPg\u03b8 [ log Pg\u03b8 (x)\nPg\u03b80 (x)\n]\n= \u2212Ex\u223cPg\u03b8\n[ log D\u2217(x)\n1\u2212D\u2217(x)\n] \u2212KL(Pg\u03b8\u2016Pg\u03b80 )\n= \u2212Ez\u223cp(z) [ log D\u2217(g\u03b8(z))\n1\u2212D\u2217(g\u03b8(z))\n] \u2212KL(Pg\u03b8\u2016Pg\u03b80 )\nTaking derivatives in \u03b8 at \u03b80 we get \u2207\u03b8KL(Pg\u03b8\u2016Pr)|\u03b8=\u03b80 = \u2212\u2207\u03b8Ez\u223cp(z) [ log D\u2217(g\u03b8(z))\n1\u2212D\u2217(g\u03b8(z))\n] |\u03b8=\u03b80 \u2212\u2207\u03b8KL(Pg\u03b8\u2016Pg\u03b80 )|\u03b8=\u03b80\n= Ez\u223cp(z) [ \u2212\u2207\u03b8 log D\u2217(g\u03b8(z))\n1\u2212D\u2217(g\u03b8(z))\n] |\u03b8=\u03b80\nSubstracting this last equation with the result for the JSD, we obtain our desired result.\nWe now turn to our result regarding the instability of a noisy version of the true distriminator.\nTheorem 2.6 (Instability of generator gradient updates). Let g\u03b8 : Z \u2192 X be a differentiable function that induces a distribution Pg . Let Pr be the real data distribution, with either conditions of Theorems 2.1 or 2.2 satisfied. Let D be a discriminator such that D\u2217 \u2212 D = is a centered Gaussian process indexed by x and independent for every x (popularly known as white noise) and \u2207xD\u2217 \u2212\u2207xD = r another independent centered Gaussian process indexed by x and independent for every x. Then, each coordinate of\nEz\u223cp(z) [\u2212\u2207\u03b8 logD(g\u03b8(z))]\nis a centered Cauchy distribution with infinite expectation and variance.4\nProof. Let us remember again that in this case D is locally constant equal to 0 on the support of Pg . We denote r(z), (z) the random variables r(g\u03b8(z)), (g\u03b8(z)). By the chain rule and the definition of r, , we get\nEz\u223cp(z) [\u2212\u2207\u03b8 logD(g\u03b8(z))] = Ez\u223cp(z) [ \u2212J\u03b8g\u03b8(z)\u2207xD(g\u03b8(z))\nD(g\u03b8(z)) ] = Ez\u223cp(z) [ \u2212J\u03b8g\u03b8(z)r(z)\n(z)\n]\nSince r(z) is a centered Gaussian distribution, multiplying by a matrix doesn\u2019t change this fact. Furthermore, when we divide by (z), a centered Gaussian independent from the numerator, we get a centered Cauchy random variable on every coordinate. Averaging over z the different independent Cauchy random variables again yields a centered Cauchy distribution. 5\n4Note that the theorem holds regardless of the variance of r and . As the approximation gets better, this error looks more and more as centered random noise due to the finite precision.\n5A note on technicality: when is defined as such, the remaining process is not measurable in x, so we can\u2019t take the expectation in z trivially. This is commonly bypassed, and can be formally worked out by stating the expectation as the result of a stochastic differential equation.\nNote that even if we ignore the fact that the updates have infinite variance, we still arrive to the fact that the distribution of the updates is centered, meaning that if we bound the updates the expected update will be 0, providing no feedback to the gradient.\nSince the assumption that the noises of D and \u2207D are decorrelated is albeit too strong, we show in Figure 3 how the norm of the gradient grows drastically as we train the discriminator closer to optimality, at any stage in training of a well stabilized DCGAN except when it has already converged. In all cases, using this updates lead to a notorious decrease in sample quality. The noise in the curves also shows that the variance of the gradients is increasing, which is known to delve into slower convergence and more unstable behaviour in the optimization (Bottou et al., 2016)."}, {"heading": "3 TOWARDS SOFTER METRICS AND DISTRIBUTIONS", "text": "An important question now is how to fix the instability and vanishing gradients issues. Something we can do to break the assumptions of these theorems is add continuous noise to the inputs of the discriminator, therefore smoothening the distribution of the probability mass. Theorem 3.1. If X has distribution PX with support on M and is an aboslutely continuous distribution with density P , then PX+ is absolutely continuous with density\nPX+ (x) = Ey\u223cPX [P (x\u2212 y)]\n= \u222b M P (x\u2212 y) dPX(y)\nProof. See Appendix A.\nCorollary 3.1. \u2022 If \u223c N (0, \u03c32I) then\nPX+ (x) = 1\nZ \u222b M e\u2212 \u2016y\u2212x\u20162 2\u03c32 dPX(y)\n\u2022 If \u223c N (0,\u03a3) then PX+ (x) = 1\nZ Ey\u223cPX\n[ e\u2212 1 2\u2016y\u2212x\u2016 2 \u03a3\u22121 ]\n\u2022 If P (x) \u221d 1\u2016x\u2016d+1 then\nPX+ (x) = 1\nZ Ey\u223cPX\n[ 1\n\u2016x\u2212 y\u2016d+1\n]\nThis theorem therefore tells us that the density PX+ (x) is inversely proportional to the average distance to points in the support of PX , weighted by the probability of these points. In the case of the support of PX being a manifold, we will have the weighted average of the distance to the points along the manifold. How we choose the distribution of the noise will impact the notion of distance we are choosing. In our corolary, for example, we can see the effect of changing the covariance matrix by altering the norm inside the exponential. Different noises with different types of decays can therefore be used.\nNow, the optimal discriminator between Pg+ and Pr+ is\nD\u2217(x) = Pr+ (x)\nPr+ (x) + Pg+ (x)\nand we want to calculate what the gradient passed to the generator is.\nTheorem 3.2. Let Pr and Pg be two distributions with support on M and P respectively, with \u223c N (0, \u03c32I). Then, the gradient passed to the generator has the form\nEz\u223cp(z) [\u2207\u03b8 log(1\u2212D\u2217(g\u03b8(z)))] (4) = Ez\u223cp(z) [ a(z) \u222b M P (g\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPr(y)\n\u2212 b(z) \u222b P P (g\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPg(y) ] where a(z) and b(z) are positive functions. Furthermore, b > a if and only if Pr+ > Pg+ , and b < a if and only if Pr+ < Pg+ .\nThis theorem proves that we will drive our samples g\u03b8(z) towards points along the data manifold, weighted by their probability and the distance from our samples. Furthermore, the second term drives our points away from high probability samples, again, weighted by the sample manifold and distance to these samples. This is similar in spirit to contrastive divergence, where we lower the free energy of our samples and increase the free energy of data points. The importance of this term is seen more clearly when we have samples that have higher probability of coming from Pg than from Pr. In this case, we will have b > a and the second term will have the strength to lower the probability of this too likely samples. Finally, if there\u2019s an area around x that has the same probability to come from Pg than Pr, the gradient contributions between the two terms will cancel, therefore stabilizing the gradient when Pr is similar to Pg .\nThere is one important problem with taking gradient steps exactly of the form (4), which is that in that case, D will disregards errors that lie exactly in g(Z), since this is a set of measure 0. However, g will be optimizing its cost only on that space. This will make the discriminator extremely susceptible to adversarial examples, and will render low cost on the generator without high cost on the discriminator, and lousy meaningless samples. This is easilly seen when we realize the term inside the expectation of equation (4) will be a positive scalar times \u2207x log(1 \u2212 D\u2217(x))\u2207\u03b8g\u03b8(z), which is the directional derivative towards the exact adversarial term of Goodfellow et al. (2014b). Because of this, it is important to backprop through noisy samples in the generator as well. This will yield a crucial benefit: the generator\u2019s backprop term will be through samples on a set of positive measure that the discriminator will care about. Formalizing this notion, the actual gradient through the generator will now be proportional to \u2207\u03b8JSD(Pr+ \u2016Pg+ ), which will make the two noisy distributions match. As we anneal the noise, this will make Pr and Pg match as well. For completeness, we show the smooth gradient we get in this case. The proof is identical to the one of Theorem 3.2, so we leave it to the reader.\nCorollary 3.2. Let , \u2032 \u223c N (0, \u03c32I) and g\u0303\u03b8(z) = g\u03b8(z) + \u2032, then\nEz\u223cp(z), \u2032 [\u2207\u03b8 log(1\u2212D\u2217(g\u0303\u03b8(z)))] (5) = Ez\u223cp(z), \u2032 [ a(z) \u222b M P (g\u0303\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u0303\u03b8(z)\u2212 y\u20162 dPr(y)\n\u2212 b(z) \u222b P P (g\u0303\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u0303\u03b8(z)\u2212 y\u20162 dPg(y) ] = 2\u2207\u03b8JSD(Pr+ \u2016Pg+ )\nIn the same as with Theorem 3.2, a and b will have the same properties. The main difference is that we will be moving all our noisy samples towards the data manifold, which can be thought of as moving a small neighbourhood of samples towards it. This will protect the discriminator against measure 0 adversarial examples.\nProof of theorem 3.2. Since the discriminator is assumed fixed when backproping to the generator, the only thing that depends on \u03b8 is g\u03b8(z) for every z. By taking derivatives on our cost function\nEz\u223cp(z) [\u2207\u03b8 log(1\u2212D\u2217(g\u03b8(z)))] = Ez\u223cp(z) [ \u2207\u03b8 log Pg+ (g\u03b8(z))\nPr+ (g\u03b8(z)) + Pg+ (g\u03b8(z)) ] = Ez\u223cp(z) [\u2207\u03b8 logPg+ (g\u03b8(z))\u2212\u2207\u03b8 log (Pg+ (g\u03b8(z)) + Pr+ (g\u03b8(z)))]\n= Ez\u223cp(z) [ \u2207\u03b8Pg+ (g\u03b8(z)) Pg+ (g\u03b8(z)) \u2212 \u2207\u03b8Pg+ (g\u03b8(z)) +\u2207\u03b8Pr+ (g\u03b8(z)) Pg+ (g\u03b8(z)) + Pr+ (g\u03b8(z)) ] = Ez\u223cp(z) [ 1\nPg+ (g\u03b8(z)) + Pr+ (g\u03b8(z)) \u2207\u03b8[\u2212Pr+ (g\u03b8(z))]\u2212\n1\nPg+ (g\u03b8(z)) + Pr+ (g\u03b8(z))\nPr+ (g\u03b8(z)) Pg+ (g\u03b8(z)) \u2207\u03b8[\u2212Pg+ (g\u03b8(z))] ] Let the density of be 1Z e \u2212 \u2016x\u2016 2 2\u03c32 . We now define\na(z) = 1 2\u03c32 1\nPg+ (g\u03b8(z)) + Pr+ (g\u03b8(z))\nb(z) = 1 2\u03c32 1\nPg+ (g\u03b8(z)) + Pr+ (g\u03b8(z))\nPr+ (g\u03b8(z))\nPg+ (g\u03b8(z))\nTrivially, a and b are positive functions. Since b = aPr+ Pg+ , we know that b > a if and only if Pr+ > Pg+ , and b < a if and only if Pr+ < Pg+ as we wanted. Continuing the proof, we know\nEz\u223cp(z) [\u2207\u03b8 log(1\u2212D\u2217(g\u03b8(z)))] = Ez\u223cp(z) [ 2\u03c32a(z)\u2207\u03b8[\u2212Pr+ (g\u03b8(z))]\u2212 2\u03c32b(z)\u2207\u03b8[\u2212Pg+ (g\u03b8(z)) ] = Ez\u223cp(z) [ 2\u03c32a(z) \u222b M \u2212\u2207\u03b8 1 Z e \u2212\u2016g\u03b8(z)\u2212y\u2016 2 2 2\u03c32 dPr(y)\u2212 2\u03c32b(z) \u222b P \u2212\u2207\u03b8 1 Z e \u2212\u2016g\u03b8(z)\u2212y\u2016 2 2 2\u03c32 dPg(y) ]\n= Ez\u223cp(z) [ a(z) \u222b M 1 Z e \u2212\u2016g\u03b8(z)\u2212y\u2016 2 2 2\u03c32 \u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPr(y)\n\u2212 b(z) \u222b P 1 Z e \u2212\u2016g\u03b8(z)\u2212y\u2016 2 2 2\u03c32 \u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPg(y) ]\n= Ez\u223cp(z) [ a(z) \u222b M P (g\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPr(y)\n\u2212 b(z) \u222b P P (g\u03b8(z)\u2212 y)\u2207\u03b8\u2016g\u03b8(z)\u2212 y\u20162 dPg(y) ] Finishing the proof.\nAn interesting observation is that if we have two distributions Pr and Pg with support on manifolds that are close, the noise terms will make the noisy distributions Pr+ and Pg+ almost overlap, and the JSD between them will be small. This is in drastic contrast to the noiseless variants Pr and Pg , where all the divergences are maxed out, regardless of the closeness of the manifolds. We could argue to use the JSD of the noisy variants to measure a similarity between the original distributions, but this would depend on the amount of noise, and is not an intrinsic measure of Pr and Pg . Luckilly, there are alternatives.\nDefinition 3.1. We recall the definition of the Wasserstein metric W (P,Q) for P and Q two distributions over X . Namely,\nW (P,Q) = inf \u03b3\u2208\u0393 \u222b X\u00d7X \u2016x\u2212 y\u20162d\u03b3(x, y)\nwhere \u0393 is the set of all possible joints on X \u00d7 X that have marginals P and Q.\nThe Wasserstein distance also goes by other names, most commonly the transportation metric and the earth mover\u2019s distance. This last name is most explicative: it\u2019s the minimum cost of transporting the whole probability mass of P from its support to match the probability mass ofQ onQ\u2019s support. This identification of transporting points from P toQ is done via the coupling \u03b3. We refer the reader to Villani (2009) for an in-depth explanation of these ideas. It is easy to see now that the Wasserstein metric incorporates the notion of distance (as also seen inside the integral) between the elements in the support of P and the ones in the support of Q, and that as the supports of P and Q get closer and closer, the metric will go to 0, inducing as well a notion of distance between manifolds.\nIntuitively, as we decrease the noise, PX and PX+ become more similar. However, it is easy to see again that JSD(PX\u2016PX+ ) is maxed out, regardless of the amount of noise. The following Lemma shows that this is not the case for the Wasserstein metric, and that it goes to 0 smoothly when we decrease the variance of the noise.\nLemma 4. If is a random vector with mean 0, then we have\nW (PX ,PX+ ) \u2264 V 1 2\nwhere V = E[\u2016 \u201622] is the variance of .\nProof. Let x \u223c PX , and y = x+ with independent from x. We call \u03b3 the joint of (x, y), which clearly has marginals PX and PX+ . Therefore,\nW (PX ,PX+ ) \u2264 \u222b \u2016x\u2212 y\u20162d\u03b3(x, y)\n= Ex\u223cPXEy\u223cx+ [\u2016x\u2212 y\u20162] = Ex\u223cPXEy\u223cx+ [\u2016 \u20162] = Ex\u223cPXE [\u2016 \u20162] = E [\u2016 \u20162]\n\u2264 E [\u2016 \u201622] 1 2 = V 1 2\nwhere the last inequality was due to Jensen.\nWe now turn to one of our main results. We are interested in studying the distance between Pr and Pg without any noise, even when their supports lie on different manifolds, since (for example) the closer these manifolds are, the closer to actual points on the data manifold the samples will be. Furthermore, we eventually want a way to evaluate generative models, regardless of whether they are continuous (as in a VAE) or not (as in a GAN), a problem that has for now been completely unsolved. The next theorem relates the Wasserstein distance of Pr and Pg , without any noise or modification, to the divergence of Pr+ and Pg+ , and the variance of the noise. Since Pr+ and Pg+ are continuous distributions, this divergence is a sensible estimate, which can even be attempted to minimize, since a discriminator trained on those distributions will approximate the JSD between them, and provide smooth gradients as per Corolary 3.2.\nTheorem 3.3. Let Pr and Pg be any two distributions, and be a random vector with mean 0 and variance V . If Pr+ and Pg+ have support contained on a ball of diameter C, then 6\nW (Pr,Pg) \u2264 2V 1 2 + 2C \u221a JSD(Pr+ \u2016Pg+ ) (6)\nProof.\nW (Pr,Pg) \u2264W (Pr,Pr+ ) +W (Pr+ ,Pg+ ) +W (Pg+ ,Pg)\n\u2264 2V 12 +W (Pr+ ,Pg+ )\n\u2264 2V 12 + C\u03b4(Pr+ ,Pg+ )\n\u2264 2V 12 + C (\u03b4(Pr+ ,Pm) + \u03b4(Pg+ ,Pm))\n\u2264 2V 12 + C\n(\u221a 1\n2 KL(Pr+ \u2016Pm) +\n\u221a 1\n2 KL(Pg+ \u2016Pm) ) \u2264 2V 12 + 2C \u221a JSD(Pr+ \u2016Pg+ )\nWe first used the Lemma 4 to bound everything but the middle term as a function of V . After that, we followed by the fact that W (P,Q) \u2264 C\u03b4(P,Q) wih \u03b4 the total variation, which is a popular Lemma arizing from the Kantorovich-Rubinstein duality. After that, we used the triangular inequality on \u03b4 and Pm the mixture distribution between Pg+ and Pr+ . Finally, we used Pinsker\u2019s inequality and later the fact that each individual KL is only one of the non-negative sumands of the JSD.\nTheorem 3.3 points us to an interesting idea. The two terms in equation (6) can be controlled. The first term can be decreased by annealing the noise, and the second term can be minimized by a GAN when the discriminator is trained on the noisy inputs, since it will be approximating the JSD between the two continuous distributions. One great advantage of this is that we no longer have to worry about training schedules. Because of the noise, we can train the discriminator till optimality without any problems and get smooth interpretable gradients by Corollary 3.2. All this while still minimizing the distance between Pr and Pg , the two noiseless distributions we in the end care about."}, {"heading": "ACKNOWLEDGMENTS", "text": "The first author would like to especially thank Luis Scoccola for help with the proof of Lemma 1.\nThe authors would also like to thank Ishmael Belghazi, Yoshua Bengio, Gerry Che, Soumith Chintala, Caglar Gulcehre, Daniel Jiwoong Im, Alex Lamb, Luis Scoccola, Pablo Sprechmann, Arthur Szlam, Jake Zhao for insightful comments and advice."}, {"heading": "A PROOFS OF THINGS", "text": "Proof of Lemma 1. We first consider the case where the nonlinearities are rectifiers or leaky rectifiers of the form \u03c3(x) = 1[x < 0]c1x + 1[x \u2265 0]c2x for some c1, c2 \u2208 R. In this case, g(z) = DnWn . . .D1W1z, where Wi are affine transformations and Di are some diagonal matrices dependent on z that have diagonal entries c1 or c2. If we consider D to be the (finite) set of all diagonal matrices with diagonal entries c1 or c2, then g(Z) \u2286 \u22c3 Di\u2208DDnWn . . .D1W1Z , which is a finite union of linear manifolds.\nThe proof for the second case is technical and slightly more involved. When \u03c3 is a pointwise smooth strictly increasing nonlinearity, then applying it vectorwise it\u2019s a diffeomorphism to its image. Therefore, it sends a countable union of manifolds of dimension d to a countable union of manifolds of dimension d. If we can prove the same thing for affine transformations we will be finished, since g(Z) is just a composition of these applied to a dimZ dimensional manifold. Of course, it suffices to prove that an affine transformation sends a manifold to a countable union of manifolds without increasing dimension, since a countable union of countable unions is still a countable union. Furthermore, we only need to show this for linear transformations, since applying a bias term is a diffeomorphism.\nLet W \u2208 Rn\u00d7m be a matrix. Note that by the singular value decomposition, W = U\u03a3V, where \u03a3 is a square diagonal matrix with diagonal positive entries and U,V are compositions of changes of basis, inclusions (meaning adding 0s to new coordinates) and projections to a subset of the coordinates. Multiplying by \u03a3 and applying a change of basis are diffeomorphisms, and adding 0s to new coordinates is a manifold embedding, so we only need to prove our statement for projections onto a subset of the coordinates. Let \u03c0 : Rn+k \u2192 Rn, where \u03c0(x1, . . . , xn+k) = (x1, . . . , xn) be our projection andM\u2286 Rn+k our d-dimensional manifold. If n \u2264 d, we are done since the image of \u03c0 is contained in all Rn, a manifold with at most dimension d. We now turn to the case where n > d. Let \u03c0i(x) = xi be the projection onto the i-th coordinate. If x is a critical point of \u03c0, since the coordinates of \u03c0 are independent, then x has to be a critical point of a \u03c0i. By a consequence of the Morse Lemma, the critical points of \u03c0i are isolated, and therefore so are the ones of \u03c0, meaning that there is at most a countable number of them. Since \u03c0 maps the non-critical points onto a d dimensional manifold (because it acts as an embedding) and the countable number of critical points into a countable number of points (or 0 dimensional manifolds), the proof is finished.\nProof of Lemma 2. For now we assume thatM and P are without boundary. If dimM+ dimP \u2265 d it is known that under arbitrarilly small perturbations defined as the ones in the statement of this Lemma, the two dimensions will intersect only transversally with probability 1 by the General Position Lemma. If dimM+ dimP < d, we will show that with probability 1,M+ \u03b7 and P + \u03b7\u2032 will not intersect, thereby getting our desired result. Let us then assume dimM+ dimP < d. Note that M\u0302 \u2229 P\u0302 6= \u2205 if and only if there are x \u2208 M, y \u2208 P such that x + \u03b7 = y + \u03b7\u2032, or equivalently x \u2212 y = \u03b7\u2032 \u2212 \u03b7. Therefore, M\u0302 and P\u0302 intersect if and only if \u03b7\u2032 \u2212 \u03b7 \u2208 M \u2212 P . Since \u03b7, \u03b7\u2032 are independent continuous random variables, the difference is also continuous. IfM\u2212P has measure 0 in Rd then P(\u03b7\u2032 \u2212 \u03b7 \u2208 M\u2212 P) = 0, concluding the proof. We will therefore show thatM\u2212P has measure 0. Let f :M\u00d7P \u2192 Rd be f(x, y) = x\u2212 y. If m and p are the dimensions ofM and P , then f is a smooth function between an m + p-dimensional manifold and a d dimensional one. Clearly, the image of f isM\u2212P . Therefore,\nM\u2212P = f({z \u2208M\u00d7P|rank(dzf) < m+ p}) \u222a f({z \u2208M\u00d7P|rank(dzf) = m+ p})\nThe first set is the image of the critical points, namely the critical values. By Sard\u2019s Lemma, this set has measure 0. Let\u2019s call A = {z \u2208 M \u00d7 P|rank(dzf) = m + p}. Let z be an element of A. By the inverse function theorem, there is a neighbourhood Uz \u2286 M \u00d7 P of z such that f |Uz is an embedding. Since every manifold has a countable topological basis, we can cover A by countable sets Uzn , where n \u2208 N. We will just note them by Un. Since f |Un is an embedding, f(Un) is an m+ p-dimensional manifold, and since m+ p < d, this set has measure 0 in Rd. Now, f(A) = \u22c3 n\u2208N f(Un), which therefore has measure 0 in Rd, finishing the proof of the boundary free case.\nNow we consider the case whereM and P are manifolds with boundary. By a simple union bound, P\u03b7,\u03b7\u2032(M\u0303 perfectly aligns with P\u0303) \u2264 P\u03b7,\u03b7\u2032(Int M\u0303 perfectly aligns with Int P\u0303)\n+ P\u03b7,\u03b7\u2032(Int M\u0303 perfectly aligns with \u2202P\u0303) + P\u03b7,\u03b7\u2032(\u2202M\u0303 perfectly aligns with Int P\u0303) + P\u03b7,\u03b7\u2032(\u2202M\u0303 perfectly aligns with \u2202P\u0303) = 0\nwhere the last equality arizes when combining the facts that \u02dcIntM = \u03b7 + IntM = Int (\u03b7 +M) = Int M\u0303 (and analogously for the boundary and P), that the boundary and interiors ofM and P are boundary free regular submanifolds of Rd without full dimension, and then applying the boundary free case of the proof.\nProof of Lemma 3. Let m = dimM and p = dimP . We again consider first the case whereM and P are manifolds without boundary. If m+p < d, then L = \u2205 so the statement is obviously true. If m+ p \u2265 d, thenM and P intersect transversally. This implies that L is a manifold of dimension m + p \u2212 d < m, p. Since L is a submanifold of bothM and P that has lower dimension, it has measure 0 on both of them.\nWe now tackle the case whereM and P have boundaries. Let us remember thatM = IntM\u222a\u2202M and the union is disjoint (and analogously for P). By using elementary properties of sets, we can trivially see that"}, {"heading": "L =M\u2229P = (IntM\u2229 Int P) \u222a (IntM\u2229 \u2202P) \u222a (\u2202M\u2229 Int P) \u222a (\u2202M\u2229 \u2202P)", "text": "where the unions are disjoint. This is the disjoint union of 4 strictly lower dimensional manifolds, by using the first part of the proof. Since each one of these intersections has measure 0 on either the interior or boundary ofM (again, by the first part of the proof), and interior and boundary are contained in M, each one of the four intersections has measure 0 in M. Analogously, they have measure 0 in P , and by a simple union bound we see that L has measure 0 inM and P finishing the remaining case of the proof.\nProof of Theorem 3.1. We first need to show that PX+ is absolutely continuous. Let A be a Borel set with Lebesgue measure 0. Then, by the fact that and X are independent, we know by Fubini\nPX+ (A) = \u222b Rd P (A\u2212 x) dPX(x)\n= \u222b Rd 0 dPX(x) = 0\nWhere we used the fact that if A has Lebesgue measure zero, then so does A \u2212 x and since P is absolutely continuous, P (A\u2212 x) = 0. Now we calculate the density of PX+ . Again, by using the independence of X and , for any Borel set B we know\nPX+ (B) = \u222b Rd P (B \u2212 y) dPX(y)\n= Ey\u223cPX [P (B \u2212 y)] = Ey\u223cPx [\u222b\nB\u2212y P (x)dx ] = Ey\u223cPx [\u222b B P (x\u2212 y)dx ]\n= \u222b B Ey\u223cPx [P (x\u2212 y)] dx\nTherefore, PX+ (B) = \u222b B PX+ (x)dx for our proposed PX+ and all Borel sets B. By the uniqueness of the Radon-Nikodym theorem, this implies the proposed PX+ is the density of PX+ . The equivalence of the formula changing the expectation for \u222b M PX is trivial by the definition of expectation and the fact that the support of PX lies onM."}, {"heading": "B FURTHER CLARIFICATIONS", "text": "In this appendix we further explain some of the terms and ideas mentioned in the paper, which due to space constrains, and to keep the flow of the paper, couldn\u2019t be extremely developed in the main text. Some of these have to do with notation, others with technical elements of the proofs. On the latter case, we try to convey more intuition than we previously could. We present these clarifications in a very informal fashion in the following item list.\n\u2022 There are two different but very related properties a random variable can have. A random variable X is said to be continuous if P (X = x) = 0 for all single points x \u2208 X . Note that a random variable concentrated on a low dimensional manifold such as a plane can have this property. However, an absolutely continuous random variable has the following property: if a set A has Lebesgue measure 0, then P (X \u2208 A) = 0. Since points have measure 0 with the Lebesgue measure, absolute continuity implies continuity. A random variable that\u2019s supported on a low dimensional manifold therefore will not be absolutely continuous: let M a low dimensional manifold be the support of X . Since a low dimensional manifold has 0 Lebesgue measure, this would imply P (X \u2208 M) = 0, which is an absurd since M was the support of X . The property of X being absolutely continuous can be shown to be equivalent to X having a density: the existence of a function f : X \u2192 R such that P (X \u2208 A) = \u222b A f(x) dx (this is a consequence of the Radon-Nikodym theorem).\nThe annoying part is that in everyday paper writing when we talk about continuous random variables, we omit the \u201dabsolutely\u201d word to keep the text concise and actually talk about absolutely continuous random variables (ones that have a density), this is done through almost all sciences and throughout mathematics as well, annoying as it is. However we made the clarification in here since it\u2019s relevant to our paper not to mistake the two terms. \u2022 The notation Pr[D(x) = 1] = 1 is the abbreviation of Pr[{x \u2208 X : D(x) = 1}] = 1 for a\nmeasure Pr. Another way of expressing this more formally is Pr[D\u22121(1)] = 1. \u2022 In the proof of Theorem 2.1, the distance between sets d(A,B) is defined as the usual\ndistance between sets in a metric space\nd(A,B) = inf x\u2208A,y\u2208B d(x, y)\nwhere d(x, y) is the distance between points (in our case the Euclidean distance). \u2022 Note that not everything that\u2019s outside of the support of Pr has to be a generated image.\nGenerated images are only things that lie in the support of Pg , and there are things that don\u2019t need to be in the support of either Pr or Pg (these could be places where 0 < D < 1 for example). This is because the discriminator is not trained to discriminate Pr from all things that are not Pr, but to distinguish Pr from Pg . Points that don\u2019t lie in the support of Pr or Pg are not important to the performance of the discriminator (as is easily evidenced in its cost). Why we define accuracy 1 as is done in the text is to avoid the identification of a single \u2018tight\u2019 support, since this typically leads to problems (if I take a measure 0 set from any support it still is the support of the distribution). In the end, what we aim for is:\n\u2013 We want D(x) = 1 with probability 1 when x \u223c Pr. \u2013 We want D(x) = 0 with probability 1 when x \u223c Pg . \u2013 Whatever happens elsewhere is irrelevant (as it is also reflected by the cost of the\ndiscriminator) \u2022 We say that a discriminator D\u2217 is optimal for g\u03b8 (or its corresponding Pg) if for all mea-\nsurable functions D : X \u2192 [0, 1] we have\nL(D\u2217, g\u03b8) \u2265 L(D, g\u03b8)\nfor L defined as in equation (1)."}], "references": [{"title": "Optimization methods for large-scale machine", "author": ["L\u00e9on Bottou", "Frank E. Curtis", "Jorge Nocedal"], "venue": "learning. CoRR,", "citeRegEx": "Bottou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "CoRR, abs/1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural Networks,", "citeRegEx": "Hornik.,? \\Q1991\\E", "shortCiteRegEx": "Hornik.", "year": 1991}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "CoRR, abs/1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Professor forcing: A new algorithm for training recurrent", "author": ["Alex Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron Courville", "Yoshua Bengio"], "venue": "networks. Corr,", "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["Hariharan Narayanan", "Sanjoy Mitter"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Narayanan and Mitter.,? \\Q2010\\E", "shortCiteRegEx": "Narayanan and Mitter.", "year": 2010}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Improved techniques for training", "author": ["Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "gans. CoRR,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften", "author": ["C\u00e9dric Villani"], "venue": "URL http://opac.inria. fr/record=b1129524", "citeRegEx": "Villani.,? \\Q2009\\E", "shortCiteRegEx": "Villani.", "year": 2009}, {"title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial", "author": ["Jiajun Wu", "Chengkai Zhang", "Tianfan Xue", "William T. Freeman", "Joshua B. Tenenbaum"], "venue": "modeling. Corr,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "(Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016) However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures.", "startOffset": 0, "endOffset": 102}, {"referenceID": 8, "context": "(Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016) However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures.", "startOffset": 0, "endOffset": 102}, {"referenceID": 9, "context": "(Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016) However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures.", "startOffset": 0, "endOffset": 102}, {"referenceID": 6, "context": "(Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016) However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures.", "startOffset": 0, "endOffset": 102}, {"referenceID": 12, "context": "(Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016; Lamb et al., 2016; Wu et al., 2016) However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures.", "startOffset": 0, "endOffset": 102}, {"referenceID": 8, "context": "(Radford et al., 2015; Salimans et al., 2016) Despite their success, there is little to no theory explaining the unstable behaviour of GAN training.", "startOffset": 0, "endOffset": 45}, {"referenceID": 9, "context": "(Radford et al., 2015; Salimans et al., 2016) Despite their success, there is little to no theory explaining the unstable behaviour of GAN training.", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "The use of this norm is to make the proofs simpler, but could have been done in another Sobolev norm \u2016 \u00b7 \u20161,p for p < \u221e covered by the universal approximation theorem in the sense that we can guarantee a neural network approximation in this norm (Hornik, 1991).", "startOffset": 246, "endOffset": 260}, {"referenceID": 0, "context": "The noise in the curves also shows that the variance of the gradients is increasing, which is known to delve into slower convergence and more unstable behaviour in the optimization (Bottou et al., 2016).", "startOffset": 181, "endOffset": 202}], "year": 2017, "abstractText": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_5"}