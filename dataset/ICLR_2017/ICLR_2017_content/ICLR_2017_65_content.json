{"name": "ICLR_2017_65.pdf", "metadata": {"source": "CRF", "title": "AUTOENCODING VARIATIONAL INFERENCE FOR TOPIC MODELS", "authors": ["Akash Srivastava", "Charles Sutton"], "emails": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "Topic models (Blei, 2012) are among the most widely used models for learning unsupervised representations of text, with hundreds of different model variants in the literature, and have have found applications ranging from the exploration of the scientific literature (Blei & Lafferty, 2007) to computer vision (Fei-Fei & Perona, 2005), bioinformatics (Rogers et al., 2005), and archaeology (Mimno, 2009). A major challenge in applying topic models and developing new models is the computational cost of computing the posterior distribution. Therefore a large body of work has considered approximate inference methods, the most popular methods being variational methods, especially mean field methods, and Markov chain Monte Carlo, particularly methods based on collapsed Gibbs sampling.\nBoth mean-field and collapsed Gibbs have the drawback that applying them to new topic models, even if there is only a small change to the modeling assumptions, requires re-deriving the inference methods, which can be mathematically arduous and time consuming, and limits the ability of practitioners to freely explore the space of different modeling assumptions. This has motivated the development of black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kucukelbir et al., 2016; Kingma & Welling, 2014) which require only very limited and easy to compute information from the model, and hence can be applied automatically to new models given a simple declarative specification of the generative process.\nAutoencoding variational Bayes (AEVB) (Kingma & Welling, 2014; Rezende et al., 2014) is a particularly natural choice for topic models, because it trains an inference network (Dayan et al., 1995), a neural network that directly maps a document to an approximate posterior distribution,\n\u2217Additional affiliation: Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB\nwithout the need to run further variational updates. This is intuitively appealing because in topic models, we expect the mapping from documents to posterior distributions to be well behaved, that is, that a small change in the document will produce only a small change in topics. This is exactly the type of mapping that a universal function approximator like a neural network should be good at representing. Essentially, the inference network learns to mimic the effect of probabilistic inference, so that on test data, we can enjoy the benefits of probabilistic modeling without paying a further cost for inference.\nHowever, despite some notable successes for latent Gaussian models, black box inference methods are significantly more challenging to apply to topic models. For example, in initial experiments, we tried to apply ADVI (Kucukelbir et al., 2016), a recent black-box variational method, but it was difficult to obtain any meaningful topics. Two main challenges are: first, the Dirichlet prior is not a location scale family, which hinders reparameterisation, and second, the well known problem of component collapsing (Dinh & Dumoulin, 2016), in which the inference network becomes stuck in a bad local optimum in which all topics are identical.\nIn this paper, we present what is, to our knowledge, the first effective AEVB inference method for topic models, which we call Autoencoded Variational Inference for Topic Models or AVITM1. On several data sets, we find that AVITM yields topics of equivalent quality to standard mean-field inference, with a large decrease in training time. We also find that the inference network learns to mimic the process of approximate inference highly accurately, so that it is not necessary to run variational optimization at all on test data.\nBut perhaps more important is that AVITM is a black-box method that is easy to apply to new models. To illustrate this, we present a new topic model, called ProdLDA, in which the distribution over individual words is a product of experts rather than the mixture model used in LDA. We find that ProdLDA consistently produces better topics than standard LDA, whether measured by automatically determined topic coherence or qualitative examination. Furthermore, because we perform probabilistic inference using a neural network, we can fit a topic model on roughly a one million documents in under 80 minutes on a single GPU, and because we are using a black box inference method, implementing ProdLDA requires a change of only one line of code from our implementation of standard LDA.\nTo summarize, the main advantages of our methods are:\n1. Topic coherence: ProdLDA returns consistently better topics than LDA, even when LDA is trained using Gibbs sampling.\n2. Computational efficiency: Training AVITM is fast and efficient like standard mean-field. On new data, AVITM is much faster than standard mean field, because it requires only one forward pass through a neural network.\n3. Black box: AVITM does not require rigorous mathematical derivations to handle changes in the model, and can be easily applied to a wide range of topic models.\nOverall, our results suggest that AVITM is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models."}, {"heading": "2 BACKGROUND", "text": "To fix notation, we begin by describing topic modelling and AVITM."}, {"heading": "2.1 LATENT DIRICHLET ALLOCATION", "text": "We describe the most popular topic model, latent Dirichlet allocation (LDA). In LDA, each document of the collection is represented as a mixture of topics, where each topic \u03b2k is a probability distribution over the vocabulary. We also use \u03b2 to denote the matrix \u03b2 = (\u03b21 . . . \u03b2K). The generative process is then as described in Algorithm 1. Under this generative model, the marginal likelihood of\n1Code available at https://github.com/akashgit/autoencoding_vi_for_topic_models\nfor each document w do Draw topic distribution \u03b8 \u223c Dirichlet(\u03b1); for each word at position n do\nSample topic zn \u223c Multinomial(1, \u03b8); Sample word wn \u223c Multinomial(1, \u03b2zn);\nend end\nAlgorithm 1: LDA as a generative model.\na document w is\n(1)p(w|\u03b1, \u03b2) = \u222b \u03b8 ( N\u220f n=1 k\u2211 zn=1 p(wn|zn, \u03b2)p(zn|\u03b8) ) p(\u03b8|\u03b1)d\u03b8.\nPosterior inference over the hidden variables \u03b8 and z is intractable due to the coupling between the \u03b8 and \u03b2 under the multinomial assumption (Dickey, 1983)."}, {"heading": "2.2 MEAN FIELD AND AEVB", "text": "A popular approximation for efficient inference in topic models is mean field variational inference, which breaks the coupling between \u03b8 and z by introducing free variational parameters \u03b3 over \u03b8 and \u03c6 over z and dropping the edges between them. This results in an approximate variational posterior q(\u03b8, z|\u03b3, \u03c6) = q\u03b3(\u03b8) \u220f n q\u03c6(zn), which is optimized to best approximate the true posterior p(\u03b8, z|w, \u03b1, \u03b2). The optimization problem is to minimize\n(2)L(\u03b3, \u03c6 | \u03b1, \u03b2) = DKL [q(\u03b8, z|\u03b3, \u03c6)||p(\u03b8, z|w, \u03b1, \u03b2)]\u2212 log p(w|\u03b1, \u03b2).\nIn fact the above equation is a lower bound to the marginal log likelihood, sometimes called an evidence lower bound (ELBO), a fact which can be easily verified by multiplying and dividing (1) by the variational posterior and then applying Jensen\u2019s inequality on its logarithm. Note that the mean field method optimizes over an independent set of variational parameters for each document. To emphasize this, we will refer to this standard method by the non-standard name of Decoupled Mean-Field Variational Inference (DMFVI).\nFor LDA, this optimization has closed form coordinate descent equations due to the conjugacy between the Dirichlet and multinomial distributions. Although this is a computationally convenient aspect of DMFVI, it also limits its flexibility. Applying DMFVI to new models relies on the practitioner\u2019s ability to derive the closed form updates, which can be impractical and sometimes impossible.\nAEVB (Kingma & Welling, 2014; Rezende et al., 2014) is one of several recent methods that aims at \u201cblack box\u201d inference methods to sidestep this issue. First, rewrite the ELBO as\n(3)L(\u03b3, \u03c6 | \u03b1, \u03b2) = \u2212DKL [q(\u03b8, z|\u03b3, \u03c6)||p(\u03b8, z|\u03b1)] + Eq(\u03b8,z|\u03b3,\u03c6)[log p(w|z, \u03b8, \u03b1, \u03b2)]\nThis form is intuitive. The first term attempts to match the variational posterior over latent variables to the prior on the latent variables, while the second term ensures that the variational posterior favors values of the latent variables that are good at explaining the data. By analogy to autoencoders, this second term is referred to as a reconstruction term.\nWhat makes this method \u201cAutoencoding,\u201d and in fact the main difference from DMFVI, is the parameterization of the variational distribution. In AEVB, the variational parameters are computed by using a neural network called an inference network that takes the observed data as input. For example, if the model prior p(\u03b8) were Gaussian, we might define the inference network as a feedforward neural network (\u00b5(w),v(w)) = f(w, \u03b3), where \u00b5(w) and v(w) are both vectors of length k, and \u03b3 are the network\u2019s parameters. Then we might choose a Gaussian variational distribution q\u03b3(\u03b8) = N(\u03b8;\u00b5(w), diag(v(w))), where diag(\u00b7 \u00b7 \u00b7) produces a diagonal matrix from a column vector. The variational parameters \u03b3 can then be chosen by optimizing the ELBO (3). Note that we have\nnow, unlike DMFVI, coupled the variational parameters for different documents because they are all computed from the same neural network. To compute the expectations with respect to q in (3), Kingma & Welling (2014); Rezende et al. (2014) use a Monte Carlo estimator which they call the \u201creparameterization trick\u201d (RT; appears also in Williams (1992)). In the RT, we define a variate U with a simple distribution that is independent of all variational parameters, like a uniform or standard normal, and a reparameterization function F such that F (U, \u03b3) has distribution q\u03b3 . This is always possible, as we could choose F to be the inverse cumulative distribution function of q\u03b3 , although we will additionally want F to be easy to compute and differentiable. If we can determine a suitable F , then we can approximate (3) by taking Monte Carlo samples of U , and optimize \u03b3 using stochastic gradient descent."}, {"heading": "3 AUTOENCODING VARIATIONAL BAYES IN LATENT DIRICHLET ALLOCATION", "text": "Although simple conceptually, applying AEVB to topic models raises several practical challenges. The first is the need to determine a reparameterization function for q(\u03b8) and q(zn) to use the RT. The zn are easily dealt with, but \u03b8 is more difficult; if we choose q(\u03b8) to be Dirichlet, it is difficult to apply the RT, whereas if we choose q to be Gaussian or logistic normal, then the KL divergence in (3) becomes more problematic. The second issue is the well known problem of component collapsing (Dinh & Dumoulin, 2016), which a type of bad local optimum that is particularly endemic to AEVB and similar methods. We describe our solutions to each of those problems in the next few subsections.\n3.1 COLLAPSING z\u2019S\nDealing with discrete variables like z using reparameterization can be problematic, but fortunately in LDA the variable z can be conveniently summed out. By collapsing z we are left with having to sample from \u03b8 only, reducing (1) to\n(4)p(w|\u03b1, \u03b2) = \u222b \u03b8 ( N\u220f n=1 p(wn|\u03b2, \u03b8) ) p(\u03b8|\u03b1)d\u03b8.\nwhere the distribution of wn|\u03b2, \u03b8 is Multinomial(1, \u03b2\u03b8), recalling that \u03b2 denotes the matrix of all topic-word probability vectors."}, {"heading": "3.2 WORKING WITH DIRICHLET BELIEFS: LAPLACE APPROXIMATION", "text": "LDA gets its name from the Dirichlet prior on the topic proportions \u03b8, and the choice of Dirichlet prior is important to obtaining interpretable topics (Wallach et al., 2009). But it is difficult to handle the Dirichlet within AEVB because it is difficult to develop an effective reparameterization function for the RT. Fortunately, a RT does exist for the Gaussian distribution and has been shown to perform quite well in the context of variational autoencoder (VAE) (Kingma & Welling, 2014).\nWe resolve this issue by constructing a Laplace approximation to the Dirichlet prior. Following MacKay (1998), we do so in the softmax basis instead of the simplex. There are two benefits of this choice. First, Dirichlet distributions are unimodal in the softmax basis with their modes coinciding with the means of the transformed densities. Second, the softmax basis also allows for carrying out unconstrained optimization of the cost function without the simplex constraints. The Dirichlet probability density function in this basis over the softmax variable h is given by\n(5)P (\u03b8(h)|\u03b1) = \u0393( \u2211 k \u03b1k)\u220f\nk \u0393(\u03b1k) \u220f k \u03b8\u03b1kk g(1 Th).\nHere \u03b8 = \u03c3(h), where \u03c3(.) represents the softmax function. Recall that the Jacobian of \u03c3 is proportional to \u220f k \u03b8k and g(\u00b7) is an arbitrary density that ensures integrability by constraining the redundant degree of freedom. We use the Laplace approximation of Hennig et al. (2012), which\nhas the property that the covariance matrix becomes diagonal for large k (number of topics). This approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution over the softmax variables h as a multivariate normal with mean \u00b51 and covariance matrix \u03a31 where\n\u00b51k = log\u03b1k \u2212 1\nK \u2211 i log\u03b1i\n\u03a31kk = 1\n\u03b1k\n( 1\u2212 2\nK\n) + 1\nK2 \u2211 i 1 \u03b1k . (6)\nFinally, we approximate p(\u03b8|\u03b1) in the simplex basis with p\u0302(\u03b8|\u00b51,\u03a31) = LN (\u03b8|\u00b51,\u03a31) where LN is a logistic normal distribution with parameters \u00b51,\u03a31. Although we approximate the Dirichlet prior in LDA with a logistic normal, this is not the same idea as a correlated topic model (Blei & Lafferty, 2006), because we use a diagonal covariance matrix. Rather, it is an approximation to standard LDA."}, {"heading": "3.3 VARIATIONAL OBJECTIVE", "text": "Now we can write the modified variational objective function. We use a logistic normal variational distribution over \u03b8 with diagonal covariance. More precisely, we define two inference networks as feed forward neural networks f\u00b5 and f\u03a3 with parameters \u03b4; the output of each network is a vector in RK . Then for a document w, we define q(\u03b8) to be logistic normal with mean \u00b50 = f\u00b5(w, \u03b4) and diagonal covariance \u03a30 = diag(f\u03a3(w, \u03b4)), where diag converts a column vector to a diagonal matrix. Note that we can generate samples from q(\u03b8) by sampling \u223c N (0, I) and computing \u03b8 = \u03c3(\u00b50 + \u03a3 1/2 0 ).\nWe can now write the ELBO as\nL(\u0398) = D\u2211 d=1\n[ \u2212 ( 1\n2\n{ tr(\u03a3\u221211 \u03a30) + (\u00b51 \u2212\u00b50)T\u03a3 \u22121 1 (\u00b51 \u2212\u00b50)\u2212K + log\n|\u03a31| |\u03a30|\n}) (7)\n+E \u223cN (0,I) [ w>d log ( \u03c3(\u03b2)\u03c3(\u00b50 + \u03a3 1/2 0 ) )]] ,\nwhere \u0398 represents the set of all the model and variational parameters and w1 . . .wD are the documents in the corpus. The first line in this equation arises from the KL divergence between the two logistic normal distributions q and p\u0302, while the second line is the reconstruction error.\nIn order to impose the simplex constraint on the \u03b2 matrix during the optimization, we apply the softmax transformation. That is, each topic \u03b2k \u2208 RV is unconstrained, and the notation \u03c3(\u03b2) means to apply the softmax function separately to each column of the matrix \u03b2. Note that the mixture of multinomials for each word wn can then be written as p(wn|\u03b2, \u03b8) = [\u03c3(\u03b2)\u03b8]wn , which explains the dot product in (7). To optimize (7), we use stochastic gradient descent using Monte Carlo samples from , following the Law of the Unconscious Statistician."}, {"heading": "3.4 TRAINING AND PRACTICAL CONSIDERATIONS: DEALING WITH COMPONENT COLLAPSING", "text": "AEVB is prone to component collapsing (Dinh & Dumoulin, 2016), which is a particular type of local optimum very close to the prior belief, early on in the training. As the latent dimensionality of the model is increased, the KL regularization in the variational objective dominates, so that the outgoing decoder weights collapse for the components of the latent variable that reach close to the prior and do not show any posterior divergence. In our case, the collapsing specifically occurs because of the inclusion of the softmax transformation to produce \u03b8. The result is that the k inferred topics are identical as shown in table 7.\nWe were able to resolve this issue by tweaking the optimization. Specifically, we train the network with the ADAM optimizer (Kingma & Ba, 2015) using high moment weight (\u03b21) and learning rate (\u03b7). Through training at higher rates, early peaks in the functional space can be easily avoided. The\nproblem is that momentum based training coupled with higher learning rate causes the optimizer to diverge. While explicit gradient clipping helps to a certain extent, we found that batch normalization (Ioffe & Szegedy, 2015) does even better by smoothing out the functional space and hence curbing sudden divergence.\nFinally, we also found an increase in performance with dropout units when applied to \u03b8 to force the network to use more of its capacity.\nWhile more prominent in the AEVB framework, the collapsing can also occurs in DMFVI if the learning offset (referred to as the \u03c4 parameter (Hofmann, 1999)) is not set properly. Interestingly, a similar learning offset or annealing based approach can also be used to down-weight the KL term in early iterations of the training to avoid local optima."}, {"heading": "4 PRODLDA: LATENT DIRICHLET ALLOCATION WITH PRODUCTS OF EXPERTS", "text": "In LDA, the distribution p(w|\u03b8, \u03b2) is a mixture of multinomials. A problem with this assumption is that it can never make any predictions that are sharper than the components that are being mixed (Hinton & Salakhutdinov, 2009). This can result in some topics appearing that are poor quality and do not correspond well with human judgment. One way to resolve this issue is to replace this word-level mixture with a weighted product of experts which by definition is capable of making sharper predictions than any of the constituent experts (Hinton, 2002). In this section we present a novel topic model PRODLDA that replaces the mixture assumption at the word-level in LDA with a weighted product of experts, resulting in a drastic improvement in topic coherence. This is a good illustration of the benefits of a black box inference method, like AVITM, to allow exploration of new models."}, {"heading": "4.1 MODEL", "text": "The PRODLDA model can be simply described as latent Dirichlet allocation where the word-level mixture over topics is carried out in natural parameter space, i.e. the topic matrix is not constrained to exist in a multinomial simplex prior to mixing. In other words, the only changes from LDA are that \u03b2 is unnormalized, and that the conditional distribution of wn is defined as wn|\u03b2, \u03b8 \u223c Multinomial(1, \u03c3(\u03b2\u03b8)).\nThe connection to a product of experts is straightforward, as for the multinomial, a mixture of natural parameters corresponds to a weighted geometric average of the mean parameters. That is, consider two N dimensional multinomials parametrized by mean vectors p and q. Define the corresponding natural parameters as p = \u03c3(r) and q = \u03c3(s), and let \u03b4 \u2208 [0, 1]. It is then easy to show that\nP ( x|\u03b4r + (1\u2212 \u03b4)s ) \u221d N\u220f i=1 \u03c3(\u03b4ri + (1\u2212 \u03b4)si)xi \u221d N\u220f i=1 [r\u03b4i \u00b7 s (1\u2212\u03b4) i ] xi .\nSo the PRODLDA model can be simply described as a product of experts, that is, p(wn|\u03b8, \u03b2) \u221d\u220f k p(wn|zn = k, \u03b2)\u03b8k . PRODLDA is an instance of the exponential-family PCA (Collins et al., 2001) class, and relates to the exponential-family harmoniums (Welling et al., 2004) but with nonGaussian priors."}, {"heading": "5 RELATED WORK", "text": "For an overview of topic modeling, see Blei (2012). There are several examples of topic models based on neural networks and neural variational inference (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Mnih & Gregor, 2014; Miao et al., 2016) but we are unaware of methods that apply AEVB generically to a topic model specified by an analyst, or even of a successful application of AEVB to the most widely used topic model, latent Dirichlet allocation.\nRecently, Miao et al. (2016) introduced a closely related model called the Neural Variational Document Model (NVDM). This method uses a latent Gaussian distribution over topics, like probabilistic latent semantic indexing, and averages over topic-word distributions in the logit space. However,\nthey do not use either of the two key aspects of our work: explicitly approximating the Dirichlet prior using a Gaussian, or high-momentum training. In the experiments we show that these aspects lead to much improved training and much better topics."}, {"heading": "6 EXPERIMENTS AND RESULTS", "text": "Qualitative evaluation of topic models is a challenging task and consequently a large body of work has developed automatic evaluation metrics that attempt to match human judgment of topic quality. Traditionally, perplexity has been used to measure the goodness-of-fit of the model but it has been repeatedly shown that perplexity is not a good metric for qualitative evaluation of topics (Newman et al., 2010). Several new metrics of topic coherence evaluation have thus been proposed; see Lau et al. (2014) for a comparative review. Lau et al. (2014) showed that among all the competing metrics, normalized pointwise mutual information (NPMI) between all the pairs of words in a set of topics matches human judgment most closely, so we adopt it in this work. We also report perplexity, primarily as a way of evaluating the capability of different optimizers. Following standard practice (Blei et al., 2003), for variational methods we use the ELBO to calculate perplexity. For AEVB methods, we calculate the ELBO using the same Monte Carlo approximation as for training.\nWe run experiments on both the 20 Newsgroups (11,000 training instances with 2000 word vocabulary) and RCV1 Volume 2 ( 800K training instances with 10000 word vocabulary) datasets. Our preprocessing involves tokenization, removal of some non UTF-8 characters for 20 Newsgroups and English stop word removal. We first compare our AVITM inference method with the standard online mean-field variational inference (Hoffman et al., 2010) and collapsed Gibbs sampling (Griffiths & Steyvers, 2004) on the LDA model. We use standard implementations of both methods, scikit-learn for DMFVI and mallet (McCallum, 2002) for collapsed Gibbs. Then we compare two autoencoding inference methods on three different topic models: standard LDA, PRODLDA using our inference method and the Neural Variational Document Model (NVDM) (Miao et al., 2016), using the inference described in the paper.2\nTables 1 and 2 show the average topic coherence values for all the models for two different settings of k, the number of topics. Comparing the different inference methods for LDA, we find that, consistent with previous work, collapsed Gibbs sampling yields better topics than mean-field methods. Among the variational methods, we find that VAE-LDA model (AVITM) 3 yields similar topic coherence and perplexity to the standard DMFVI (although in some cases, VAE-LDA yields significantly better topics). However, AVITM is significantly faster to train than DMFVI. It takes 46 seconds on 20 Newsgroup compared to 18 minutes for DMFVI. Whereas for a million document corpus of RCV1 it only under 1.5 hours while scikit-learn\u2019s implementation of DMFVI failed to return any results even after running for 24 hours.4\nComparing the new topic models than LDA, it is clear that PRODLDA finds significantly better topics than LDA, even when trained by collapsed Gibbs sampling. To verify this qualitatively, we display examples of topics from all the models in Table 6. The topics from ProdLDA appear visually more coherent than NVDM or LDA. Unfortunately, NVDM does not perform comparatively to LDA\n2We have used both https://github.com/carpedm20/variational-text-tensorflow and the NVDM author\u2019s (Miao et al., 2016) implementation.\n3We recently found that \u2019whitening\u2019 the topic matrix significantly improves the topic coherence for VAELDA. Manuscript in preparation.\n4Therefore, we were not able to report topic coherence for DMFVI on RCV1\nfor any value of k. To avoid any training dissimilarities we train all the competing models until we reach the perplexities that were reported in previous work. These are reported in Table 35.\nA major benefit of AVITM inference is that it does not require running variational optimization, which can be costly, for new data. Rather, the inference network can be used to obtain topic proportions for new documents for new data points without running any optimization. We evaluate whether this approximation is accurate, i.e. whether the neural network effectively learns to mimic probabilistic inference. We verify this by training the model on the training set, then on the test set, holding the topics (\u03b2 matrix) fixed, and comparing the test perplexity if we obtain topic proportions by running the inference neural network directly, or by the standard method of variational optimization of the inference network on the test set. As shown in Table 4, the perplexity remains practically un-changed. The computational benefits of this are remarkable. On both the datasets, computing perplexity using the neural network takes well under a minute, while running the standard variational approximation takes \u223c 3 minutes even on the smaller 20 Newsgroups data. Finally, we investigate the reasons behind the improved topic coherence in PRODLDA. First, Table 5 explores the effects of each of our two main ideas separately. In this table, \u201cDirichlet\u201d means that the prior is the Laplace approximation to Dirichlet(\u03b1 = 0.02), while \u201cGaussian\u201d indicates that we use a standard Gaussian as prior. \u2018High Learning Rate\u201d training means we use \u03b21 > 0.8 and 0.1 > \u03b7 > 0.0016 with batch normalization, whereas \u201cLow Learning Rate\u201d means \u03b21 > 0.8 and 0.0009 > \u03b7 > 0.00009 without batch normalization. (For both parameters, the precise value was chosen by Bayesian optimization. We found that these values in the \u201dwith BN\u201d cases were close to the default settings in the Adam optimizer.) We find that the high topic coherence that we achieve in this work is only possible if we use both tricks together. In fact the high learning rates with momentum is required to avoid local minima in the beginning of the training and batch-normalization is required to be able to train the network at these values without diverging. If trained at a lower momentum value or at a lower learning rate PRODLDA shows component collapsing. Interestingly, if we choose a Gaussian prior, rather than the logistic normal approximation used in ProdLDA or NVLDA, the model is easier to train even with low learning rate without any momentum or batch normalization.\nThe main advantage of AVITM topic models as opposed to NVDM is that the Laplace approximation allows us to match a specific Dirichlet prior of interest. As pointed out by Wallach et al. (2009), the choice of Dirichlet hyperparameter is important to the topic quality of LDA. Following this reasoning, we hypothesize that AVITM topics are higher quality than those of NVDM because they are much more focused, i.e., apply to a more specific subset of documents of interest. We provide support for this hypothesis in Figure 1, by evaluating the sparsity of the posterior proportions over topics, that is, how many of the model\u2019s topics are typically used to explain each document. In order to estimate the sparsity in topic proportions, we project samples from the Gaussian latent spaces of PRODLDA and NVDM in the simplex and average them across documents. We compare the topic\n5We note that much recent work follows Hinton & Salakhutdinov (2009) in reporting perplexity for the LDA Gibbs sampler on only a small subset of the test data. Our results are different because we use the entire test dataset.\n6\u03b21 is the weight on the average of the gradients from the previous time step and \u03b7 refers to the learning rate.\nsparsity for the standard Gaussian prior used by NVDM to the Laplace approximation of Dirichlet priors with different hyperparameters. Clearly the Laplace approximation to the Dirichlet prior significantly promotes sparsity, providing support for our hypothesis that preserving the Dirichlet prior explains the the increased topic coherence in our method.\nThe inference network architecture can be found in figure 2 in the appendix."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "We present what is to our knowledge the first effective AEVB inference algorithm for latent Dirichlet allocation. Although this combination may seem simple in principle, in practice this method is difficult to train because of the Dirichlet prior and because of the component collapsing problem. By addressing both of these problems, we presented a black-box inference method for topic models with the notable advantage that the neural network allows computing topic proportions for new documents without the need to run any variational optimization. As an illustration of the advantages of"}, {"heading": "1. write article get thanks like anyone please know look one", "text": "black box inference techniques, we presented a new topic model, ProdLDA, which achieves significantly better topics than LDA, while requiring a change of only one line of code from AVITM for LDA. Our results suggest that AVITM inference is ready to take its place alongside mean field and collapsed Gibbs as one of the workhorse inference methods for topic models. Future work could include extending our inference methods to handle dynamic and correlated topic models."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Andriy Mnih, Chris Dyer, Chris Russell, David Blei, Hannah Wallach, Max Welling, Mirella Lapata and Yishu Miao for helpful comments, discussions and feedback."}, {"heading": "A NETWORK ARCHITECTURE", "text": ""}], "references": [{"title": "Probabilistic topic models", "author": ["David Blei"], "venue": "Communications of the ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Correlated topic models", "author": ["David M. Blei", "John D. Lafferty"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Blei and Lafferty.,? \\Q2006\\E", "shortCiteRegEx": "Blei and Lafferty.", "year": 2006}, {"title": "A correlated topic model of science", "author": ["David M. Blei", "John D. Lafferty"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Blei and Lafferty.,? \\Q2007\\E", "shortCiteRegEx": "Blei and Lafferty.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A generalization of principal component analysis to the exponential family", "author": ["Michael Collins", "Sanjoy Dasgupta", "Robert E Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "The helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel"], "venue": "Neural Computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Multiple hypergeometric functions: Probabilistic interpretations and statistical uses", "author": ["James M Dickey"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Dickey.,? \\Q1983\\E", "shortCiteRegEx": "Dickey.", "year": 1983}, {"title": "Training neural Bayesian nets. http://www. iro.umontreal.ca/ \u0303bengioy/cifar/NCAP2014-summerschool/slides/ Laurent_dinh_cifar_presentation.pdf", "author": ["Laurent Dinh", "Vincent Dumoulin"], "venue": null, "citeRegEx": "Dinh and Dumoulin.,? \\Q2016\\E", "shortCiteRegEx": "Dinh and Dumoulin.", "year": 2016}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Pietro Perona"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Fei.Fei and Perona.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona.", "year": 2005}, {"title": "Finding scientific topics", "author": ["Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National academy of Sciences,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Kernel topic models", "author": ["Philipp Hennig", "David H Stern", "Ralf Herbrich", "Thore Graepel"], "venue": "In AISTATS, pp", "citeRegEx": "Hennig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hennig et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "Replicated softmax: an undirected topic model", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2009}, {"title": "Online learning for latent dirichlet allocation", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "3rd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "The International Conference on Learning Representations (ICLR), Banff,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Automatic differentiation variational inference", "author": ["Alp Kucukelbir", "Dustin Tran", "Rajesh Ranganath", "Andrew Gelman", "David M Blei"], "venue": "arXiv preprint arXiv:1603.00788,", "citeRegEx": "Kucukelbir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2016}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["Jey Han Lau", "David Newman", "Timothy Baldwin"], "venue": "In EACL,", "citeRegEx": "Lau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "Choice of basis for Laplace approximation", "author": ["David JC MacKay"], "venue": "Machine learning,", "citeRegEx": "MacKay.,? \\Q1998\\E", "shortCiteRegEx": "MacKay.", "year": 1998}, {"title": "Mallet: A machine learning for language toolkit", "author": ["Andrew McCallum"], "venue": "http://mallet.cs. umass.edu,", "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Reconstructing Pompeian households", "author": ["David Mimno"], "venue": "In Applications of Topic Models Workshop,", "citeRegEx": "Mimno.,? \\Q2009\\E", "shortCiteRegEx": "Mimno.", "year": 2009}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": null, "citeRegEx": "Mnih and Gregor.,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor.", "year": 2014}, {"title": "Automatic evaluation of topic coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Newman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Black box variational inference", "author": ["Rajesh Ranganath", "Sean Gerrish", "David M Blei"], "venue": "In AISTATS, pp", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "The latent process decomposition of cdna microarray data sets", "author": ["Simon Rogers", "Mark Girolami", "Colin Campbell", "Rainer Breitling"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "Rogers et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2005}, {"title": "Rethinking LDA: Why priors matter", "author": ["Hanna Wallach", "David Mimno", "Andrew McCallum"], "venue": "In NIPS,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["Max Welling", "Michal Rosen-Zvi", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Welling et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2004}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "Topic models (Blei, 2012) are among the most widely used models for learning unsupervised representations of text, with hundreds of different model variants in the literature, and have have found applications ranging from the exploration of the scientific literature (Blei & Lafferty, 2007) to computer vision (Fei-Fei & Perona, 2005), bioinformatics (Rogers et al.", "startOffset": 13, "endOffset": 25}, {"referenceID": 29, "context": "Topic models (Blei, 2012) are among the most widely used models for learning unsupervised representations of text, with hundreds of different model variants in the literature, and have have found applications ranging from the exploration of the scientific literature (Blei & Lafferty, 2007) to computer vision (Fei-Fei & Perona, 2005), bioinformatics (Rogers et al., 2005), and archaeology (Mimno, 2009).", "startOffset": 351, "endOffset": 372}, {"referenceID": 27, "context": "This has motivated the development of black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kucukelbir et al., 2016; Kingma & Welling, 2014) which require only very limited and easy to compute information from the model, and hence can be applied automatically to new models given a simple declarative specification of the generative process.", "startOffset": 66, "endOffset": 160}, {"referenceID": 18, "context": "This has motivated the development of black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kucukelbir et al., 2016; Kingma & Welling, 2014) which require only very limited and easy to compute information from the model, and hence can be applied automatically to new models given a simple declarative specification of the generative process.", "startOffset": 66, "endOffset": 160}, {"referenceID": 28, "context": "Autoencoding variational Bayes (AEVB) (Kingma & Welling, 2014; Rezende et al., 2014) is a particularly natural choice for topic models, because it trains an inference network (Dayan et al.", "startOffset": 38, "endOffset": 84}, {"referenceID": 5, "context": ", 2014) is a particularly natural choice for topic models, because it trains an inference network (Dayan et al., 1995), a neural network that directly maps a document to an approximate posterior distribution, \u2217Additional affiliation: Alan Turing Institute, British Library, 96 Euston Road, London NW1 2DB", "startOffset": 98, "endOffset": 118}, {"referenceID": 18, "context": "For example, in initial experiments, we tried to apply ADVI (Kucukelbir et al., 2016), a recent black-box variational method, but it was difficult to obtain any meaningful topics.", "startOffset": 60, "endOffset": 85}, {"referenceID": 6, "context": "Posterior inference over the hidden variables \u03b8 and z is intractable due to the coupling between the \u03b8 and \u03b2 under the multinomial assumption (Dickey, 1983).", "startOffset": 142, "endOffset": 156}, {"referenceID": 28, "context": "AEVB (Kingma & Welling, 2014; Rezende et al., 2014) is one of several recent methods that aims at \u201cblack box\u201d inference methods to sidestep this issue.", "startOffset": 5, "endOffset": 51}, {"referenceID": 30, "context": "LDA gets its name from the Dirichlet prior on the topic proportions \u03b8, and the choice of Dirichlet prior is important to obtaining interpretable topics (Wallach et al., 2009).", "startOffset": 152, "endOffset": 174}, {"referenceID": 14, "context": "While more prominent in the AEVB framework, the collapsing can also occurs in DMFVI if the learning offset (referred to as the \u03c4 parameter (Hofmann, 1999)) is not set properly.", "startOffset": 139, "endOffset": 154}, {"referenceID": 11, "context": "One way to resolve this issue is to replace this word-level mixture with a weighted product of experts which by definition is capable of making sharper predictions than any of the constituent experts (Hinton, 2002).", "startOffset": 200, "endOffset": 214}, {"referenceID": 4, "context": "PRODLDA is an instance of the exponential-family PCA (Collins et al., 2001) class, and relates to the exponential-family harmoniums (Welling et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 31, "context": ", 2001) class, and relates to the exponential-family harmoniums (Welling et al., 2004) but with nonGaussian priors.", "startOffset": 64, "endOffset": 86}, {"referenceID": 23, "context": "There are several examples of topic models based on neural networks and neural variational inference (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Mnih & Gregor, 2014; Miao et al., 2016) but we are unaware of methods that apply AEVB generically to a topic model specified by an analyst, or even of a successful application of AEVB to the most widely used topic model, latent Dirichlet allocation.", "startOffset": 101, "endOffset": 197}, {"referenceID": 26, "context": "Traditionally, perplexity has been used to measure the goodness-of-fit of the model but it has been repeatedly shown that perplexity is not a good metric for qualitative evaluation of topics (Newman et al., 2010).", "startOffset": 191, "endOffset": 212}, {"referenceID": 3, "context": "Following standard practice (Blei et al., 2003), for variational methods we use the ELBO to calculate perplexity.", "startOffset": 28, "endOffset": 47}, {"referenceID": 13, "context": "We first compare our AVITM inference method with the standard online mean-field variational inference (Hoffman et al., 2010) and collapsed Gibbs sampling (Griffiths & Steyvers, 2004) on the LDA model.", "startOffset": 102, "endOffset": 124}, {"referenceID": 22, "context": "We use standard implementations of both methods, scikit-learn for DMFVI and mallet (McCallum, 2002) for collapsed Gibbs.", "startOffset": 83, "endOffset": 99}, {"referenceID": 23, "context": "Then we compare two autoencoding inference methods on three different topic models: standard LDA, PRODLDA using our inference method and the Neural Variational Document Model (NVDM) (Miao et al., 2016), using the inference described in the paper.", "startOffset": 182, "endOffset": 201}, {"referenceID": 23, "context": "com/carpedm20/variational-text-tensorflow and the NVDM author\u2019s (Miao et al., 2016) implementation.", "startOffset": 64, "endOffset": 83}], "year": 2017, "abstractText": "Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven difficult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_65"}