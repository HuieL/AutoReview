{"name": "ICLR_2017_66.pdf", "metadata": {"source": "CRF", "title": "OPTIMAL BINARY AUTOENCODING WITH PAIRWISE CORRELATIONS", "authors": ["Akshay Balsubramani"], "emails": ["abalsubr@stanford.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Consider a general autoencoding scenario, in which an algorithm learns a compression scheme for independently, identically distributed (i.i.d.) V -dimensional bit vector data { x\u0302(1), . . . , x\u0302(n) } . For\nsome encoding dimension H , the algorithm encodes each data example x\u0302(i) = (x\u0302(i)1 , . . . , x\u0302 (i) V ) > into an H-dimensional representation e(i), with H < V . It then decodes each e(i) back into a reconstructed example x\u0303(i) using some small amount of additional memory, and is evaluated on the quality of the reconstruction by the cross-entropy loss commonly used to compare bit vectors. A good autoencoder learns to compress the data into H bits so as to reconstruct it with low loss.\nWhen the loss is squared reconstruction error and the goal is to compress data in RV to RH , this is often accomplished with principal component analysis (PCA), which projects the input data on the top H eigenvectors of their covariance matrix (Bourlard & Kamp (1988); Baldi & Hornik (1989)). These eigenvectors in RV constitute V H real values of additional memory needed to decode the compressed data in RH back to the reconstructions in RV , which are linear combinations of the eigenvectors. Crucially, this total additional memory does not depend on the amount of data n, making it applicable when data are abundant.\nThis paper considers a similar problem, except using bit-vector data and the cross-entropy reconstruction loss. Since we are compressing samples of i.i.d. V -bit data into H-bit encodings, a natural approach is to remember the pairwise statistics: the V H average correlations between pairs of bits in the encoding and decoding, constituting as much additional memory as the eigenvectors used in PCA. The decoder uses these along with the H-bit encoded data, to produce V -bit reconstructions.\nWe show how to efficiently learn the autoencoder with the worst-case optimal loss in this scenario, without any further assumptions, parametric or otherwise. It has some striking properties.\nThe decoding function is identical in form to the one used in a standard binary autoencoder with one hidden layer (Bengio et al. (2013a)) and cross-entropy reconstruction loss. Specifically, each bit v of the decoding is the output of a logistic sigmoid artificial neuron of the encoded bits, with some learned weights wv \u2208 RH . This form emerges as the uniquely optimal decoding function, and is not assumed as part of any explicit model.\nWe show that the worst-case optimal reconstruction loss suffered by the autoencoder is convex in these decoding weights W = {wv}v\u2208[V ], and in the encoded representations E. Though it is not \u2217Most of the work was done as a PhD student at UC San Diego.\njointly convex in both, the situation still admits a natural and efficient optimization algorithm in which the loss is alternately minimized in E and W while the other is held fixed. The algorithm is practical and performs well empirically, learning incrementally from minibatches of data in a stochastic optimization setting."}, {"heading": "1.1 NOTATION", "text": "The observed data and encodings can be written in matrix form, representing bits as \u00b11:\nX\u0302 = x\u0302 (1) 1 \u00b7 \u00b7 \u00b7 x\u0302 (n) 1 ... . . .\n... x\u0302 (1) V \u00b7 \u00b7 \u00b7 x\u0302 (n) V\n \u2208 [\u22121, 1]V\u00d7n , E = e (1) 1 \u00b7 \u00b7 \u00b7 e (n) 1 ... . . .\n... e (1) H \u00b7 \u00b7 \u00b7 e (n) H  \u2208 [\u22121, 1]H\u00d7n (1) Here the encodings are allowed to be randomized, represented by values in [\u22121, 1] instead of just the two values {\u22121, 1}; e.g. e(1)i = 12 is +1 w.p. 3 4 and \u22121 w.p. 1 4 . The data in X are also allowed to be randomized, which we will see essentially loses no generality (Appendix B). We write the columns of X\u0302,E as x\u0302(i), e(i) for i \u2208 [n] (where [s] := {1, . . . , s}), representing the data. The rows are written as x\u0302v = (x (1) v , . . . , x (n) v )> for v \u2208 [V ] and eh = (e(1)h , . . . , e (n) h ) > for h \u2208 [H].\nWe also consider the correlation of each bit h of the encoding with each decoded bit v over the data, i.e. bv,h := 1n \u2211n i=1 x (i) v e (i) h . This too can be written in matrix form as B := 1 nX\u0302E\n> \u2208 RV\u00d7H , whose rows and columns we respectively write as bv = (bv,1, . . . , bv,H)> over v \u2208 [V ] and bh = (b1,h, . . . , bV,h) > over h \u2208 [H]; the indexing will be clear from context.\nAs alluded to earlier, the loss incurred on any example x(i) is the cross-entropy between the example and its reconstruction x\u0303(i), in expectation over the randomness in x(i). Defining `\u00b1(x\u0303 (i) v ) =\nln ( 2\n1\u00b1x\u0303(i)v\n) (the partial losses to true labels \u00b11), the loss is written as:\n`(x(i), x\u0303(i)) := V\u2211 v=1\n[( 1 + x (i) v\n2\n) `+(x\u0303 (i) v ) + ( 1\u2212 x(i)v\n2\n) `\u2212(x\u0303 (i) v ) ] (2)\nIn addition, define a potential well \u03a8(m) := ln (1 + em) + ln (1 + e\u2212m) with derivative \u03a8\u2032(m) := 1\u2212e\u2212m 1+e\u2212m . Univariate functions like this are applied componentwise to matrices in this paper."}, {"heading": "1.2 PROBLEM SETUP", "text": "With these definitions, the autoencoding problem we address can be precisely stated as two tasks, encoding and decoding. These share only the side information B. Our goal is to perform these steps so as to achieve the best possible guarantee on reconstruction loss, with no further assumptions. This can be written as a zero-sum game of an autoencoding algorithm seeking to minimize loss against an adversary, by playing encodings and reconstructions:\n\u2022 Using X\u0302, algorithm plays (randomized) encodings E, resulting in pairwise correlations B. \u2022 Using E and B, algorithm plays reconstructions X\u0303 = ( x\u0303(1); . . . ; x\u0303(n) ) \u2208 [\u22121, 1]V\u00d7n.\n\u2022 Given X\u0303,E,B, adversary plays X \u2208 [\u22121, 1]V\u00d7n to maximize reconstruction loss 1 n \u2211n i=1 `(x (i), x\u0303(i)).\nTo incur low loss, the algorithm must use an E and B such that no adversary playing X can inflict higher loss. The algorithm never sees X, which represents the worst the data could be given the algorithm\u2019s incomplete memory of it (E,B) and reconstructions (X\u0303).\nWe find the autoencoding algorithm\u2019s best strategy in two parts. First, we find the optimal decoding function of any encodings E given B, in Section 2. Then, we use the resulting optimal reconstruction function to outline the best encoding procedure, i.e. one that finds the E,B that lead to the best reconstruction, in Section 3.1. Combining these ideas yields an autoencoding algorithm in Section\n3.2 (Algorithm 1), where its implementation and interpretation are specified. Further discussion and related work in Section 4 are followed by more extensions of the framework in Section 5. Experiments in Section 6 show extremely competitive results with equivalent fully-connected autoencoders trained with backpropagation."}, {"heading": "2 OPTIMALLY DECODING AN ENCODED REPRESENTATION", "text": "To address the game of Section 1.2, we first assume E and B are fixed, and derive the optimal decoding rule given this information. We show in this section that the form of this optimal decoder is precisely the same as in a classical autoencoder: having learned a weight vector wv \u2208 RH for each v \u2208 [V ], the vth bit of each reconstruction x\u0303i is expressed as a logistic function of a wv-weighted combination of the H encoded bits ei \u2013 a logistic artificial neuron with weights wv. The weight vectors are learned by convex optimization, despite the nonconvexity of the transfer functions.\nTo develop this, we minimize the worst-case reconstruction error, where X is constrained by our prior knowledge that B = 1nXE >, i.e. 1nExv = bv \u2200v \u2208 [V ]. This can be written as a function of E:\nL\u2217B(E) := min x\u0303(1),...,x\u0303(n)\u2208[\u22121,1]V\nmax x(1),...,x(n)\u2208[\u22121,1]V , \u2200v\u2208[V ]: 1nExv=bv\n1\nn n\u2211 i=1 `(x(i), x\u0303(i)) (3)\nWe solve this minimax problem for the optimal reconstructions played by the minimizing player in (3), written as x\u0303(1)\u2217, . . . , x\u0303(n)\u2217.\nTheorem 1. Define the bitwise slack function \u03b3E(w,b) := \u2212b>w + 1n \u2211n i=1 \u03a8(w\n>e(i)), which is convex in w. W.r.t. any bv, this has minimizing weights w\u2217v := w \u2217 v(E,B) := arg min\nw\u2208RH \u03b3E(w,bv).\nThen the minimax value of the game (3) is L\u2217B(E) = 1\n2 V\u2211 v=1 \u03b3E(w\u2217v,bv). For any example i \u2208 [n],\nthe minimax optimal reconstruction can be written for any bit v as x\u0303(i)\u2217v := 1\u2212e \u2212w\u2217>v e (i)\n1+e\u2212w \u2217> v e\n(i) .\nThis tells us that the optimization problem of finding the minimax optimal reconstructions x\u0303(i) is extremely convenient in several respects. The learning problem decomposes over the V bits in the decoding, reducing to solving for a weight vector w\u2217v \u2208 RH for each bit v, by optimizing each bitwise slack function. Given the weights, the optimal reconstruction of any example i can be specified by a layer of logistic sigmoid artificial neurons of its encoded bits, with w\u2217>v e (i) as the bitwise logits.\nHereafter, we write W \u2208 RV\u00d7H as the matrix of decoding weights, with rows {wv}Vv=1. In particular, the optimal decoding weights W\u2217(E,B) are the matrix with rows {w\u2217v(E,B)} V v=1."}, {"heading": "3 LEARNING AN AUTOENCODER", "text": ""}, {"heading": "3.1 FINDING AN ENCODED REPRESENTATION", "text": "Having computed the optimal decoding function in the previous section given any E and B, we now switch perspectives to the encoder, which seeks to compress the input data X\u0302 into encoded representations E (from which B is easily calculated to pass to the decoder). We seek to find (E,B) to ensure the lowest worst-case reconstruction loss after decoding; recall that this is L\u2217B(E) from (3).\nObserve that 1nX\u0302E > = B by definition, and that the encoder is given X\u0302. Therefore, by using Thm. 1 and substituting bv = 1nEx\u0302v \u2200v \u2208 [V ],\nL\u2217B(E) = 1\n2n n\u2211 i=1 V\u2211 v=1 [ \u2212x\u0302(i)v (w\u2217>v e(i)) + \u03a8(w\u2217>v e(i)) ] := L(W\u2217,E) (4)\nSo it is convenient to define the feature distortion 1 for any v \u2208 [V ] with respect to W, between any example x and its encoding e:\n\u03b2Wv (e,x) := \u2212xvw>v e + \u03a8(w>v e) (5)\nFrom the above discussion, the best E given any decoding W, written as E\u2217(W), solves the minimization\nmin E\u2208[\u22121,1]H\u00d7n L(W,E) = 1 2n n\u2211 i=1 min e(i)\u2208[\u22121,1]H V\u2211 v=1 \u03b2Wv (e (i), x\u0302(i))\nwhich immediately yields the following result.\nProposition 2. Define the optimal encodings for decoding weights W as E\u2217(W) := arg min\nE\u2208[\u22121,1]H\u00d7n L(W,E). Then e(i)\u2217(W) can be computed separately for each example x\u0302(i) \u2208 [\u22121, 1]V , minimizing its total feature distortion over the decoded bits w.r.t. W:\nENC(x\u0302(i);W) := e(i)\u2217(W) := arg min e\u2208[\u22121,1]H V\u2211 v=1 \u03b2Wv (e, x\u0302 (i)) (6)\nObserve that the encoding function ENC(x\u0302(i);W) can be efficiently computed to any desired precision since the feature distortion \u03b2Wv (e, x\u0302\n(i)) of each bit v is convex and Lipschitz in e; an L1 error of can be reached in O( \u22122) linear-time first-order optimization iterations. Note that the encodings need not be bits, and can be e.g. unconstrained \u2208 RH instead; the proof of Thm. 1 assumes no structure on them, and the optimization will proceed as above but without projecting into the hypercube."}, {"heading": "3.2 AN AUTOENCODER LEARNING ALGORITHM", "text": "Our ultimate goal is to minimize the worst-case reconstruction loss. As we have seen in (3) and (6), it is convex in the encoding E and in the decoding parameters W, each of which can be fixed while minimizing with respect to the other. This suggests a learning algorithm that alternately performs two steps: finding encodings E that minimize L(W,E) as in (6) with a fixed W, and finding decoding parameters W\u2217(E,B), as given in Algorithm 1.\nAlgorithm 1 Pairwise Correlation Autoencoder (PC-AE)\nInput: Size-n dataset X\u0302, number of epochs T Initialize W0 (e.g. with each element being i.i.d. \u223c N (0, 1)) for t = 1 to T do\nEncode each example to ensure accurate reconstruction using weights Wt\u22121, and compute the associated pairwise bit correlations Bt:\n\u2200i \u2208 [n] : [e(i)]t = ENC(x\u0302(i);Wt\u22121) , Bt = 1\nn X\u0302E>t\nUpdate weight vectors [wv]t for each v \u2208 [V ] to minimize slack function, using encodings Et:\n\u2200v \u2208 [V ] : [wv]t = arg min w\u2208RH\n[ \u2212[bv]>t w + 1\nn n\u2211 i=1 \u03a8(w>e (i) t ) ] end for Output: Weights WT\n1Noting that \u03a8(w>v e) \u2248 \u2223\u2223w>v e\u2223\u2223, we see that \u03b2Wv (e, x\u0302) \u2248 w>v e (sgn(w>v e) \u2212 x\u0302v). So the optimizer tends\nto change e so that w>v e matches signs with x\u0302v , motivating the name."}, {"heading": "3.3 EFFICIENT IMPLEMENTATION", "text": "Our derivation of the encoding and decoding functions involves no model assumptions at all, only using the minimax structure and pairwise statistics that the algorithm is allowed to remember. Nevertheless, the (en/de)coders can be learned and implemented efficiently.\nDecoding is a convex optimization in H dimensions, which can be done in parallel for each bit v \u2208 [V ]. This is relatively easy to solve in the parameter regime of primary interest when data are abundant, in which H < V n. Similarly, encoding is also a convex optimization problem in only H dimensions. If the data examples are instead sampled in minibatches of size n, they can be encoded in parallel, with a new minibatch being sampled to start each epoch t. The number of examples n (per batch) is essentially only limited by nH , the number of compressed representations that fit in memory.\nSo far in this paper, we have stated our results in the transductive setting, in which all data are given together a priori, with no assumptions whatsoever made about the interdependences between the V features. However, PC-AE operates much more efficiently than this might suggest. Crucially, the encoding and decoding tasks both depend on n only to average a function of x(i) or e(i) over i \u2208 [n], so they can both be solved by stochastic optimization methods that use first-order gradient information, like variants of stochastic gradient descent (SGD). We find it remarkable that the minimax optimal encoding and decoding can be efficiently learned by such methods, which do not scale computationally in n. Note that the result of each of these steps involves \u2126(n) outputs (E and X\u0303), which are all coupled together in complex ways.\nFurthermore, efficient first-order convex optimization methods for both encoding and decoding steps manipulate more intermediate gradient-related quantities, with facile interpretations. For details, see Appendix A.2."}, {"heading": "3.4 CONVERGENCE AND WEIGHT REGULARIZATION", "text": "As we noted previously, the objective function of the optimization is biconvex. This means that the alternating minimization algorithm we specify is an instance of alternating convex search, shown in that literature to converge under broad conditions (Gorski et al. (2007)). It is not guaranteed to converge to the global optimum, but each iteration will monotonically decrease the objective function. In light of our introductory discussion, the properties and rate of such convergence would be interesting to compare to stochastic optimization algorithms for PCA, which converge efficiently under broad conditions (Balsubramani et al. (2013); Shamir (2016)).\nThe basic game used so far has assumed perfect knowledge of the pairwise correlations, leading to equality constraints \u2200v \u2208 [V ] : 1nExv = bv. This makes sense in PC-AE, where the encoding phase of each epoch gives the exact Bt for the decoding phase. However, in other stochastic settings as for denoising autoencoders (see Sec. 5.2), it may be necessary to relax this constraint. A relaxed constraint of \u2225\u2225 1 nExv \u2212 bv \u2225\u2225 \u221e \u2264 exactly corresponds to an extra additive regularization term of\n\u2016wv\u20161 on the corresponding weights in the convex optimization used to find W (Appendix D.1). Such regularization leads to provably better generalization (Bartlett (1998)) and is often practical to use, e.g. to encourage sparsity. But we do not use it for our PC-AE experiments in this paper."}, {"heading": "4 DISCUSSION AND RELATED WORK", "text": "Our approach PC-AE is quite different from existing autoencoding work in several ways.\nFirst and foremost, we posit no explicit decision rule, and avoid optimizing the highly non-convex decision surface traversed by traditional autoencoding algorithms that learn with backpropagation (Rumelhart et al. (1986)). The decoding function, given the encodings, is a single layer of artificial neurons only because of the minimax structure of the problem when minimizing worst-case loss. This differs from reasoning typically used in neural net work (see Jordan (1995)), in which the loss is the negative log-likelihood (NLL) of the joint probability, which is assumed to follow a form specified by logistic artificial neurons and their weights. We instead interpret the loss in the usual direct way as the NLL of the predicted probability of the data given the visible bits, and avoid any assumptions on the decision rule (e.g. not monotonicity in the score w>v e (i), or even dependence on such a score).\nThis justification of artificial neurons \u2013 as the minimax optimal decision rules given information on pairwise correlations \u2013 is one of our more distinctive contributions (see Sec. 5.1).\nCrucially, we make no assumptions whatsoever on the form of the encoding or decoding, except on the memory used by the decoding. Some such \u201cregularizing\" restriction is necessary to rule out the autoencoder just memorizing the data, and is typically expressed by assuming a model class of compositions of artificial neuron layers. We instead impose it axiomiatically by limiting the amount of information transmitted through B, which does not scale in n; but we do not restrict how this information is used. This confers a clear theoretical advantage, allowing us to attain the strongest robust loss guarantee among all possible autoencoders that use the correlations B.\nMore importantly in practice, avoiding an explicit model class means that we do not have to optimize the typically non-convex model, which has long been a central issue for backpropagation-based learning methods (e.g. Dauphin et al. (2014)). Prior work related in spirit has attempted to avoid this through convex relaxations, including for multi-layer optimization under various structural assumptions (Aslan et al. (2014); Zhang et al. (2016)), and when the number of hidden units is varied by the algorithm (Bengio et al. (2005); Bach (2014)).\nOur approach also isolates the benefit of higher n in dealing with overfitting, as the pairwise correlations B can be measured progressively more accurately as n increases. In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep (Arora et al. (2014)) and shallow networks (using tensor decompositions, e.g. Livni et al. (2014); Janzamin et al. (2015)). Our use of ensemble aggregation techniques (from Balsubramani & Freund (2015a; 2016)) to study these problems is anticipated in spirit by prior work as well, as discussed at length by Bengio (2009) in the context of distributed representations."}, {"heading": "4.1 OPTIMALITY, OTHER ARCHITECTURES, AND DEPTH", "text": "We have established that a single layer of logistic artificial neurons is an optimal decoder, given only indirect information about the data through pairwise correlations. This is not a claim that autoencoders need only a single-layer architecture in the worst case. Sec. 3.1 establishes that the best representations E are the solution to a convex optimization, with no artificial neurons involved in computing them from the data. Unlike the decoding function, the optimal encoding function ENC cannot be written explicitly in terms of artificial neurons, and is incomparable to existing architectures (though it is analogous to PCA in prescribing an efficient operation that yields the encodings from unlabeled data). Also, the encodings are only optimal given the pairwise correlations; training algorithms like backpropagation, which communicate other knowledge of the data through derivative composition, can learn final decoding layers that outperform ours, as we see in experiments.\nIn our framework so far, we explore using all the pairwise correlations between hidden and visible bits to inform learning by constraining the adversary, resulting in a Lagrange parameter \u2013 a weight \u2013 for each constraint. These V H weights W constitute the parameters of the optimal decoding layer, describing a fully connected architecture. If just a select few of these correlations were used, only they would constrain the adversary in the minimax problem of Sec. 2, so weights would only be introduced for them, giving rise to sparser architectures.\nOur central choices \u2013 to store only pairwise correlations and minimize worst-case reconstruction loss \u2013 play a similar regularizing role to explicit model assumptions, and other autoencoding methods may achieve better performance on data for which these choices are too conservative, by e.g. making distributional assumptions on the data. From our perspective, other architectures with more layers \u2013 particularly highly successful ones like convolutional, recurrent, residual, and ladder networks (LeCun et al. (2015); He et al. (2015); Rasmus et al. (2015)) \u2013 lend the autoencoding algorithm more power by allowing it to measure more nuanced correlations using more parameters, which decreases the worst-case loss. Applying our approach with these would be interesting future work.\nExtending this paper\u2019s convenient minimax characterization to deep representations with empirical success is a very interesting open problem. Prior work on stacking autoencoders/RBMs (Vincent et al.\n(2010)) and our learning algorithm PC-AE suggest that we could train a deep network in alternating forward and backward passes. Using this paper\u2019s ideas, the forward pass would learn the weights to each layer given the previous layer\u2019s activations (and inter-layer pairwise correlations) by minimizing the slack function, with the backward pass learning the activations for each layer given the weights to / activations of the next layer by convex optimization (as we learn E). Both passes would consist of successive convex optimizations dictated by our approach, quite distinct from backpropagation, though loosely resembling the wake-sleep algorithm (Hinton et al. (1995))."}, {"heading": "4.2 GENERATIVE APPLICATIONS", "text": "Particularly recently, autoencoders have been of interest largely for their many applications beyond compression, especially for their generative uses. The most directly relevant to us involve repurposing denoising autoencoders (Bengio et al. (2013b); see Sec. 5.2); moment matching among hidden and visible units (Li et al. (2015)); and generative adversarial network ideas (Goodfellow et al. (2014); Makhzani et al. (2015)), the latter particularly since the techniques of this paper have been applied to binary classification (Balsubramani & Freund (2015a;b)). These are outside this paper\u2019s scope, but suggest themselves as future extensions of our approach."}, {"heading": "5 EXTENSIONS", "text": ""}, {"heading": "5.1 OTHER RECONSTRUCTION LOSSES", "text": "It may make sense to use another reconstruction loss other than cross-entropy, for instance the expected Hamming distance between x(i) and x\u0303(i). It turns out that the minimax manipulations we use work under very broad conditions, for nearly any loss that additively decomposes over the V bits as cross-entropy does. In such cases, all that is required is that the partial losses `+(x\u0303 (i) v ), `\u2212(x\u0303 (i) v ) are monotonically decreasing and increasing respectively (recall that for cross-entropy loss, this is true as `\u00b1(x\u0303 (i) v ) = ln ( 2\n1\u00b1x\u0303(i)v\n) ); they need not even be convex. This monotonicity is a natural condition,\nbecause the loss measures the discrepancy to the true label, and holds for all losses in common use.\nChanging the partial losses only changes the structure of the minimax solution in two respects: by altering the form of the transfer function on the decoding neurons, and the univariate potential well \u03a8 optimized to learn the decoding weights. Otherwise, the problem remains convex and the algorithm is identical. Formal statements of these general results are in Appendix E."}, {"heading": "5.2 DENOISING AUTOENCODING", "text": "Our framework can be easily applied to learn a denoising autoencoder (DAE; Vincent et al. (2008; 2010)), which uses noise-corrupted data (call it X\u0307) for training, and uncorrupted data for evaluation. From our perspective, this corresponds to leaving the learning of W unchanged, but using corrupted data when learning E. Consequently, the minimization problem over encodings must be changed to account for the bias on B introduced by the noise; so the algorithm plays given the noisy data, but to minimize loss against X. This is easiest to see for zero-mean noise, for which our algorithms are completely unchanged because B does not change (in expectation) after the noise is added.\nAnother common scenario illustrating this technique is to mask a \u03c1 fraction of the input bits uniformly at random (in our notation, changing 1s to\u22121s). This masking noise changes each pairwise correlation bv,h by an amount \u03b4v,h := 1n \u2211n i=1(x\u0307 (i) v \u2212x(i)v )e(i)h . Therefore, the optimand Eq. (4) must be modified by subtracting this factor \u03b4v,h. This \u03b4v,h can be estimated (w.h.p.) given x\u0307v, eh, \u03c1,xv . But even with just the noisy data and not xv , we can estimate \u03b4v,h w.h.p. by extrapolating the correlation of the bits of x\u0307v that are left as +1 (a 1\u2212 \u03c1 fraction) with the corresponding values in eh (see Appendix C)."}, {"heading": "6 EXPERIMENTS", "text": "In this section we compare our approach 2 empirically to a standard autoencoder with one hidden layer (termed AE here) trained with backpropagation, and a thresholded PCA baseline. Our goal is simply to verify that our approach, though very different, is competitive in reconstruction performance.\nThe datasets we use are first normalized to [0, 1], and then binarized by sampling each pixel stochastically in proportion to its intensity, following prior work (Salakhutdinov & Murray (2008)). Changing between binary and real-valued encodings in PC-AE requires just a line of code, to project the encodings into [\u22121, 1]H after convex optimization updates to compute ENC(\u00b7). We use Adagrad (Duchi et al. (2011)) for the convex minimizations of our algorithms; we observed that their performance is not very sensitive to the choice of optimization method, explained by our approach\u2019s convexity.\nWe compare to a basic AE with a single hidden layer, trained using the Adam method with default parameters (Kingma & Ba (2014)). Other models like variational autoencoders (Kingma & Welling (2013)) are not shown here because they do not aim to optimize reconstruction loss or are not comparably general autoencoding architectures. We also use a sign-thresholded PCA baseline (essentially a completely linear autoencoder, but with the output layer thresholded to be in [\u22121, 1]); see Appendix A for more details. We vary the number of hidden units H for all algorithms, and try both binary and unconstrained real-valued encodings where appropriate; the respective AE uses logistic sigmoid and ReLU transfer functions for the encoding neurons. The results are in Table 1.\nThe reconstruction performance of PC-AE indicates that it can encode information very well using pairwise correlations, compared to the directly learned AE and PCA approaches. Loss can become extremely low when H is raised, giving B the capacity to robustly encode almost all the information in the input bits X\u0302. The performance is roughly equal between binary hidden units and unconstrained ones, which is expected by our derivations.\nWe also try learning just the decoding layer of Sec. 2, on the encoded representation of the AE. This is motivated by the fact that Sec. 2 establishes our decoding method to be worst-case optimal given any E and B. We find the results to be significantly worse than the AE alone in all datasets used (e.g. reconstruction loss of \u223c 171/133 on MNIST, and \u223c 211/134 on Omniglot, with 32/100 hidden units respectively). This reflects the AE\u2019s training backpropagating information about the data beyond pairwise correlations, through non-convex function compositions \u2013 however, this comes at the cost of being more difficult to optimize. The representations learned by the ENC function of PC-AE are quite different and capture much more of the pairwise correlation information, which is used by the decoding layer in a worst-case optimal fashion. We attempt to visually depict the differences between the representations in Fig. 3.\nAs discussed in Sec. 4, we do not claim that this paper\u2019s method will always achieve the best empirical reconstruction loss, even among single-layer autoencoders. We would like to make the encoding\n2TensorFlow code available at https://github.com/aikanor/pc-autoencoder .\nfunction quicker to compute, as well. But we believe this paper\u2019s results, especially when H is high, illustrate the potential of using pairwise correlations for autoencoding as in our approach, learning to encode with alternating convex minimization and extremely strong worst-case robustness guarantees."}, {"heading": "ACKNOWLEDGMENTS", "text": "I am grateful to Jack Berkowitz, Sanjoy Dasgupta, and Yoav Freund for helpful discussions; Daniel Hsu and Akshay Krishnamurthy for instructive examples; and Gary Cottrell for an enjoyable chat. I acknowledge funding from the NIH (grant R01ES02500902)."}, {"heading": "A EXPERIMENTAL DETAILS", "text": "In addition to MNIST, we use the preprocessed version of the Omniglot dataset found in Burda et al. (2016), split 1 of the Caltech-101 Silhouettes dataset, the small notMNIST dataset, and the UCI Adult (a1a) dataset. The results reported are the mean of 10 Monte Carlo runs, and the PC-AE significance results use 95% Monte Carlo confidence intervals. Only notMNIST comes without a predefined split, so the displayed results use 10-fold cross-validation. Non-binarized versions of all datasets (grayscale pixels) resulted in nearly identical PC-AE performance (not shown); this is as expected from its derivation using expected pairwise correlations, which with high probability are nearly invariant under binarization (by e.g. Hoeffding bounds).\nWe used minibatches of size 250. All standard autoencoders use the \u2019Xavier\u2019 initialization and trained for 500 epochs or using early stopping on the test set. The \u201cPCA\" baseline was run on exactly the same input data as the others; it finds decodings by mean-centering this input, finding the top H principal components with standard PCA, reconstructing the mean-centered input with these components, adding back the means, and finally thresholding the result to [\u22121, 1]V . We did not evaluate against other types of autoencoders which regularize (Kingma & Welling (2013)) or are otherwise not trained for direct reconstruction loss minimization. Also, not shown is the performance of a standard convolutional autoencoder (32-bit representation, depth-3 64-64-32 (en/de)coder) which performs better than the standard autoencoder, but is still outperformed by PC-AE on our image-based datasets. A deeper architecture could quite possibly achieve superior performance, but the greater number of channels through which information is propagated makes fair comparison with our flat fully-connected approach difficult. We consider extension of our PC-AE approach to such architectures to be fascinating future work.\nA.1 FURTHER RESULTS\nOur bound on worst-case loss is invariably quite tight, as shown in Fig. 4. Similar results are found on all datasets. This is consistent with our conclusions about the nature of the PC-AE representations \u2013 conveying almost exactly the information available in pairwise correlations.\nA 2D visualization of MNIST is in Fig. 6, showing that even with just two hidden units there is enough information in pairwise correlations for PC-AE to learn a sensible embedding. We also include more pictures of our autoencoders\u2019 reconstructions, and visualizations of the hidden units when H = 100 in Fig. 5.\nA.2 PC-AE INTERPRETATION AND IMPLEMENTATION DETAILS\nHere we give some details that are useful for interpretation and implementation of the proposed method.\nA.2.1 ENCODING\nProposition 2 defines the encoding function for any data example x as the vector that minimizes the total feature distortion, summed over the bits in the decoding, rewritten here for convenience:\nENC(x(i);W) := arg min e\u2208[\u22121,1]H V\u2211 v=1 [ \u2212x(i)v w>v e(i) + \u03a8(w>v e(i)) ] (7)\nDoing this on multiple examples at once (in memory as a minibatch) can be much faster than on each example separately. We can now compute the gradient of the objective function w.r.t. each example i \u2208 [n], writing the gradient w.r.t. example i as column i of a matrix G \u2208 RH\u00d7n. G can be calculated efficiently in a number of ways, for example as follows:\n\u2022 Compute matrix of hallucinated data X\u0306 := \u03a8\u2032(WE) \u2208 RV\u00d7n. \u2022 Subtract X to compute residuals R := X\u0306\u2212X \u2208 RV\u00d7n. \u2022 Compute G = 1nW >R \u2208 RH\u00d7n.\nOptimization then proceeds with gradient descent using G, with the step size found using line search. Note that since the objective function is convex, the optimum E\u2217 leads to optimal residuals R\u2217 \u2208 RV\u00d7n such that G = 1nW\n>R\u2217 = 0H\u00d7n, so each column of R\u2217 is in the null space of W>, which maps the residual vectors to the encoded space. We conclude that although the compression is not perfect (so the optimal residuals R\u2217 6= 0V\u00d7n in general), each column of R\u2217 is orthogonal to the decoding weights at an equilibrium towards which the convex minimization problem of (7) is guaranteed to stably converge.\nA.2.2 DECODING\nThe decoding step finds W to ensure accurate decoding of the given encodings E with correlations B, solving the convex minimization problem:\nW\u2217 = arg min W\u2208RV\u00d7H V\u2211 v=1 [ \u2212b>v wv + 1 n n\u2211 i=1 \u03a8(w>v e (i)) ] (8)\nThis can be minimized by first-order convex optimization. The gradient of (8) at W is:\n\u2212B + 1 n [\u03a8\u2032(WE)]E> (9)\nThe second term can be understood as \u201challucinated\" pairwise correlations B\u0306, between bits of the encoded examples E and bits of their decodings under the current weights, X\u0306 := \u03a8\u2032(WE). The hallucinated correlations can be written as B\u0306 := 1nX\u0306E\n>. Therefore, (9) can be interpreted as the residual correlations B\u0306 \u2212B. Since the slack function of (8) is convex, the optimum W\u2217 leads to hallucinated correlations B\u0306\u2217 = B, which is the limit reached by the optimization algorithm after many iterations."}, {"heading": "B ALLOWING RANDOMIZED DATA AND ENCODINGS", "text": "In this paper, we represent the bit-vector data in a randomized way in [\u22121, 1]V . Randomizing the data only relaxes the constraints on the adversary in the game we play; so at worst we are working with an upper bound on worst-case loss, instead of the exact minimax loss itself, erring on the conservative side. Here we briefly justify the bound as being essentially tight, which we also see empirically in this paper\u2019s experiments.\nIn the formulation of Section 2, the only information we have about the data is its pairwise correlations with the encoding units. When the data are abundant (n large), then w.h.p. these correlations are close to their expected values over the data\u2019s internal randomization, so representing them as continuous values w.h.p. results in the same B and therefore the same solutions for E,W. We are effectively allowing the adversary to play each bit\u2019s conditional probability of firing, rather than the binary realization of that probability.\nThis allows us to apply minimax theory and duality to considerably simplify the problem to a convex optimization, when it would otherwise be nonconvex, and computationally hard (Baldi (2012)). The fact that we are only using information about the data through its expected pairwise correlations with the hidden units makes this possible.\nThe above also applies to the encodings and their internal randomization, allowing us to learn binary randomized encodings by projecting to the convex set [\u22121, 1]H ."}, {"heading": "C DENOISING AUTOENCODER WITH MASKING NOISE: DETAILS", "text": "This section elaborates on the discussion of Sec. 5.2.\nRecall the correlation correction term \u03b4v,h from Sec. 5.2:\n\u03b4v,h = 1\nn n\u2211 i=1 (x\u0307(i)v \u2212 x(i)v )e (i) h\nHere, we express this in terms of the known quantities x\u0307v, eh, \u03c1, and not the unknown denoised data xv .\nConsider that\n(x\u0307(i)v \u2212 x(i)v )e (i) h = 1 ( x(i)v = \u22121 ) (x\u0307(i)v \u2212 x(i)v )e (i) h + 1 ( x(i)v = +1 ) (x\u0307(i)v \u2212 x(i)v )e (i) h\nNow if x(i)v = \u22121, then x\u0307(i)v = \u22121, so (x\u0307(i)v \u2212 x(i)v )e(i)h = 0. Therefore the first term above is zero, and the expression can be simplified:\n(x\u0307(i)v \u2212 x(i)v )e (i) h = 1 ( x(i)v = +1 ) (x\u0307(i)v \u2212 x(i)v )e (i) h = 1 ( x(i)v = +1 \u2227 x\u0307(i)v = \u22121 ) (\u22122)e(i)h (10)\nNow on any example i, independent of the value of e(i)h , a \u03c1 fraction of the bits where x (i) v = +1 are flipped to get x\u0307(i)v . Therefore,\n1\n\u03c1 n\u2211 i=1 1 ( x(i)v = +1 \u2227 x\u0307(i)v = \u22121 ) e (i) h \u2248 1 1\u2212 \u03c1 n\u2211 i=1 1 ( x(i)v = +1 \u2227 x\u0307(i)v = +1 ) e (i) h\nPutting it all together,\n\u03b4v,h = 1\nn n\u2211 i=1 (x\u0307(i)v \u2212 x(i)v )e (i) h = \u2212 2 n n\u2211 i=1 1 ( x(i)v = +1 \u2227 x\u0307(i)v = \u22121 ) e (i) h\n\u2248 \u2212 2 n \u03c1 1\u2212 \u03c1 n\u2211 i=1 1 ( x(i)v = +1 \u2227 x\u0307(i)v = +1 ) e (i) h = \u2212 2 n \u03c1 1\u2212 \u03c1 n\u2211 i=1 1 ( x\u0307(i)v = +1 ) e (i) h"}, {"heading": "D PROOFS", "text": "Proof of Theorem 1. Writing \u0393(x\u0303(i)v ) := `\u2212(x\u0303 (i) v ) \u2212 `+(x\u0303(i)v ) = ln ( 1+x\u0303(i)v 1\u2212x\u0303(i)v ) for convenience, we can simplify L\u2217, using the definition of the loss (2), and Lagrange duality for all V H constraints involving B.\nThis leads to the following chain of equalities, where for brevity the constraint sets are sometimes omitted when clear, and we write X as shorthand for the data x(1), . . . ,x(n) and X\u0303 analogously for the reconstructions.\nL\u2217 = 1 2 min x\u0303(1),...,x\u0303(n)\n\u2208[\u22121,1]V max x(1),...,x(n)\u2208[\u22121,1]V , \u2200v\u2208[V ]: 1nExv=bv\n1\nn n\u2211 i=1 V\u2211 v=1 [( 1 + x(i)v ) `+(x\u0303 (i) v ) + ( 1\u2212 x(i)v ) `\u2212(x\u0303 (i) v ) ]\n= 1\n2 min X\u0303 max X min W\u2208RV\u00d7H\n[ 1\nn n\u2211 i=1 V\u2211 v=1 ( `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v )\u2212 x(i)v \u0393(x\u0303(i)v ) ) + V\u2211 v=1 w>v ( 1 n Exv \u2212 bv )] (a) = 1\n2 min w1,...,wV\n[ \u2212\nV\u2211 v=1 b>v wv + 1 n min X\u0303 max X V\u2211 v=1 [ n\u2211 i=1 ( `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v )\u2212 x(i)v \u0393(x\u0303(i)v ) ) + w>v Exv ]]\n= 1\n2 min w1,...,wV\n[ \u2212\nV\u2211 v=1 b>v wv + 1 n min X\u0303 n\u2211 i=1 V\u2211 v=1 [ `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v ) + max x(i)\u2208[\u22121,1]V x(i)v ( w>v e (i) \u2212 \u0393(x\u0303(i)v ) )]]\n(11)\nwhere (a) uses the minimax theorem (Cesa-Bianchi & Lugosi (2006)), which can be applied as in linear programming, because the objective function is linear in x(i) and wv. Note that the weights are introduced merely as Lagrange parameters for the pairwise correlation constraints, not as model assumptions.\nThe strategy x(i) which solves the inner maximization of (11) is to simply match signs with w>v e (i)\u2212 \u0393(x\u0303 (i) v ) coordinate-wise for each v \u2208 [V ]. Substituting this into the above,\nL\u2217 = 1 2 min w1,...,wV\n[ \u2212\nV\u2211 v=1 b>v wv + 1 n n\u2211 i=1 min x\u0303(i)\u2208[\u22121,1]V V\u2211 v=1 ( `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v ) + \u2223\u2223\u2223w>v e(i) \u2212 \u0393(x\u0303(i)v )\u2223\u2223\u2223) ]\n= 1\n2 V\u2211 v=1 min wv\u2208RH [ \u2212b>v wv + 1 n n\u2211 i=1 min x\u0303 (i) v \u2208[\u22121,1] ( `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v ) + \u2223\u2223\u2223w>v e(i) \u2212 \u0393(x\u0303(i)v )\u2223\u2223\u2223) ]\nThe absolute value breaks down into two cases, so the inner minimization\u2019s objective can be simplified:\n`+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v ) + \u2223\u2223\u2223w>v e(i) \u2212 \u0393(x\u0303(i)v )\u2223\u2223\u2223 = { 2`+(x\u0303 (i) v ) + w>v e (i) if w>v e (i) \u2265 \u0393(x\u0303(i)v )\n2`\u2212(x\u0303 (i) v )\u2212w>v e(i) if w>v e(i) < \u0393(x\u0303 (i) v )\n(12)\nSuppose x\u0303(i)v falls in the first case of (12), so that w>v e (i) \u2265 \u0393(x\u0303(i)v ). By definition of `+(\u00b7), 2`+(x\u0303 (i) v ) +w>v e (i) is decreasing in x\u0303(i)v , so it is minimized for the greatest x\u0303 (i)\u2217 v \u2264 1 s.t. \u0393(x\u0303(i)\u2217v ) \u2264 w>v e (i). This means \u0393(x\u0303(i)\u2217v ) = w>v e (i), so the minimand (12) is `+(x\u0303 (i)\u2217 v ) + `\u2212(x\u0303 (i)\u2217 v ), where x\u0303i\u2217v = 1\u2212e\u2212w > v e (i)\n1+e\u2212w > v e\n(i) .\nA precisely analogous argument holds if x\u0303(i)v falls in the second case of (12), where w>v e (i) < \u0393(x\u0303 (i) v ).\nPutting the cases together, we have shown the form of the summand \u03a8. We have also shown the dependence of x\u0303(i)\u2217v on w\u2217>v e\n(i), where w\u2217v is the minimizer of the outer minimization of (11). This completes the proof.\nD.1 L\u221e CORRELATION CONSTRAINTS AND L1 WEIGHT REGULARIZATION\nHere we formalize the discussion of Sec. 3.4 with the following result. Theorem 3.\nmin x\u0303(1),...,x\u0303(n)\u2208[\u22121,1]V\nmax x(1),...,x(n)\u2208[\u22121,1]V ,\n\u2200v\u2208[V ]: \u2016 1nExv\u2212bv\u2016\u221e\u2264 v\n1\nn n\u2211 i=1 `(x(i), x\u0303(i))\n= 1\n2 V\u2211 v=1 min wv\u2208RH [ \u2212b>v wv + 1 n n\u2211 i=1 \u03a8(w>v e (i)) + v \u2016wv\u20161 ]\nFor each v, i, the minimizing x\u0303(i)v is a logistic function of the encoding e(i) with weights equal to the minimizing w\u2217v above, exactly as in Theorem 1.\nProof. The proof adapts the proof of Theorem 1, following the result on L1 regularization in Balsubramani & Freund (2016) in a very straightforward way; we describe this here.\nWe break each L\u221e constraint into two one-sided constraints for each v, i.e. 1nExv \u2212 bv \u2264 v1 n and 1 nExv \u2212 bv \u2265 \u2212 v1\nn. These respectively give rise to two sets of Lagrange parameters \u03bbv, \u03bev \u2265 0H for each v, replacing the unconstrained Lagrange parameters wv \u2208 RH . The conditions for the minimax theorem apply here just as in the proof of Theorem 1, so that (11) is replaced by\n1 2 min\n\u03bb1,...,\u03bbV \u03be1,...,\u03beV\n[ \u2212\nV\u2211 v=1 ( b>v (\u03bev \u2212 \u03bbv)\u2212 v1>(\u03bev + \u03bbv) ) (13)\n+ 1\nn min X\u0303 n\u2211 i=1 V\u2211 v=1 [ `+(x\u0303 (i) v ) + `\u2212(x\u0303 (i) v ) + max x(i) x(i)v ( (\u03bev \u2212 \u03bbv)>e(i) \u2212 \u0393(x\u0303(i)v ) )]] (14)\nSuppose for some h \u2208 [H] that \u03bev,h > 0 and \u03bbv,h > 0. Then subtracting min(\u03bev,h, \u03bbv,h) from both does not affect the value [\u03bev \u2212 \u03bbv]h, but always decreases [\u03bev + \u03bbv]h, and therefore always decreases the objective function. Therefore, we can w.l.o.g. assume that \u2200h \u2208 [H] : min(\u03bev,h, \u03bbv,h) = 0. Defining wv = \u03bev \u2212 \u03bbv (so that \u03bev,h = [wv,h]+ and \u03bbv,h = [wv,h]\u2212 for all h), we see that the term v1 >(\u03bev + \u03bbv) in (13) can be replaced by v \u2016wv\u20161.\nProceeding as in the proof of Theorem 1 gives the result."}, {"heading": "E GENERAL RECONSTRUCTION LOSSES", "text": "In this section we extend Theorem 1 to a larger class of reconstruction losses for binary autoencoding, of which cross-entropy loss is a special case. This uses techniques recently employed by Balsubramani & Freund (2016) for binary classification.\nSince the data X are still randomized binary, we first broaden the definition of (2), rewritten here:\n`(x(i), x\u0303(i)) := V\u2211 v=1\n[( 1 + x (i) v\n2\n) `+(x\u0303 (i) v ) + ( 1\u2212 x(i)v\n2\n) `\u2212(x\u0303 (i) v ) ] (15)\nWe do this by redefining the partial losses `\u00b1(x\u0303 (i) v ), to any functions satisfying the following monotonicity conditions. Assumption 1. Over the interval (\u22121, 1), `+(\u00b7) is decreasing and `\u2212(\u00b7) is increasing, and both are twice differentiable.\nAssumption 1 is a very natural one and includes many non-convex losses (see Balsubramani & Freund (2016) for a more detailed discussion, much of which applies bitwise here). This and the additive decomposability of (15) over the V bits are the only assumptions we make on the reconstruction loss `(x(i), x\u0303(i)). The latter decomposability assumption is often natural when the loss is a log-likelihood, where it is tantamount to conditional independence of the visible bits given the hidden ones.\nGiven such a reconstruction loss, define the increasing function \u0393(y) := `\u2212(y)\u2212`+(y) : [\u22121, 1] 7\u2192 R, for which there exists an increasing (pseudo)inverse \u0393\u22121. Using this we broaden the definition of the potential function \u03a8 in terms of `\u00b1:\n\u03a8(m) :=  \u2212m+ 2`\u2212(\u22121) if m \u2264 \u0393(\u22121) `+(\u0393 \u22121(m)) + `\u2212(\u0393 \u22121(m)) if m \u2208 (\u0393(\u22121),\u0393(1))\nm+ 2`+(1) if m \u2265 \u0393(1)\nThen we may state the following result, describing the optimal decoding function for a general reconstruction loss. Theorem 4. Define the potential function\nmin x\u0303(1),...,x\u0303(n)\u2208[\u22121,1]V max x(1),...,x(n)\u2208[\u22121,1]V , \u2200v\u2208[V ]: 1nExv=bv\n1\nn n\u2211 i=1 `(x(i), x\u0303(i))\n= 1\n2 V\u2211 v=1 min wv\u2208RH [ \u2212b>v wv + 1 n n\u2211 i=1 \u03a8(w>v e (i)) ]\nFor each v \u2208 [V ], i \u2208 [n], the minimizing x\u0303(i)v is a sigmoid function of the encoding e(i) with weights equal to the minimizing w\u2217v above, as in Theorem 1. The sigmoid is defined as\nx\u0303(i)\u2217v :=  \u22121 if w\u2217v>e(i) \u2264 \u0393(\u22121) \u0393\u22121(w\u2217v >e(i)) if w\u2217v >e(i) \u2208 (\u0393(\u22121),\u0393(1))\n1 if w\u2217v >e(i) \u2265 \u0393(1)\n(16)\nThe proof is nearly identical to that of the main theorem of Balsubramani & Freund (2016). That proof is essentially recapitulated here for each bit v \u2208 [V ] due to the additive decomposability of the loss, through algebraic manipulations (and one application of the minimax theorem) identical to the proof of Theorem 1, but using the more general specifications of \u03a8 and \u0393 in this section. So we do not rewrite it in full here.\nA notable special case of interest is the Hamming loss, for which `\u00b1(x\u0303 (i) v ) = 1 2 ( 1\u2213 x\u0303(i)v ) , where the reconstructions are allowed to be randomized binary values. In this case, we have \u03a8(m) = max(|m| , 1), and the sigmoid used for each decoding neuron is the clipped linearity max(\u22121,min(w\u2217v>e(i), 1))."}, {"heading": "F ALTERNATE APPROACHES", "text": "We made some technical choices in the derivation of PC-AE, which prompt possible alternatives not explored here for a variety of reasons. Recounting these choices gives more insight into our framework.\nThe output reconstructions could have restricted pairwise correlations, i.e. 1nX\u0303E > = B. One option is to impose such restrictions instead of the existing constraints on X, leaving X unrestricted. However, this is not in the spirit of this paper, because B is our means of indirectly conveying information to the decoder about how X is decoded.\nAnother option is to restrict both X\u0303 and X. This is possible and may be useful in propagating correlation information between layers of deeper architectures while learning, but its minimax solution does not have the conveniently clean structure of the PC-AE derivation.\nIn a similar vein, we could restrict E during the encoding phase, using B and X. As B is changed only during this phase to better conform to the true data X, this tactic fixes B during the optimization, which is not in the spirit of this paper\u2019s approach. It also performed significantly worse in our experiments."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["David H Ackley", "Geoffrey E Hinton", "Terrence J Sejnowski"], "venue": "Cognitive science,", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Convex deep learning via normalized kernels", "author": ["\u00d6zlem Aslan", "Xinhua Zhang", "Dale Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Aslan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Aslan et al\\.", "year": 2014}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1412.8690,", "citeRegEx": "Bach.,? \\Q2014\\E", "shortCiteRegEx": "Bach.", "year": 2014}, {"title": "Autoencoders, unsupervised learning, and deep architectures. Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, pp", "author": ["Pierre Baldi"], "venue": null, "citeRegEx": "Baldi.,? \\Q2012\\E", "shortCiteRegEx": "Baldi.", "year": 2012}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Pierre Baldi", "Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik.,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik.", "year": 1989}, {"title": "Optimally combining classifiers using unlabeled data", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Balsubramani and Freund.,? \\Q2015\\E", "shortCiteRegEx": "Balsubramani and Freund.", "year": 2015}, {"title": "Scalable semi-supervised classifier aggregation", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani and Freund.,? \\Q2015\\E", "shortCiteRegEx": "Balsubramani and Freund.", "year": 2015}, {"title": "Optimal binary classifier aggregation for general losses", "author": ["Akshay Balsubramani", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani and Freund.,? \\Q2016\\E", "shortCiteRegEx": "Balsubramani and Freund.", "year": 2016}, {"title": "The fast convergence of incremental pca", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balsubramani et al\\.", "year": 2013}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["Peter L Bartlett"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett.", "year": 1998}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Convex neural networks. In Advances in neural information processing systems (NIPS)", "author": ["Yoshua Bengio", "Nicolas L Roux", "Pascal Vincent", "Olivier Delalleau", "Patrice Marcotte"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2005}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["Herv\u00e9 Bourlard", "Yves Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "Bourlard and Kamp.,? \\Q1988\\E", "shortCiteRegEx": "Bourlard and Kamp.", "year": 1988}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e0bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Yoav Freund", "David Haussler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Freund and Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1992}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Biconvex sets and optimization with biconvex functions: a survey and extensions", "author": ["Jochen Gorski", "Frank Pfeuffer", "Kathrin Klamroth"], "venue": "Mathematical Methods of Operations Research,", "citeRegEx": "Gorski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gorski et al\\.", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "The\" wake-sleep\" algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Why the logistic function? a tutorial discussion on probabilities and neural networks", "author": ["Michael I Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q1995\\E", "shortCiteRegEx": "Jordan.", "year": 1995}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Rich Zemel"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Semi-supervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Parallel distributed processing, explorations in the microstructure of cognition. vol. 1: Foundations. Computational Models of Cognition and Perception", "author": ["David E Rumelhart", "James L McClelland"], "venue": null, "citeRegEx": "Rumelhart and McClelland.,? \\Q1987\\E", "shortCiteRegEx": "Rumelhart and McClelland.", "year": 1987}, {"title": "Learning representations by back-propagating", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Convergence of stochastic gradient descent for pca", "author": ["Ohad Shamir"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Shamir.,? \\Q2016\\E", "shortCiteRegEx": "Shamir.", "year": 2016}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["P Smolensky"], "venue": "In Parallel distributed processing: explorations in the microstructure of cognition,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning (ICML),", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Convexified convolutional neural networks", "author": ["Yuchen Zhang", "Percy Liang", "Martin J Wainwright"], "venue": "arXiv preprint arXiv:1609.01000,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "When the loss is squared reconstruction error and the goal is to compress data in R to R , this is often accomplished with principal component analysis (PCA), which projects the input data on the top H eigenvectors of their covariance matrix (Bourlard & Kamp (1988); Baldi & Hornik (1989)).", "startOffset": 267, "endOffset": 289}, {"referenceID": 4, "context": "When the loss is squared reconstruction error and the goal is to compress data in R to R , this is often accomplished with principal component analysis (PCA), which projects the input data on the top H eigenvectors of their covariance matrix (Bourlard & Kamp (1988); Baldi & Hornik (1989)). These eigenvectors in R constitute V H real values of additional memory needed to decode the compressed data in R back to the reconstructions in R , which are linear combinations of the eigenvectors. Crucially, this total additional memory does not depend on the amount of data n, making it applicable when data are abundant. This paper considers a similar problem, except using bit-vector data and the cross-entropy reconstruction loss. Since we are compressing samples of i.i.d. V -bit data into H-bit encodings, a natural approach is to remember the pairwise statistics: the V H average correlations between pairs of bits in the encoding and decoding, constituting as much additional memory as the eigenvectors used in PCA. The decoder uses these along with the H-bit encoded data, to produce V -bit reconstructions. We show how to efficiently learn the autoencoder with the worst-case optimal loss in this scenario, without any further assumptions, parametric or otherwise. It has some striking properties. The decoding function is identical in form to the one used in a standard binary autoencoder with one hidden layer (Bengio et al. (2013a)) and cross-entropy reconstruction loss.", "startOffset": 267, "endOffset": 1439}, {"referenceID": 20, "context": "This means that the alternating minimization algorithm we specify is an instance of alternating convex search, shown in that literature to converge under broad conditions (Gorski et al. (2007)).", "startOffset": 172, "endOffset": 193}, {"referenceID": 9, "context": "In light of our introductory discussion, the properties and rate of such convergence would be interesting to compare to stochastic optimization algorithms for PCA, which converge efficiently under broad conditions (Balsubramani et al. (2013); Shamir (2016)).", "startOffset": 215, "endOffset": 242}, {"referenceID": 9, "context": "In light of our introductory discussion, the properties and rate of such convergence would be interesting to compare to stochastic optimization algorithms for PCA, which converge efficiently under broad conditions (Balsubramani et al. (2013); Shamir (2016)).", "startOffset": 215, "endOffset": 257}, {"referenceID": 9, "context": "In light of our introductory discussion, the properties and rate of such convergence would be interesting to compare to stochastic optimization algorithms for PCA, which converge efficiently under broad conditions (Balsubramani et al. (2013); Shamir (2016)). The basic game used so far has assumed perfect knowledge of the pairwise correlations, leading to equality constraints \u2200v \u2208 [V ] : 1 nExv = bv. This makes sense in PC-AE, where the encoding phase of each epoch gives the exact Bt for the decoding phase. However, in other stochastic settings as for denoising autoencoders (see Sec. 5.2), it may be necessary to relax this constraint. A relaxed constraint of \u2225\u2225 1 nExv \u2212 bv \u2225\u2225 \u221e \u2264 exactly corresponds to an extra additive regularization term of \u2016wv\u20161 on the corresponding weights in the convex optimization used to find W (Appendix D.1). Such regularization leads to provably better generalization (Bartlett (1998)) and is often practical to use, e.", "startOffset": 215, "endOffset": 922}, {"referenceID": 32, "context": "First and foremost, we posit no explicit decision rule, and avoid optimizing the highly non-convex decision surface traversed by traditional autoencoding algorithms that learn with backpropagation (Rumelhart et al. (1986)).", "startOffset": 198, "endOffset": 222}, {"referenceID": 26, "context": "This differs from reasoning typically used in neural net work (see Jordan (1995)), in which the loss is the negative log-likelihood (NLL) of the joint probability, which is assumed to follow a form specified by logistic artificial neurons and their weights.", "startOffset": 67, "endOffset": 81}, {"referenceID": 0, "context": "Prior work related in spirit has attempted to avoid this through convex relaxations, including for multi-layer optimization under various structural assumptions (Aslan et al. (2014); Zhang et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 0, "context": "Prior work related in spirit has attempted to avoid this through convex relaxations, including for multi-layer optimization under various structural assumptions (Aslan et al. (2014); Zhang et al. (2016)), and when the number of hidden units is varied by the algorithm (Bengio et al.", "startOffset": 162, "endOffset": 203}, {"referenceID": 0, "context": "Prior work related in spirit has attempted to avoid this through convex relaxations, including for multi-layer optimization under various structural assumptions (Aslan et al. (2014); Zhang et al. (2016)), and when the number of hidden units is varied by the algorithm (Bengio et al. (2005); Bach (2014)).", "startOffset": 162, "endOffset": 290}, {"referenceID": 0, "context": "Prior work related in spirit has attempted to avoid this through convex relaxations, including for multi-layer optimization under various structural assumptions (Aslan et al. (2014); Zhang et al. (2016)), and when the number of hidden units is varied by the algorithm (Bengio et al. (2005); Bach (2014)).", "startOffset": 162, "endOffset": 303}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)).", "startOffset": 198, "endOffset": 219}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)).", "startOffset": 198, "endOffset": 237}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)).", "startOffset": 198, "endOffset": 268}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)).", "startOffset": 198, "endOffset": 294}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep (Arora et al. (2014)) and shallow networks (using tensor decompositions, e.", "startOffset": 198, "endOffset": 553}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep (Arora et al. (2014)) and shallow networks (using tensor decompositions, e.g. Livni et al. (2014); Janzamin et al.", "startOffset": 198, "endOffset": 630}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep (Arora et al. (2014)) and shallow networks (using tensor decompositions, e.g. Livni et al. (2014); Janzamin et al. (2015)).", "startOffset": 198, "endOffset": 654}, {"referenceID": 0, "context": "In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines (Ackley et al. (1985); Smolensky (1986); Rumelhart & McClelland (1987); Freund & Haussler (1992)). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep (Arora et al. (2014)) and shallow networks (using tensor decompositions, e.g. Livni et al. (2014); Janzamin et al. (2015)). Our use of ensemble aggregation techniques (from Balsubramani & Freund (2015a; 2016)) to study these problems is anticipated in spirit by prior work as well, as discussed at length by Bengio (2009) in the context of distributed representations.", "startOffset": 198, "endOffset": 854}, {"referenceID": 23, "context": "(2015); He et al. (2015); Rasmus et al. (2015)) \u2013 lend the autoencoding algorithm more power by allowing it to measure more nuanced correlations using more parameters, which decreases the worst-case loss.", "startOffset": 8, "endOffset": 47}, {"referenceID": 24, "context": "Both passes would consist of successive convex optimizations dictated by our approach, quite distinct from backpropagation, though loosely resembling the wake-sleep algorithm (Hinton et al. (1995)).", "startOffset": 176, "endOffset": 197}, {"referenceID": 11, "context": "The most directly relevant to us involve repurposing denoising autoencoders (Bengio et al. (2013b); see Sec.", "startOffset": 77, "endOffset": 99}, {"referenceID": 11, "context": "The most directly relevant to us involve repurposing denoising autoencoders (Bengio et al. (2013b); see Sec. 5.2); moment matching among hidden and visible units (Li et al. (2015)); and generative adversarial network ideas (Goodfellow et al.", "startOffset": 77, "endOffset": 180}, {"referenceID": 11, "context": "The most directly relevant to us involve repurposing denoising autoencoders (Bengio et al. (2013b); see Sec. 5.2); moment matching among hidden and visible units (Li et al. (2015)); and generative adversarial network ideas (Goodfellow et al. (2014); Makhzani et al.", "startOffset": 77, "endOffset": 249}, {"referenceID": 11, "context": "The most directly relevant to us involve repurposing denoising autoencoders (Bengio et al. (2013b); see Sec. 5.2); moment matching among hidden and visible units (Li et al. (2015)); and generative adversarial network ideas (Goodfellow et al. (2014); Makhzani et al. (2015)), the latter particularly since the techniques of this paper have been applied to binary classification (Balsubramani & Freund (2015a;b)).", "startOffset": 77, "endOffset": 273}, {"referenceID": 19, "context": "We use Adagrad (Duchi et al. (2011)) for the convex minimizations of our algorithms; we observed that their performance is not very sensitive to the choice of optimization method, explained by our approach\u2019s convexity.", "startOffset": 16, "endOffset": 36}, {"referenceID": 19, "context": "We use Adagrad (Duchi et al. (2011)) for the convex minimizations of our algorithms; we observed that their performance is not very sensitive to the choice of optimization method, explained by our approach\u2019s convexity. We compare to a basic AE with a single hidden layer, trained using the Adam method with default parameters (Kingma & Ba (2014)). Other models like variational autoencoders (Kingma & Welling (2013)) are not shown here because they do not aim to optimize reconstruction loss or are not comparably general autoencoding architectures.", "startOffset": 16, "endOffset": 416}], "year": 2017, "abstractText": "We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion.", "creator": "LaTeX with hyperref package"}, "id": "ICLR_2017_66"}