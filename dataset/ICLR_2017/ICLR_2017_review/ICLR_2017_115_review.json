{"id": "ICLR_2017_115", "reviews": [{"review": "The authors of the paper present a novel distribution for discrete variables called the \"concrete distribution\". The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs.\n\nI think the paper is well written and sound, definitely of interest for the conference program.\n\nAs to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets?\nSimilarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not.\n\nI believe the paper should be accepted to the conference, however please consider that I'm not an expert in this field.\n\nSome minor observations/comments/issues:\n-Section 2.1: there is a repetition \"be be\" in the first paragraph.\n-Section 2.4: I would add a reference for the \"multi-sample variational objective\"\n-Section 3.1, just before Section 3.2: \"the Gumbel is a crucial 1\". Why 1 and not \"one\"?\n-Section 3.3, last paragraph: \"Thus, in addition to relaxing the sampling pass of a SCG the log...\" I would add a comma after \"SCG\". More in general, the second part of the paragraph is very dense and not easy to \"absorb\". I don't think it's an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader.\n-Section 5.1, second paragraph: \"All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k\". It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}.\n-Section 5.2: After \"this distribution.\" and \"We will\" there is an extra space.\n-If a compare the last formula in Section 5.3 with Eq. 8, I don't see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "The authors describe the concrete distribution, a continuous approximation to\ndiscrete distributions parameterized by a vector of continuous positive numbers\nproportional to the probability of each discrete result. The concrete\ndistribution is obtained by using the softmax function to approximate the\nargmax operator. The paper is clearly written, original and significant.\nThe experiments clearly illustrate the advantages of the proposed method.\n\nSome minor questions:\n\n\"for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized\nfor other additive noise distributions\"\n\nWhat do you mean by this? Can you be more specific?\n\nWhat is the temperature values used to obtain Table 1 and the table in Figure 4.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"review": "Thank you for an interesting read.\n\nI think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables. Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051).\n\nI've got one suggestion to make the paper even better, but maybe the authors want to leave it to future work. I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases. It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way. My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this.\n\n**to area chair: concurrent paper by Jang et al. 2016**\nIt seems there's a concurrent submission by Jang et al. I havent' read that paper in detail, but maybe the conference should accept or reject both?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}