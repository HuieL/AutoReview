{"id": "ICLR_2017_12", "reviews": [{"review": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. \n\nPros and Cons:\nAlthough there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. \n\nSignificance:\nI think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n\nComments:\nEarlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.\n\n\n\n", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "I think that the paper is quite interesting and useful. \nIt might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}