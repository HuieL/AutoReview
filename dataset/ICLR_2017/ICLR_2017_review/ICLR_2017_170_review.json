{"id": "ICLR_2017_170", "reviews": [{"review": "This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.\n\nIt is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.\n\nThe experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.\n\nMinor comments:\nThird line in abstract: where model -> where the model\nSecond line in section 7: into space -> into the space\nShouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.\nExperiments on PTB shows significant improvement.\nThe idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.\nI was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.\nHave you considered using character or sub-word units in that context?\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. \n\nI see two main drawbacks from this framework:\nThe augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. \nThe augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. \n\nNevertheless the argument is very interesting, and clearly written.\nThe simulated results indeed validate the argument, and the PTB results seem promising.\n\nMinor comments:\nSection 3:\nCan you clarify if y~ is conditioned on the t example or on the entire history.\nEq. 3.5: i is enumerated over V (not |V|) ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}