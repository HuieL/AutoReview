{"id": "ICLR_2017_178", "reviews": [{"review": "This paper has two main contributions: \n(1) Applying adversarial training to imagenet, a larger dataset than previously considered \n(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.\n\nThis paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.\nThe paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.\n\nPros:\nFor the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.\nUsing the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. \n\nCons:\nAlthough the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the \u2018large-scale\u2019 in the title and in the introduction seems improper. \nBasically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}