{"id": "ICLR_2017_193", "reviews": [{"review": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n\nQuestion) Can you extend it to bidirectional RNN? \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}