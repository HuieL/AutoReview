{"id": "ICLR_2017_197", "reviews": [{"review": "This paper addresses the problem of efficient neural stylization.  Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style.  The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.  Qualitative results are shown.\n\nI have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style.  The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.  As far as I can tell the results look compelling.  As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to champion this paper.\n\nMy main comments are on the paper writing.  As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art.  If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it\u2019s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it\u2019s clear.  In fact, I\u2019m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures.  \n\nI was at first confused by Eq (5) as x and z are not defined anywhere.  Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper.  \n\nIn Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles.  \n\nFig 5 left: I\u2019m assuming the different colors correspond to the different styles.  If so, perhaps mention this in the caption.  Also, this figure is hard to read.  Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles.\n\nTypos:\nPage 1: Shouldn\u2019t it be \u201cVGG-16\u201d network (not \"VGG-19\u201d)?\nPage 2: \u201cnewtork\u201d => \u201cnetwork\u201d.\nParagraph after Eq. (5): \u201cmuch less\u201d => \u201cfewer\u201d.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. \n\nThis enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  \n\nFinally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space.\nHere I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as \u201cThe parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles\u201d can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting.\n\nIn conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"review": "CONTRIBUTIONS\nThe authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.\n\nNOVELTY\nThe problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style.\n\nMISSING CITATION\nThe following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al:\n\nLi and Wand, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\", ECCV 2016\n\nCLARITY\nThe paper is very well written and easy to follow.\n\nSIGNIFICANCE\nThough simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device.\n\nEVALUATION\nLike many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively.\n\nSUMMARY\nThe problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference.\n\nPros\n- Simple modification to feedforward neural style transfer with several improvements over prior work\n- Strong qualitative results\n- Well-written\n- Open-source code has already been released\n\nCons\n- Slightly incremental\n- Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}]}