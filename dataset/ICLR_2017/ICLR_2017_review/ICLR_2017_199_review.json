{"id": "ICLR_2017_199", "reviews": [{"review": "The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well.\n\nAs expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.\n\nAt the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).\n\nMy main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "\nThis work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space.  The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression.  To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function.  The authors also use a \u201csubexpression forcing\u201d mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality.\n\nResults are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly.  I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space \u2014 it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers.  \n\nThe weakest part of the paper is probably that the setting seems somewhat contrived \u2014 I can\u2019t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions.   The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain.  This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting.  For example, the proposed methods would not be useable in an \u201cequation search engine\u201d unless we were able to accurately canonicalize variable names in some way.\n\nOther miscellaneous points:\n* Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable \u2014 see the \u201cword problem for Thue systems\u201d.  Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets.  They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is \u2014 is it possible that equivalent expressions in the training data would have been mapped to different canonical forms?  Would it have been easier/possible to construct and compare truth tables?\n* The \u201cCOMBINE\u201d operation uses what the authors describe as a residual-like connection.  Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features.  A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion\u2026. so is there a reason why this was used rather than an identity connection?\n* In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c))\n* Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}