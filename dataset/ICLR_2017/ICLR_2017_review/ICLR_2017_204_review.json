{"id": "ICLR_2017_204", "reviews": [{"review": "Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples. I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv. noise but not better for new unseen samples from the task in consideration. Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotation\u2026 than robustness to adv. noise.\n\nI can not see any direct conclusion from table 5 results. \n\nEssentially i am not convinced about the necessity to measure the robustness against adversarial noise.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, {"review": "This paper aims at making three contributions:\n- Charecterizing robustness to adversarials in a topological manner.\n- Connecting the topological characterization to more quantitative measurements and evaluating deep networks.\n- Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties.\n\nIn my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether.\n\nA central notion of the paper is the abstract characterization of robustness. The main weakness is the notion of strong robustness itself, which is an extremely rigid notion. It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle. This robustness is almost never the case in real life: it requires that the predictor is almost perfect.\n\nThe main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier. Continuity of the classifier wrt. a discrete output is also never really satisfied. However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance. Based on those definition, several uninteresting, trivial consequences follow. They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes.\n\nThe second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint. The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials. This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness.\n\nThe experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function. In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space. \n\nThe main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important. \n\nHowever, this paper also has the following problems. \n\n1)\tIt is not clear how the classifier c can affect the overall robustness to adversarial noises. The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter. (Please correct me if it is not true) This is counter-intuitive. For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold. \nI am also wondering if the metric d has anything to do with the classifier c.\n2)\tA very relevant problem is how to decompose f into g and c. For examples, one can take any intermediate layer or the input space as the feature space for a neural network. Will this affect the analysis of the adversarial robustness?\n3)\tThe oracle is a good concept. However, it is hard to explicitly define it. In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric. This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion). \n\nDue to the above problems, I feel that some aspects of the paper are not ready. If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper. \n\nIn addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise.\n \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "A promising attempt to quantify the difference between human annotator and DNN. The contribution of defining topological equivalence of two metric spaces is significant to understand the origin of adversarial examples. Appreciate the examples showed in Figure2 and Figure 3, straightforward and intuitively helpful. More interestingly, a useful measure is provided for quantify the robustness of DNN model. Researchers can use this measure to compare different DNN models.\n \nSome critiques:\nIt might be nicer to think more about the detailed mechanics of human vision. In the early state of primary visual cortex, feature space might be linear, L2 norm might be sufficient to capture the key difference between DNN and human metrics. However, when the information flow goes deep, single one metric might not be sufficient when the Oracle feature space is highly curved. Tracking and reducing the metric distance in cognitive level would be extremely hard. \n\nThis paper offers a pioneering advance to tackle this problem. Hope more work can be done to further our understanding towards the limitation of DNN.", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}