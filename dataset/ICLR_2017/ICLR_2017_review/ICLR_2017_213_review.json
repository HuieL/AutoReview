{"id": "ICLR_2017_213", "reviews": [{"review": "Pros:\n* Part of the paper addresses an industrially important topic, namely how to make deep networks work properly on point clouds, i.e. in many (most?) potential applications they should be invariant to permutations of the points within the cloud, as well as rigid transformations of the cloud (depends on the application).\n* The authors propose a formalism for dealing with compositions of different kinds of invariance.\n\nCons:\n* For me the explanation of the generalization is really hard to follow. For me, the paper would be stronger if were less broad, but went into more depth for the permutation-invariance case.\n* It is very easy to sit down and come up with network structures that are permutation invariant. It seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there's not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant. i.e. what about just using a FC layer that is shared between the points instead of your three \"set invariant\" layers? Seems simpler, more general, and also permutation invariant...\n* It is not clear to me how valuable the author's definition of \"minimally invariant\" is. Is a sufficiently large composition of \"set invariant\" layers a universal approximator for permutation invariant functions?\n* I'm concerned that proposed \"set invariant layer\" might be strongly variant to spatial transformations, as well as vulnerable to large outliers. In particular there is a term that subtracts a corner of the clouds bounding box (i.e. the max over set operator inside the first layer), before the cloud goes through a learned affine transform and pixelwise nonlinearity. Seems like that could saturate the whole network...\n\nI'm reviewing with low confidence, because there's a chance the formalism in the first part of the paper is more valuable than I realize; I haven't fully understood it. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, {"review": "Pros : \n- New and clear formalism for invariance on signals with known structure\n- Good numerical results\n\nCons :\n- The structure must be specified.\n- The set structure dataset is too simple\n- There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing\n\nBesides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content.  Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that's an issue I had to deal with at some points)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. ", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, {"review": "\nThis paper discusses ways to enforce invariance in neural networks using weight sharing.  The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a \u201cset-invariant\u201d function, which is used in an anomaly detection setting and a point cloud classification problem.  \n\n\u201cInvariance\u201d is, at a high level, an important issue of course, since we don\u2019t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper.  However, there are a few weaknesses that I feel prevent this from being a strong submission.  First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning.\n\nSecond, \u201cset invariance\u201d, which is the main type of invariance studied in the paper is defined via the author\u2019s formalization of invariance, but is never explicitly related to what I might think of as \u201cset invariance\u201d \u2014 e.g. to permutations of input or output dimensions.  Explicitly defining set invariance in some other way, then relating it to the  \u201cstructural invariance\u201d formulation may be a better way to explain things.  It is never made clear, for example, why Figure 1(b) is *the* set data-structure.\n\nI like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?).  But the authors have ignored the other kind of compositionality that is important to neural networks \u2014 specifically that relating the proposed notion of invariance to function composition seems important \u2014 i.e. under what conditions do compositions of invariant functions remain invariant?  And  It is clear to me that just by having one layer of invariance in a network doesn\u2019t make the entire network invariant, for example.  So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is \u201cset invariant\u201d in some sense?  \n\nRegarding experiments, there are no baselines presented for anomaly detection.  Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed.  (I should say that I don\u2019t know enough about the dataset to say whether these are exactly fair comparisons or not).  It is also never really made clear why set invariance is a desirable property for a point cloud classification setting.  As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance.  Also, what about classical set kernels?\n\nOther random things:\n* Example 2.2: Shouldn\u2019t |S|=5 in the case of left-right and up-down symmetry?\n* \u201cParameters shared within a relation\u201d is vague and undefined.\n* Why is \u201cset convolution\u201d called \u201cset convolution\u201d in the appendix?  What is convolutional about it?\n* Is there a relationship to symmetric function theory?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}