{"id": "ICLR_2017_218", "reviews": [{"review": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.\n\nPoints\n\n1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.\n\n2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.\n\n3) Section 4 is nice, and I applaud the authors for doing such an analysis.\n\n\nList of typos etc.\n\n- maintain -> maintain\n- requisits -> requisites\n- a LSTM -> an LSTM\n- \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation.\n- \u201cbeacause\"-> because\n- One of the \u03b3x > \u03b3h at the end of page 5 is wrong.\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. \n\nAll in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}