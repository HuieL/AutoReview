{"id": "ICLR_2017_22", "reviews": [{"review": "In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the \u2018gradient workers\u2019 and the quantities required by the K-FAC method are computed asynchronously by the \u2018stats workers\u2019 and \u2018additional workers\u2019. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD.\n\nThe main contributions of the paper are:\n1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating.\n2) The authors propose a \u201cdoubly-factored\u201d Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers.\n3) Empirically illustrate the performance of the method, and show:\n- Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)\n- K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)\n- Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)\n- Favorable scaling properties of K-FAC with mini-batch size\n\nPros:\n- Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. \n- Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).\n\nCons: \n- \u201c\u2026it should be possible to scale our implementation to a larger distributed system with hundreds of workers.\u201d The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point.\n- Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective.\n\nThe paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. \n\nTypos:\n- \u201cupdates that accounts for\u201d \u2014 \u201cupdates that account for\u201d\n- \u201cKronecker product of their inverse\u201d \u2014 \u201cKronecker product of their inverses\u201d\n- \u201cwhere P is distribution over\u201d \u2014 \u201cwhere P is the distribution over\u201d\n- \u201cback-propagated loss derivativesas\u201d \u2014 \u201cback-propagated loss derivatives as\u201d\n- \u201cinverse of the Fisher\u201d \u2014 \u201cinverse of the Fisher Information matrix\u201d\n- \u201cwhich amounts of several matrix\u201d \u2014 \u201cwhich amounts to several matrix\u201d\n- \u201cThe diagram illustrate the distributed\u201d \u2014 \u201cThe diagram illustrates the distributed\u201d\n- \u201cGradient workers computes\u201d \u2014 \u201cGradient workers compute\u201d \n- \u201cStat workers computes\u201d \u2014 \u201cStat workers compute\u201d \n- \u201coccasionally and uses stale values\u201d \u2014 \u201coccasionally and using stale values\u201d \n- \u201cThe factors of rank-1 approximations\u201d \u2014 \u201cThe factors of the rank-1 approximations\u201d\n- \u201cbe the first singular value and its left and right singular vectors\u201d \u2014 \u201cbe the first singular value and the left and right singular vectors \u2026 , respectively.\u201d\n- \u201c\\Psi is captures\u201d \u2014 \u201c\\Psi captures\u201d\n- \u201cmultiplying the inverses of the each smaller matrices\u201d \u2014 \u201cmultiplying the inverses of each of the smaller matrices\u201d\n- \u201cwhich is a nested applications of the reshape\u201d \u2014 \u201cwhich is a nested application of the reshape\u201d\n- \u201cprovides a computational feasible alternative\u201d \u2014 \u201cprovides a computationally feasible alternative\u201d\n- \u201caccording the geometric mean\u201d \u2014 \u201caccording to the geometric mean\u201d\n- \u201canalogous to shrink\u201d \u2014 \u201canalogous to shrinking\u201d\n- \u201capplied to existing model-specification code\u201d \u2014 \u201capplied to the existing model-specification code\u201d\n- \u201c: that the alternative parametrization\u201d \u2014 \u201c: the alternative parameterization\u201d\n\nMinor Issues:\n- In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example:\n1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231)\n3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1)\n4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html)\n5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl)\n- Page 2, equation s = WA, is there a dimension issue in this expression?\n- x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots.\n- \u201cJames Martens. Deep Learning via Hessian-Free Optimization\u201d appears twice in References section.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper proposes an asynchronous distributed K-FAC method for efficient optimization of \ndeep networks. The authors introduce interesting ideas that many computationally demanding \nparts of the original K-FAC algorithm can be efficiently implemented in distributed fashion. The\ngradients and the second-order statistics are computed by distributed workers separately and \naggregated at the parameter server along with the inversion of the approximate Fisher matrix \ncomputed by a separate CPU machine. The experiments are performed in CIFAR-10 and ImageNet\nclassification problems using models such as AlexNet, ResNet, and GoogleReNet.\n\nThe paper includes many interesting ideas and techniques to derive an asynchronous distributed \nversion from the original K-FAC. And the experiments also show good results on a few \ninteresting cases. However, I think the empirical results are not thorough and convincing \nenough yet. Particularly, experiments on various and large number of GPU workers (in the same machine, \nor across multiple workers) are desired. For example, as pointed by the authors in the answer of a comment,\nChen et.al. (Revisiting Distributed Synchronous SGD, 2015) used 100 workers to test their distributed deep \nlearning algorithm. Even considering that the authors have a limitation in computing resource under the \nacademic research setting, the maximum number of 4 or 8 GPUs seems too limited as the only test case of \ndemonstrating the efficiency of a distributed learning algorithm.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}