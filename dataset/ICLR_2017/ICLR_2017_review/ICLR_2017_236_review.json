{"id": "ICLR_2017_236", "reviews": [{"review": "The authors propose a semi-supervised technique for neural networks which includes two objectives: (1) the neural net embeddings of two samples with identical labels is constrained to be closer than the embeddings of samples with different labels (2) the embedding of an unlabeled example is constrained to be close to the embeddings of the closest labeled sample (and far away from the other ones).\n\nWhile the authors list a number of previous works, they do not relate them very well with their approach, and actual differences appear unclear. In particular, the approach seems rather incremental with respect to (Hadsell et al. 2006), the way the neighbors are chosen being the main difference; in that respect, (Weston et al, 2008 or 2012) (which could be viewed as an application of DrLim from Hadsell et al, applied to semi-supervised learning) is even closer to the approach proposed here.\n\nThere is also no balancing constraint in the proposed approach, which was known to be crucial in all these models from the 2000s.\n\nConcerning the experimental results, reported performance are very good; given the approach is very close to previous work, it is hard to know if good performance come from better neural net architectures or something else. This should be clarified.\n\nIn summary, the novelty is not clearly defined in this paper; differences with existing literature should be highlighted. Experimental results are very good, but it is hard to know where comes the difference in performance with previous (very related) work.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This work presents an embedding approach for semi-supervised learning with neural nets, in the presence of little labeled data. The intuition is to learn a metric embedding that forms \u201cclusters\u201d with the following desiderata: two labeled examples from the class should have a smaller distance in this embedding compared to any example from another class & a given unlabeled example embedding will be closer to all of the embeddings of *some* label (i.e. that a given unlabeled example will be \u201cmatched\u201d to one cluster). The paper formulates these intuitions as two differentiable losses and does gradient descent on their sum.\n\nIt\u2019s unclear to me how different is this work from the sum of Hoffer & Ailon (2015) (which is eq. 3) and Grandvalet & Bengio (2004) (seems to be related to eq. 4). Would be nice if the authors not only cited the previous work but summarized the actual differences.\nIn Section 5.1, the authors say that Szegedy et al. (2015) use random noise in the targets -- is that actually true? I think only soft targets are used (which are not noisy).\n\nDoes the choice of \\lambda_{1,2} make a difference?\n\nHow is k for k-NN actually chosen? Is there a validation set?\n\nFigure 1 would benefit from showing where the labeled examples were at the beginning of training (relative to each other / rest of the data).\n\nThe submission seems overall OK, but somewhat light on actual data-driven or theoretical insights. I would\u2019ve liked experiments showing the influence of data set sizes at the very least, and ablation experiments that showed the influence of each of the corresponding losses.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}