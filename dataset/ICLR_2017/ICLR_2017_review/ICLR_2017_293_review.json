{"id": "ICLR_2017_293", "reviews": [{"review": "In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. \n\nPros:\n\n- The proposed approach demonstrates strong performance on FB15k dataset. \n\n- The idea of using shared memory for knowledge base completion is new and interesting. \n\n- The proposed approach is general and can be applied in various tasks. \n\nCons:\n\n- There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. \n\n- The introduction section can be improved. Specifically, the authors should motivate \"shared memory\" more in the introduction and how it different from existing methods that using \"unshared memory\" for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion.  The concept of shared memory and search controller only make sense to me after reading through section 2. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.\n\n\nThe paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.\n\n* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:\n  - What are the performance without the shared memory? And when its size is grown? \n  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.\n  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.\n  - What is the proportion of examples for which the prediction changed along several inference iterations?\n\n* A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?\n\n* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "\n[Summary]\nThis paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate.\n\nThe experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq.\n\nThe paper is well-written and it is easy to follow.\n\n[Major comments]\nI actually do like the idea and am also impressed that this model can work well.\nThe main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k.\n\nOne key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don\u2019t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size?\n\nThe RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps?\n\nI appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB  (e.g., Freebase) entities and relations.\n\n[Minor comments]\nI am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}