{"id": "ICLR_2017_299", "reviews": [{"review": "Summary:\n\nThe authors propose a multi-hop \"gated attention\" model, which models the interactions between query and document representations, for answering cloze-style questions. The document representation is attended to sequentially over multiple-hops using similarity with the query representation (using a dot-product) as the scoring/attention function. \nThe proposed method improves upon (CNN, Daily Mail, Who-Did-What datasets) or is comparable to (CBT dataset) the state-of-the-art results.\n\n\nPros:\n\n1. Nice idea on heirarchical attention for modulating the context (document) representation by the task-specific (query) representation.\n2. The presentation is clear with thorough experimental comparison with the latest results.\n\n\nComments:\n\n1. The overall system presents a number of architectural elements: (1) attention at multiple layers (multi-hop), (2) query based attention for the context (or gated attention), (3) encoding the query vector at each layer independently.\nIt is important to breakdown the gain in performance due to the above factors: the ablation study presented in section 4.4 helps establish the importance of Gated Attention (#2 above). However, it is not clear:\n\n  (1) how much multiple-hops of gated-attention contribute to the performance.\n  (2) how important is it to have a specialized query encoder for each layer.\n\nUnderstanding the above better, will help simplify the architecture.\n\n\n2. The tokens are represented using L(w) and C(w). It is not clear if C(w) is crucial for the performance of the proposed method.\nThere is a significant performance drop when C(w) is absent (e.g. in \"GA Reader--\"; although there are other changes in \"GA Reader--\" which could affect the performance). Hence, it is not clear how much does the main idea, i.e., gated attention contributes towards the superior performance of the proposed method.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}