{"id": "ICLR_2017_300", "reviews": [{"review": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "Pros:\n  using neural network on a new domain.\nCons:\n  It is not clear how it is guaranteed that the network generates syntactically correct code.\n\nQuestions, comments:\n  How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications?\n  Were all combinations of NT2N decision with all possible NTN2T considered?\n\n  Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the\n  lexicon, thus limited the possible predictions.\n  The question should then more likely be what is the optimal value of alpha for UNK.\n  See also my previous comment on estimating and using UNK.\n\n  Section 5.5, second paragraph, compares numbers which are not comparable.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "While the overall direction is promising, there are several serious issues with the paper which affect the novelty and validity of the results:\n\n1. Incorrect claims about related work affecting novelty:\n\n  - This work is not the first to explore a deep learning approach to automatic code completion: \u201cToward Deep Learning Software Repositories\u201d, MSR\u201915 also uses deep learning for code completion, and is not cited.\n\n  - \u201cCode Completion with Statistical Language Models\u201d, PLDI\u201914 is cited incorrectly -- it also does code completion with recurrent neural networks.\n\n  - PHOG is independent of JavaScript -- it does representation learning and has been applied to other languages (e.g., Python, see OOPSLA\u201916 below). \n\n  - This submission is not the only one that \u201ccan automatically extract features\u201d. Some high-precision (cited) baselines do it.\n\n  - \u201cStructured generative models of natural source code\u201d is an incorrect citation. It is from ICML\u201914 and has more authors. It is also a log-linear model and conditions on more context than claimed in this submission.\n\n\n2. Uses a non-comparable prediction task for non-terminal symbols: The type of prediction made here is simpler than the one used in PHOG and state-of-the-art (see OOPSLA\u201916 paper below) and thus the claimed 11 point improvement is not substantiated. In particular, in JavaScript there are 44 types of nodes. However, a PHOG and OOPSLA\u201916 predictions considers not only these 44 types, but also whether there are right siblings and children of a node. This is necessary for predicting tree fragments instead of a sequence of nodes. It however makes the prediction harder than the one considered here (it leads to 150+ labels, a >3x increase).\n\n\n3. Not comparing to state-of-the-art: the state-of-the-art however is not the basic PHOG cited here, but \u201cProbabilistic Model for Code with Decision Trees\u201d, (OOPSLA 2016) which appeared before the submission deadline for ICLR\u201917:\n\nhttp://dl.acm.org/citation.cfm?id=2984041\n\nOn the same dataset, OOPSLA\u201916 has accuracy of 83.9%, and on the more difficult task than considered here (see above point). Further, state-of-the-art (OOPSLA\u201916) has 2% higher accuracy than PHOG for tree values, which makes state of the art better than the current sub by 5%.\n\n\nThe work requires a deep revision pass w.r.t to novelty and precision claims which are currently incorrect.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}]}