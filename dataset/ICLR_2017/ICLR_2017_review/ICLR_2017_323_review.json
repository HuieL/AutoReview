{"id": "ICLR_2017_323", "reviews": [{"review": "Summary:\nThis paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces.\n\nPros:\nFormalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks\n\nCons:\nAnalysis seems quite preliminary, and no novel theoretical results or clear practical conclusions.\n\nThe main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn't seem ready yet.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "I have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}]}