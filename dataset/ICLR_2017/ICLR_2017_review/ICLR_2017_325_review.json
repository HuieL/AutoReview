{"id": "ICLR_2017_325", "reviews": [{"review": "The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided.\n\nSummary:\n\u2014\u2014\u2014\nI think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is great.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetailed comments:\n\u2014\u2014\u2014\n1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., \\alpha-expansion and belief propagation are not `scene-understanding algorithms\u2019 but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model\u2019 should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)\u2019 and I\u2019m wondering how the `approximate MAP state' can be optimal. Etc.\n\n2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible.\n\n3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011.  The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials.  The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines.\n\nPros:\n+ New formulation of SPNs \n+ New inference algorithm\n\nCons:\n- The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level)\n- The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). \n\nThe idea is great, however, the paper needs more work to be published.  I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}