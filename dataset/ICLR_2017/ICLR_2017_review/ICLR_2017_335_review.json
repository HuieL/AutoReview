{"id": "ICLR_2017_335", "reviews": [{"review": "The paper is about channel sparsity in Convolution layer.\nThe paper is well written and it elaborately discussed and investigated different approaches for applying sparsity.  The paper contains detailed literature review.\nIn result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices.\nThe main point is that the paper needs more detailed investigation on different dropout schedule.\nAs mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "This paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer.  Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters.\n\nUnfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights.  In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case:  The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored.  The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent.\n\nThe section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized.  The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original.\n\nOverall, the method and evaluations show that the basic approach has promise.  However, it is unclear how real gains (in either speed or accuracy) might actually be found with it.  Without this last step, it still seems incomplete to me for a conference paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper experiments with channel to channel sparse neural networks.\nThe paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers.\nThe results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. \nThis paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}