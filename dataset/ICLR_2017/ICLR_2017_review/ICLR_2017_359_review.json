{"id": "ICLR_2017_359", "reviews": [{"review": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}