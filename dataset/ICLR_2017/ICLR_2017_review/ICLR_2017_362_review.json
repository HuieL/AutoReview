{"id": "ICLR_2017_362", "reviews": [{"review": "Paper Summary: \nThis paper presents a new large scale machine reading comprehension dataset called MS MARCO. It is different from existing datasets in that the questions are real user queries, the context passages are real web documents, and free form answers are generated by humans instead of spans in the context. The paper also includes some analysis of the dataset and performance of QA models on the dataset.\n\nPaper Strengths: \n-- The questions in the dataset are real queries from users instead of humans writing questions given some context.\n-- Context passages are extracted from real web documents which are used by search engines to find answers to the given query.\n-- Answers are generated by humans instead of being spans in context.\n-- It is large scale dataset, with an aim of 1 million queries. Current release includes 100,000 queries.\n\nPaper Weaknesses: \n-- The authors say, \"We have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text.\", but the statement is not backed up with any study.\n-- The paper doesn't clearly present what additional information can today's QA models learn from MS MARCO which they can't from existing datasets. \n-- The paper should talk about what challenges are involved in obtaining a good performance on this dataset.\n-- What are the human performances as compared to the models presented in the paper?\n-- In section 4.1, what are the train/test splits? The results are for the subset of MS MARCO where every query has multiple answers. How big is that subset?\n-- What is DSSM mentioned in row 2, Table 5?\n-- The authors should include in the paper how experiments in section 4.2 prove that MS MARCO is a better dataset.\n-- In Table 6, the performance of Memory Networks is already close to Best Passage. Does that mean there is not enough room for improvement there?\n-- The paper seems to be written in hurry, with partial analysis, evaluation and various mistakes in the text.\n\nPreliminary Evaluation: \nThe proposed dataset MS MARCO is unique from existing datasets as it is a good representative of the QA task encountered by search engines. I think it can be a very useful dataset for the community to benefit from. Given the huge potential in the dataset, this paper lacks the analysis and evaluation needed to present the dataset's worth. I think it can benefit a lot with a more comprehensive analysis of the dataset.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}