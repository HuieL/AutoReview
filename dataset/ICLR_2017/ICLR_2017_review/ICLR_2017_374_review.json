{"id": "ICLR_2017_374", "reviews": [{"review": "This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets.\n\nStrength:\n1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. \n2, The paper is well organized.\n\nWeakness:\n1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. \n2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings.\n\nDetailed Questions:\n1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line \u201cl_min >= (n_k - 1), \u2026 >= l_max\u201d in section 2.2.2 is a mistake.\n2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I\u2019m curious about how stable the algorithm is.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper studies the graph embedding problem by using the encoder-decoder method. The experimental study on real network data sets show the features extracted by the proposed model is good for classification.\n\nStrong points of this paper:\n  1. The idea of using the methods from natural language processing to graph mining is quite interesting.\n  2. The organization of the paper is clear\n\nWeak points of this paper:\n  1. Comparisons with state-of-art methods (Graph Kernels) is missing. \n  2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing. \n  3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.)\n  4. Presentation of the paper is weak. There are lots of typos and unclear statements. \n  5. The author mentioned about the graph kernel things, but in the experiment they didn't compare them. Also, only compare the classification accuracy by using the proposed method is not enough.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, {"review": "The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.\n\nIn a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. \n\nAn anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))\n\nTo sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.  \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}