{"id": "ICLR_2017_383", "reviews": [{"review": "The paper proposed a very complex compression and reconstruction method (with additional parameters) for reducing the memory footprint of deep networks.\n\nThe authors show that this complex proposal is better than simple hashed net proposal. One question: Are you also counting the extra parameters for reconstruction network for the memory comparison? Otherwise, the experiments are unfair.  \n\nSince hashing and reconstruction cost will dominate the feed-forward and back-propagation updates, it is imperative to compare the two methods on running time. For hashed net, this is quite simple, yet it created an additional bottleneck.  Please also show the impact on running time. Small improvements for a big loss in computational cost may not be acceptable. I am not convinced that this method will be lightweight. If we are allowed complicated compression and reconstruction then we can use any off-shelf methods, but the cost will be huge\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"review": "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}