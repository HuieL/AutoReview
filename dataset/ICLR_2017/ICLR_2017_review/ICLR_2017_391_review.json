{"id": "ICLR_2017_391", "reviews": [{"review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.\n\nOverall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "- Proven again that end to end training with deep networks gives large\ngains over traditional hybrid systems with hand crafted features. The results \nare very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.\n\n- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.\n\n- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}