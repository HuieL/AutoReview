{"id": "ICLR_2017_401", "reviews": [{"review": "The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations.\n\nThe paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%).\n\nOverall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning.\n\nFinally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer.\nLearning and inference are performed on artificially generated questions using a probabilistic grammar.\nOverall, the paper is clearly written and seems to be novel in its approach.\n\nThe main problems I see with this work are:\n1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense.\n2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}