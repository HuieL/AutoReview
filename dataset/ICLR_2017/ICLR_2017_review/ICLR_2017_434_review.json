{"id": "ICLR_2017_434", "reviews": [{"review": "Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "Strengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}