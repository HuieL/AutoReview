{"id": "ICLR_2017_467", "reviews": [{"review": "This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.\n\nThe authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.\n\nAt a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.\n\nHowever, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. \n- I cannot find where  f_{symbkey} / f_{voca} / f_{copy} are defined\n- w^v, w^s are confusing.\n- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.\n- (h_t, c_t) = f_LSTM(x_{t\u22121}, h_{t\u22121})  c_t is not used?\n- The notion of \u201cfact embeddings\u201d is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor / \u201ctopic-itself\u201d facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?\n\nOn generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention \u201cObama\u201d for the entity \u201cBarack Obama\u201d?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.\n\nThe writing could be improved. The beginning of Section 3 in particular is hard to parse.\n\nThere have been similar efforts recently (like \"Pointer Sentinel Mixture Models\" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.\n\nThe architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. \n\n\nOther comments:\n* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.\n* What is the performance on standard benchmarks like Penn Tree Bank?\n* How long is it to train compare to a standard RNNLM?\n* What is the importance of the knowledge context $e$?\n* How is initialized the fact embedding $a_{t-1}$ for the first word?\n* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if \"Michelle\" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}