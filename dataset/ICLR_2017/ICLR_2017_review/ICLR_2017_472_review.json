{"id": "ICLR_2017_472", "reviews": [{"review": "The paper is straightforward, easy to read, and has clear results. \n\nSince all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?\n\nWould we get the same result if there was no reference-pose cost, only a locomotion cost?\n\nWould we get the same result if the task was to spin a top? My guess is no. \n\nThis work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.\n\nThe video is nice.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}