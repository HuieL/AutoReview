{"id": "ICLR_2017_478", "reviews": [{"review": "The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The goal of this paper is to learn \u201c a collection of experts that are individually\nmeaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.\n\nComments:\n\nThe paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.\n\nThe subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d \n\nSince you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.\n\nThe horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.\nI find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?\nWe also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf\nI tried to derive the update rule on top of page 4 from the \u201cconditional objective for p(x|h)\u201d in sec. 3.2 But I am getting something different (apart form the extra smoothing factors eps and mu_o). Does this follow? (If we define R=R_nk, mu-mu_k and X=X_n, I get mu = (XR)*inv(R^TR) as the optimal solution, which then needs to be projected back onto the probability simplex).\nThe experiments are only illustrative. They don\u2019t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images. I have no idea whether what we see is impressive or not. \n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}