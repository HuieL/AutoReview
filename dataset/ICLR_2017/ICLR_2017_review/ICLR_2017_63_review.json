{"id": "ICLR_2017_63", "reviews": [{"review": "This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.\n\nComments: \n- p. 3 \u201ccan be seen has a way\u201d -> \u201ccan be seen as a way\u201d (?)\n- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?\n- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.\n- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "This paper discusses data noising as a regularization technique for language modelling as an alternative to dropout regularization. The key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise. Through this motivation, the authors present noising analogies to standard discounting, as well as Kneser-Ney smoothing.\n\nThe experiments are convincing in that the smoothed (noised) models outperform their unregularized baselines.\n\nMy main issue with the evaluation in this paper is that there is no comparison between the noising/smoothing idea and more conventional regularizers (such as L2 and dropout) which were discussed in the paper. Likewise, it would have been interesting if the techniques proposed here had been applied to stronger base models (such as the models compared with in Table 2). Seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in Zaremba's or Gal's!\n\nThose weaknesses aside (and I recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly. I strongly recommend the paper for acceptance.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}