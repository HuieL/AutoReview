{"id": "ICLR_2017_71", "reviews": [{"review": "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, {"review": "A new memory module based on k-NN is presented.\nThe paper is very well written and the results are convincing. \n\nOmniglot is a good sanity test and the performance is surprisingly good.\nThe artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.\nAnd the translation task eventually makes a very strong point on practical usefulness of the proposed model.\n\nI am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "The paper proposes a new memory module to be used as an addition to existing neural network models.\n\nPros:\n* Clearly written and original idea.\n* Useful memory module, shows nice improvements.\n* Tested on some big tasks.\n\nCons:\n* No comparisons to other memory modules such as associative LSTMs etc.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}