{"id": "ICLR_2017_73", "reviews": [{"review": "This paper proposes a new gating mechanism to combine word and character representations. The proposed model sets a new state-of-the-art on the CBT dataset; the new gating mechanism also improves over scalar gates without linguistic features on SQuAD and a twitter classification task. \n\nIntuitively, the vector-based gate working better than the scalar gate is unsurprising, as it is more similar to LSTM and GRU gates. The real contribution of the paper for me is that using features such as POS tags and NER help learn better gates. The visualization in Figure 3 and examples in Table 4 effectively confirm the utility of these features, very nice! \n\nIn sum, while the proposed gate is nothing technically groundbreaking, the paper presents a very focused contribution that I think will be useful to the NLP community. Thus, I hope it is accepted.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "I think the problem here is well motivated, the approach is insightful and intuitive, and the results are convincing of the approach (although lacking in variety of applications). I like the fact that the authors use POS and NER in terms of an intermediate signal for the decision. Also they compare against a sufficient range of baselines to show the effectiveness of the proposed model.\n\nI am also convinced by the authors' answers to my question, I think there is sufficient evidence provided in the results to show the effectiveness of the inductive bias introduced by the fine-grained gating model.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}]}