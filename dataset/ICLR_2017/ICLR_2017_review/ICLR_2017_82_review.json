{"id": "ICLR_2017_82", "reviews": [{"review": "This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices.\n\nStrengths:\n- A novel approach for automatic design of neural network architectures.\n- Shows quite promising results on several datasets (MNIST, CIFAR-10).\n\nWeakness:\n- Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.)\n- The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space.\n\nOverall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, {"review": "The paper looks solid and the idea is natural. Results seem promising as well.\n\nI am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter.\n I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like.\n\nIf you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication.\n\nMinor: \n- ResNets should be mentioned in Table ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, {"review": "Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}]}